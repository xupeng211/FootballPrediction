#!/usr/bin/env python3
"""
InfluxDBæ—¶åºæ•°æ®åº“å®¢æˆ·ç«¯
InfluxDB Time Series Database Client

æä¾›é«˜è´¨é‡æŒ‡æ ‡æ•°æ®çš„æ—¶åºå­˜å‚¨,æŸ¥è¯¢å’Œåˆ†æåŠŸèƒ½
"""

import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union

try:
    from influxdb_client import InfluxDBClient, Point
    from influxdb_client.client.write_api import SYNCHRONOUS
    from influxdb_client.client.query_api import QueryApi
except ImportError:
    print("è­¦å‘Š: influxdb-client æœªå®‰è£…,InfluxDBåŠŸèƒ½å°†ä¸å¯ç”¨")
    InfluxDBClient = None
    Point = None

from src.core.logging_system import get_logger
from src.core.config import get_config

logger = get_logger(__name__)


@dataclass
class TimeSeriesMetric:
    """ç±»æ–‡æ¡£å­—ç¬¦ä¸²"""
    pass  # æ·»åŠ passè¯­å¥
    """æ—¶åºæŒ‡æ ‡æ•°æ®æ¨¡å‹"""

    measurement: str  # æµ‹é‡åç§°,å¦‚ "quality_metrics"
    tags: Dict[str, str]  # æ ‡ç­¾,ç”¨äºè¿‡æ»¤å’Œåˆ†ç»„
    fields: Dict[str, Union[float, int, str, bool]]  # å­—æ®µ,å®é™…çš„æ•°æ®å€¼
    timestamp: datetime  # æ—¶é—´æˆ³

    def to_influx_point(self) -> "Point":
        """è½¬æ¢ä¸ºInfluxDB Pointå¯¹è±¡"""
        if Point is None:
            return None

        point = Point(self.measurement)

        # æ·»åŠ æ ‡ç­¾
        for tag_key, tag_value in self.tags.items():
            point.tag(tag_key, str(tag_value))

        # æ·»åŠ å­—æ®µ
        for field_key, field_value in self.fields.items():
            point.field(field_key, field_value)

        # è®¾ç½®æ—¶é—´æˆ³
        point.time(self.timestamp)

        return point


@dataclass
class MetricQuery:
    """ç±»æ–‡æ¡£å­—ç¬¦ä¸²"""
    pass  # æ·»åŠ passè¯­å¥
    """æŒ‡æ ‡æŸ¥è¯¢é…ç½®"""

    measurement: str
    fields: List[str]
    tags: Optional[Dict[str, str]] = None
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    aggregation: Optional[str] = None  # mean, max, min, sum, count
    window: Optional[str] = None  # æ—¶é—´çª—å£,å¦‚ "1h", "30m"
    limit: Optional[int] = None


class InfluxDBManager:
    """ç±»æ–‡æ¡£å­—ç¬¦ä¸²"""
    pass  # æ·»åŠ passè¯­å¥
    """InfluxDBç®¡ç†å™¨"""

    def __init__(self):
        """å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²"""
        pass
  # æ·»åŠ passè¯­å¥
        self.config = get_config()
        self.logger = get_logger(self.__class__.__name__)

        # InfluxDBè¿æ¥é…ç½®
        self.influx_url = self.config.get("influxdb", {}).get("url", "http://localhost:8086")
        self.influx_token = self.config.get("influxdb", {}).get("token", "")
        self.influx_org = self.config.get("influxdb", {}).get("org", "football-prediction")
        self.influx_bucket = self.config.get("influxdb", {}).get("bucket", "quality-metrics")

        self.client: Optional[InfluxDBClient] = None
        self.write_api = None
        self.query_api: Optional[QueryApi] = None

        # è¿æ¥çŠ¶æ€
        self.is_connected = False
        self.connection_attempts = 0
        self.max_connection_attempts = 3

    async def connect(self) -> bool:
        """è¿æ¥åˆ°InfluxDB"""
        if InfluxDBClient is None:
            self.logger.error("influxdb-client æœªå®‰è£…,æ— æ³•è¿æ¥åˆ°InfluxDB")
            return False

        try:
            self.client = InfluxDBClient(
                url=self.influx_url, token=self.influx_token, org=self.influx_org
            )

            # æµ‹è¯•è¿æ¥
            health = self.client.health()
            if health.status == "pass":
                self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
                self.query_api = self.client.query_api()
                self.is_connected = True
                self.connection_attempts = 0
                self.logger.info(f"InfluxDBè¿æ¥æˆåŠŸ: {self.influx_url}")
                return True
            else:
                self.logger.error(f"InfluxDBå¥åº·æ£€æŸ¥å¤±è´¥: {health.message}")
                return False

        except Exception as e:
            self.connection_attempts += 1
            self.logger.error(
                f"InfluxDBè¿æ¥å¤±è´¥ (å°è¯• {self.connection_attempts}/{self.max_connection_attempts}): {e}"
            )

            if self.connection_attempts < self.max_connection_attempts:
                # ç­‰å¾…åé‡è¯•
                await asyncio.sleep(2**self.connection_attempts)
                return await self.connect()

            return False

    async def disconnect(self):
        """æ–­å¼€InfluxDBè¿æ¥"""
        if self.client:
            self.client.close()
            self.is_connected = False
            self.logger.info("InfluxDBè¿æ¥å·²æ–­å¼€")

    def _ensure_connection(self):
        """å‡½æ•°æ–‡æ¡£å­—ç¬¦ä¸²"""
        pass
  # æ·»åŠ passè¯­å¥
        """ç¡®ä¿è¿æ¥å¯ç”¨"""
        if not self.is_connected or not self.client:
            raise ConnectionError("InfluxDBæœªè¿æ¥")

    async def write_metric(self, metric: TimeSeriesMetric) -> bool:
        """å†™å…¥å•ä¸ªæ—¶åºæŒ‡æ ‡"""
        try:
            self._ensure_connection()

            point = metric.to_influx_point()
            if point is None:
                self.logger.error("æ— æ³•åˆ›å»ºInfluxDB Pointå¯¹è±¡")
                return False

            self.write_api.write(bucket=self.influx_bucket, record=point)

            self.logger.debug(f"æ—¶åºæŒ‡æ ‡å·²å†™å…¥: {metric.measurement} at {metric.timestamp}")
            return True

        except Exception as e:
            self.logger.error(f"å†™å…¥æ—¶åºæŒ‡æ ‡å¤±è´¥: {e}")
            return False

    async def write_metrics_batch(self, metrics: List[TimeSeriesMetric]) -> bool:
        """æ‰¹é‡å†™å…¥æ—¶åºæŒ‡æ ‡"""
        try:
            self._ensure_connection()

            points = []
            for metric in metrics:
                point = metric.to_influx_point()
                if point:
                    points.append(point)

            if points:
                self.write_api.write(bucket=self.influx_bucket, record=points)
                self.logger.debug(f"æ‰¹é‡å†™å…¥ {len(points)} ä¸ªæ—¶åºæŒ‡æ ‡")
                return True
            else:
                self.logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„æŒ‡æ ‡æ•°æ®å¯å†™å…¥")
                return False

        except Exception as e:
            self.logger.error(f"æ‰¹é‡å†™å…¥æ—¶åºæŒ‡æ ‡å¤±è´¥: {e}")
            return False

    async def write_quality_metrics(self, quality_data: Dict[str, Any]) -> bool:
        """å†™å…¥è´¨é‡æŒ‡æ ‡æ•°æ®"""
        try:
            timestamp = datetime.now()

            # æ„å»ºæ ‡ç­¾
            tags = {
                "source": "quality_gate_system",
                "environment": self.config.get("environment", "development"),
            }

            # æ·»åŠ çŠ¶æ€æ ‡ç­¾
            if "overall_status" in quality_data:
                tags["status"] = quality_data["overall_status"]

            # æ„å»ºå­—æ®µ
            fields = {
                "overall_score": float(quality_data.get("overall_score", 0)),
                "gates_checked": int(quality_data.get("gates_checked", 0)),
                "should_block": bool(quality_data.get("should_block", False)),
            }

            # æ·»åŠ æ€§èƒ½æŒ‡æ ‡
            if "performance" in quality_data:
                perf = quality_data["performance"]
                fields.update(
                    {
                        "response_time_ms": float(perf.get("response_time_ms", 0)),
                        "cpu_usage": float(perf.get("cpu_usage", 0)),
                        "memory_usage": float(perf.get("memory_usage", 0)),
                        "active_connections": int(perf.get("active_connections", 0)),
                    }
                )

            # æ·»åŠ æ‘˜è¦ç»Ÿè®¡
            if "summary" in quality_data:
                summary = quality_data["summary"]
                fields.update(
                    {
                        "passed_count": int(summary.get("passed", 0)),
                        "failed_count": int(summary.get("failed", 0)),
                        "warning_count": int(summary.get("warning", 0)),
                        "skipped_count": int(summary.get("skipped", 0)),
                    }
                )

            # åˆ›å»ºæ—¶åºæŒ‡æ ‡
            metric = TimeSeriesMetric(
                measurement="quality_metrics",
                tags=tags,
                fields=fields,
                timestamp=timestamp,
            )

            return await self.write_metric(metric)

        except Exception as e:
            self.logger.error(f"å†™å…¥è´¨é‡æŒ‡æ ‡å¤±è´¥: {e}")
            return False

    async def write_gate_results(self, gate_results: List[Dict[str, Any]]) -> bool:
        """å†™å…¥é—¨ç¦æ£€æŸ¥ç»“æœ"""
        try:
            metrics = []
            timestamp = datetime.now()

            for result in gate_results:
                # æ„å»ºæ ‡ç­¾
                tags = {
                    "gate_name": result.get("gate_name", "unknown"),
                    "status": result.get("status", "unknown"),
                    "source": "quality_gate_system",
                }

                # æ„å»ºå­—æ®µ
                fields = {
                    "score": float(result.get("score", 0)),
                    "threshold": float(result.get("threshold", 0)),
                    "duration_ms": int(result.get("duration_ms", 0)),
                }

                metric = TimeSeriesMetric(
                    measurement="gate_results",
                    tags=tags,
                    fields=fields,
                    timestamp=timestamp,
                )
                metrics.append(metric)

            return await self.write_metrics_batch(metrics)

        except Exception as e:
            self.logger.error(f"å†™å…¥é—¨ç¦ç»“æœå¤±è´¥: {e}")
            return False

    async def query_metrics(self, query: MetricQuery) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ—¶åºæŒ‡æ ‡"""
        try:
            self._ensure_connection()

            # æ„å»ºFluxæŸ¥è¯¢è¯­å¥
            flux_query = self._build_flux_query(query)

            # æ‰§è¡ŒæŸ¥è¯¢
            result = self.query_api.query(flux_query)

            # è§£æç»“æœ
            data_points = []
            for table in result:
                for record in table.records:
                    data_point = {
                        "time": record.get_time(),
                        "measurement": record.get_measurement(),
                        "field": record.get_field(),
                        "value": record.get_value(),
                    }

                    # æ·»åŠ æ ‡ç­¾
                    for tag_key, tag_value in record.values.items():
                        if tag_key.startswith("_") or tag_key in [
                            "time",
                            "measurement",
                            "field",
                            "value",
                            "result",
                            "table",
                        ]:
                            continue
                        data_point[tag_key] = tag_value

                    data_points.append(data_point)

            self.logger.debug(f"æŸ¥è¯¢è¿”å› {len(data_points)} ä¸ªæ•°æ®ç‚¹")
            return data_points

        except Exception as e:
            self.logger.error(f"æŸ¥è¯¢æ—¶åºæŒ‡æ ‡å¤±è´¥: {e}")
            return []

    def _build_flux_query(self, query: MetricQuery) -> str:
        """æ„å»ºFluxæŸ¥è¯¢è¯­å¥"""

        # åŸºç¡€æŸ¥è¯¢
        flux_parts = [f'from(bucket: "{self.influx_bucket}")']

        # æ—¶é—´èŒƒå›´
        if query.start_time:
            start_str = query.start_time.strftime("%Y-%m-%dT%H:%M:%SZ")
            flux_parts.append(f"|> range(start: {start_str}")

            if query.end_time:
                end_str = query.end_time.strftime("%Y-%m-%dT%H:%M:%SZ")
                flux_parts.append(f", stop: {end_str}")

            flux_parts.append(")")
        else:
            # é»˜è®¤æŸ¥è¯¢æœ€è¿‘24å°æ—¶
            flux_parts.append("|> range(start: -24h)")

        # æµ‹é‡åç§°è¿‡æ»¤
        flux_parts.append(f'|> filter(fn: (r) => r._measurement == "{query.measurement}")')

        # æ ‡ç­¾è¿‡æ»¤
        if query.tags:
            for tag_key, tag_value in query.tags.items():
                flux_parts.append(f'|> filter(fn: (r) => r.{tag_key} == "{tag_value}")')

        # å­—æ®µè¿‡æ»¤
        if query.fields:
            field_filter = " or ".join([f'r._field == "{field}"' for field in query.fields])
            flux_parts.append(f"|> filter(fn: (r) => {field_filter})")

        # èšåˆæ“ä½œ
        if query.aggregation:
            if query.window:
                flux_parts.append(
                    f"|> aggregateWindow(every: {query.window}, fn: {query.aggregation}, createEmpty: false)"
                )
            else:
                flux_parts.append(f"|> {query.aggregation}()")

        # é™åˆ¶ç»“æœæ•°é‡
        if query.limit:
            flux_parts.append(f"|> limit(n: {query.limit})")

        return " ".join(flux_parts)

    async def get_quality_metrics_history(self, hours: int = 24) -> List[Dict[str, Any]]:
        """è·å–è´¨é‡æŒ‡æ ‡å†å²æ•°æ®"""
        query = MetricQuery(
            measurement="quality_metrics",
            fields=["overall_score", "gates_checked", "cpu_usage", "memory_usage"],
            start_time=datetime.now() - timedelta(hours=hours),
            aggregation="mean",
            window="5m",
        )

        return await self.query_metrics(query)

    async def get_gate_results_history(
        self, gate_name: str = None, hours: int = 24
    ) -> List[Dict[str, Any]]:
        """è·å–é—¨ç¦ç»“æœå†å²æ•°æ®"""
        query = MetricQuery(
            measurement="gate_results",
            fields=["score", "duration_ms"],
            start_time=datetime.now() - timedelta(hours=hours),
            aggregation="mean",
            window="10m",
        )

        if gate_name:
            query.tags = {"gate_name": gate_name}

        return await self.query_metrics(query)

    async def get_metric_statistics(
        self, measurement: str, field: str, hours: int = 24
    ) -> Dict[str, float]:
        """è·å–æŒ‡æ ‡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            # æŸ¥è¯¢åŸå§‹æ•°æ®
            query = MetricQuery(
                measurement=measurement,
                fields=[field],
                start_time=datetime.now() - timedelta(hours=hours),
            )

            data_points = await self.query_metrics(query)

            if not data_points:
                return {}

            values = [
                point["value"] for point in data_points if isinstance(point["value"], (((int, float)
            ]

            if not values:
                return {}

            return {
                "count": len(values)) / len(values)))
            return {}

    async def cleanup_old_data(self, days_to_keep: int = 30):
        """æ¸…ç†æ—§æ•°æ®"""
        try:
            self._ensure_connection()

            # è®¡ç®—åˆ é™¤æ—¶é—´ç‚¹
            delete_time = (datetime.now() - timedelta(days=days_to_keep)).strftime(
                "%Y-%m-%dT%H:%M:%SZ"
            )

            # æ„å»ºåˆ é™¤æŸ¥è¯¢
            delete_query = f"""
            from(bucket: "{self.influx_bucket}")
                |> range(start: -365d, stop: {delete_time})
                |> drop()
            """
            # è®°å½•åˆ é™¤æŸ¥è¯¢ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logger.debug(
                f"Generated delete query for data older than {delete_time}: {delete_query.strip()}"
            )

            # æ‰§è¡Œåˆ é™¤ (æ³¨æ„:è¿™éœ€è¦é€‚å½“çš„æƒé™)
            # self.query_api.query(delete_query)

            self.logger.info(f"å·²æ¸…ç† {days_to_keep} å¤©å‰çš„æ•°æ®")

        except Exception as e:
            self.logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")

    async def get_bucket_info(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨æ¡¶ä¿¡æ¯"""
        try:
            self._ensure_connection()

            # æŸ¥è¯¢å­˜å‚¨æ¡¶ä¿¡æ¯
            buckets_api = self.client.buckets_api()
            bucket = buckets_api.find_bucket_by_name(self.influx_bucket)

            if bucket:
                return {
                    "name": bucket.name,
                    "id": bucket.id,
                    "org_id": bucket.org_id,
                    "retention_rules": bucket.retention_rules,
                    "created_at": bucket.created_at,
                }
            else:
                return {}

        except Exception as e:
            self.logger.error(f"è·å–å­˜å‚¨æ¡¶ä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æ£€æŸ¥"""
        try:
            if not self.is_connected or not self.client:
                return {
                    "status": "unhealthy",
                    "message": "InfluxDBæœªè¿æ¥",
                    "url": self.influx_url,
                }

            health = self.client.health()

            return {
                "status": health.status,
                "message": health.message,
                "url": self.influx_url,
                "bucket": self.influx_bucket,
                "org": self.influx_org,
                "connection_attempts": self.connection_attempts,
            }

        except Exception as e:
            return {"status": "error", "message": str(e), "url": self.influx_url}


# å…¨å±€InfluxDBç®¡ç†å™¨å®ä¾‹
influxdb_manager = InfluxDBManager()


# è¾…åŠ©å‡½æ•°
async def initialize_influxdb() -> bool:
    """åˆå§‹åŒ–InfluxDBè¿æ¥"""
    success = await influxdb_manager.connect()
    if success:
        logger.info("InfluxDBåˆå§‹åŒ–æˆåŠŸ")
    else:
        logger.error("InfluxDBåˆå§‹åŒ–å¤±è´¥")
    return success


async def save_quality_metrics(quality_data: Dict[str, Any]) -> bool:
    """ä¿å­˜è´¨é‡æŒ‡æ ‡åˆ°InfluxDB"""
    return await influxdb_manager.write_quality_metrics(quality_data)


async def get_quality_history(hours: int = 24) -> List[Dict[str, Any]]:
    """è·å–è´¨é‡æŒ‡æ ‡å†å²"""
    return await influxdb_manager.get_quality_metrics_history(hours)


if __name__ == "__main__":
    import asyncio

    async def test_influxdb():
        """æµ‹è¯•InfluxDBåŠŸèƒ½"""
        print("ğŸ§ª æµ‹è¯•InfluxDBæ—¶åºæ•°æ®åº“åŠŸèƒ½...")

        # è¿æ¥æµ‹è¯•
        connected = await initialize_influxdb()
        if not connected:
            print("âŒ InfluxDBè¿æ¥å¤±è´¥")
            return None
        print("âœ… InfluxDBè¿æ¥æˆåŠŸ")

        # å†™å…¥æµ‹è¯•æ•°æ®
        test_data = {
            "overall_score": 9.2,
            "overall_status": "PASSED",
            "gates_checked": 6,
            "should_block": False,
            "performance": {
                "response_time_ms": 145,
                "cpu_usage": 42.5,
                "memory_usage": 67.8,
                "active_connections": 8,
            },
            "summary": {"passed": 5, "failed": 0, "warning": 1, "skipped": 0},
        }

        success = await save_quality_metrics(test_data)
        if success:
            print("âœ… æµ‹è¯•æ•°æ®å†™å…¥æˆåŠŸ")
        else:
            print("âŒ æµ‹è¯•æ•°æ®å†™å…¥å¤±è´¥")
            return None
        # æŸ¥è¯¢æµ‹è¯•æ•°æ®
        history = await get_quality_history(1)  # æŸ¥è¯¢æœ€è¿‘1å°æ—¶
        print(f"âœ… æŸ¥è¯¢åˆ° {len(history)} æ¡å†å²æ•°æ®")

        # å¥åº·æ£€æŸ¥
        health = influxdb_manager.health_check()
        print(f"âœ… å¥åº·æ£€æŸ¥: {health['status']}")

        # æ–­å¼€è¿æ¥
        await influxdb_manager.disconnect()
        print("âœ… æµ‹è¯•å®Œæˆ")

    asyncio.run(test_influxdb())
}