#!/usr/bin/env python3
"""
FotMob HTML æ•°æ®é‡‡é›†å™¨ - QAéªŒè¯ç‰ˆæœ¬
FotMob HTML Data Collector - QA Verified Version

ç»è¿‡è°ƒè¯•éªŒè¯çš„ç¨³å®šç‰ˆæœ¬ï¼Œä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½å’ŒxGæ•°æ®æå–
"""

import asyncio
import json
import logging
import random
import re
import time
from typing import Optional, Dict, Any, List
from datetime import datetime

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from .user_agent import UserAgentManager

logger = logging.getLogger(__name__)


class HTMLFotMobCollector:
    """FotMob HTML æ•°æ®é‡‡é›†å™¨ - QAéªŒè¯ç‰ˆæœ¬"""

    def __init__(
        self,
        max_retries: int = 3,
        timeout: int = 30,
        enable_stealth: bool = True,  # å¼ºåˆ¶å¯ç”¨éšèº«æ¨¡å¼å¯¹æŠ—Dockeræ£€æµ‹
        enable_proxy: bool = False,
    ):
        self.max_retries = max_retries
        self.timeout = (10, 30)  # è¿æ¥è¶…æ—¶10ç§’ï¼Œè¯»å–è¶…æ—¶30ç§’
        self.enable_stealth = enable_stealth
        self.enable_proxy = enable_proxy

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "requests_made": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "matches_collected": 0,
            "ua_switches": 0,
            "retry_count": 0,
        }

        # ä¼šè¯å’Œç”¨æˆ·ä»£ç†
        self.session = None
        # å¼ºåˆ¶åˆå§‹åŒ–ç”¨æˆ·ä»£ç†ç®¡ç†å™¨ä»¥å¯¹æŠ—Dockeræ£€æµ‹
        self.user_manager = UserAgentManager()
        self.current_headers = None
        self.last_rotation = time.time()

        logger.info("ğŸ•·ï¸ FotMob HTMLé‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆ - QAéªŒè¯ç‰ˆæœ¬")

    async def initialize(self):
        """åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯"""
        # ä¸ä½¿ç”¨Sessioné¿å…Dockerç¯å¢ƒä¸‹çš„åçˆ¬æ£€æµ‹
        self.session = None

        # åˆå§‹åŒ–ä¼ªè£…
        await self._refresh_disguise()

        logger.info("âœ… HTTPå®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

    async def _refresh_disguise(self):
        """åˆ·æ–°User-Agentä¼ªè£…"""
        if not self.enable_stealth:
            return

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®æ¢
        now = time.time()
        rotation_interval = 300  # 5åˆ†é’Ÿ
        if now - self.last_rotation < rotation_interval:
            return

        if self.user_manager:
            self.current_headers = self.user_manager.get_realistic_headers()
            self.stats["ua_switches"] += 1

        self.last_rotation = now

    def _get_current_headers(self) -> dict[str, str]:
        """è·å–å½“å‰è¯·æ±‚å¤´"""
        # ä½¿ç”¨æ ‡å‡†çš„æµè§ˆå™¨è¯·æ±‚å¤´ï¼Œè®©requestsè‡ªåŠ¨å¤„ç†GZIPè§£å‹
        return {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-GB,en;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',  # è®©requestsè‡ªåŠ¨å¤„ç†GZIP
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }

    async def collect_match_data(self, match_id: str) -> Optional[dict[str, Any]]:
        """
        é‡‡é›†å•åœºæ¯”èµ›æ•°æ® - QAéªŒè¯ç‰ˆæœ¬

        å…³é”®é€»è¾‘ï¼š
        1. é‡åˆ°404çŠ¶æ€ç ä¸è¿”å›Noneï¼Œç»§ç»­è§£æresponse.text
        2. ä½¿ç”¨æ­£åˆ™æå–__NEXT_DATA__ JSON
        3. è§£æprops.pageProps.contentæå–æ ¸å¿ƒæ•°æ®
        """
        try:
            url = f"https://www.fotmob.com/match/{match_id}"
            logger.info(f"ğŸ•·ï¸ è¯·æ±‚æ¯”èµ›æ•°æ®: {url}")

            # å®šæœŸåˆ·æ–°ä¼ªè£…
            if random.random() < 0.2:  # 20%æ¦‚ç‡åˆ·æ–°ä¼ªè£…
                await self._refresh_disguise()

            # å‘èµ·è¯·æ±‚
            headers = self._get_current_headers()

            response = requests.get(
                url,
                headers=headers,
                timeout=self.timeout,
                allow_redirects=True,
                verify=False  # ç¦ç”¨SSLéªŒè¯ï¼Œé¿å…Dockerç¯å¢ƒè¯ä¹¦é—®é¢˜
            )

            self.stats["requests_made"] += 1

            logger.info(f"ğŸ“Š å“åº”çŠ¶æ€: {response.status_code}, å¤§å°: {len(response.text):,} å­—ç¬¦")

            # ğŸ¯ å…³é”®å¤„ç†ï¼šå³ä½¿æ˜¯404ä¹Ÿè¦ç»§ç»­è§£æ
            if response.status_code in [200, 404]:
                self.stats["successful_requests"] += 1

                # æ£€æŸ¥æ˜¯å¦åŒ…å«Next.jsæ•°æ®
                if '__NEXT_DATA__' in response.text:
                    logger.info("âœ… å‘ç°Next.js SSRæ•°æ®")

                    # ğŸ¯ å…³é”®ï¼šæå–Next.jsæ•°æ®
                    nextjs_data = self._extract_nextjs_data(response.text, match_id)

                    if nextjs_data:
                        # ğŸ¯ å…³é”®ï¼šæå–contentæ•°æ®
                        content_data = self._extract_content_data(nextjs_data, match_id)

                        if content_data:
                            self.stats["matches_collected"] += 1
                            logger.info(f"âœ… æ•°æ®æå–æˆåŠŸ: {match_id}")

                            # è¿”å›æ ‡å‡†APIæ ¼å¼
                            return {
                                "match": {"id": match_id},
                                "content": content_data
                            }
                        else:
                            logger.warning(f"âš ï¸ contentæ•°æ®æå–å¤±è´¥: {match_id}")
                            return None
                    else:
                        logger.warning(f"âš ï¸ Next.jsæ•°æ®è§£æå¤±è´¥: {match_id}")
                        return None
                else:
                    if response.status_code == 404:
                        logger.info(f"â„¹ï¸ 404é¡µé¢æ— Next.jsæ•°æ®: {match_id}")
                    else:
                        logger.warning(f"âš ï¸ é¡µé¢æ— Next.jsæ•°æ®: {match_id}")
                    return None

            elif response.status_code == 429:
                logger.warning("âš ï¸ è§¦å‘é¢‘ç‡é™åˆ¶")
                self.stats["retry_count"] += 1
                await asyncio.sleep(random.uniform(10, 20))
                return await self.collect_match_data(match_id)

            elif response.status_code == 403:
                logger.warning("âš ï¸ è§¦å‘åçˆ¬æ£€æµ‹")
                self.stats["retry_count"] += 1
                await self._refresh_disguise()
                await asyncio.sleep(random.uniform(5, 10))
                return await self.collect_match_data(match_id)

            else:
                logger.error(f"âŒ æœªå¤„ç†çš„çŠ¶æ€ç : {response.status_code}")
                self.stats["failed_requests"] += 1
                return None

        except Exception as e:
            logger.error(f"âŒ é‡‡é›†å¼‚å¸¸ {match_id}: {e}")
            self.stats["failed_requests"] += 1
            return None

    def _manual_decompress_response(self, response) -> str:
        """æ‰‹åŠ¨è§£å‹å“åº”å†…å®¹ï¼ˆå¤„ç†GZIPå‹ç¼©é—®é¢˜ï¼‰"""
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰‹åŠ¨è§£å‹GZIP
            if hasattr(response, 'content') and response.content:
                # æ£€æŸ¥GZIPé­”æ•° (1f 8b)
                if response.content[:2] == b'\x1f\x8b':
                    import gzip
                    import io
                    try:
                        decompressed = gzip.GzipFile(fileobj=io.BytesIO(response.content)).read().decode('utf-8')
                        self.logger.info("âœ… æ‰‹åŠ¨GZIPè§£å‹æˆåŠŸ")
                        return decompressed
                    except Exception as e:
                        self.logger.error(f"âŒ æ‰‹åŠ¨GZIPè§£å‹å¤±è´¥: {e}")
                        # å›é€€åˆ°åŸå§‹æ–‡æœ¬
                        if hasattr(response, 'text'):
                            return response.text
                        else:
                            return response.content.decode('utf-8', errors='ignore')

            # å¦‚æœä¸æ˜¯GZIPï¼Œå°è¯•æ­£å¸¸æ–¹å¼
            if hasattr(response, 'text'):
                return response.text
            else:
                return response.content.decode('utf-8', errors='ignore')

        except Exception as e:
            self.logger.error(f"âŒ å“åº”è§£å‹å¼‚å¸¸: {e}")
            # æœ€åå›é€€æ–¹æ¡ˆ
            try:
                return str(response.content, errors='ignore')
            except:
                return ""

    def _extract_nextjs_data(self, html: str, match_id: str) -> Optional[dict[str, Any]]:
        """
        ä»HTMLä¸­æå–Next.jsæ•°æ® - QAéªŒè¯ç‰ˆæœ¬

        ğŸ¯ å…³é”®ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®åŒ¹é…__NEXT_DATA__ JSON
        """
        try:
            # æ”¹è¿›çš„æ­£åˆ™è¡¨è¾¾å¼ï¼Œç²¾ç¡®åŒ¹é…scriptæ ‡ç­¾
            patterns = [
                r'<script[^>]*id=["\']__NEXT_DATA__["\'][^>]*type=["\']application/json["\'][^>]*>(.*?)</script>',
                r'<script[^>]*id=["\']__NEXT_DATA__["\'][^>]*>(.*?)</script>',
                r'window\.__NEXT_DATA__\s*=\s*(\{.*?\});?\s*<\/script>'
            ]

            for pattern in patterns:
                matches = re.findall(pattern, html, re.DOTALL)
                if matches:
                    nextjs_data_str = matches[0].strip()

                    # æ¸…ç†å¯èƒ½çš„JavaScriptåŒ…è£…
                    if nextjs_data_str.startswith('window.__NEXT_DATA__'):
                        nextjs_data_str = nextjs_data_str.replace('window.__NEXT_DATA__', '').replace('=', '').strip()
                        if nextjs_data_str.endswith(';'):
                            nextjs_data_str = nextjs_data_str[:-1]

                    try:
                        nextjs_data = json.loads(nextjs_data_str)
                        logger.info(f"âœ… Next.js JSONè§£ææˆåŠŸ: {match_id}")
                        return nextjs_data
                    except json.JSONDecodeError as e:
                        logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥ {match_id}: {e}")
                        logger.debug(f"   æ•°æ®é¢„è§ˆ: {nextjs_data_str[:200]}...")
                        continue

            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°__NEXT_DATA__: {match_id}")
            return None

        except Exception as e:
            logger.error(f"âŒ Next.jsæå–å¼‚å¸¸ {match_id}: {e}")
            return None

    def _extract_content_data(self, nextjs_data: dict[str, Any], match_id: str) -> Optional[dict[str, Any]]:
        """
        ä»Next.jsæ•°æ®ä¸­æå–content - QAéªŒè¯ç‰ˆæœ¬

        ğŸ¯ å…³é”®ï¼šè§£æprops.pageProps.contentå¹¶æå–MLç‰¹å¾
        """
        try:
            props = nextjs_data.get('props', {})
            if not props:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°props: {match_id}")
                return None

            page_props = props.get('pageProps', {})
            if not page_props:
                # æ£€æŸ¥æ˜¯å¦æ˜¯404é¡µé¢
                url = props.get('url', '')
                if '/404' in url:
                    logger.info(f"â„¹ï¸ è·³è¿‡404é¡µé¢: {match_id}")
                return None

            content = page_props.get('content', {})
            if not content:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°content: {match_id}")
                return None

            logger.info(f"âœ… æˆåŠŸæå–content: {match_id}")
            logger.info(f"   Content Keys: {list(content.keys())}")

            # ğŸ¯ å…³é”®ï¼šéªŒè¯MLç‰¹å¾å­—æ®µ
            required_features = ['matchFacts', 'stats', 'lineup', 'shotmap', 'playerStats']
            found_features = [feature for feature in required_features if feature in content]

            logger.info(f"   æ‰¾åˆ°MLç‰¹å¾: {found_features}/{len(required_features)}")

            # ğŸ¯ å…³é”®ï¼šæ£€æŸ¥xGæ•°æ®
            if 'stats' in content:
                stats = content.get('stats', {})
                if isinstance(stats, dict):
                    periods = stats.get('Periods', {})
                    all_stats = periods.get('All', {})
                    stats_list = all_stats.get('stats', [])

                    xg_found = False
                    for stat_group in stats_list:
                        if isinstance(stat_group, dict) and 'stats' in stat_group:
                            for stat in stat_group.get('stats', []):
                                if isinstance(stat, dict):
                                    title = stat.get('title', '').lower()
                                    if 'expected goals' in title or 'xg' in title:
                                        xg_values = stat.get('stats', [])
                                        if xg_values and len(xg_values) >= 2:
                                            logger.info(f"ğŸ¯ æ‰¾åˆ°xGæ•°æ®: ä¸»é˜Ÿ={xg_values[0]}, å®¢é˜Ÿ={xg_values[1]}")
                                            xg_found = True
                                            break
                        if xg_found:
                            break

                    if not xg_found:
                        logger.info(f"â„¹ï¸ æœªæ‰¾åˆ°xGæ•°æ®: {match_id}")

            return content

        except Exception as e:
            logger.error(f"âŒ contentæå–å¼‚å¸¸ {match_id}: {e}")
            import traceback
            logger.debug(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None

    def get_stats(self) -> dict[str, Any]:
        """è·å–é‡‡é›†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.stats.copy()
        if stats["requests_made"] > 0:
            stats["success_rate"] = stats["successful_requests"] / stats["requests_made"]
        else:
            stats["success_rate"] = 0.0

        stats["stealth_mode"] = self.enable_stealth
        stats["proxy_enabled"] = self.enable_proxy

        return stats

    async def close(self):
        """å…³é—­é‡‡é›†å™¨"""
        # ä¸ä½¿ç”¨Sessionï¼Œæ— éœ€æ¸…ç†
        self.session = None
        logger.info("ğŸ”’ é‡‡é›†å™¨å·²å…³é—­")
