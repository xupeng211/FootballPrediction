"""
FBref é‡‡é›†å™¨ V2 - å¼‚æ­¥åŸºç¡€è®¾æ–½ç‰ˆæœ¬
FBref Collector V2 - Async Infrastructure Version

åŸºäº AsyncBaseCollector é‡æ–°è®¾è®¡çš„ FBref æ•°æ®é‡‡é›†å™¨ï¼Œé›†æˆï¼š
1. RateLimiter é™æµä¿æŠ¤ (QPS=0.5)
2. ProxyPool ä»£ç†è½®æ¢
3. httpx/curl_cffi HTTP å®¢æˆ·ç«¯
4. æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
5. å®Œæ•´çš„åçˆ¬è™«ç­–ç•¥

æŠ€æœ¯ç‰¹æ€§:
- ç»§æ‰¿ AsyncBaseCollector ç»Ÿä¸€åŸºç¡€è®¾æ–½
- æ”¯æŒ curl_cffi ç»•è¿‡ TLS æŒ‡çº¹æ£€æµ‹
- å®ç° Slow Crawl ç­–ç•¥ï¼ˆè‡³å°‘2ç§’é—´éš”ï¼‰
- ä¿æŒä¸æ—§ç‰ˆ 100% å…¼å®¹çš„è¾“å‡ºæ ¼å¼

ä½œè€…: Data Engineer (P2-2 Migration)
åˆ›å»ºæ—¶é—´: 2025-12-06
ç‰ˆæœ¬: 2.0.0
"""

import asyncio
import gzip
import hashlib
import logging
import random
import re
import time
from datetime import datetime, timedelta
from io import StringIO
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from bs4 import BeautifulSoup

# åŸºç¡€è®¾æ–½å¯¼å…¥
from src.core.async_base import AsyncBaseCollector, AsyncConfig
from src.collectors.rate_limiter import RateLimiter, RateLimitConfig
from src.collectors.proxy_pool import ProxyPool, Proxy

# HTTP å®¢æˆ·ç«¯å¯¼å…¥ - æ”¯æŒä¸¤ç§é€‰æ‹©
try:
    import curl_cffi
    from curl_cffi import requests
    HAVE_CURL_CFFI = True
except ImportError:
    HAVE_CURL_CFFI = False
    curl_cffi = None
    requests = None

import httpx
from httpx import AsyncClient

logger = logging.getLogger(__name__)


class FBrefCollectorV2(AsyncBaseCollector):
    """
    FBref æ•°æ®é‡‡é›†å™¨ V2

    åŸºäº AsyncBaseCollector çš„å¼‚æ­¥å®ç°ï¼Œé›†æˆé™æµå’Œä»£ç†åŠŸèƒ½
    """

    def __init__(
        self,
        rate_limiter: Optional[RateLimiter] = None,
        proxy_pool: Optional[ProxyPool] = None,
        use_curl_cffi: bool = True,
        config: Optional[AsyncConfig] = None,
        raw_data_dir: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– FBref é‡‡é›†å™¨ V2

        Args:
            rate_limiter: é€Ÿç‡é™åˆ¶å™¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºé»˜è®¤é…ç½®
            proxy_pool: ä»£ç†æ± ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ä½¿ç”¨ä»£ç†
            use_curl_cffi: æ˜¯å¦ä½¿ç”¨ curl_cffiï¼ˆç»•è¿‡ TLS æŒ‡çº¹ï¼‰
            config: å¼‚æ­¥é…ç½®
            raw_data_dir: åŸå§‹æ•°æ®å­˜å‚¨ç›®å½•
        """
        # åˆå§‹åŒ–åŸºç¡€é…ç½® - FBref éœ€è¦"æ…¢é€Ÿé‡‡é›†"
        if config is None:
            config = AsyncConfig(
                http_timeout=45.0,  # å¢åŠ è¶…æ—¶
                max_retries=5,      # å¢åŠ é‡è¯•æ¬¡æ•°
                retry_delay=8.0,    # å¢åŠ é‡è¯•å»¶è¿Ÿ
                rate_limit_delay=2.0,  # å¼ºåˆ¶è‡³å°‘2ç§’é—´éš”
                enable_performance_monitoring=True
            )

        super().__init__(config=config, name="FBrefCollectorV2")

        # åˆå§‹åŒ–é™æµå™¨ - FBref é™åˆ¶ä¸¥æ ¼ï¼šQPS=0.5, Burst=1
        if rate_limiter:
            self.rate_limiter = rate_limiter
        else:
            # åˆ›å»ºé…ç½®å­—å…¸è€Œä¸æ˜¯ç›´æ¥ä¼ é€’RateLimitConfigå¯¹è±¡
            rate_limit_config = {
                "default": {
                    "rate": 0.5,
                    "burst": 1,
                    "max_wait_time": 60.0
                }
            }
            self.rate_limiter = RateLimiter(rate_limit_config)

        # åˆå§‹åŒ–ä»£ç†æ± 
        self.proxy_pool = proxy_pool

        # HTTP å®¢æˆ·ç«¯é…ç½®
        self.use_curl_cffi = use_curl_cffi and HAVE_CURL_CFFI
        self.async_client: Optional[AsyncClient] = None

        # åŸå§‹æ•°æ®å­˜å‚¨é…ç½®
        self.raw_data_dir = Path(raw_data_dir or Path(__file__).parent.parent.parent / "data" / "raw_landing" / "fbref_v2")
        self.raw_data_dir.mkdir(parents=True, exist_ok=True)

        # FBref ç‰¹å®šçš„æµè§ˆå™¨æŒ‡çº¹è½®æ¢
        self.browser_configs = [
            "chrome", "chrome99", "chrome100", "chrome101", "chrome104"
        ]
        self.current_config_index = 0

        self.logger.info(f"ğŸš€ FBref Collector V2 åˆå§‹åŒ–å®Œæˆ")
        self.logger.info(f"   - HTTPå®¢æˆ·ç«¯: {'curl_cffi' if self.use_curl_cffi else 'httpx'}")
        self.logger.info(f"   - é™æµé…ç½®: QPS=0.5, Burst=1, Interval=2s")
        self.logger.info(f"   - ä»£ç†æ± : {'å¯ç”¨' if self.proxy_pool else 'ç¦ç”¨'}")

    async def _get_headers(self) -> Dict[str, str]:
        """
        è·å– FBref ä¸“ç”¨è¯·æ±‚å¤´

        Returns:
            Dict[str, str]: HTTPè¯·æ±‚å¤´
        """
        headers = {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,en-GB;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Google Chrome";v="120", "Chromium";v="120", "Not.A/Brand";v="24"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }

        # å¦‚æœä½¿ç”¨ä»£ç†ï¼Œæ·»åŠ ä»£ç†ç›¸å…³å¤´
        if self.proxy_pool:
            headers['X-Forwarded-For'] = self._get_random_ip()
            headers['X-Real-IP'] = self._get_random_ip()

        return headers

    async def _get_user_agent(self) -> str:
        """
        è·å–éšæœº User-Agent

        Returns:
            str: User-Agentå­—ç¬¦ä¸²
        """
        user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
        ]
        return random.choice(user_agents)

    def _get_random_ip(self) -> str:
        """ç”Ÿæˆéšæœº IP åœ°å€"""
        return f"{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}.{random.randint(1, 255)}"

    async def _setup_async_client(self) -> None:
        """è®¾ç½®å¼‚æ­¥ HTTP å®¢æˆ·ç«¯"""
        if self.async_client and not self.async_client.is_closed:
            return

        headers = await self._get_headers()
        timeout = httpx.Timeout(self.config.http_timeout)

        # è·å–ä»£ç†é…ç½®
        proxies = None
        if self.proxy_pool:
            proxy = await self.proxy_pool.get_proxy()
            if proxy:
                proxies = proxy.url
                self.logger.debug(f"Using proxy: {proxy.url}")

        # æ³¨æ„ï¼šhttpxçš„ä»£ç†å‚æ•°åæ˜¯proxyè€Œä¸æ˜¯proxies
        self.async_client = AsyncClient(
            headers=headers,
            timeout=timeout,
            proxy=proxies,
            limits=httpx.Limits(max_connections=self.config.max_connections),
            follow_redirects=True
        )

    def _create_curl_session(self):
        """åˆ›å»º curl_cffi ä¼šè¯"""
        if not self.use_curl_cffi:
            return None

        # è½®æ¢æµè§ˆå™¨é…ç½®
        browser_type = self.browser_configs[self.current_config_index]
        self.current_config_index = (self.current_config_index + 1) % len(self.browser_configs)

        headers = asyncio.run(self._get_headers())

        try:
            session = requests.Session(
                impersonate=browser_type,
                headers=headers
            )
            self.logger.debug(f"Created curl_cffi session with {browser_type}")
            return session
        except Exception as e:
            self.logger.warning(f"Failed to create curl_cffi session: {e}")
            return None

    async def fetch_html(self, url: str) -> Optional[str]:
        """
        è·å– HTML å†…å®¹ï¼ˆç»Ÿä¸€æ¥å£ï¼‰

        Args:
            url: ç›®æ ‡URL

        Returns:
            HTMLæ–‡æœ¬å†…å®¹æˆ–None
        """
        self.logger.info(f"ğŸ”— è¯·æ±‚HTML: {url}")

        # åº”ç”¨é€Ÿç‡é™åˆ¶ - Slow Crawl ç­–ç•¥
        # ä»URLä¸­æå–åŸŸå
        from urllib.parse import urlparse
        parsed_url = urlparse(url)
        domain = parsed_url.netloc or "fbref.com"

        async with self.rate_limiter.acquire(domain):
            # é¢å¤–çš„é—´éš”æ—¶é—´ - ç¡®ä¿è‡³å°‘2ç§’é—´éš”
            await asyncio.sleep(2.0)

            for attempt in range(self.config.max_retries):
                try:
                    # é‡è¯•æ—¶çš„é¢å¤–å»¶è¿Ÿ
                    if attempt > 0:
                        delay = self.config.retry_delay * (2 ** attempt) + random.uniform(3, 8)
                        self.logger.warning(f"â¸ï¸ é‡è¯• {attempt + 1}/{self.config.max_retries}ï¼Œå»¶è¿Ÿ {delay:.1f}s")
                        await asyncio.sleep(delay)

                    # æ·»åŠ æ—¶é—´æˆ³é¿å…ç¼“å­˜
                    timestamp = int(time.time() * 1000)
                    url_with_timestamp = f"{url}&_t={timestamp}" if '?' not in url else f"{url}&_t={timestamp}"

                    # å°è¯•ä½¿ç”¨ curl_cffiï¼ˆå¦‚æœå¯ç”¨ä¸”æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼‰
                    if attempt == 0 and self.use_curl_cffi:
                        html_content = await self._fetch_with_curl_cffi(url_with_timestamp)
                        if html_content:
                            return html_content

                    # å›é€€åˆ° httpx
                    html_content = await self._fetch_with_httpx(url_with_timestamp)
                    if html_content:
                        return html_content

                except Exception as e:
                    self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}): {e}")
                    if attempt < self.config.max_retries - 1:
                        continue

            self.logger.error("ğŸ’¥ æ‰€æœ‰è¯·æ±‚å°è¯•å‡å¤±è´¥")
            return None

    async def _fetch_with_curl_cffi(self, url: str) -> Optional[str]:
        """ä½¿ç”¨ curl_cffi è·å–å†…å®¹"""
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œ curl_cffiï¼ˆå®ƒæ˜¯åŒæ­¥çš„ï¼‰
            loop = asyncio.get_event_loop()
            session = self._create_curl_session()

            if not session:
                return None

            def sync_fetch():
                response = session.get(url, timeout=self.config.http_timeout)
                if response.status_code == 200:
                    return response.text
                elif response.status_code == 403:
                    raise Exception("403 Forbidden: å¯èƒ½è¢«æ£€æµ‹åˆ°")
                elif response.status_code == 429:
                    raise Exception("429 Too Many Requests: è¯·æ±‚è¿‡äºé¢‘ç¹")
                else:
                    raise Exception(f"HTTP {response.status_code}")

            html_content = await loop.run_in_executor(None, sync_fetch)

            # å…³é—­ä¼šè¯
            session.close()

            self.logger.info(f"âœ… curl_cffi è·å–æˆåŠŸï¼Œå¤§å°: {len(html_content):,} å­—èŠ‚")

            # æ‰§è¡Œ HTML è§£å°
            unsealed_html = self._unseal_html(html_content)

            if '<table' in unsealed_html.lower():
                self.logger.info("âœ… HTMLåŒ…å«è¡¨æ ¼æ•°æ®")
                return unsealed_html
            else:
                self.logger.warning("âš ï¸ HTMLå¯èƒ½ä¸åŒ…å«è¡¨æ ¼æ•°æ®")
                return unsealed_html

        except Exception as e:
            self.logger.error(f"âŒ curl_cffi è¯·æ±‚å¤±è´¥: {e}")
            return None

    async def _fetch_with_httpx(self, url: str) -> Optional[str]:
        """ä½¿ç”¨ httpx è·å–å†…å®¹"""
        try:
            await self._setup_async_client()

            response = await self.async_client.get(url)

            if response.status_code == 200:
                html_content = response.text
                self.logger.info(f"âœ… httpx è·å–æˆåŠŸï¼Œå¤§å°: {len(html_content):,} å­—èŠ‚")

                # æ‰§è¡Œ HTML è§£å°
                unsealed_html = self._unseal_html(html_content)

                if '<table' in unsealed_html.lower():
                    self.logger.info("âœ… HTMLåŒ…å«è¡¨æ ¼æ•°æ®")
                    return unsealed_html
                else:
                    self.logger.warning("âš ï¸ HTMLå¯èƒ½ä¸åŒ…å«è¡¨æ ¼æ•°æ®")
                    return unsealed_html

            elif response.status_code == 403:
                raise Exception("403 Forbidden: å¯èƒ½è¢«æ£€æµ‹åˆ°")
            elif response.status_code == 429:
                raise Exception("429 Too Many Requests: è¯·æ±‚è¿‡äºé¢‘ç¹")
            else:
                raise Exception(f"HTTP {response.status_code}")

        except Exception as e:
            self.logger.error(f"âŒ httpx è¯·æ±‚å¤±è´¥: {e}")
            return None

    def _unseal_html(self, html_content: str) -> str:
        """
        HTML æ³¨é‡Šè§£å°æŠ€æœ¯ - æå– FBref éšè—çš„ xG æ•°æ®

        Args:
            html_content: åŸå§‹HTMLå†…å®¹

        Returns:
            è§£å°åçš„HTMLå†…å®¹
        """
        try:
            self.logger.info("ğŸ”“ å¼€å§‹HTMLæ³¨é‡Šè§£å°...")

            # ç»Ÿè®¡åŸå§‹æ³¨é‡Šæ•°é‡
            original_comment_count = html_content.count('<!--')
            self.logger.info(f"ğŸ“‹ å‘ç° {original_comment_count} ä¸ªHTMLæ³¨é‡Š")

            unsealed_html = html_content
            unsealed_count = 0

            # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åŒ…å«è¡¨æ ¼çš„æ³¨é‡Š
            comment_pattern = r'<!--(.*?)-->'
            comments = re.findall(comment_pattern, unsealed_html, re.DOTALL)

            for comment in comments:
                comment_str = comment.strip()

                # æ£€æŸ¥æ˜¯å¦ä¸ºåŒ…å«ç»Ÿè®¡è¡¨æ ¼çš„æ³¨é‡Š
                if (any(keyword in comment_str.lower() for keyword in ['id="sched_', 'id="all_', 'table', 'tbody']) and
                    any(keyword in comment_str.lower() for keyword in ['xg', 'xga', 'shot', 'possession', 'touches'])):

                    # è§£å°ï¼šç§»é™¤æ³¨é‡Šæ ‡è®°
                    sealed_html = f'<!--{comment_str}-->'
                    unsealed_content = comment_str

                    # æ‰§è¡Œæ›¿æ¢
                    unsealed_html = unsealed_html.replace(sealed_html, unsealed_content)
                    unsealed_count += 1

                    self.logger.debug(f"ğŸ”“ è§£å°è¡¨æ ¼ {unsealed_count}: {comment_str[:100]}...")

            # ç»Ÿè®¡è§£å°ç»“æœ
            remaining_comments = unsealed_html.count('<!--')
            self.logger.info(f"âœ… è§£å°å®Œæˆ: {unsealed_count} ä¸ªè¡¨æ ¼å·²è§£å°")
            self.logger.info(f"ğŸ“Š æ³¨é‡Šç»Ÿè®¡: {original_comment_count} â†’ {remaining_comments}")

            # éªŒè¯è§£å°æ•ˆæœ
            new_table_count = unsealed_html.count('<table')
            original_table_count = html_content.count('<table')
            if new_table_count > original_table_count:
                self.logger.info(f"ğŸ¯ è¡¨æ ¼æ•°é‡å¢åŠ : {original_table_count} â†’ {new_table_count} (+{new_table_count - original_table_count})")

            return unsealed_html

        except Exception as e:
            self.logger.error(f"âŒ HTMLæ³¨é‡Šè§£å°å¤±è´¥: {e}")
            return html_content

    def _save_raw_html(self, html_content: str, url: str, league_id: str = None, season: str = None) -> Optional[str]:
        """
        ä¿å­˜åŸå§‹HTMLå†…å®¹åˆ°æ–‡ä»¶ç³»ç»Ÿ

        Args:
            html_content: HTMLæ–‡æœ¬å†…å®¹
            url: åŸå§‹è¯·æ±‚URL
            league_id: è”èµ›ID
            season: èµ›å­£

        Returns:
            ä¿å­˜çš„æ–‡ä»¶ç»å¯¹è·¯å¾„ï¼Œå¦‚æœä¿å­˜å¤±è´¥è¿”å›None
        """
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            content_hash = hashlib.sha256(html_content.encode('utf-8')).hexdigest()[:16]
            filename = f"{timestamp}_{content_hash}.html.gz"

            # åˆ›å»ºç›®å½•ç»“æ„
            if league_id and season:
                season_normalized = season.replace("-", "")
                target_dir = self.raw_data_dir / league_id / season_normalized
            else:
                # ä»URLä¸­æå–league_id
                league_match = re.search(r'/comps/(\d+)/', url)
                if league_match:
                    league_id_fallback = league_match.group(1)
                    target_dir = self.raw_data_dir / league_id_fallback / "unknown_season"
                else:
                    target_dir = self.raw_data_dir / "unknown_league" / "unknown_season"

            target_dir.mkdir(parents=True, exist_ok=True)
            file_path = target_dir / filename

            # å‹ç¼©ä¿å­˜HTMLå†…å®¹
            with gzip.open(file_path, 'wt', encoding='utf-8') as f:
                f.write(html_content)

            file_size = file_path.stat().st_size
            self.logger.info(f"ğŸ—‚ï¸ åŸå§‹HTMLå·²ä¿å­˜: {file_path} ({file_size:,} å­—èŠ‚å‹ç¼©å)")

            return str(file_path.absolute())

        except Exception as e:
            self.logger.warning(f"âš ï¸ åŸå§‹HTMLä¿å­˜å¤±è´¥ (ä¸å½±å“é‡‡é›†): {e}")
            return None

    def parse_html_tables(self, html_content: str) -> Tuple[List[pd.DataFrame], Dict[str, pd.DataFrame]]:
        """
        è§£æHTMLä¸­çš„è¡¨æ ¼æ•°æ® - ä¸æ—§ç‰ˆä¿æŒå…¼å®¹

        Args:
            html_content: HTMLå†…å®¹

        Returns:
            è§£æå‡ºçš„DataFrameåˆ—è¡¨ï¼ŒåŒ…å«match_report_urlåˆ—å’Œç»Ÿè®¡æ•°æ®
        """
        try:
            self.logger.info("ğŸ”— å¼€å§‹HTMLè¡¨æ ¼è§£æ...")

            # 1. ä½¿ç”¨BeautifulSoupè¯†åˆ«æ‰€æœ‰è¡¨æ ¼
            soup = BeautifulSoup(html_content, 'html.parser')

            # æŸ¥æ‰¾ç»Ÿè®¡è¡¨æ ¼å…³é”®è¯
            table_keywords = [
                'stats', 'shooting', 'schedule', 'passing', 'defending', 'possession',
                'misc', 'team', 'summary', 'keeper', 'keeper_adv'
            ]
            all_table_divs = soup.find_all('div', id=lambda x: x and any(keyword in x.lower() for keyword in table_keywords))
            self.logger.info(f"ğŸ¯ å‘ç° {len(all_table_divs)} ä¸ªåŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„è¡¨æ ¼div")

            # æŸ¥æ‰¾æˆ˜æœ¯ç»Ÿè®¡è¡¨æ ¼
            tactical_table_divs = soup.find_all('table', id=lambda x: x and any(
                keyword in str(x).lower() for keyword in ['shots', 'passes', 'fouls', 'tackles', 'possession']
            ))
            self.logger.info(f"ğŸ¯ å‘ç° {len(tactical_table_divs)} ä¸ªæˆ˜æœ¯ç»Ÿè®¡è¡¨æ ¼")

            # 2. ç”¨pandasè§£ææ‰€æœ‰è¡¨æ ¼æ•°æ®
            tables = pd.read_html(StringIO(html_content))
            self.logger.info(f"ğŸ“Š pandasè§£æå‡º {len(tables)} ä¸ªåŸºç¡€è¡¨æ ¼")

            # 3. ä½¿ç”¨BeautifulSoupæå–Match Reporté“¾æ¥
            match_report_links = self._extract_match_report_links(html_content)
            self.logger.info(f"ğŸ”— BeautifulSoupæå–åˆ° {len(match_report_links)} ä¸ªMatch Reporté“¾æ¥")

            # 4. æå–é«˜çº§ç»Ÿè®¡æ•°æ®
            advanced_stats = self._extract_advanced_stats(tables)
            self.logger.info(f"ğŸ“ˆ æå–åˆ° {len(advanced_stats)} ä¸ªç»Ÿè®¡è¡¨æ ¼")

            # 5. å°†ç»Ÿè®¡æ•°æ®é™„åŠ åˆ°è¿”å›ç»“æœä¸­
            stats_result = advanced_stats if advanced_stats else {}

            # 6. å°†é“¾æ¥åˆå¹¶åˆ°èµ›ç¨‹è¡¨æ ¼ä¸­
            if tables and match_report_links:
                schedule_df = tables[0]

                if len(match_report_links) <= len(schedule_df):
                    schedule_df = schedule_df.copy()
                    schedule_df['match_report_url'] = None
                    schedule_df.loc[:len(match_report_links)-1, 'match_report_url'] = match_report_links
                    self.logger.info(f"âœ… æˆåŠŸå°† {len(match_report_links)} ä¸ªé“¾æ¥åˆå¹¶åˆ°èµ›ç¨‹è¡¨")

                tables[0] = schedule_df

            return tables, stats_result

        except Exception as e:
            self.logger.error(f"âŒ HTMLè¡¨æ ¼è§£æå¤±è´¥: {e}")
            return [], {}

    def _extract_match_report_links(self, html_content: str) -> List[str]:
        """ä»HTMLå†…å®¹ä¸­æå–Match Reporté“¾æ¥"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            match_report_urls = []

            # å¯»æ‰¾FBrefçš„Match Reporté“¾æ¥
            for link in soup.find_all('a', href=True):
                href = link['href']
                if '/matches/' in href and 'fbref.com' not in href:  # ç›¸å¯¹è·¯å¾„
                    full_url = f"https://fbref.com{href}"
                    match_report_urls.append(full_url)

            # å¦‚æœæ–¹æ³•1å¤±è´¥ï¼ŒæŸ¥æ‰¾ç‰¹å®šdata-statå±æ€§
            if not match_report_urls:
                for td in soup.find_all('td', {'data-stat': 'match_report'}):
                    link = td.find('a', href=True)
                    if link and '/matches/' in link['href']:
                        full_url = f"https://fbref.com{link['href']}"
                        match_report_urls.append(full_url)

            # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼
            if not match_report_urls:
                pattern = r'href="(/en/matches/[^"]+)"'
                matches = re.findall(pattern, html_content)
                for match in matches:
                    full_url = f"https://fbref.com{match}"
                    match_report_urls.append(full_url)

            self.logger.info(f"ğŸ”— æ€»å…±æå–åˆ° {len(match_report_urls)} ä¸ªMatch Reporté“¾æ¥")
            return match_report_urls

        except Exception as e:
            self.logger.error(f"âŒ Match Reporté“¾æ¥æå–å¤±è´¥: {e}")
            return []

    def _extract_advanced_stats(self, tables: List[pd.DataFrame]) -> Dict[str, pd.DataFrame]:
        """ä»è¡¨æ ¼ä¸­æå–é«˜çº§ç»Ÿè®¡æ•°æ®"""
        stats_dict = {}

        try:
            for i, table in enumerate(tables):
                if table.empty:
                    continue

                columns_str = [str(col).lower() for col in table.columns]

                # æ£€æŸ¥ä¸åŒç±»å‹çš„ç»Ÿè®¡è¡¨
                is_shooting_stats = any(keyword in ' '.join(columns_str) for keyword in
                    ['xg', 'xga', 'shots', 'possession', 'touches', 'pressures', 'passes'])

                if is_shooting_stats:
                    self.logger.info(f"ğŸ“ˆ å‘ç°å°„å‡»ç»Ÿè®¡è¡¨ (ç´¢å¼• {i}): {table.shape}")
                    stats_dict['shooting'] = table
                elif any(keyword in ' '.join(columns_str) for keyword in
                    ['keeper', 'gk', 'goalkeeper', 'defending']):
                    self.logger.info(f"ğŸ›¡ï¸ å‘ç°é˜²å®ˆç»Ÿè®¡è¡¨ (ç´¢å¼• {i}): {table.shape}")
                    stats_dict['defending'] = table
                elif any(keyword in ' '.join(columns_str) for keyword in
                    ['passing', 'pass_types', 'gca', 'scca']):
                    self.logger.info(f"ğŸ¯ å‘ç°ä¼ çƒç»Ÿè®¡è¡¨ (ç´¢å¼• {i}): {table.shape}")
                    stats_dict['passing'] = table
                elif any(keyword in ' '.join(columns_str) for keyword in
                    ['dribbles', 'take_ons', 'carries']):
                    self.logger.info(f"ğŸƒ å‘ç°è¿çƒç»Ÿè®¡è¡¨ (ç´¢å¼• {i}): {table.shape}")
                    stats_dict['dribbling'] = table
                elif any(keyword in ' '.join(columns_str) for keyword in
                    ['misc', 'fouls', 'cards']):
                    self.logger.info(f"ğŸ“ å‘ç°æ‚é¡¹ç»Ÿè®¡è¡¨ (ç´¢å¼• {i}): {table.shape}")
                    stats_dict['misc'] = table

        except Exception as e:
            self.logger.error(f"âŒ ç»Ÿè®¡è¡¨æå–å¤±è´¥: {e}")

        return stats_dict

    def extract_schedule_table(self, tables: List[pd.DataFrame]) -> Optional[pd.DataFrame]:
        """
        ä»è¡¨æ ¼ä¸­æå–èµ›ç¨‹è¡¨ - ä¸æ—§ç‰ˆä¿æŒå…¼å®¹

        Args:
            tables: DataFrameåˆ—è¡¨

        Returns:
            èµ›ç¨‹è¡¨DataFrameæˆ–None
        """
        if not tables:
            return None

        # æ™ºèƒ½è¯†åˆ«èµ›ç¨‹è¡¨
        for i, table in enumerate(tables):
            columns_str = [str(col).lower() for col in table.columns]

            # æ£€æŸ¥èµ›ç¨‹è¡¨ç‰¹å¾åˆ—
            has_date = any(keyword in col for col in columns_str
                         for keyword in ['date', 'wk', 'week', 'day', 'time'])
            has_home = any(keyword in col for col in columns_str
                         for keyword in ['home', 'squad', 'team'])
            has_score = any(keyword in col for col in columns_str
                          for keyword in ['score', 'result', 'goals'])
            has_away = any(keyword in col for col in columns_str
                         for keyword in ['away', 'opponent', 'visitor'])

            # æ’é™¤ç§¯åˆ†æ¦œç‰¹å¾åˆ—
            standings_keywords = ['mp', 'w', 'd', 'l', 'pts', 'ga', 'gd', 'points', 'played']
            is_standings = any(keyword in col for col in columns_str
                              for keyword in standings_keywords
                              if 'away' not in col)

            self.logger.info(f"ğŸ” è¡¨æ ¼ {i}: {table.shape}, åˆ—: {columns_str[:5]}...")
            self.logger.info(f"   ç‰¹å¾: date={has_date}, home={has_home}, score={has_score}, away={has_away}, standings={is_standings}")

            # èµ›ç¨‹è¡¨è¯†åˆ«æ¡ä»¶
            if (has_date and has_home and has_away and has_score and
                table.shape[0] >= 10 and table.shape[1] >= 3):

                self.logger.info(f"ğŸ¯ æ‰¾åˆ°èµ›ç¨‹è¡¨ (ç´¢å¼• {i}): {table.shape}")
                self.logger.info(f"ğŸ“‹ è¡¨æ ¼åˆ—å: {list(table.columns)}")

                # æ£€æŸ¥æ˜¯å¦åŒ…å«xGæ•°æ®
                has_xg = any('xg' in col for col in columns_str)
                if has_xg:
                    self.logger.info("âœ… ç¡®è®¤åŒ…å«xGæ•°æ®åˆ—")

                return table

        self.logger.error("âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„èµ›ç¨‹è¡¨")
        return None

    async def collect_season_stats(self, season_url: str, season_year: Optional[str] = None) -> pd.DataFrame:
        """
        é‡‡é›†èµ›å­£ç»Ÿè®¡æ•°æ® - æ ¸å¿ƒä¸šåŠ¡é€»è¾‘

        Args:
            season_url: FBrefèµ›å­£URL
            season_year: èµ›å­£å¹´ä»½ (å¦‚ "2023-2024")

        Returns:
            åŒ…å«æ¯”èµ›æ•°æ®çš„DataFrame
        """
        self.logger.info(f"ğŸ•µï¸ FBrefèµ›å­£æ•°æ®é‡‡é›†: {season_url}")

        # æ„å»ºå®Œæ•´URL
        if season_year:
            if '?' in season_url:
                url = f"{season_url}&season={season_year.replace('-', '')}"
            else:
                url = f"{season_url}?season={season_year.replace('-', '')}"
        else:
            url = season_url

        self.logger.info(f"ğŸ¯ ç›®æ ‡URL: {url}")

        # è·å–HTMLå†…å®¹
        html_content = await self.fetch_html(url)
        if not html_content:
            self.logger.error("âŒ æ— æ³•è·å–HTMLå†…å®¹")
            return pd.DataFrame()

        # ä¿å­˜åŸå§‹HTMLå†…å®¹
        import re
        league_match = re.search(r'/comps/(\d+)/', url)
        league_id = league_match.group(1) if league_match else None
        season_normalized = season_year.replace("-", "") if season_year else None

        raw_file_path = self._save_raw_html(
            html_content=html_content,
            url=url,
            league_id=league_id,
            season=season_normalized
        )

        # è§£æHTMLè¡¨æ ¼
        tables, advanced_stats = self.parse_html_tables(html_content)
        if not tables:
            self.logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•HTMLè¡¨æ ¼")
            return pd.DataFrame()

        # æå–èµ›ç¨‹è¡¨
        schedule_table = self.extract_schedule_table(tables)
        if schedule_table is None:
            self.logger.error("âŒ æ— æ³•æå–èµ›ç¨‹è¡¨")
            return pd.DataFrame()

        # å°†åŸå§‹æ–‡ä»¶è·¯å¾„æ·»åŠ åˆ°DataFrameä¸­
        if raw_file_path and not schedule_table.empty:
            schedule_table = schedule_table.copy()
            schedule_table['raw_file_path'] = raw_file_path
            self.logger.info(f"âœ… åŸå§‹æ–‡ä»¶è·¯å¾„å·²æ·»åŠ åˆ°DataFrame")

        self.logger.info(f"ğŸ‰ èµ›å­£æ•°æ®é‡‡é›†æˆåŠŸï¼Œè¡¨æ ¼å½¢çŠ¶: {schedule_table.shape}")
        return schedule_table

    async def collect_match_stats(self, match_url: str) -> Dict[str, Any]:
        """
        é‡‡é›†å•åœºæ¯”èµ›çš„è¯¦ç»†ç»Ÿè®¡æ•°æ®

        Args:
            match_url: æ¯”èµ›è¯¦æƒ…é¡µURL

        Returns:
            åŒ…å«æ¯”èµ›ç»Ÿè®¡æ•°æ®çš„å­—å…¸
        """
        self.logger.info(f"âš½ FBrefæ¯”èµ›è¯¦æƒ…é‡‡é›†: {match_url}")

        # è·å–HTMLå†…å®¹
        html_content = await self.fetch_html(match_url)
        if not html_content:
            self.logger.error("âŒ æ— æ³•è·å–æ¯”èµ›HTMLå†…å®¹")
            return {}

        # ä¿å­˜åŸå§‹HTML
        raw_file_path = self._save_raw_html(html_content, match_url)

        # è§£æè¡¨æ ¼
        tables, advanced_stats = self.parse_html_tables(html_content)

        # æ„å»ºæ¯”èµ›ç»Ÿè®¡ç»“æœ
        match_stats = {
            'match_url': match_url,
            'raw_file_path': raw_file_path,
            'tables_count': len(tables),
            'advanced_stats': advanced_stats,
            'has_shooting_stats': 'shooting' in advanced_stats,
            'has_passing_stats': 'passing' in advanced_stats,
            'has_defending_stats': 'defending' in advanced_stats,
        }

        self.logger.info(f"âœ… æ¯”èµ›ç»Ÿè®¡é‡‡é›†å®Œæˆï¼Œè¡¨æ ¼æ•°: {len(tables)}")
        return match_stats

    def _clean_schedule_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—èµ›ç¨‹æ•°æ® - ä¸æ—§ç‰ˆä¿æŒå®Œå…¨å…¼å®¹

        Args:
            df: åŸå§‹DataFrame

        Returns:
            æ¸…æ´—åçš„DataFrame
        """
        self.logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")

        if df.empty:
            return df

        # å¤„ç†MultiIndexåˆ—å
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = ['_'.join(col).strip() if col[1] else col[0] for col in df.columns.values]

        # æ™ºèƒ½åˆ—åæ˜ å°„ï¼ˆä¸æ—§ç‰ˆä¿æŒä¸€è‡´ï¼‰
        column_mapping = {}
        for col in df.columns:
            col_str = str(col).lower()

            # åŸºç¡€æ¯”èµ›ä¿¡æ¯
            if 'date' in col_str and 'date' not in column_mapping:
                column_mapping['date'] = col
            elif 'home' in col_str and 'away' not in col_str and 'home' not in column_mapping:
                column_mapping['home'] = col
            elif 'away' in col_str and 'home' not in col_str and 'away' not in column_mapping:
                column_mapping['away'] = col
            elif 'score' in col_str and 'score' not in column_mapping:
                column_mapping['score'] = col

            # xGæ•°æ®æ˜ å°„ï¼ˆä¸æ—§ç‰ˆå®Œå…¨ä¸€è‡´ï¼‰
            elif col_str == 'xg' and 'xg_home' not in column_mapping:
                column_mapping['xg_home'] = col
                self.logger.info(f"ğŸ¯ æ˜ å°„xGåˆ—: {col} -> xg_home")
            elif col_str == 'xg.1' and 'xg_away' not in column_mapping:
                column_mapping['xg_away'] = col
                self.logger.info(f"ğŸ¯ æ˜ å°„xGåˆ—: {col} -> xg_away")
            elif 'expected goals' in col_str and 'xg_home' not in column_mapping:
                column_mapping['xg_home'] = col
                self.logger.info(f"ğŸ¯ æ˜ å°„Expected Goalsåˆ—: {col} -> xg_home")
            elif 'expected goals' in col_str and 'away' in col_str and 'xg_away' not in column_mapping:
                column_mapping['xg_away'] = col
                self.logger.info(f"ğŸ¯ æ˜ å°„Expected Goalsåˆ—: {col} -> xg_away")

            # å…¶ä»–ç»Ÿè®¡å­—æ®µ
            elif 'shots' in col_str and 'shots_on_target' not in col_str and 'shots' not in column_mapping:
                column_mapping['shots'] = col
            elif 'shots on target' in col_str and 'shots_on_target' not in column_mapping:
                column_mapping['shots_on_target'] = col
            elif 'possession' in col_str and 'possession_home' not in column_mapping and 'possession' not in column_mapping:
                column_mapping['possession_home'] = col
            elif 'match_report_url' in col_str and 'match_report_url' not in column_mapping:
                column_mapping['match_report_url'] = col

        # æ„å»ºæ¸…æ´—åçš„DataFrame
        cleaned_df = pd.DataFrame()
        for new_name, old_name in column_mapping.items():
            if old_name in df.columns:
                cleaned_df[new_name] = df[old_name].copy()

        # å¼ºåˆ¶ä¿ç•™match_report_urlåˆ—
        if 'match_report_url' in df.columns and 'match_report_url' not in cleaned_df.columns:
            cleaned_df['match_report_url'] = df['match_report_url'].copy()
            self.logger.info("ğŸ”— å¼ºåˆ¶ä¿ç•™match_report_urlåˆ—")

        # ä¿ç•™æ‰€æœ‰æœªæ˜ å°„çš„åˆ—
        for col in df.columns:
            if col not in column_mapping.values() and col not in cleaned_df.columns:
                cleaned_df[f'raw_{col}'] = df[col].copy()

        # å¼ºåˆ¶è½¬æ¢xGåˆ—ä¸ºfloatç±»å‹
        if 'xg_home' in cleaned_df.columns:
            cleaned_df['xg_home'] = pd.to_numeric(cleaned_df['xg_home'], errors='coerce')
            xg_home_valid = cleaned_df['xg_home'].notna().sum()
            self.logger.info(f"   xg_homeæœ‰æ•ˆæ•°æ®: {xg_home_valid}/{len(cleaned_df)}")

        if 'xg_away' in cleaned_df.columns:
            cleaned_df['xg_away'] = pd.to_numeric(cleaned_df['xg_away'], errors='coerce')
            xg_away_valid = cleaned_df['xg_away'].notna().sum()
            self.logger.info(f"   xg_awayæœ‰æ•ˆæ•°æ®: {xg_away_valid}/{len(cleaned_df)}")

        self.logger.info(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {len(cleaned_df.columns)} åˆ—")
        return cleaned_df

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.async_client and not self.async_client.is_closed:
            await self.async_client.aclose()
            self.async_client = None

        await super()._cleanup()
        self.logger.info("ğŸ§¹ FBref Collector V2 èµ„æºæ¸…ç†å®Œæˆ")

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.cleanup()
        await super().__aexit__(exc_type, exc_val, exc_tb)