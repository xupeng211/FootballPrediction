"""
Prefect æ•°æ®ç®¡é“å·¥ä½œæµ
Football Prediction é¡¹ç›®çš„ä¸»è¦æ•°æ®å¤„ç†å’Œæœºå™¨å­¦ä¹ ç®¡é“
ä½¿ç”¨ Prefect 2.0+ çš„ @flow å’Œ @task è£…é¥°å™¨å®ç°
"""

from prefect import flow, task
import logging
from typing import Dict, Any, Optional, List
import asyncio
from datetime import datetime, timedelta
import json
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@task(
    name="è·å–æ¯”èµ›æ•°æ®",
    description="ä»å¤–éƒ¨APIè·å–æœ€æ–°æ¯”èµ›æ•°æ®",
    retries=3,
    retry_delay_seconds=60,
    cache_expiration=timedelta(hours=1),
)
async def fetch_data() -> Dict[str, Any]:
    """
    ä»å¤–éƒ¨æ•°æ®æºè·å–è¶³çƒæ¯”èµ›æ•°æ®

    Returns:
        Dict[str, Any]: åŒ…å«æ¯”èµ›æ•°æ®çš„å­—å…¸

    Raises:
        Exception: æ•°æ®è·å–å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    logger.info("ğŸ”„ å¼€å§‹è·å–æ¯”èµ›æ•°æ®...")

    try:
        # æ¨¡æ‹Ÿæ•°æ®è·å–è¿‡ç¨‹
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„æ•°æ®æºAPI

        # å ä½ç¬¦æ•°æ® - å®é™…å®ç°æ—¶æ›¿æ¢ä¸ºçœŸå®æ•°æ®æº
        mock_data = {
            "matches": [
                {
                    "id": 1,
                    "home_team": "Team A",
                    "away_team": "Team B",
                    "date": "2024-01-15",
                    "league": "Premier League",
                    "home_score": 2,
                    "away_score": 1,
                    "stats": {
                        "possession": {"home": 55, "away": 45},
                        "shots": {"home": 12, "away": 8},
                        "corners": {"home": 6, "away": 4},
                    },
                },
                {
                    "id": 2,
                    "home_team": "Team C",
                    "away_team": "Team D",
                    "date": "2024-01-16",
                    "league": "La Liga",
                    "home_score": 0,
                    "away_score": 0,
                    "stats": {
                        "possession": {"home": 50, "away": 50},
                        "shots": {"home": 5, "away": 7},
                        "corners": {"home": 3, "away": 5},
                    },
                },
            ],
            "metadata": {
                "source": "external_api",
                "fetch_time": datetime.now().isoformat(),
                "total_matches": 2,
            },
        }

        logger.info(f"âœ… æˆåŠŸè·å– {len(mock_data['matches'])} åœºæ¯”èµ›æ•°æ®")

        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“æˆ–ç¼“å­˜
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ

        return mock_data

    except Exception as e:
        logger.error(f"âŒ æ•°æ®è·å–å¤±è´¥: {str(e)}")
        raise


@task(
    name="ç‰¹å¾å·¥ç¨‹å¤„ç†",
    description="å¯¹åŸå§‹æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹å¤„ç†",
    retries=2,
    retry_delay_seconds=30,
)
async def engineer_features(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    å¯¹è·å–çš„åŸå§‹æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹å¤„ç†

    Args:
        data (Dict[str, Any]): åŸå§‹æ¯”èµ›æ•°æ®

    Returns:
        Dict[str, Any]: å¤„ç†åçš„ç‰¹å¾æ•°æ®
    """
    logger.info("ğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹å¤„ç†...")

    try:
        features = []
        labels = []

        for match in data.get("matches", []):
            # åŸºç¡€ç‰¹å¾æå–
            feature_vector = {
                "match_id": match["id"],
                "home_team": match["home_team"],
                "away_team": match["away_team"],
                # ç»Ÿè®¡ç‰¹å¾
                "home_possession": match["stats"]["possession"]["home"],
                "away_possession": match["stats"]["possession"]["away"],
                "home_shots": match["stats"]["shots"]["home"],
                "away_shots": match["stats"]["shots"]["away"],
                "home_corners": match["stats"]["corners"]["home"],
                "away_corners": match["stats"]["corners"]["away"],
                # æ´¾ç”Ÿç‰¹å¾
                "possession_diff": match["stats"]["possession"]["home"]
                - match["stats"]["possession"]["away"],
                "shots_diff": match["stats"]["shots"]["home"]
                - match["stats"]["shots"]["away"],
                "corners_diff": match["stats"]["corners"]["home"]
                - match["stats"]["corners"]["away"],
                "total_shots": match["stats"]["shots"]["home"]
                + match["stats"]["shots"]["away"],
                "total_corners": match["stats"]["corners"]["home"]
                + match["stats"]["corners"]["away"],
                # ç›®æ ‡å˜é‡
                "home_goals": match["home_score"],
                "away_goals": match["away_score"],
                "goal_difference": match["home_score"] - match["away_score"],
            }

            features.append(feature_vector)

            # æ ‡ç­¾æ„å»º (è¿™é‡Œç®€åŒ–ä¸ºèƒœè´Ÿå¹³)
            if match["home_score"] > match["away_score"]:
                result = "home_win"
            elif match["home_score"] < match["away_score"]:
                result = "away_win"
            else:
                result = "draw"

            labels.append(
                {
                    "match_id": match["id"],
                    "result": result,
                    "over_2_5_goals": (match["home_score"] + match["away_score"]) > 2.5,
                    "both_teams_score": match["home_score"] > 0
                    and match["away_score"] > 0,
                }
            )

        processed_data = {
            "features": features,
            "labels": labels,
            "metadata": {
                "total_samples": len(features),
                "feature_count": len(features[0]) if features else 0,
                "processing_time": datetime.now().isoformat(),
                "original_data_source": data.get("metadata", {}).get(
                    "source", "unknown"
                ),
            },
        }

        logger.info(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆï¼Œç”Ÿæˆäº† {len(features)} ä¸ªæ ·æœ¬")

        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        await asyncio.sleep(1)

        return processed_data

    except Exception as e:
        logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤„ç†å¤±è´¥: {str(e)}")
        raise


@task(
    name="æ¨¡å‹è®­ç»ƒ",
    description="ä½¿ç”¨å¤„ç†åçš„ç‰¹å¾æ•°æ®è®­ç»ƒé¢„æµ‹æ¨¡å‹",
    retries=1,
    retry_delay_seconds=10,
)
async def train_model(features: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä½¿ç”¨ç‰¹å¾æ•°æ®è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹

    Args:
        features (Dict[str, Any]): ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®

    Returns:
        Dict[str, Any]: è®­ç»ƒç»“æœå’Œæ¨¡å‹è¯„ä¼°
    """
    logger.info("ğŸ¤– å¼€å§‹æ¨¡å‹è®­ç»ƒ...")

    try:
        # è¿™é‡Œæ˜¯å ä½ç¬¦å®ç°
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨çœŸå®çš„æœºå™¨å­¦ä¹ è®­ç»ƒä»£ç 

        feature_list = features.get("features", [])
        label_list = features.get("labels", [])

        if not feature_list or not label_list:
            raise ValueError("ç‰¹å¾æˆ–æ ‡ç­¾æ•°æ®ä¸ºç©º")

        # æ¨¡æ‹Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹
        logger.info(f"ğŸ“Š ä½¿ç”¨ {len(feature_list)} ä¸ªæ ·æœ¬è¿›è¡Œæ¨¡å‹è®­ç»ƒ")

        # å ä½ç¬¦è®­ç»ƒç»“æœ
        training_results = {
            "model_type": "ensemble",  # é›†æˆæ¨¡å‹
            "training_samples": len(feature_list),
            "feature_dimensions": len(feature_list[0]) if feature_list else 0,
            "training_time_seconds": 5.0,
            "model_performance": {
                "accuracy": 0.75,
                "precision": 0.73,
                "recall": 0.78,
                "f1_score": 0.75,
                "roc_auc": 0.82,
            },
            "feature_importance": {
                "possession_diff": 0.15,
                "shots_diff": 0.25,
                "corners_diff": 0.10,
                "total_shots": 0.20,
                "total_corners": 0.12,
                "home_possession": 0.08,
                "away_possession": 0.06,
                "other_features": 0.04,
            },
            "training_metadata": {
                "algorithm": "XGBoost + LSTM Ensemble",
                "hyperparameters": {
                    "learning_rate": 0.01,
                    "max_depth": 6,
                    "n_estimators": 100,
                    "random_state": 42,
                },
                "cross_validation_folds": 5,
                "training_timestamp": datetime.now().isoformat(),
            },
        }

        # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
        await asyncio.sleep(3)

        logger.info(
            f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {training_results['model_performance']['accuracy']:.2%}"
        )

        return training_results

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}")
        raise


@task(name="æ¨¡å‹è¯„ä¼°", description="è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½")
async def evaluate_model(model_results: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹æ€§èƒ½

    Args:
        model_results (Dict[str, Any]): æ¨¡å‹è®­ç»ƒç»“æœ

    Returns:
        Dict[str, Any]: è¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š
    """
    logger.info("ğŸ“ˆ å¼€å§‹æ¨¡å‹è¯„ä¼°...")

    try:
        performance = model_results.get("model_performance", {})

        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        evaluation_report = {
            "overall_score": (
                performance.get("accuracy", 0)
                + performance.get("precision", 0)
                + performance.get("recall", 0)
                + performance.get("f1_score", 0)
            )
            / 4,
            "detailed_metrics": performance,
            "quality_assessment": {
                "excellent": performance.get("accuracy", 0) > 0.85,
                "good": 0.75 < performance.get("accuracy", 0) <= 0.85,
                "acceptable": 0.65 < performance.get("accuracy", 0) <= 0.75,
                "needs_improvement": performance.get("accuracy", 0) <= 0.65,
            },
            "recommendations": [],
            "evaluation_timestamp": datetime.now().isoformat(),
        }

        # åŸºäºæ€§èƒ½ç»™å‡ºå»ºè®®
        if performance.get("accuracy", 0) < 0.7:
            evaluation_report["recommendations"].append("è€ƒè™‘å¢åŠ æ›´å¤šè®­ç»ƒæ•°æ®")
            evaluation_report["recommendations"].append("å°è¯•ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–")

        if performance.get("precision", 0) < 0.75:
            evaluation_report["recommendations"].append("è°ƒæ•´åˆ†ç±»é˜ˆå€¼")

        if performance.get("recall", 0) < 0.75:
            evaluation_report["recommendations"].append("è€ƒè™‘ä½¿ç”¨ä¸åŒçš„ç®—æ³•")

        logger.info(
            f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆï¼Œç»¼åˆè¯„åˆ†: {evaluation_report['overall_score']:.2%}"
        )

        return evaluation_report

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹è¯„ä¼°å¤±è´¥: {str(e)}")
        raise


@task(name="ä¿å­˜æ¨¡å‹", description="å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åˆ°å­˜å‚¨")
async def save_model(model_results: Dict[str, Any], evaluation: Dict[str, Any]) -> bool:
    """
    ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°ç»“æœ

    Args:
        model_results (Dict[str, Any]): æ¨¡å‹è®­ç»ƒç»“æœ
        evaluation (Dict[str, Any]): æ¨¡å‹è¯„ä¼°ç»“æœ

    Returns:
        bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
    """
    logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜æ¨¡å‹...")

    try:
        # æ¨¡æ‹Ÿæ¨¡å‹ä¿å­˜è¿‡ç¨‹
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå°†æ¨¡å‹ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿæˆ–äº‘å­˜å‚¨

        model_metadata = {
            "model_results": model_results,
            "evaluation": evaluation,
            "save_timestamp": datetime.now().isoformat(),
            "version": "1.0.0",
        }

        # æ¨¡æ‹Ÿä¿å­˜æ“ä½œ
        await asyncio.sleep(1)

        logger.info("âœ… æ¨¡å‹ä¿å­˜æˆåŠŸ")

        return True

    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}")
        return False


@flow(
    name="è¶³çƒé¢„æµ‹æ•°æ®ç®¡é“",
    description="å®Œæ•´çš„è¶³çƒé¢„æµ‹æ•°æ®å¤„ç†å’Œæ¨¡å‹è®­ç»ƒç®¡é“",
    log_prints=True,
)
async def main_data_flow() -> Dict[str, Any]:
    """
    ä¸»è¦çš„æ•°æ®ç®¡é“æµç¨‹

    è¿™ä¸ªæµç¨‹åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
    1. ä»å¤–éƒ¨æ•°æ®æºè·å–æ¯”èµ›æ•°æ®
    2. å¯¹æ•°æ®è¿›è¡Œç‰¹å¾å·¥ç¨‹å¤„ç†
    3. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹
    4. è¯„ä¼°æ¨¡å‹æ€§èƒ½
    5. ä¿å­˜æ¨¡å‹å’Œç»“æœ

    Returns:
        Dict[str, Any]: ç®¡é“æ‰§è¡Œç»“æœæ‘˜è¦
    """
    logger.info("ğŸš€ å¯åŠ¨è¶³çƒé¢„æµ‹æ•°æ®ç®¡é“...")

    pipeline_start_time = datetime.now()

    try:
        # æ­¥éª¤1: è·å–æ•°æ®
        raw_data = await fetch_data()

        # æ­¥éª¤2: ç‰¹å¾å·¥ç¨‹
        processed_data = await engineer_features(raw_data)

        # æ­¥éª¤3: æ¨¡å‹è®­ç»ƒ
        model_results = await train_model(processed_data)

        # æ­¥éª¤4: æ¨¡å‹è¯„ä¼°
        evaluation = await evaluate_model(model_results)

        # æ­¥éª¤5: ä¿å­˜æ¨¡å‹
        save_success = await save_model(model_results, evaluation)

        pipeline_end_time = datetime.now()
        pipeline_duration = (pipeline_end_time - pipeline_start_time).total_seconds()

        # ç”Ÿæˆç®¡é“æ‰§è¡ŒæŠ¥å‘Š
        pipeline_summary = {
            "pipeline_status": "success" if save_success else "partial_success",
            "execution_time_seconds": pipeline_duration,
            "data_processed": {
                "raw_matches": len(raw_data.get("matches", [])),
                "feature_samples": len(processed_data.get("features", [])),
                "model_accuracy": model_results.get("model_performance", {}).get(
                    "accuracy", 0
                ),
            },
            "model_performance": {
                "accuracy": model_results.get("model_performance", {}).get(
                    "accuracy", 0
                ),
                "evaluation_score": evaluation.get("overall_score", 0),
                "quality_assessment": evaluation.get("quality_assessment", {}),
            },
            "outputs": {
                "model_saved": save_success,
                "evaluation_completed": True,
                "recommendations": evaluation.get("recommendations", []),
            },
            "timestamps": {
                "start_time": pipeline_start_time.isoformat(),
                "end_time": pipeline_end_time.isoformat(),
                "duration": f"{pipeline_duration:.2f} seconds",
            },
        }

        logger.info(f"ğŸ‰ æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆ! è€—æ—¶: {pipeline_duration:.2f}ç§’")
        logger.info(
            f"ğŸ“Š æ¨¡å‹å‡†ç¡®ç‡: {pipeline_summary['model_performance']['accuracy']:.2%}"
        )

        return pipeline_summary

    except Exception as e:
        pipeline_end_time = datetime.now()
        pipeline_duration = (pipeline_end_time - pipeline_start_time).total_seconds()

        error_summary = {
            "pipeline_status": "failed",
            "error_message": str(e),
            "execution_time_seconds": pipeline_duration,
            "timestamps": {
                "start_time": pipeline_start_time.isoformat(),
                "failure_time": pipeline_end_time.isoformat(),
                "duration": f"{pipeline_duration:.2f} seconds",
            },
        }

        logger.error(f"âŒ æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {str(e)}")

        return error_summary


@flow(name="å¿«é€Ÿæ•°æ®éªŒè¯æµç¨‹", description="ç”¨äºæµ‹è¯•å’ŒéªŒè¯çš„è½»é‡çº§æ•°æ®æµç¨‹")
async def quick_validation_flow() -> Dict[str, Any]:
    """
    å¿«é€ŸéªŒè¯æµç¨‹ï¼Œç”¨äºæµ‹è¯•ç¯å¢ƒ

    Returns:
        Dict[str, Any]: éªŒè¯ç»“æœ
    """
    logger.info("âš¡ å¯åŠ¨å¿«é€Ÿæ•°æ®éªŒè¯æµç¨‹...")

    try:
        # åªè¿è¡Œå…³é”®æ­¥éª¤è¿›è¡ŒéªŒè¯
        raw_data = await fetch_data()
        processed_data = await engineer_features(raw_data)

        validation_result = {
            "status": "success",
            "data_validation": {
                "matches_count": len(raw_data.get("matches", [])),
                "features_count": len(processed_data.get("features", [])),
                "feature_dimensions": (
                    len(processed_data.get("features", [{}])[0])
                    if processed_data.get("features")
                    else 0
                ),
            },
            "pipeline_ready": True,
            "timestamp": datetime.now().isoformat(),
        }

        logger.info("âœ… å¿«é€ŸéªŒè¯æµç¨‹å®Œæˆ")
        return validation_result

    except Exception as e:
        logger.error(f"âŒ å¿«é€ŸéªŒè¯æµç¨‹å¤±è´¥: {str(e)}")
        return {
            "status": "failed",
            "error": str(e),
            "pipeline_ready": False,
            "timestamp": datetime.now().isoformat(),
        }


if __name__ == "__main__":
    """
    å‘½ä»¤è¡Œå…¥å£ç‚¹
    å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼è¿è¡Œï¼š
    python src/workflows/data_pipeline.py
    """
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "quick":
        # è¿è¡Œå¿«é€ŸéªŒè¯
        result = asyncio.run(quick_validation_flow())
    else:
        # è¿è¡Œå®Œæ•´æ•°æ®ç®¡é“
        result = asyncio.run(main_data_flow())

    print("\n" + "=" * 50)
    print("ğŸ“Š æ‰§è¡Œç»“æœæ‘˜è¦")
    print("=" * 50)
    print(json.dumps(result, indent=2, ensure_ascii=False))
