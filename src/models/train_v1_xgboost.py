#!/usr/bin/env python3
"""
é¦–å¸­AIç§‘å­¦å®¶ä¸“ç”¨ - V1 XGBoostæ¨¡å‹è®­ç»ƒå™¨
åŸºäºé«˜è´¨é‡æ•°æ®è®­ç»ƒæ–°ä¸€ä»£é¢„æµ‹æ¨¡å‹
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
import logging
from datetime import datetime
from pathlib import Path
import warnings

warnings.filterwarnings("ignore")

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class V1XGBoostTrainer:
    """V1 XGBoostè®­ç»ƒå™¨"""

    def __init__(self):
        self.data = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = []

    def load_dataset(self):
        """åŠ è½½æ•°æ®é›†"""
        try:
            dataset_path = Path("data/training_sets/v1_dataset.csv")
            if not dataset_path.exists():
                logger.error(f"âŒ æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_path}")
                return False

            self.data = pd.read_csv(dataset_path)
            logger.info(f"âœ… åŠ è½½æ•°æ®é›†: {self.data.shape}")

            # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
            logger.info(
                f"ğŸ“Š æ¯”èµ›æ—¶é—´èŒƒå›´: {self.data['match_date'].min()} åˆ° {self.data['match_date'].max()}"
            )
            logger.info(f"ğŸ¯ ç›®æ ‡åˆ†å¸ƒ: {self.data['result'].value_counts().to_dict()}")

            return True

        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
            return False

    def prepare_features(self):
        """å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
        logger.info("ğŸ”§ å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡...")

        # é€‰æ‹©ç‰¹å¾åˆ—
        feature_cols = [
            col for col in self.data.columns if "avg_" in col or "games_" in col
        ]
        self.feature_names = feature_cols

        logger.info(f"ğŸ”§ ç‰¹å¾åˆ—: {feature_cols}")
        logger.info(f"ğŸ“Š ç‰¹å¾æ•°é‡: {len(feature_cols)}")

        # å‡†å¤‡ç‰¹å¾çŸ©é˜µXå’Œç›®æ ‡å˜é‡y
        X = self.data[feature_cols].copy()
        y = self.data["result"].copy()

        # å¤„ç†NaNå€¼ï¼ˆç”¨0å¡«å……ï¼‰
        X = X.fillna(0)

        # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
        logger.info("ğŸ“ˆ ç‰¹å¾ç»Ÿè®¡:")
        for col in feature_cols:
            null_count = X[col].isnull().sum()
            mean_val = X[col].mean()
            std_val = X[col].std()
            logger.info(
                f"   {col}: å‡å€¼={mean_val:.3f}, æ ‡å‡†å·®={std_val:.3f}, NaN={null_count}"
            )

        return X, y

    def time_split_data(self, X, y):
        """æŒ‰æ—¶é—´åˆ‡åˆ†æ•°æ®ï¼ˆæ¨¡æ‹ŸçœŸå®é¢„æµ‹åœºæ™¯ï¼‰"""
        logger.info("ğŸ“… æŒ‰æ—¶é—´åˆ‡åˆ†æ•°æ®...")

        # æŒ‰æ—¶é—´æ’åº
        self.data = self.data.sort_values("match_date")

        # å‰80%åšè®­ç»ƒï¼Œå20%åšæµ‹è¯•
        split_idx = int(len(self.data) * 0.8)
        train_idx = self.data.iloc[:split_idx].index
        test_idx = self.data.iloc[split_idx:].index

        X_train = X.loc[train_idx]
        X_test = X.loc[test_idx]
        y_train = y.loc[train_idx]
        y_test = y.loc[test_idx]

        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.y_test = y_test

        logger.info(f"ğŸ“Š è®­ç»ƒé›†: {len(X_train)} åœºæ¯”èµ›")
        logger.info(f"ğŸ“Š æµ‹è¯•é›†: {len(X_test)} åœºæ¯”èµ›")
        logger.info(
            f"ğŸ“… è®­ç»ƒæ—¶é—´èŒƒå›´: {self.data.iloc[train_idx]['match_date'].min()} åˆ° {self.data.iloc[train_idx]['match_date'].max()}"
        )
        logger.info(
            f"ğŸ“… æµ‹è¯•æ—¶é—´èŒƒå›´: {self.data.iloc[test_idx]['match_date'].min()} åˆ° {self.data.iloc[test_idx]['match_date'].max()}"
        )

        return True

    def train_model(self):
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")

        # æ ‡å‡†åŒ–ç‰¹å¾
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

        # é…ç½®XGBoostå‚æ•°
        params = {
            "objective": "multi:softmax",  # å¤šåˆ†ç±»
            "num_class": 3,  # 3ä¸ªç±»åˆ«ï¼š0=å®¢èƒœ, 1=å¹³å±€, 2=ä¸»èƒœ
            "max_depth": 6,
            "learning_rate": 0.1,
            "n_estimators": 200,
            "subsample": 0.8,
            "colsample_bytree": 0.8,
            "random_state": 42,
            "eval_metric": "mlogloss",
            "early_stopping_rounds": 20,
            "verbosity": 1,
        }

        # è®­ç»ƒæ¨¡å‹
        self.model = xgb.XGBClassifier(**params)

        # ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœ
        self.model.fit(
            self.X_train_scaled,
            self.y_train,
            eval_set=[(self.X_test_scaled, self.y_test)],
            verbose=False,
        )

        logger.info("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        logger.info(f"ğŸŒ³ æœ€ä½³è¿­ä»£æ¬¡æ•°: {self.model.best_iteration}")

        return True

    def evaluate_model(self):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

        # é¢„æµ‹
        y_pred = self.model.predict(self.X_test_scaled)
        y_pred_proba = self.model.predict_proba(self.X_test_scaled)

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(self.y_test, y_pred)
        logger.info(f"ğŸ¯ å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        logger.info("ğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(
            self.y_test, y_pred, target_names=["å®¢èƒœ", "å¹³å±€", "ä¸»èƒœ"], digits=4
        )
        logger.info(report)

        # æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(self.y_test, y_pred)
        logger.info("ğŸ”€ æ··æ·†çŸ©é˜µ:")
        logger.info(f"   å®é™…\\é¢„æµ‹  å®¢èƒœ  å¹³å±€  ä¸»èƒœ")
        for i, row in enumerate(cm):
            logger.info(
                f"   {['å®¢èƒœ','å¹³å±€','ä¸»èƒœ'][i]:6} {row[0]:6} {row[1]:6} {row[2]:6}"
            )

        return accuracy, cm, y_pred_proba

    def analyze_feature_importance(self):
        """åˆ†æç‰¹å¾é‡è¦æ€§"""
        logger.info("ğŸ” åˆ†æç‰¹å¾é‡è¦æ€§...")

        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = self.model.feature_importances_
        feature_importance_df = pd.DataFrame(
            {"feature": self.feature_names, "importance": importance}
        ).sort_values("importance", ascending=False)

        logger.info("ğŸ“Š ç‰¹å¾é‡è¦æ€§æ’å:")
        for idx, row in feature_importance_df.iterrows():
            logger.info(f"   {row['feature']:25} {row['importance']:.4f}")

        # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
        try:
            plt.figure(figsize=(10, 6))
            sns.barplot(
                data=feature_importance_df.head(10), x="importance", y="feature"
            )
            plt.title("XGBoost ç‰¹å¾é‡è¦æ€§ (Top 10)")
            plt.xlabel("é‡è¦æ€§")
            plt.ylabel("ç‰¹å¾")
            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            plot_path = Path("results/feature_importance_v1.png")
            plot_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(plot_path, dpi=300, bbox_inches="tight")
            logger.info(f"ğŸ’¾ ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜: {plot_path}")

        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•ä¿å­˜ç‰¹å¾é‡è¦æ€§å›¾: {e}")

        return feature_importance_df

    def simulate_betting(self, y_pred_proba):
        """æ¨¡æ‹Ÿä¸‹æ³¨ROI"""
        logger.info("ğŸ’° æ¨¡æ‹Ÿä¸‹æ³¨ROIåˆ†æ...")

        # è·å–é¢„æµ‹æ¦‚ç‡
        predicted_probs = y_pred_proba
        predicted_classes = np.argmax(predicted_probs, axis=1)
        actual_classes = self.y_test.values

        # æ¨¡æ‹ŸæŠ•æ³¨ï¼šæ¯åœºæ¯”èµ›æŠ•1å…ƒåˆ°æ¦‚ç‡æœ€é«˜çš„ç»“æœ
        total_bet = len(predicted_classes)
        wins = 0
        total_payout = 0

        for i in range(len(predicted_classes)):
            if predicted_classes[i] == actual_classes[i]:
                # å‡è®¾å…¬å¹³èµ”ç‡ï¼š1/predicted_probability
                prob = predicted_probs[i][predicted_classes[i]]
                fair_odds = 1.0 / prob if prob > 0 else 1.0

                # èµ¢å¾—æŠ•æ³¨
                total_payout += fair_odds
                wins += 1

        # è®¡ç®—ROI
        total_investment = total_bet  # æ¯åœºæŠ•1å…ƒ
        net_profit = total_payout - total_investment
        roi = (net_profit / total_investment) * 100
        hit_rate = (wins / total_bet) * 100

        logger.info("ğŸ’° ä¸‹æ³¨æ¨¡æ‹Ÿç»“æœ:")
        logger.info(f"   æ€»æŠ•æ³¨: {total_bet} å…ƒ")
        logger.info(f"   èƒœåœºæ•°: {wins} åœº")
        logger.info(f"   å‘½ä¸­ç‡: {hit_rate:.2f}%")
        logger.info(f"   æ€»èµ”ä»˜: {total_payout:.2f} å…ƒ")
        logger.info(f"   å‡€æ”¶ç›Š: {net_profit:.2f} å…ƒ")
        logger.info(f"   ROI: {roi:.2f}%")

        # æŒ‰ç±»åˆ«åˆ†æ
        logger.info("ğŸ“Š æŒ‰ç»“æœç±»åˆ«åˆ†æ:")
        for result_class, class_name in enumerate(["å®¢èƒœ", "å¹³å±€", "ä¸»èƒœ"]):
            class_mask = actual_classes == result_class
            class_total = class_mask.sum()
            class_wins = (
                predicted_classes[class_mask] == actual_classes[class_mask]
            ).sum()
            class_hit_rate = (class_wins / class_total * 100) if class_total > 0 else 0

            logger.info(
                f"   {class_name}: {class_total} åœº, å‘½ä¸­ {class_wins} åœº, å‘½ä¸­ç‡ {class_hit_rate:.2f}%"
            )

        return roi, hit_rate

    def save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        try:
            model_dir = Path("models")
            model_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜æ¨¡å‹
            model_path = model_dir / "v1_xgboost_model.json"
            self.model.save_model(str(model_path))

            # ä¿å­˜æ ‡å‡†åŒ–å™¨
            import joblib

            scaler_path = model_dir / "v1_scaler.pkl"
            joblib.dump(self.scaler, scaler_path)

            # ä¿å­˜ç‰¹å¾åç§°
            features_path = model_dir / "v1_features.json"
            import json

            with open(features_path, "w") as f:
                json.dump(self.feature_names, f, indent=2)

            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_path}")
            logger.info(f"ğŸ’¾ æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {scaler_path}")
            logger.info(f"ğŸ’¾ ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜: {features_path}")

            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨é¦–å¸­AIç§‘å­¦å®¶ - V1 XGBoostè®­ç»ƒå™¨")
        start_time = datetime.now()

        # 1. åŠ è½½æ•°æ®é›†
        if not self.load_dataset():
            return False

        # 2. å‡†å¤‡ç‰¹å¾
        X, y = self.prepare_features()

        # 3. æ—¶é—´åˆ‡åˆ†
        if not self.time_split_data(X, y):
            return False

        # 4. è®­ç»ƒæ¨¡å‹
        if not self.train_model():
            return False

        # 5. è¯„ä¼°æ¨¡å‹
        accuracy, cm, y_pred_proba = self.evaluate_model()

        # 6. åˆ†æç‰¹å¾é‡è¦æ€§
        feature_importance = self.analyze_feature_importance()

        # 7. æ¨¡æ‹Ÿä¸‹æ³¨
        roi, hit_rate = self.simulate_betting(y_pred_proba)

        # 8. ä¿å­˜æ¨¡å‹
        self.save_model()

        # è®¡ç®—æ€»è€—æ—¶
        duration = (datetime.now() - start_time).total_seconds()
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f}ç§’")

        # æœ€ç»ˆæ€»ç»“
        logger.info("=" * 60)
        logger.info("ğŸ‰ V1 XGBoostæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"ğŸ¯ å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)")
        logger.info(f"ğŸ’° ROI: {roi:.2f}%")
        logger.info(f"ğŸ”« å‘½ä¸­ç‡: {hit_rate:.2f}%")
        logger.info(f"ğŸ“Š æµ‹è¯•æ ·æœ¬: {len(self.y_test)} åœºæ¯”èµ›")

        # ä¸baselineæ¯”è¾ƒ
        baseline_accuracy = 0.46  # ä¹‹å‰æåˆ°çš„46%
        improvement = ((accuracy - baseline_accuracy) / baseline_accuracy) * 100
        logger.info(f"ğŸ“ˆ ç›¸æ¯”Baselineæå‡: {improvement:+.2f}%")

        if accuracy > baseline_accuracy:
            logger.info("ğŸŠ æ–°æ¨¡å‹è¶…è¶Šäº†Baselineï¼")
        else:
            logger.warning("âš ï¸ æ–°æ¨¡å‹æœªè¾¾åˆ°Baselineæ°´å¹³")

        return True


def main():
    """ä¸»å‡½æ•°"""
    try:
        trainer = V1XGBoostTrainer()
        success = trainer.run()

        if success:
            logger.info("âœ… V1 XGBoostæ¨¡å‹è®­ç»ƒæˆåŠŸ")
            return 0
        else:
            logger.error("âŒ V1 XGBoostæ¨¡å‹è®­ç»ƒå¤±è´¥")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
