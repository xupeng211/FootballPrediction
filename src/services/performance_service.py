#!/usr/bin/env python3
"""
æ€§èƒ½æœåŠ¡
æ•´åˆæ‰€æœ‰æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½ï¼Œæä¾›ç»Ÿä¸€çš„æ€§èƒ½ç®¡ç†æ¥å£
"""

import asyncio
import time
from dataclasses import dataclass
from typing import Any

from src.core.logger import get_logger
from src.database.connection import DatabaseManager
from src.performance.async_optimizer import (
    get_batch_processor,
    get_connection_pool,
    get_file_optimizer,
    get_query_optimizer,
)
from src.performance.db_query_optimizer import get_database_optimizer

logger = get_logger(__name__)


@dataclass
class PerformanceReport:
    """æ€§èƒ½æŠ¥å‘Šæ•°æ®ç±»"""

    timestamp: float
    database_metrics: dict[str, Any]
    async_metrics: dict[str, Any]
    query_metrics: dict[str, Any]
    connection_pool_metrics: dict[str, Any]
    recommendations: list[str]
    overall_score: float


class PerformanceService:
    """æ€§èƒ½æœåŠ¡ - ç»Ÿä¸€æ€§èƒ½ç®¡ç†"""

    def __init__(self, db_manager: DatabaseManager | None = None):
        self.db_manager = db_manager or DatabaseManager()
        self.batch_processor = get_batch_processor()
        self.connection_pool = get_connection_pool()
        self.file_optimizer = get_file_optimizer()
        self.query_optimizer = get_query_optimizer()
        self.db_optimizer = get_database_optimizer()

        # æ€§èƒ½åŸºçº¿
        self.baseline_metrics = {
            "query_response_time": 0.5,  # 500ms
            "batch_throughput": 1000,  # 1000 ops/sec
            "connection_utilization": 0.7,  # 70%
            "error_rate": 0.05,  # 5%
        }

    async def initialize(self):
        """åˆå§‹åŒ–æ€§èƒ½æœåŠ¡"""
        try:
            logger.info("åˆå§‹åŒ–æ€§èƒ½æœåŠ¡...")
            # ç¡®ä¿æ•°æ®åº“ç®¡ç†å™¨å·²åˆå§‹åŒ–
            if (
                not hasattr(self.db_manager, "initialized")
                or not self.db_manager.initialized
            ):
                self.db_manager.initialize()

            logger.info("âœ… æ€§èƒ½æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"æ€§èƒ½æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    # ========================================
    # æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–
    # ========================================

    async def optimize_database_queries(
        self, queries: list[dict[str, Any]]
    ) -> list[Any]:
        """
        ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ‰§è¡Œ

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨ [{"query": "...", "params": {...}}, ...]

        Returns:
            ä¼˜åŒ–æ‰§è¡Œç»“æœ
        """
        start_time = time.time()

        try:
            # ä½¿ç”¨æŸ¥è¯¢ä¼˜åŒ–å™¨æ‰§è¡Œ
            results = await self.query_optimizer.execute_batch_with_optimization(
                queries, max_concurrent=10
            )

            execution_time = time.time() - start_time
            logger.info(
                f"æ‰¹é‡æŸ¥è¯¢ä¼˜åŒ–å®Œæˆ: {len(queries)}ä¸ªæŸ¥è¯¢, è€—æ—¶ {execution_time:.3f}ç§’"
            )

            return results

        except Exception as e:
            logger.error(f"æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {e}")
            raise

    async def analyze_database_performance(
        self, table_names: list[str] | None = None
    ) -> dict[str, Any]:
        """åˆ†ææ•°æ®åº“æ€§èƒ½"""
        try:
            if table_names is None:
                # è·å–æ‰€æœ‰ç”¨æˆ·è¡¨
                async with self.db_manager.get_async_session() as session:
                    tables_query = """
                    SELECT tablename FROM pg_tables WHERE schemaname = 'public'
                    """
                    from sqlalchemy import text

                    stmt = text(tables_query)
                    result = await session.execute(stmt)
                    table_names = [row[0] for row in result.fetchall()]

            # åˆ†æè¡¨æ€§èƒ½
            performance_data = await self.db_optimizer.analyze_table_performance(
                table_names
            )

            # è·å–æŸ¥è¯¢æ€§èƒ½æŠ¥å‘Š
            query_report = self.query_optimizer.get_performance_report()

            return {
                "tables": performance_data,
                "queries": query_report,
                "timestamp": time.time(),
            }

        except Exception as e:
            logger.error(f"æ•°æ®åº“æ€§èƒ½åˆ†æå¤±è´¥: {e}")
            raise

    # ========================================
    # å¼‚æ­¥I/Oæ€§èƒ½ä¼˜åŒ–
    # ========================================

    async def optimize_batch_processing(
        self,
        items: list[Any],
        processor_func: callable,
        batch_size: int = 100,
        max_concurrent: int = 5,
    ) -> list[Any]:
        """
        ä¼˜åŒ–æ‰¹é‡å¤„ç†æ€§èƒ½

        Args:
            items: è¦å¤„ç†çš„æ•°æ®
            processor_func: å¤„ç†å‡½æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            # é…ç½®æ‰¹é‡å¤„ç†å™¨
            self.batch_processor.batch_size = batch_size
            self.batch_processor.max_concurrent_batches = max_concurrent

            # æ‰§è¡Œæ‰¹é‡å¤„ç†
            results = await self.batch_processor.process_batch(items, processor_func)

            logger.info(f"æ‰¹é‡å¤„ç†ä¼˜åŒ–å®Œæˆ: {len(items)}é¡¹æ•°æ®")

            return results

        except Exception as e:
            logger.error(f"æ‰¹é‡å¤„ç†ä¼˜åŒ–å¤±è´¥: {e}")
            raise

    async def optimize_file_operations(
        self, file_path: str, operation: str = "read"
    ) -> Any:
        """ä¼˜åŒ–æ–‡ä»¶æ“ä½œæ€§èƒ½"""
        try:
            if operation == "read":
                # è¯»å–æ–‡ä»¶ç»Ÿè®¡
                import aiofiles.os

                file_size = (await aiofiles.os.stat(file_path)).st_size
                logger.info(f"æ–‡ä»¶ {file_path} å¤§å°: {file_size} bytes")

                return {"file_size": file_size, "optimized": True}
            else:
                logger.info(f"æ–‡ä»¶æ“ä½œ {operation} ä¼˜åŒ–å®Œæˆ")
                return {"optimized": True}

        except Exception as e:
            logger.error(f"æ–‡ä»¶æ“ä½œä¼˜åŒ–å¤±è´¥: {e}")
            raise

    # ========================================
    # è¿æ¥æ± ä¼˜åŒ–
    # ========================================

    def get_connection_pool_status(self) -> dict[str, Any]:
        """è·å–è¿æ¥æ± çŠ¶æ€"""
        return {
            "pool_stats": self.connection_pool.get_pool_stats(),
            "baseline": self.baseline_metrics["connection_utilization"],
            "status": "healthy" if self._is_pool_healthy() else "warning",
        }

    def _is_pool_healthy(self) -> bool:
        """æ£€æŸ¥è¿æ¥æ± å¥åº·çŠ¶æ€"""
        stats = self.connection_pool.get_pool_stats()
        utilization = stats.get("utilization", 0)
        return utilization <= self.baseline_metrics["connection_utilization"]

    # ========================================
    # æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Š
    # ========================================

    async def generate_performance_report(self) -> PerformanceReport:
        """ç”Ÿæˆç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        try:
            timestamp = time.time()

            # æ”¶é›†å„é¡¹æ€§èƒ½æŒ‡æ ‡
            database_metrics = await self.analyze_database_performance()
            async_metrics = {
                "batch_processor": self.batch_processor.metrics.__dict__,
                "file_optimizer": self.file_optimizer.get_performance_stats(),
            }
            query_metrics = self.query_optimizer.get_performance_report()
            connection_pool_metrics = self.get_connection_pool_status()

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = await self._generate_recommendations(
                database_metrics, async_metrics, query_metrics, connection_pool_metrics
            )

            # è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•°
            overall_score = self._calculate_performance_score(
                database_metrics, async_metrics, query_metrics, connection_pool_metrics
            )

            return PerformanceReport(
                timestamp=timestamp,
                database_metrics=database_metrics,
                async_metrics=async_metrics,
                query_metrics=query_metrics,
                connection_pool_metrics=connection_pool_metrics,
                recommendations=recommendations,
                overall_score=overall_score,
            )

        except Exception as e:
            logger.error(f"æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise

    async def _generate_recommendations(
        self,
        db_metrics: dict,
        async_metrics: dict,
        query_metrics: dict,
        pool_metrics: dict,
    ) -> list[str]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # æ•°æ®åº“å»ºè®®
        if db_metrics.get("queries", {}).get("summary", {}).get("error_rate", 0) > 5:
            recommendations.append("æ•°æ®åº“æŸ¥è¯¢é”™è¯¯ç‡è¿‡é«˜ï¼Œå»ºè®®æ£€æŸ¥SQLè¯­æ³•å’Œç´¢å¼•")

        slow_queries = db_metrics.get("queries", {}).get("slow_queries_count", 0)
        if slow_queries > 0:
            recommendations.append(
                f"å‘ç° {slow_queries} ä¸ªæ…¢æŸ¥è¯¢ï¼Œå»ºè®®æ·»åŠ ç´¢å¼•æˆ–ä¼˜åŒ–SQL"
            )

        # å¼‚æ­¥å¤„ç†å»ºè®®
        batch_avg_time = async_metrics.get("batch_processor", {}).get("avg_time", 0)
        if batch_avg_time > 1.0:
            recommendations.append(
                "æ‰¹é‡å¤„ç†å¹³å‡æ—¶é—´è¿‡é•¿ï¼Œå»ºè®®å‡å°‘æ‰¹æ¬¡å¤§å°æˆ–ä¼˜åŒ–å¤„ç†é€»è¾‘"
            )

        # è¿æ¥æ± å»ºè®®
        if pool_metrics.get("status") != "healthy":
            recommendations.append("è¿æ¥æ± åˆ©ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ è¿æ¥æ± å¤§å°æˆ–ä¼˜åŒ–æŸ¥è¯¢")

        # é€šç”¨å»ºè®®
        if not recommendations:
            recommendations.append("å½“å‰æ€§èƒ½æŒ‡æ ‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒä¼˜åŒ–é…ç½®")

        return recommendations

    def _calculate_performance_score(
        self,
        db_metrics: dict,
        async_metrics: dict,
        query_metrics: dict,
        pool_metrics: dict,
    ) -> float:
        """è®¡ç®—ç»¼åˆæ€§èƒ½åˆ†æ•° (0-100)"""
        scores = []

        # æŸ¥è¯¢æ€§èƒ½åˆ†æ•° (40%)
        query_summary = query_metrics.get("summary", {})
        if query_summary:
            avg_time = query_summary.get("avg_query_time", 0)
            error_rate = query_summary.get("error_rate", 0) / 100

            # å“åº”æ—¶é—´åˆ†æ•° (0.5ç§’ä¸ºæ»¡åˆ†)
            time_score = max(0, 100 - (avg_time - 0.5) * 100)
            # é”™è¯¯ç‡åˆ†æ•° (5%ä¸ºæ»¡åˆ†)
            error_score = max(0, 100 - error_rate * 2000)

            scores.append((time_score + error_score) / 2 * 0.4)

        # å¼‚æ­¥å¤„ç†åˆ†æ•° (30%)
        batch_metrics = async_metrics.get("batch_processor", {})
        if batch_metrics:
            error_count = batch_metrics.get("errors_count", 0)
            operation_count = batch_metrics.get("operation_count", 1)
            batch_error_rate = error_count / operation_count
            batch_score = max(0, 100 - batch_error_rate * 1000)
            scores.append(batch_score * 0.3)

        # è¿æ¥æ± åˆ†æ•° (30%)
        pool_utilization = pool_metrics.get("pool_stats", {}).get("utilization", 0.7)
        pool_score = max(0, 100 - abs(pool_utilization - 0.7) * 200)
        scores.append(pool_score * 0.3)

        return round(sum(scores), 1) if scores else 70.0

    # ========================================
    # æ€§èƒ½è°ƒä¼˜æ“ä½œ
    # ========================================

    async def auto_tune_performance(self) -> dict[str, Any]:
        """è‡ªåŠ¨æ€§èƒ½è°ƒä¼˜"""
        try:
            logger.info("å¼€å§‹è‡ªåŠ¨æ€§èƒ½è°ƒä¼˜...")

            # ç”Ÿæˆå½“å‰æ€§èƒ½æŠ¥å‘Š
            report = await self.generate_performance_report()

            tuning_actions = []

            # æ ¹æ®å»ºè®®æ‰§è¡Œè°ƒä¼˜æ“ä½œ
            for recommendation in report.recommendations:
                if "æ…¢æŸ¥è¯¢" in recommendation:
                    # æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜ï¼Œé‡æ–°ä¼˜åŒ–
                    self.query_optimizer.clear_cache()
                    tuning_actions.append("æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜")

                elif "è¿æ¥æ± åˆ©ç”¨ç‡è¿‡é«˜" in recommendation:
                    # å¢åŠ è¿æ¥æ± å¤§å°ï¼ˆå¦‚æœå¯èƒ½ï¼‰
                    if hasattr(self.connection_pool, "max_size"):
                        self.connection_pool.max_size = min(
                            self.connection_pool.max_size + 5, 50
                        )
                        tuning_actions.append("å¢åŠ è¿æ¥æ± å¤§å°")

                elif "æ‰¹é‡å¤„ç†" in recommendation:
                    # è°ƒæ•´æ‰¹é‡å¤„ç†å‚æ•°
                    if hasattr(self.batch_processor, "batch_size"):
                        self.batch_processor.batch_size = max(
                            self.batch_processor.batch_size - 20, 50
                        )
                        tuning_actions.append("å‡å°‘æ‰¹é‡å¤„ç†å¤§å°")

            logger.info(f"è‡ªåŠ¨è°ƒä¼˜å®Œæˆï¼Œæ‰§è¡Œäº† {len(tuning_actions)} ä¸ªè°ƒä¼˜æ“ä½œ")

            return {
                "actions_performed": tuning_actions,
                "performance_score_before": report.overall_score,
                "recommendations_applied": report.recommendations,
            }

        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ€§èƒ½è°ƒä¼˜å¤±è´¥: {e}")
            raise

    async def benchmark_performance(self, duration_seconds: int = 60) -> dict[str, Any]:
        """æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        try:
            logger.info(f"å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ŒæŒç»­æ—¶é—´: {duration_seconds}ç§’")

            start_time = time.time()
            benchmark_data = {
                "queries_executed": 0,
                "total_response_time": 0.0,
                "errors_count": 0,
                "peak_memory_usage": 0.0,
            }

            # æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
            test_queries = [
                {"query": "SELECT 1 as test", "params": None},
                {"query": "SELECT 2 as test", "params": None},
                {"query": "SELECT COUNT(*) FROM (SELECT 1 as dummy) t", "params": None},
            ]

            while time.time() - start_time < duration_seconds:
                query_start = time.time()

                try:
                    # æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢
                    await self.query_optimizer.execute_optimized_query(
                        test_queries[0]["query"], test_queries[0]["params"]
                    )
                    benchmark_data["queries_executed"] += 1

                except Exception as e:
                    benchmark_data["errors_count"] += 1
                    logger.warning(f"åŸºå‡†æµ‹è¯•æŸ¥è¯¢å¤±è´¥: {e}")

                query_time = time.time() - query_start
                benchmark_data["total_response_time"] += query_time

                # é¿å…è¿‡äºé¢‘ç¹çš„æŸ¥è¯¢
                await asyncio.sleep(0.01)

            # è®¡ç®—åŸºå‡†æŒ‡æ ‡
            actual_duration = time.time() - start_time
            benchmark_results = {
                "duration": actual_duration,
                "queries_per_second": benchmark_data["queries_executed"]
                / actual_duration,
                "avg_response_time": (
                    benchmark_data["total_response_time"]
                    / max(1, benchmark_data["queries_executed"])
                ),
                "error_rate": (
                    benchmark_data["errors_count"]
                    / max(1, benchmark_data["queries_executed"])
                ),
                "total_queries": benchmark_data["queries_executed"],
                "total_errors": benchmark_data["errors_count"],
            }

            logger.info(
                f"æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆ: {benchmark_results['queries_per_second']:.1f} QPS, "
                f"å¹³å‡å“åº”æ—¶é—´ {benchmark_results['avg_response_time']:.3f}ç§’"
            )

            return benchmark_results

        except Exception as e:
            logger.error(f"æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            raise

    # ========================================
    # æ¸…ç†å’Œç»´æŠ¤
    # ========================================

    async def cleanup(self):
        """æ¸…ç†æ€§èƒ½æœåŠ¡èµ„æº"""
        try:
            # æ¸…ç©ºç¼“å­˜
            self.query_optimizer.clear_cache()

            # é‡ç½®æŒ‡æ ‡
            self.batch_processor.metrics = type(self.batch_processor.metrics)()

            logger.info("æ€§èƒ½æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.error(f"æ€§èƒ½æœåŠ¡æ¸…ç†å¤±è´¥: {e}")


# å…¨å±€æ€§èƒ½æœåŠ¡å®ä¾‹
_global_performance_service: PerformanceService | None = None


def get_performance_service() -> PerformanceService:
    """è·å–å…¨å±€æ€§èƒ½æœåŠ¡å®ä¾‹"""
    global _global_performance_service
    if _global_performance_service is None:
        _global_performance_service = PerformanceService()
    return _global_performance_service


async def initialize_performance_service():
    """åˆå§‹åŒ–å…¨å±€æ€§èƒ½æœåŠ¡"""
    service = get_performance_service()
    await service.initialize()
    return service


if __name__ == "__main__":

    async def demo_performance_service():
        """æ¼”ç¤ºæ€§èƒ½æœåŠ¡åŠŸèƒ½"""
        print("ğŸš€ æ¼”ç¤ºæ€§èƒ½æœåŠ¡åŠŸèƒ½")

        # åˆå§‹åŒ–æ€§èƒ½æœåŠ¡
        service = await initialize_performance_service()

        # æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        benchmark_results = await service.benchmark_performance(duration_seconds=5)
        print(f"ğŸ“Š åŸºå‡†æµ‹è¯•ç»“æœ: {benchmark_results}")

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = await service.generate_performance_report()
        print(f"ğŸ“ˆ æ€§èƒ½æŠ¥å‘Š: æ€»ä½“è¯„åˆ† {report.overall_score}")

        # è‡ªåŠ¨è°ƒä¼˜
        tuning_results = await service.auto_tune_performance()
        print(f"ğŸ”§ è‡ªåŠ¨è°ƒä¼˜: {tuning_results}")

        # æ¸…ç†
        await service.cleanup()
        print("âœ… æ€§èƒ½æœåŠ¡æ¼”ç¤ºå®Œæˆ")

    asyncio.run(demo_performance_service())
