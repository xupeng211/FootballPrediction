#!/usr/bin/env python3
"""
AI ä»£ç åˆ†æå™¨

é›†æˆ Claude Code CLIï¼Œå®ç°æ™ºèƒ½çš„æµ‹è¯•å¤±è´¥åˆ†æå’Œä¿®å¤å»ºè®®ç”Ÿæˆã€‚
"""

import json
import re
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

from .claude_cli_wrapper import ClaudeCodeWrapper, TestFailure, FixSuggestion, TestFailureParser

logger = logging.getLogger(__name__)


@dataclass
class AnalysisResult:
    """åˆ†æç»“æœ"""
    test_failures: List[TestFailure]
    fix_suggestions: List[FixSuggestion]
    analysis_summary: Dict
    timestamp: datetime
    confidence_score: float


class CodeAnalyzer:
    """AI ä»£ç åˆ†æå™¨ä¸»ç±»"""

    def __init__(self):
        self.claude_wrapper = ClaudeCodeWrapper()
        self.parser = TestFailureParser()
        logger.info("CodeAnalyzer initialized")

    def analyze_test_results(self, pytest_output: str, coverage_data: Optional[Dict] = None) -> AnalysisResult:
        """
        åˆ†ææµ‹è¯•ç»“æœï¼Œç”Ÿæˆä¿®å¤å»ºè®®

        Args:
            pytest_output: pytest è¾“å‡ºæ—¥å¿—
            coverage_data: è¦†ç›–ç‡æ•°æ®ï¼ˆå¯é€‰ï¼‰

        Returns:
            åˆ†æç»“æœ
        """
        logger.info("Starting test results analysis...")

        # 1. è§£ææµ‹è¯•å¤±è´¥
        test_failures = self.parser.parse_pytest_output(pytest_output)
        logger.info(f"Found {len(test_failures)} test failures")

        fix_suggestions = []

        # 2. å¯¹æ¯ä¸ªå¤±è´¥è¿›è¡Œåˆ†æ
        for failure in test_failures:
            try:
                logger.info(f"Analyzing failure: {failure.test_name}")

                # åˆ†æå¤±è´¥åŸå› 
                analysis = self.claude_wrapper.analyze_test_failure(failure)

                # ç”Ÿæˆä¿®å¤å»ºè®®
                fix_suggestion = self.claude_wrapper.generate_fix(failure, analysis)

                # éªŒè¯ä¿®å¤å»ºè®®
                if self.claude_wrapper.validate_fix(fix_suggestion):
                    fix_suggestions.append(fix_suggestion)
                    logger.info(f"Generated valid fix for {failure.test_name}")
                else:
                    logger.warning(f"Invalid fix suggestion for {failure.test_name}")

            except Exception as e:
                logger.error(f"Failed to analyze failure {failure.test_name}: {e}")
                continue

        # 3. ç”Ÿæˆåˆ†ææ‘˜è¦
        analysis_summary = self._generate_analysis_summary(
            test_failures, fix_suggestions, coverage_data
        )

        # 4. è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦
        confidence_score = self._calculate_confidence_score(test_failures, fix_suggestions)

        return AnalysisResult(
            test_failures=test_failures,
            fix_suggestions=fix_suggestions,
            analysis_summary=analysis_summary,
            timestamp=datetime.now(),
            confidence_score=confidence_score
        )

    def prioritize_fixes(self, analysis_result: AnalysisResult) -> List[FixSuggestion]:
        """
        å¯¹ä¿®å¤å»ºè®®è¿›è¡Œä¼˜å…ˆçº§æ’åº

        Args:
            analysis_result: åˆ†æç»“æœ

        Returns:
            æ’åºåçš„ä¿®å¤å»ºè®®åˆ—è¡¨
        """
        suggestions = analysis_result.fix_suggestions

        # æŒ‰å¤šä¸ªç»´åº¦æ’åºï¼š
        # 1. ç½®ä¿¡åº¦ï¼ˆé«˜åˆ°ä½ï¼‰
        # 2. é”™è¯¯ç±»å‹ï¼ˆè¯­æ³•é”™è¯¯ä¼˜å…ˆï¼‰
        # 3. æ–‡ä»¶é‡è¦æ€§ï¼ˆæ ¸å¿ƒæ–‡ä»¶ä¼˜å…ˆï¼‰

        def get_priority_score(suggestion: FixSuggestion) -> Tuple[float, int, int]:
            # åŸºç¡€åˆ†æ•°ï¼šç½®ä¿¡åº¦
            base_score = suggestion.confidence

            # é”™è¯¯ç±»å‹æƒé‡
            error_type_weight = self._get_error_type_weight(suggestion)

            # æ–‡ä»¶é‡è¦æ€§æƒé‡
            file_importance_weight = self._get_file_importance_weight(suggestion.file_path)

            return (base_score, error_type_weight, file_importance_weight)

        return sorted(suggestions, key=get_priority_score, reverse=True)

    def generate_bugfix_todo(self, analysis_result: AnalysisResult, output_path: Path) -> None:
        """
        ç”Ÿæˆ BUGFIX_TODO.md æ–‡ä»¶

        Args:
            analysis_result: åˆ†æç»“æœ
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        logger.info(f"Generating BUGFIX_TODO at {output_path}")

        # è·å–ä¼˜å…ˆçº§æ’åºçš„ä¿®å¤å»ºè®®
        prioritized_fixes = self.prioritize_fixes(analysis_result)

        # ç”Ÿæˆ TODO å†…å®¹
        todo_content = self._generate_todo_content(analysis_result, prioritized_fixes)

        # å†™å…¥æ–‡ä»¶
        output_path.write_text(todo_content, encoding='utf-8')
        logger.info("BUGFIX_TODO generated successfully")

    def apply_fix(self, fix_suggestion: FixSuggestion, dry_run: bool = True) -> bool:
        """
        åº”ç”¨ä¿®å¤å»ºè®®

        Args:
            fix_suggestion: ä¿®å¤å»ºè®®
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼

        Returns:
            ä¿®å¤æ˜¯å¦æˆåŠŸ
        """
        file_path = Path(fix_suggestion.file_path)

        if not file_path.exists():
            logger.error(f"File not found: {file_path}")
            return False

        try:
            # è¯»å–åŸæ–‡ä»¶
            original_content = file_path.read_text(encoding='utf-8')

            # å¦‚æœæ˜¯é¢„è§ˆæ¨¡å¼ï¼Œåªæ˜¾ç¤ºå·®å¼‚
            if dry_run:
                print(f"ğŸ“‹ Preview fix for {file_path}:")
                print("Original code:")
                print("-" * 40)
                print(original_content)
                print("\nFixed code:")
                print("-" * 40)
                print(fix_suggestion.fixed_code)
                print("\nExplanation:", fix_suggestion.explanation)
                return True

            # åº”ç”¨ä¿®å¤
            file_path.write_text(fix_suggestion.fixed_code, encoding='utf-8')
            logger.info(f"Applied fix to {file_path}")
            return True

        except Exception as e:
            logger.error(f"Failed to apply fix to {file_path}: {e}")
            return False

    def _generate_analysis_summary(self, test_failures: List[TestFailure],
                                 fix_suggestions: List[FixSuggestion],
                                 coverage_data: Optional[Dict]) -> Dict:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "total_failures": len(test_failures),
            "generated_fixes": len(fix_suggestions),
            "success_rate": len(fix_suggestions) / len(test_failures) if test_failures else 0,
            "error_types": {},
            "confidence_distribution": {
                "high": len([f for f in fix_suggestions if f.confidence >= 0.8]),
                "medium": len([f for f in fix_suggestions if 0.5 <= f.confidence < 0.8]),
                "low": len([f for f in fix_suggestions if f.confidence < 0.5])
            }
        }

        # ç»Ÿè®¡é”™è¯¯ç±»å‹
        for failure in test_failures:
            error_type = failure.error_type
            summary["error_types"][error_type] = summary["error_types"].get(error_type, 0) + 1

        # æ·»åŠ è¦†ç›–ç‡ä¿¡æ¯
        if coverage_data:
            summary["coverage"] = {
                "total_coverage": coverage_data.get("totals", {}).get("percent_covered", 0),
                "low_coverage_files": len([
                    f for f, data in coverage_data.get("files", {}).items()
                    if data.get("summary", {}).get("percent_covered", 100) < 50
                ])
            }

        return summary

    def _calculate_confidence_score(self, test_failures: List[TestFailure],
                                  fix_suggestions: List[FixSuggestion]) -> float:
        """è®¡ç®—æ•´ä½“ç½®ä¿¡åº¦åˆ†æ•°"""
        if not test_failures:
            return 1.0

        if not fix_suggestions:
            return 0.0

        # åŸºäºä¿®å¤å»ºè®®çš„å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(f.confidence for f in fix_suggestions) / len(fix_suggestions)

        # åŸºäºä¿®å¤è¦†ç›–ç‡
        coverage_rate = len(fix_suggestions) / len(test_failures)

        # ç»¼åˆè¯„åˆ†
        return (avg_confidence * 0.7) + (coverage_rate * 0.3)

    def _get_error_type_weight(self, suggestion: FixSuggestion) -> int:
        """è·å–é”™è¯¯ç±»å‹æƒé‡"""
        # è¿™é‡Œå¯ä»¥æ ¹æ®å®é™…é”™è¯¯ç±»å‹è®¾ç½®æƒé‡
        # è¯­æ³•é”™è¯¯ > å¯¼å…¥é”™è¯¯ > é€»è¾‘é”™è¯¯
        error_priorities = {
            "SyntaxError": 3,
            "ImportError": 2,
            "ModuleNotFoundError": 2,
            "AssertionError": 1,
            "TypeError": 1,
            "ValueError": 1
        }
        return error_priorities.get("UnknownError", 0)

    def _get_file_importance_weight(self, file_path: str) -> int:
        """è·å–æ–‡ä»¶é‡è¦æ€§æƒé‡"""
        path = Path(file_path)

        # æ ¸å¿ƒæ–‡ä»¶ä¼˜å…ˆçº§æ›´é«˜
        if "src/api/" in str(path):
            return 3
        elif "src/core/" in str(path):
            return 2
        elif "src/services/" in str(path):
            return 2
        elif "tests/" in str(path):
            return 1
        else:
            return 0

    def _generate_todo_content(self, analysis_result: AnalysisResult,
                             prioritized_fixes: List[FixSuggestion]) -> str:
        """ç”Ÿæˆ TODO å†…å®¹"""
        lines = []
        lines.append("# ğŸ¤– AI Bugfix TODO Board")
        lines.append("")
        lines.append(f"è‡ªåŠ¨æ›´æ–°äº: {analysis_result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}")
        lines.append("")
        lines.append("## ğŸ“Š åˆ†ææ‘˜è¦")
        lines.append(f"- æ€»æµ‹è¯•å¤±è´¥æ•°: {analysis_result.analysis_summary['total_failures']}")
        lines.append(f"- ç”Ÿæˆä¿®å¤å»ºè®®: {analysis_result.analysis_summary['generated_fixes']}")
        lines.append(f"- æˆåŠŸç‡: {analysis_result.analysis_summary['success_rate']:.1%}")
        lines.append(f"- æ•´ä½“ç½®ä¿¡åº¦: {analysis_result.confidence_score:.1%}")
        lines.append("")

        # é”™è¯¯ç±»å‹åˆ†å¸ƒ
        if analysis_result.analysis_summary["error_types"]:
            lines.append("## ğŸ“ˆ é”™è¯¯ç±»å‹åˆ†å¸ƒ")
            for error_type, count in analysis_result.analysis_summary["error_types"].items():
                lines.append(f"- {error_type}: {count} æ¬¡")
            lines.append("")

        # ç½®ä¿¡åº¦åˆ†å¸ƒ
        lines.append("## ğŸ¯ ä¿®å¤å»ºè®®ç½®ä¿¡åº¦åˆ†å¸ƒ")
        conf_dist = analysis_result.analysis_summary["confidence_distribution"]
        lines.append(f"- é«˜ç½®ä¿¡åº¦ (â‰¥80%): {conf_dist['high']} ä¸ª")
        lines.append(f"- ä¸­ç­‰ç½®ä¿¡åº¦ (50-80%): {conf_dist['medium']} ä¸ª")
        lines.append(f"- ä½ç½®ä¿¡åº¦ (<50%): {conf_dist['low']} ä¸ª")
        lines.append("")

        # å¾…ä¿®å¤ä»»åŠ¡
        lines.append("## ğŸš§ AI å»ºè®®çš„ä¿®å¤ä»»åŠ¡")
        for i, fix in enumerate(prioritized_fixes, 1):
            lines.append(f"{i}. **{fix.file_path}** (ç½®ä¿¡åº¦: {fix.confidence:.1%})")
            lines.append(f"   - é—®é¢˜: {fix.explanation}")
            lines.append(f"   - ä¿®æ”¹èŒƒå›´: ç¬¬ {fix.line_range[0]}-{fix.line_range[1]} è¡Œ")
            lines.append("")

        # å·²å®Œæˆä»»åŠ¡
        lines.append("## âœ… å·²å®Œæˆçš„ä¿®å¤")
        lines.append("ï¼ˆç­‰å¾…ä¿®å¤å®Œæˆåçš„æ›´æ–°ï¼‰")
        lines.append("")

        # å»ºè®®è¡ŒåŠ¨
        lines.append("## ğŸ”§ AI å»ºè®®çš„è¡ŒåŠ¨")
        lines.append("1. æŒ‰ä¼˜å…ˆçº§åº”ç”¨ä¿®å¤å»ºè®®")
        lines.append("2. æ¯æ¬¡ä¿®å¤åè¿è¡Œæµ‹è¯•éªŒè¯")
        lines.append("3. å¦‚æœä¿®å¤æ•ˆæœä¸ä½³ï¼Œè¯·æ‰‹åŠ¨è°ƒæ•´")
        lines.append("4. å®šæœŸè¿è¡Œæ­¤åˆ†æä»¥ä¿æŒä»£ç è´¨é‡")
        lines.append("")

        lines.append("---")
        lines.append("*æ­¤æŠ¥å‘Šç”± AI è‡ªåŠ¨ç”Ÿæˆï¼Œè¯·äººå·¥å®¡æ ¸åæ‰§è¡Œ*")

        return "\n".join(lines)


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)

    # åˆ›å»ºåˆ†æå™¨
    analyzer = CodeAnalyzer()

    # æ¨¡æ‹Ÿ pytest è¾“å‡º
    mock_pytest_output = """
    FAIL: tests/test_example.py::test_example_function
        File "/project/tests/test_example.py", line 10
        def test_example_function():
    >       assert False
    E       AssertionError: assert False

    FAIL: tests/test_another.py::test_another_function
        File "/project/tests/test_another.py", line 20
        def test_another_function():
    >       result = undefined_function()
    E       NameError: name 'undefined_function' is not defined
    """

    # åˆ†ææµ‹è¯•ç»“æœ
    print("ğŸ” Analyzing test results...")
    analysis_result = analyzer.analyze_test_results(mock_pytest_output)

    # è¾“å‡ºåˆ†æç»“æœ
    print(f"\nğŸ“Š Analysis Summary:")
    print(f"Total failures: {analysis_result.analysis_summary['total_failures']}")
    print(f"Generated fixes: {analysis_result.analysis_summary['generated_fixes']}")
    print(f"Confidence score: {analysis_result.confidence_score:.1%}")

    # ç”Ÿæˆ BUGFIX_TODO
    todo_path = Path("docs/_reports/BUGFIX_TODO_AI.md")
    analyzer.generate_bugfix_todo(analysis_result, todo_path)
    print(f"\nğŸ“ Generated BUGFIX_TODO at {todo_path}")

    # æ˜¾ç¤ºä¼˜å…ˆçº§ä¿®å¤
    prioritized = analyzer.prioritize_fixes(analysis_result)
    print(f"\nğŸ¯ Top priority fix:")
    if prioritized:
        top_fix = prioritized[0]
        print(f"File: {top_fix.file_path}")
        print(f"Confidence: {top_fix.confidence:.1%}")
        print(f"Explanation: {top_fix.explanation}")


if __name__ == "__main__":
    main()