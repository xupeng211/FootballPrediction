#!/usr/bin/env python3
"""
ç›¸å¯¹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨ - åªåœ¨å…³é”®å‡½æ•°è¿è¡Œï¼Œæ§åˆ¶æ‰§è¡Œæ—¶é—´å’Œé£é™©
"""

import subprocess
import json
import time
import logging
import psutil
import platform
from pathlib import Path
from typing import Dict, List, Optional, Set, Any
from dataclasses import dataclass, asdict
import statistics
import re

logger = logging.getLogger(__name__)

@dataclass
class PerformanceBenchmarkConfig:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•é…ç½®"""
    # é€‰æ‹©æ€§æµ‹è¯•ï¼šåªæµ‹è¯•å…³é”®å‡½æ•°
    target_functions: List[str] = None

    # æ€§èƒ½é˜ˆå€¼æ§åˆ¶ï¼ˆç™¾åˆ†æ¯”å˜åŒ–ï¼‰
    warning_threshold: float = 10.0  # 10%å˜åŒ–è­¦å‘Š
    critical_threshold: float = 20.0  # 20%å˜åŒ–ä¸¥é‡

    # æ‰§è¡Œæ§åˆ¶
    max_runs: int = 5  # å‡å°‘è¿è¡Œæ¬¡æ•°
    warmup_runs: int = 2  # é¢„çƒ­è¿è¡Œ
    timeout_per_function: int = 30  # å•ä¸ªå‡½æ•°è¶…æ—¶
    total_timeout: int = 180  # æ€»è¶…æ—¶æ§åˆ¶

    # ç¯å¢ƒä¿¡æ¯æ”¶é›†
    collect_env_info: bool = True

    # å†…å­˜æ§åˆ¶
    max_memory_mb: int = 512  # æœ€å¤§å†…å­˜ä½¿ç”¨

    def __post_init__(self):
        if self.target_functions is None:
            self.target_functions = [
                "src.models.prediction_service.predict_match",
                "src.data.collectors.scores_collector.collect_data",
                "src.features.feature_calculator.calculate_features",
                "src.services.data_processing.process_batch"
            ]

class RelativePerformanceBenchmark:
    """ç›¸å¯¹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, config: Optional[PerformanceBenchmarkConfig] = None):
        self.config = config or PerformanceBenchmarkConfig()
        self.results_dir = Path("docs/_reports/performance")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.baseline_file = self.results_dir / "performance_baselines.json"
        self.env_info_file = self.results_dir / "environment_info.json"

    def get_environment_info(self) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒä¿¡æ¯"""
        env_info = {
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
            "cpu_freq": psutil.cpu_freq().current if psutil.cpu_freq() else None,
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else None
        }

        # è·å–git commitä¿¡æ¯
        try:
            result = subprocess.run(
                ["git", "rev-parse", "--short", "HEAD"],
                capture_output=True, text=True, timeout=10
            )
            if result.returncode == 0:
                env_info["git_commit"] = result.stdout.strip()
        except Exception as e:
            logger.warning(f"Failed to get git commit: {e}")

        return env_info

    def save_environment_info(self, env_info: Dict[str, Any]):
        """ä¿å­˜ç¯å¢ƒä¿¡æ¯"""
        with open(self.env_info_file, 'w', encoding='utf-8') as f:
            json.dump(env_info, f, indent=2, ensure_ascii=False)
        logger.info(f"Environment info saved to {self.env_info_file}")

    def load_baselines(self) -> Dict:
        """åŠ è½½å†å²åŸºå‡†æ•°æ®"""
        if self.baseline_file.exists():
            try:
                with open(self.baseline_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load baselines: {e}")

        return {}

    def save_baselines(self, baselines: Dict):
        """ä¿å­˜åŸºå‡†æ•°æ®"""
        with open(self.baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baselines, f, indent=2, ensure_ascii=False)
        logger.info(f"Performance baselines saved to {self.baseline_file}")

    def create_test_script(self, function_name: str) -> str:
        """åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬"""
        script = f'''
import time
import psutil
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.getcwd())

def benchmark_function():
    """åŸºå‡†æµ‹è¯•å‡½æ•°"""
    try:
        # å¯¼å…¥å¹¶è¿è¡Œç›®æ ‡å‡½æ•°
        module_path, func_name = "{function_name}".rsplit(".", 1)
        module = __import__(module_path, fromlist=[func_name])
        func = getattr(module, func_name)

        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        if "predict_match" in func_name:
            test_data = {{"home_team": "Team A", "away_team": "Team B", "league": "Test League"}}
            result = func(**test_data)
        elif "collect_data" in func_name:
            result = func()
        elif "calculate_features" in func_name:
            result = func({{"match_data": []}})
        elif "process_batch" in func_name:
            result = func([])
        else:
            result = func()

        return {{"success": True, "result": str(result)[:100]}}
    except Exception as e:
        return {{"success": False, "error": str(e)}}

if __name__ == "__main__":
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024

    result = benchmark_function()

    end_time = time.time()
    end_memory = psutil.Process().memory_info().rss / 1024 / 1024

    # è¾“å‡ºç»“æœ
    print(json.dumps({{
        "execution_time": end_time - start_time,
        "memory_usage": end_memory - start_memory,
        "result": result
    }}, ensure_ascii=False))
'''
        return script

    def run_function_benchmark(self, function_name: str) -> Dict:
        """è¿è¡Œå•ä¸ªå‡½æ•°çš„åŸºå‡†æµ‹è¯•"""
        results = {
            "function": function_name,
            "runs": [],
            "execution_times": [],
            "memory_usages": [],
            "successful_runs": 0,
            "failed_runs": 0,
            "avg_execution_time": 0,
            "std_execution_time": 0,
            "avg_memory_usage": 0,
            "performance_grade": "unknown"
        }

        logger.info(f"Benchmarking function {function_name}...")

        # é¢„çƒ­è¿è¡Œ
        for i in range(self.config.warmup_runs):
            try:
                self._run_single_benchmark(function_name)
            except Exception as e:
                logger.warning(f"Warmup run {i+1} failed: {e}")

        # æ­£å¼è¿è¡Œ
        for run in range(self.config.max_runs):
            try:
                start_time = time.time()

                # æ£€æŸ¥å†…å­˜ä½¿ç”¨
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                if current_memory > self.config.max_memory_mb:
                    logger.warning(f"Memory limit exceeded: {current_memory}MB > {self.config.max_memory_mb}MB")
                    break

                # è¿è¡ŒåŸºå‡†æµ‹è¯•
                benchmark_result = self._run_single_benchmark(function_name)

                execution_time = time.time() - start_time

                run_data = {
                    "run_number": run + 1,
                    "execution_time": execution_time,
                    "memory_usage": benchmark_result.get("memory_usage", 0),
                    "success": benchmark_result.get("result", {}).get("success", False),
                    "result": benchmark_result.get("result", {})
                }

                results["runs"].append(run_data)
                results["execution_times"].append(execution_time)
                results["memory_usages"].append(benchmark_result.get("memory_usage", 0))

                if run_data["success"]:
                    results["successful_runs"] += 1
                else:
                    results["failed_runs"] += 1

                # æ£€æŸ¥è¶…æ—¶
                if execution_time > self.config.timeout_per_function:
                    logger.warning(f"Function {function_name} timed out on run {run + 1}")
                    break

            except Exception as e:
                logger.error(f"Function {function_name} failed on run {run + 1}: {e}")
                results["failed_runs"] += 1

        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if results["execution_times"]:
            results["avg_execution_time"] = statistics.mean(results["execution_times"])
            results["std_execution_time"] = statistics.stdev(results["execution_times"]) if len(results["execution_times"]) > 1 else 0

        if results["memory_usages"]:
            results["avg_memory_usage"] = statistics.mean(results["memory_usages"])

        # è¯„ä¼°æ€§èƒ½ç­‰çº§
        results["performance_grade"] = self._evaluate_performance_grade(results)

        return results

    def _run_single_benchmark(self, function_name: str) -> Dict:
        """è¿è¡Œå•æ¬¡åŸºå‡†æµ‹è¯•"""
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•è„šæœ¬
        script_content = self.create_test_script(function_name)
        script_path = "/tmp/benchmark_script.py"

        try:
            with open(script_path, 'w', encoding='utf-8') as f:
                f.write(script_content)

            # è¿è¡Œæµ‹è¯•è„šæœ¬
            result = subprocess.run(
                ["python", script_path],
                capture_output=True,
                text=True,
                timeout=self.config.timeout_per_function,
                cwd="."
            )

            if result.returncode == 0:
                return json.loads(result.stdout)
            else:
                logger.error(f"Script failed: {result.stderr}")
                return {"error": result.stderr, "memory_usage": 0}

        except subprocess.TimeoutExpired:
            logger.error(f"Benchmark script timed out for {function_name}")
            return {"error": "TIMEOUT", "memory_usage": 0}
        except Exception as e:
            logger.error(f"Benchmark script failed for {function_name}: {e}")
            return {"error": str(e), "memory_usage": 0}
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                Path(script_path).unlink()
            except:
                pass

    def _evaluate_performance_grade(self, results: Dict) -> str:
        """è¯„ä¼°æ€§èƒ½ç­‰çº§"""
        if results["failed_runs"] > 0:
            return "failed"

        if results["successful_runs"] < self.config.max_runs * 0.8:
            return "unstable"

        # åŸºäºæ‰§è¡Œæ—¶é—´å˜å¼‚æ€§è¯„ä¼°
        cv = results["std_execution_time"] / results["avg_execution_time"] if results["avg_execution_time"] > 0 else 1

        if cv < 0.1:  # å˜å¼‚ç³»æ•°å°äº10%
            return "excellent"
        elif cv < 0.2:  # å˜å¼‚ç³»æ•°å°äº20%
            return "good"
        elif cv < 0.3:  # å˜å¼‚ç³»æ•°å°äº30%
            return "fair"
        else:
            return "poor"

    def compare_with_baseline(self, current_results: Dict, baseline_data: Dict) -> Dict:
        """ä¸åŸºå‡†æ•°æ®å¯¹æ¯”"""
        comparison = {
            "function": current_results["function"],
            "current_performance": current_results,
            "baseline_performance": baseline_data.get(current_results["function"], {}),
            "changes": {},
            "alerts": []
        }

        baseline = baseline_data.get(current_results["function"], {})

        if baseline:
            # æ‰§è¡Œæ—¶é—´å¯¹æ¯”
            current_time = current_results.get("avg_execution_time", 0)
            baseline_time = baseline.get("avg_execution_time", 0)

            if baseline_time > 0:
                time_change_pct = ((current_time - baseline_time) / baseline_time) * 100
                comparison["changes"]["execution_time_change_pct"] = time_change_pct

                # ç”Ÿæˆè­¦å‘Š
                if abs(time_change_pct) > self.config.critical_threshold:
                    comparison["alerts"].append({
                        "type": "critical",
                        "metric": "execution_time",
                        "change_pct": time_change_pct,
                        "message": f"æ‰§è¡Œæ—¶é—´å˜åŒ–{time_change_pct:.1f}%ï¼Œè¶…è¿‡{self.config.critical_threshold}%é˜ˆå€¼"
                    })
                elif abs(time_change_pct) > self.config.warning_threshold:
                    comparison["alerts"].append({
                        "type": "warning",
                        "metric": "execution_time",
                        "change_pct": time_change_pct,
                        "message": f"æ‰§è¡Œæ—¶é—´å˜åŒ–{time_change_pct:.1f}%ï¼Œè¶…è¿‡{self.config.warning_threshold}%é˜ˆå€¼"
                    })

            # å†…å­˜ä½¿ç”¨å¯¹æ¯”
            current_memory = current_results.get("avg_memory_usage", 0)
            baseline_memory = baseline.get("avg_memory_usage", 0)

            if baseline_memory > 0:
                memory_change_pct = ((current_memory - baseline_memory) / baseline_memory) * 100
                comparison["changes"]["memory_usage_change_pct"] = memory_change_pct

                if abs(memory_change_pct) > self.config.critical_threshold:
                    comparison["alerts"].append({
                        "type": "critical",
                        "metric": "memory_usage",
                        "change_pct": memory_change_pct,
                        "message": f"å†…å­˜ä½¿ç”¨å˜åŒ–{memory_change_pct:.1f}%ï¼Œè¶…è¿‡{self.config.critical_threshold}%é˜ˆå€¼"
                    })
                elif abs(memory_change_pct) > self.config.warning_threshold:
                    comparison["alerts"].append({
                        "type": "warning",
                        "metric": "memory_usage",
                        "change_pct": memory_change_pct,
                        "message": f"å†…å­˜ä½¿ç”¨å˜åŒ–{memory_change_pct:.1f}%ï¼Œè¶…è¿‡{self.config.warning_threshold}%é˜ˆå€¼"
                    })

        return comparison

    def run_performance_benchmarks(self, update_baselines: bool = False) -> Dict:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        start_time = time.time()

        # æ”¶é›†ç¯å¢ƒä¿¡æ¯
        env_info = self.get_environment_info()
        if self.config.collect_env_info:
            self.save_environment_info(env_info)

        # åŠ è½½åŸºå‡†æ•°æ®
        baseline_data = self.load_baselines()

        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        benchmark_results = []
        comparisons = []

        summary = {
            "total_functions": len(self.config.target_functions),
            "successful_functions": 0,
            "failed_functions": 0,
            "execution_time": 0,
            "environment": env_info,
            "alerts": []
        }

        for i, function_name in enumerate(self.config.target_functions):
            logger.info(f"Processing function {i+1}/{len(self.config.target_functions)}: {function_name}")

            # æ£€æŸ¥æ€»è¶…æ—¶
            if time.time() - start_time > self.config.total_timeout:
                logger.warning(f"Performance benchmark timed out after {self.config.total_timeout}s")
                summary["timeout"] = True
                break

            try:
                # è¿è¡ŒåŸºå‡†æµ‹è¯•
                result = self.run_function_benchmark(function_name)
                benchmark_results.append(result)

                # ä¸åŸºå‡†å¯¹æ¯”
                comparison = self.compare_with_baseline(result, baseline_data)
                comparisons.append(comparison)

                # æ”¶é›†è­¦å‘Š
                summary["alerts"].extend(comparison["alerts"])

                if result["performance_grade"] != "failed":
                    summary["successful_functions"] += 1
                else:
                    summary["failed_functions"] += 1

            except Exception as e:
                logger.error(f"Failed to benchmark {function_name}: {e}")
                summary["failed_functions"] += 1

        # æ›´æ–°åŸºå‡†æ•°æ®
        if update_baselines:
            new_baselines = {}
            for result in benchmark_results:
                if result["performance_grade"] != "failed":
                    new_baselines[result["function"]] = {
                        "avg_execution_time": result["avg_execution_time"],
                        "std_execution_time": result["std_execution_time"],
                        "avg_memory_usage": result["avg_memory_usage"],
                        "performance_grade": result["performance_grade"],
                        "timestamp": time.time()
                    }

            # åˆå¹¶æ–°æ—§åŸºå‡†æ•°æ®
            baseline_data.update(new_baselines)
            self.save_baselines(baseline_data)

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        summary["execution_time"] = time.time() - start_time
        final_report = {
            "summary": summary,
            "benchmark_results": benchmark_results,
            "comparisons": comparisons,
            "timestamp": time.time()
        }

        self._save_results(final_report)

        return final_report

    def _save_results(self, results: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"performance_results_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # æ›´æ–°æœ€æ–°ç»“æœ
        latest_file = self.results_dir / "latest_performance_results.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Performance benchmark results saved to {result_file}")

    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        latest_file = self.results_dir / "latest_performance_results.json"
        if not latest_file.exists():
            return "No performance benchmark results available"

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            summary = results["summary"]
            env_info = summary.get("environment", {})

            report = f"""
## âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ

### ç¯å¢ƒä¿¡æ¯
- **ç³»ç»Ÿ**: {env_info.get('platform', 'Unknown')}
- **Pythonç‰ˆæœ¬**: {env_info.get('python_version', 'Unknown')}
- **CPUæ ¸å¿ƒæ•°**: {env_info.get('cpu_count', 'Unknown')}
- **å†…å­˜æ€»é‡**: {env_info.get('memory_total_gb', 'Unknown')}GB
- **Gitæäº¤**: {env_info.get('git_commit', 'Unknown')}

### æ‰§è¡Œä¿¡æ¯
- **æ€»å‡½æ•°æ•°**: {summary.get('total_functions', 0)}
- **æˆåŠŸå‡½æ•°**: {summary.get('successful_functions', 0)} âœ…
- **å¤±è´¥å‡½æ•°**: {summary.get('failed_functions', 0)} âŒ
- **æ‰§è¡Œæ—¶é—´**: {summary.get('execution_time', 0):.1f}ç§’
- **è¶…æ—¶è®¾ç½®**: {self.config.total_timeout}ç§’

### æ€§èƒ½è­¦æŠ¥
- **æ€»è­¦æŠ¥æ•°**: {len(summary.get('alerts', []))}
- **ä¸¥é‡è­¦æŠ¥**: {len([a for a in summary.get('alerts', []) if a.get('type') == 'critical'])} ğŸš¨
- **è­¦å‘Šè­¦æŠ¥**: {len([a for a in summary.get('alerts', []) if a.get('type') == 'warning'])} âš ï¸
"""

            # æ·»åŠ å…·ä½“è­¦æŠ¥ä¿¡æ¯
            if summary.get('alerts'):
                report += "\n### ğŸš¨ å…·ä½“è­¦æŠ¥\n"
                for alert in summary['alerts'][:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report += f"- **{alert.get('metric', 'Unknown')}**: {alert.get('message', 'No message')}\n"

            # æ·»åŠ æ€§èƒ½æ¦‚è§ˆ
            if results.get('benchmark_results'):
                report += "\n### ğŸ“Š æ€§èƒ½æ¦‚è§ˆ\n"
                for result in results['benchmark_results'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    func_name = result['function'].split('.')[-1]
                    avg_time = result.get('avg_execution_time', 0)
                    grade = result.get('performance_grade', 'unknown')
                    report += f"- **{func_name}**: {avg_time:.3f}s ({grade})\n"

            report += f"""
### é…ç½®ä¿¡æ¯
- **è¿è¡Œæ¬¡æ•°**: {self.config.max_runs}æ¬¡ï¼ˆé¢„çƒ­{self.config.warmup_runs}æ¬¡ï¼‰
- **è­¦å‘Šé˜ˆå€¼**: {self.config.warning_threshold}%å˜åŒ–
- **ä¸¥é‡é˜ˆå€¼**: {self.config.critical_threshold}%å˜åŒ–
- **å†…å­˜é™åˆ¶**: {self.config.max_memory_mb}MB
"""

            return report

        except Exception as e:
            logger.error(f"Failed to generate performance report: {e}")
            return "Failed to generate performance report"


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse

    parser = argparse.ArgumentParser(description="Relative Performance Benchmark")
    parser.add_argument("--update-baselines", action="store_true",
                       help="Update baseline performance data")
    parser.add_argument("--report-only", action="store_true",
                       help="Only show performance report")

    args = parser.parse_args()

    benchmark = RelativePerformanceBenchmark()

    if args.report_only:
        print(benchmark.generate_performance_report())
        return

    results = benchmark.run_performance_benchmarks(update_baselines=args.update_baselines)
    print(benchmark.generate_performance_report())


if __name__ == "__main__":
    main()