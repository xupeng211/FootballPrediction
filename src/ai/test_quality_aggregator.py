#!/usr/bin/env python3
"""
é£é™©æ§åˆ¶è´¨é‡èšåˆå™¨ - æ•´åˆæ‰€æœ‰æµ‹è¯•ç»“æœï¼Œæä¾›ç»¼åˆè´¨é‡è¯„ä¼°
"""

import json
import time
import logging
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Set, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import statistics

# å¯¼å…¥å…¶ä»–æµ‹è¯•æ¨¡å—
from src.ai.mutation_tester import SelectiveMutationTester
from src.ai.flaky_test_detector import SmartFlakyTestDetector
from src.ai.performance_benchmark import RelativePerformanceBenchmark

logger = logging.getLogger(__name__)

@dataclass
class QualityAggregationConfig:
    """è´¨é‡èšåˆé…ç½®"""
    # æƒé‡é…ç½®
    mutation_weight: float = 0.3  # çªå˜æµ‹è¯•æƒé‡
    flaky_weight: float = 0.25   # Flakyæµ‹è¯•æƒé‡
    performance_weight: float = 0.2  # æ€§èƒ½æµ‹è¯•æƒé‡
    coverage_weight: float = 0.25  # è¦†ç›–ç‡æƒé‡

    # è´¨é‡é˜ˆå€¼
    excellent_threshold: float = 0.85  # ä¼˜ç§€é˜ˆå€¼
    good_threshold: float = 0.70      # è‰¯å¥½é˜ˆå€¼
    acceptable_threshold: float = 0.55  # å¯æ¥å—é˜ˆå€¼

    # é£é™©æ§åˆ¶
    max_execution_time: int = 600  # æœ€å¤§æ‰§è¡Œæ—¶é—´ï¼ˆ10åˆ†é’Ÿï¼‰
    enable_mutation: bool = True   # å¯ç”¨çªå˜æµ‹è¯•
    enable_flaky: bool = True      # å¯ç”¨Flakyæµ‹è¯•
    enable_performance: bool = True # å¯ç”¨æ€§èƒ½æµ‹è¯•
    enable_coverage: bool = True   # å¯ç”¨è¦†ç›–ç‡æµ‹è¯•

    # CIé›†æˆ
    non_blocking_mode: bool = True  # éé˜»å¡æ¨¡å¼
    fail_fast: bool = False        # å¿«é€Ÿå¤±è´¥

class TestQualityAggregator:
    """æµ‹è¯•è´¨é‡èšåˆå™¨"""

    def __init__(self, config: Optional[QualityAggregationConfig] = None):
        self.config = config or QualityAggregationConfig()
        self.results_dir = Path("docs/_reports/quality")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.quality_history_file = self.results_dir / "quality_history.json"

        # åˆå§‹åŒ–æµ‹è¯•å™¨
        self.mutation_tester = SelectiveMutationTester() if self.config.enable_mutation else None
        self.flaky_detector = SmartFlakyTestDetector() if self.config.enable_flaky else None
        self.performance_benchmark = RelativePerformanceBenchmark() if self.config.enable_performance else None

    def run_comprehensive_quality_check(self, incremental: bool = True) -> Dict:
        """è¿è¡Œç»¼åˆè´¨é‡æ£€æŸ¥"""
        start_time = time.time()

        # åˆå§‹åŒ–ç»“æœ
        quality_results = {
            "timestamp": datetime.now().isoformat(),
            "execution_mode": "incremental" if incremental else "full",
            "components": {},
            "overall_score": 0,
            "quality_grade": "unknown",
            "recommendations": [],
            "risks": [],
            "execution_time": 0
        }

        logger.info("Starting comprehensive quality check...")

        # æ‰§è¡Œå„é¡¹æµ‹è¯•
        test_results = {}

        # 1. çªå˜æµ‹è¯•
        if self.config.enable_mutation:
            try:
                logger.info("Running mutation testing...")
                mutation_result = self.mutation_tester.run_mutation_tests(incremental=incremental)
                test_results["mutation"] = self._analyze_mutation_result(mutation_result)
                logger.info(f"Mutation testing completed in {mutation_result.get('execution_time', 0):.1f}s")
            except Exception as e:
                logger.error(f"Mutation testing failed: {e}")
                test_results["mutation"] = {"error": str(e), "score": 0}

        # 2. Flakyæµ‹è¯•
        if self.config.enable_flaky:
            try:
                logger.info("Running flaky test detection...")
                flaky_result = self.flaky_detector.detect_flaky_tests(incremental=incremental)
                test_results["flaky"] = self._analyze_flaky_result(flaky_result)
                logger.info(f"Flaky test detection completed in {flaky_result.get('execution_time', 0):.1f}s")
            except Exception as e:
                logger.error(f"Flaky test detection failed: {e}")
                test_results["flaky"] = {"error": str(e), "score": 0}

        # 3. æ€§èƒ½æµ‹è¯•
        if self.config.enable_performance:
            try:
                logger.info("Running performance benchmark...")
                perf_result = self.performance_benchmark.run_performance_benchmarks(update_baselines=False)
                test_results["performance"] = self._analyze_performance_result(perf_result)
                logger.info(f"Performance benchmark completed in {perf_result.get('execution_time', 0):.1f}s")
            except Exception as e:
                logger.error(f"Performance benchmark failed: {e}")
                test_results["performance"] = {"error": str(e), "score": 0}

        # 4. è¦†ç›–ç‡æµ‹è¯•
        if self.config.enable_coverage:
            try:
                logger.info("Running coverage analysis...")
                coverage_result = self._run_coverage_analysis()
                test_results["coverage"] = self._analyze_coverage_result(coverage_result)
                logger.info("Coverage analysis completed")
            except Exception as e:
                logger.error(f"Coverage analysis failed: {e}")
                test_results["coverage"] = {"error": str(e), "score": 0}

        # æ£€æŸ¥æ€»è¶…æ—¶
        if time.time() - start_time > self.config.max_execution_time:
            logger.warning(f"Quality check timed out after {self.config.max_execution_time}s")
            quality_results["timeout"] = True
            if self.config.fail_fast:
                quality_results["error"] = "Quality check timed out"
                return quality_results

        # è®¡ç®—ç»¼åˆå¾—åˆ†
        overall_score = self._calculate_overall_score(test_results)
        quality_results["overall_score"] = overall_score
        quality_results["components"] = test_results

        # è¯„ä¼°è´¨é‡ç­‰çº§
        quality_results["quality_grade"] = self._evaluate_quality_grade(overall_score)

        # ç”Ÿæˆå»ºè®®å’Œé£é™©
        quality_results["recommendations"] = self._generate_recommendations(test_results, overall_score)
        quality_results["risks"] = self._identify_risks(test_results)

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        quality_results["execution_time"] = time.time() - start_time

        # ä¿å­˜ç»“æœ
        self._save_quality_results(quality_results)

        # æ›´æ–°å†å²æ•°æ®
        self._update_quality_history(quality_results)

        return quality_results

    def _analyze_mutation_result(self, result: Dict) -> Dict:
        """åˆ†æçªå˜æµ‹è¯•ç»“æœ"""
        if "error" in result:
            return {"error": result["error"], "score": 0}

        mutation_data = result.get("results", {})
        total_mutants = mutation_data.get("total", 0)
        killed_mutants = mutation_data.get("killed", 0)

        score = killed_mutants / total_mutants if total_mutants > 0 else 0

        return {
            "score": score,
            "total_mutants": total_mutants,
            "killed_mutants": killed_mutants,
            "survived_mutants": mutation_data.get("survived", 0),
            "mutation_score": mutation_data.get("score", 0),
            "execution_time": result.get("execution_time", 0)
        }

    def _analyze_flaky_result(self, result: Dict) -> Dict:
        """åˆ†æFlakyæµ‹è¯•ç»“æœ"""
        if "error" in result:
            return {"error": result["error"], "score": 0}

        summary = result.get("summary", {})
        total_tests = summary.get("total_tests", 0)
        flaky_tests = summary.get("flaky_tests", 0)

        # Flakyæµ‹è¯•åˆ†æ•°ï¼š1 - (flaky_tests / total_tests)
        score = 1 - (flaky_tests / total_tests) if total_tests > 0 else 1

        return {
            "score": score,
            "total_tests": total_tests,
            "flaky_tests": flaky_tests,
            "external_tests": summary.get("external_tests", 0),
            "execution_time": summary.get("execution_time", 0)
        }

    def _analyze_performance_result(self, result: Dict) -> Dict:
        """åˆ†ææ€§èƒ½æµ‹è¯•ç»“æœ"""
        if "error" in result:
            return {"error": result["error"], "score": 0}

        summary = result.get("summary", {})
        total_functions = summary.get("total_functions", 0)
        successful_functions = summary.get("successful_functions", 0)
        alerts = summary.get("alerts", [])

        # æ€§èƒ½åˆ†æ•°ï¼šæˆåŠŸå‡½æ•°æ¯”ä¾‹å‡å»è­¦æŠ¥å½±å“
        success_rate = successful_functions / total_functions if total_functions > 0 else 0
        alert_penalty = min(len(alerts) * 0.1, 0.5)  # æ¯ä¸ªè­¦æŠ¥æ‰£0.1åˆ†ï¼Œæœ€å¤šæ‰£0.5åˆ†
        score = max(0, success_rate - alert_penalty)

        return {
            "score": score,
            "total_functions": total_functions,
            "successful_functions": successful_functions,
            "failed_functions": summary.get("failed_functions", 0),
            "alerts_count": len(alerts),
            "critical_alerts": len([a for a in alerts if a.get('type') == 'critical']),
            "execution_time": summary.get("execution_time", 0)
        }

    def _run_coverage_analysis(self) -> Dict:
        """è¿è¡Œè¦†ç›–ç‡åˆ†æ"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„è¦†ç›–ç‡æµ‹è¯•å‘½ä»¤
            result = subprocess.run(
                ["python", "-m", "pytest", "--cov=src", "--cov-report=json", "--cov-report=term-missing"],
                capture_output=True,
                text=True,
                timeout=60,
                cwd="."
            )

            if result.returncode == 0:
                # è§£æè¦†ç›–ç‡ç»“æœ
                coverage_file = Path("coverage.json")
                if coverage_file.exists():
                    with open(coverage_file, 'r') as f:
                        coverage_data = json.load(f)
                    return {"success": True, "data": coverage_data}
                else:
                    return {"success": False, "error": "Coverage report not found"}
            else:
                return {"success": False, "error": result.stderr}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _analyze_coverage_result(self, result: Dict) -> Dict:
        """åˆ†æè¦†ç›–ç‡ç»“æœ"""
        if not result.get("success"):
            return {"error": result.get("error", "Unknown error"), "score": 0}

        coverage_data = result.get("data", {})
        total_covered = coverage_data.get("totals", {}).get("covered_lines", 0)
        total_lines = coverage_data.get("totals", {}).get("num_statements", 0)

        coverage_pct = total_covered / total_lines if total_lines > 0 else 0

        return {
            "score": coverage_pct,
            "total_lines": total_lines,
            "covered_lines": total_covered,
            "coverage_pct": coverage_pct * 100
        }

    def _calculate_overall_score(self, test_results: Dict) -> float:
        """è®¡ç®—ç»¼åˆå¾—åˆ†"""
        weighted_scores = []

        if "mutation" in test_results:
            weighted_scores.append(test_results["mutation"]["score"] * self.config.mutation_weight)

        if "flaky" in test_results:
            weighted_scores.append(test_results["flaky"]["score"] * self.config.flaky_weight)

        if "performance" in test_results:
            weighted_scores.append(test_results["performance"]["score"] * self.config.performance_weight)

        if "coverage" in test_results:
            weighted_scores.append(test_results["coverage"]["score"] * self.config.coverage_weight)

        return sum(weighted_scores) if weighted_scores else 0

    def _evaluate_quality_grade(self, score: float) -> str:
        """è¯„ä¼°è´¨é‡ç­‰çº§"""
        if score >= self.config.excellent_threshold:
            return "excellent"
        elif score >= self.config.good_threshold:
            return "good"
        elif score >= self.config.acceptable_threshold:
            return "acceptable"
        else:
            return "poor"

    def _generate_recommendations(self, test_results: Dict, overall_score: float) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # çªå˜æµ‹è¯•å»ºè®®
        if "mutation" in test_results:
            mutation = test_results["mutation"]
            if mutation.get("score", 0) < 0.6:
                recommendations.append("åŠ å¼ºçªå˜æµ‹è¯•ï¼Œå¢åŠ æµ‹è¯•ç”¨ä¾‹è¦†ç›–ç‡")
            if mutation.get("survived_mutants", 0) > 5:
                recommendations.append(f"ä¿®å¤{mutation['survived_mutants']}ä¸ªå­˜æ´»çªå˜ï¼Œæé«˜æµ‹è¯•è´¨é‡")

        # Flakyæµ‹è¯•å»ºè®®
        if "flaky" in test_results:
            flaky = test_results["flaky"]
            if flaky.get("flaky_tests", 0) > 0:
                recommendations.append(f"ä¿®å¤{flaky['flaky_tests']}ä¸ªFlakyæµ‹è¯•ï¼Œæé«˜æµ‹è¯•ç¨³å®šæ€§")

        # æ€§èƒ½æµ‹è¯•å»ºè®®
        if "performance" in test_results:
            perf = test_results["performance"]
            if perf.get("critical_alerts", 0) > 0:
                recommendations.append("è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜ï¼Œä¼˜åŒ–å…³é”®å‡½æ•°æ‰§è¡Œæ—¶é—´")
            if perf.get("failed_functions", 0) > 0:
                recommendations.append(f"ä¿®å¤{perf['failed_functions']}ä¸ªæ€§èƒ½æµ‹è¯•å¤±è´¥")

        # è¦†ç›–ç‡å»ºè®®
        if "coverage" in test_results:
            coverage = test_results["coverage"]
            if coverage.get("coverage_pct", 0) < 80:
                recommendations.append("æé«˜ä»£ç è¦†ç›–ç‡ï¼Œæ·»åŠ æ›´å¤šæµ‹è¯•ç”¨ä¾‹")

        # æ•´ä½“å»ºè®®
        if overall_score >= self.config.excellent_threshold:
            recommendations.append("ä»£ç è´¨é‡ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒå½“å‰æ ‡å‡†")
        elif overall_score >= self.config.good_threshold:
            recommendations.append("ä»£ç è´¨é‡è‰¯å¥½ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
        elif overall_score >= self.config.acceptable_threshold:
            recommendations.append("ä»£ç è´¨é‡å¯æ¥å—ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤å…³é”®é—®é¢˜")
        else:
            recommendations.append("ä»£ç è´¨é‡éœ€è¦æ˜¾è‘—æå‡ï¼Œå»ºè®®åˆ¶å®šè´¨é‡æ”¹è¿›è®¡åˆ’")

        return recommendations

    def _identify_risks(self, test_results: Dict) -> List[Dict]:
        """è¯†åˆ«é£é™©"""
        risks = []

        # çªå˜æµ‹è¯•é£é™©
        if "mutation" in test_results:
            mutation = test_results["mutation"]
            if mutation.get("score", 0) < 0.4:
                risks.append({
                    "type": "high",
                    "category": "test_quality",
                    "message": "çªå˜æµ‹è¯•åˆ†æ•°è¿‡ä½ï¼Œæµ‹è¯•è´¨é‡å­˜åœ¨ä¸¥é‡é£é™©"
                })

        # Flakyæµ‹è¯•é£é™©
        if "flaky" in test_results:
            flaky = test_results["flaky"]
            if flaky.get("flaky_tests", 0) > 2:
                risks.append({
                    "type": "medium",
                    "category": "test_stability",
                    "message": f"å‘ç°{flaky['flaky_tests']}ä¸ªFlakyæµ‹è¯•ï¼Œå½±å“æµ‹è¯•ç¨³å®šæ€§"
                })

        # æ€§èƒ½é£é™©
        if "performance" in test_results:
            perf = test_results["performance"]
            if perf.get("critical_alerts", 0) > 0:
                risks.append({
                    "type": "high",
                    "category": "performance",
                    "message": f"å‘ç°{perf['critical_alerts']}ä¸ªä¸¥é‡æ€§èƒ½é—®é¢˜"
                })

        # è¦†ç›–ç‡é£é™©
        if "coverage" in test_results:
            coverage = test_results["coverage"]
            if coverage.get("coverage_pct", 0) < 60:
                risks.append({
                    "type": "medium",
                    "category": "coverage",
                    "message": "ä»£ç è¦†ç›–ç‡è¿‡ä½ï¼Œå­˜åœ¨æµ‹è¯•ç›²åŒº"
                })

        return risks

    def _save_quality_results(self, results: Dict):
        """ä¿å­˜è´¨é‡æ£€æŸ¥ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"quality_report_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # æ›´æ–°æœ€æ–°ç»“æœ
        latest_file = self.results_dir / "latest_quality_report.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Quality report saved to {result_file}")

    def _update_quality_history(self, results: Dict):
        """æ›´æ–°è´¨é‡å†å²æ•°æ®"""
        history = []
        if self.quality_history_file.exists():
            try:
                with open(self.quality_history_file, 'r') as f:
                    history = json.load(f)
            except Exception as e:
                logger.error(f"Failed to load quality history: {e}")

        # æ·»åŠ æ–°ç»“æœï¼ˆä¿ç•™æœ€è¿‘50æ¡è®°å½•ï¼‰
        history.append({
            "timestamp": results["timestamp"],
            "overall_score": results["overall_score"],
            "quality_grade": results["quality_grade"],
            "execution_time": results["execution_time"]
        })

        history = history[-50:]  # ä¿ç•™æœ€è¿‘50æ¡è®°å½•

        with open(self.quality_history_file, 'w', encoding='utf-8') as f:
            json.dump(history, f, indent=2, ensure_ascii=False)

    def generate_quality_report(self) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        latest_file = self.results_dir / "latest_quality_report.json"
        if not latest_file.exists():
            return "No quality report available"

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            grade_emoji = {
                "excellent": "ğŸŒŸ",
                "good": "âœ…",
                "acceptable": "âš ï¸",
                "poor": "âŒ"
            }

            report = f"""
## ğŸ“Š ç»¼åˆè´¨é‡è¯„ä¼°æŠ¥å‘Š

### æ•´ä½“è´¨é‡
- **è´¨é‡ç­‰çº§**: {grade_emoji.get(results.get('quality_grade', 'unknown'), 'â“')} {results.get('quality_grade', 'unknown').upper()}
- **ç»¼åˆå¾—åˆ†**: {results.get('overall_score', 0):.2%}
- **æ‰§è¡Œæ—¶é—´**: {results.get('execution_time', 0):.1f}ç§’
- **æ‰§è¡Œæ¨¡å¼**: {results.get('execution_mode', 'unknown')}

### å„é¡¹æŒ‡æ ‡å¾—åˆ†
"""

            # æ·»åŠ å„é¡¹æŒ‡æ ‡è¯¦æƒ…
            components = results.get("components", {})
            for component, data in components.items():
                if "error" not in data:
                    score = data.get("score", 0)
                    report += f"- **{component.title()}**: {score:.2%}\n"

            # æ·»åŠ å»ºè®®
            recommendations = results.get("recommendations", [])
            if recommendations:
                report += "\n### ğŸ¯ æ”¹è¿›å»ºè®®\n"
                for rec in recommendations[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    report += f"- {rec}\n"

            # æ·»åŠ é£é™©
            risks = results.get("risks", [])
            if risks:
                report += "\n### âš ï¸ é£é™©æç¤º\n"
                for risk in risks:
                    risk_emoji = "ğŸš¨" if risk.get("type") == "high" else "âš ï¸"
                    report += f"- {risk_emoji} {risk.get('message', 'Unknown risk')}\n"

            return report

        except Exception as e:
            logger.error(f"Failed to generate quality report: {e}")
            return "Failed to generate quality report"

    def get_ci_status(self) -> Dict:
        """è·å–CIçŠ¶æ€"""
        latest_file = self.results_dir / "latest_quality_report.json"
        if not latest_file.exists():
            return {"status": "no_data", "should_fail": False}

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            quality_grade = results.get("quality_grade", "unknown")
            overall_score = results.get("overall_score", 0)

            # CIå¤±è´¥æ¡ä»¶
            should_fail = (
                quality_grade == "poor" or
                (quality_grade == "acceptable" and not self.config.non_blocking_mode) or
                overall_score < self.config.acceptable_threshold
            )

            return {
                "status": quality_grade,
                "score": overall_score,
                "should_fail": should_fail,
                "non_blocking": self.config.non_blocking_mode
            }

        except Exception as e:
            logger.error(f"Failed to get CI status: {e}")
            return {"status": "error", "should_fail": False}


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse
    import subprocess

    parser = argparse.ArgumentParser(description="Test Quality Aggregator")
    parser.add_argument("--incremental", action="store_true", default=True,
                       help="Run incremental quality check")
    parser.add_argument("--full", action="store_true",
                       help="Run full quality check")
    parser.add_argument("--report-only", action="store_true",
                       help="Only show quality report")
    parser.add_argument("--ci-status", action="store_true",
                       help="Show CI status")

    args = parser.parse_args()

    aggregator = TestQualityAggregator()

    if args.report_only:
        print(aggregator.generate_quality_report())
        return

    if args.ci_status:
        status = aggregator.get_ci_status()
        print(f"CI Status: {status}")
        return

    incremental_mode = args.incremental and not args.full
    results = aggregator.run_comprehensive_quality_check(incremental=incremental_mode)
    print(aggregator.generate_quality_report())


if __name__ == "__main__":
    main()