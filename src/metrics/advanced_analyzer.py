#!/usr/bin/env python3
"""
é«˜çº§è´¨é‡åº¦é‡åˆ†æå™¨
Advanced Quality Metrics Analyzer

æä¾›ä»£ç å¤æ‚åº¦ã€æŠ€æœ¯å€ºåŠ¡ã€æ€§èƒ½ç›‘æ§ç­‰é«˜çº§è´¨é‡åº¦é‡æŒ‡æ ‡
"""

import ast
import os
import subprocess
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import psutil

from src.core.logging_system import get_logger
from src.core.config import get_config

logger = get_logger(__name__)


class CodeComplexityAnalyzer:
    """ä»£ç å¤æ‚åº¦åˆ†æå™¨"""

    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)

    def analyze_file_complexity(self, file_path: Path) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ–‡ä»¶çš„å¤æ‚åº¦"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content)

            complexity_metrics = {
                'file_path': str(file_path),
                'lines_of_code': len(content.splitlines()),
                'cyclomatic_complexity': self._calculate_cyclomatic_complexity(tree),
                'cognitive_complexity': self._calculate_cognitive_complexity(tree),
                'maintainability_index': self._calculate_maintainability_index(content, tree),
                'nesting_depth': self._calculate_max_nesting_depth(tree),
                'function_count': len([node for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]),
                'class_count': len([node for node in ast.walk(tree) if isinstance(node, ast.ClassDef)])
            }

            return complexity_metrics

        except Exception as e:
            self.logger.error(f"åˆ†ææ–‡ä»¶å¤æ‚åº¦å¤±è´¥ {file_path}: {e}")
            return {}

    def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, ast.With, ast.AsyncWith):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
            elif isinstance(node, ast.ListComp, ast.DictComp, ast.SetComp, ast.GeneratorExp):
                complexity += 1

        return complexity

    def _calculate_cognitive_complexity(self, tree: ast.AST) -> int:
        """è®¡ç®—è®¤çŸ¥å¤æ‚åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        complexity = 0
        nesting_level = 0

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1 + nesting_level
                nesting_level += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1 + nesting_level
            elif isinstance(node, (ast.Try, ast.With, ast.AsyncWith)):
                complexity += 1

        return complexity

    def _calculate_maintainability_index(self, content: str, tree: ast.AST) -> float:
        """è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        lines_of_code = len(content.splitlines())
        cyclomatic_complexity = self._calculate_cyclomatic_complexity(tree)
        volume = lines_of_code * 0.5  # ç®€åŒ–çš„ä½“ç§¯è®¡ç®—

        if volume == 0:
            return 100.0

        # ç®€åŒ–çš„å¯ç»´æŠ¤æ€§æŒ‡æ•°è®¡ç®—
        maintainability = max(0, 171 - 5.2 * (cyclomatic_complexity ** 0.23) - 0.23 * (volume ** 0.5))
        return round(maintainability, 2)

    def _calculate_max_nesting_depth(self, tree: ast.AST) -> int:
        """è®¡ç®—æœ€å¤§åµŒå¥—æ·±åº¦"""
        max_depth = 0

        def calculate_depth(node, current_depth=0):
            nonlocal max_depth
            max_depth = max(max_depth, current_depth)

            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor, ast.With, ast.AsyncWith, ast.Try)):
                for child in ast.iter_child_nodes(node):
                    calculate_depth(child, current_depth + 1)
            else:
                for child in ast.iter_child_nodes(node):
                    calculate_depth(child, current_depth)

        calculate_depth(tree)
        return max_depth

    def analyze_directory_complexity(self, directory: Path) -> Dict[str, Any]:
        """åˆ†æç›®å½•çš„å¤æ‚åº¦"""
        python_files = list(directory.rglob("*.py"))

        if not python_files:
            return {}

        total_metrics = {
            'total_files': len(python_files),
            'file_metrics': [],
            'summary': {
                'total_lines': 0,
                'avg_cyclomatic_complexity': 0,
                'avg_cognitive_complexity': 0,
                'avg_maintainability_index': 0,
                'max_nesting_depth': 0,
                'total_functions': 0,
                'total_classes': 0
            }
        }

        cyclomatic_complexities = []
        cognitive_complexities = []
        maintainability_indices = []

        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            metrics = self.analyze_file_complexity(file_path)
            if metrics:
                total_metrics['file_metrics'].append(metrics)

                # ç´¯åŠ ç»Ÿè®¡æ•°æ®
                total_metrics['summary']['total_lines'] += metrics['lines_of_code']
                total_metrics['summary']['total_functions'] += metrics['function_count']
                total_metrics['summary']['total_classes'] += metrics['class_count']
                total_metrics['summary']['max_nesting_depth'] = max(
                    total_metrics['summary']['max_nesting_depth'],
                    metrics['nesting_depth']
                )

                cyclomatic_complexities.append(metrics['cyclomatic_complexity'])
                cognitive_complexities.append(metrics['cognitive_complexity'])
                maintainability_indices.append(metrics['maintainability_index'])

        # è®¡ç®—å¹³å‡å€¼
        if cyclomatic_complexities:
            total_metrics['summary']['avg_cyclomatic_complexity'] = sum(cyclomatic_complexities) / len(cyclomatic_complexities)
            total_metrics['summary']['avg_cognitive_complexity'] = sum(cognitive_complexities) / len(cognitive_complexities)
            total_metrics['summary']['avg_maintainability_index'] = sum(maintainability_indices) / len(maintainability_indices)

        return total_metrics


class TechnicalDebtAnalyzer:
    """æŠ€æœ¯å€ºåŠ¡åˆ†æå™¨"""

    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)

    def analyze_technical_debt(self, directory: Path) -> Dict[str, Any]:
        """åˆ†ææŠ€æœ¯å€ºåŠ¡"""
        debt_metrics = {
            'code_smells': self._detect_code_smells(directory),
            'duplicate_code': self._detect_duplicate_code(directory),
            'long_methods': self._detect_long_methods(directory),
            'large_classes': self._detect_large_classes(directory),
            'dead_code': self._detect_dead_code(directory),
            'security_issues': self._detect_security_issues(directory)
        }

        # è®¡ç®—æŠ€æœ¯å€ºåŠ¡åˆ†æ•°
        debt_score = self._calculate_debt_score(debt_metrics)
        debt_metrics['debt_score'] = debt_score

        return debt_metrics

    def _detect_code_smells(self, directory: Path) -> List[Dict[str, Any]]:
        """æ£€æµ‹ä»£ç å¼‚å‘³"""
        smells = []
        python_files = list(directory.rglob("*.py"))

        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                # æ£€æµ‹é•¿è¡Œ
                for i, line in enumerate(lines, 1):
                    if len(line.strip()) > 88:  # PEP 8 è¡Œé•¿åº¦é™åˆ¶
                        smells.append({
                            'type': 'long_line',
                            'file': str(file_path),
                            'line': i,
                            'message': f'è¡Œé•¿åº¦è¶…è¿‡88å­—ç¬¦: {len(line.strip())}å­—ç¬¦',
                            'severity': 'medium'
                        })

                # æ£€æµ‹æœªä½¿ç”¨çš„å¯¼å…¥
                content = ''.join(lines)
                tree = ast.parse(content)
                imports = set()
                used_names = set()

                for node in ast.walk(tree):
                    if isinstance(node, ast.Import):
                        for alias in node.names:
                            imports.add(alias.name.split('.')[0])
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            imports.add(node.module.split('.')[0])
                    elif isinstance(node, ast.Name):
                        used_names.add(node.id)

                unused_imports = imports - used_names
                for import_name in unused_imports:
                    smells.append({
                        'type': 'unused_import',
                        'file': str(file_path),
                        'line': 1,
                        'message': f'æœªä½¿ç”¨çš„å¯¼å…¥: {import_name}',
                        'severity': 'low'
                    })

            except Exception as e:
                self.logger.error(f"æ£€æµ‹ä»£ç å¼‚å‘³å¤±è´¥ {file_path}: {e}")

        return smells

    def _detect_duplicate_code(self, directory: Path) -> List[Dict[str, Any]]:
        """æ£€æµ‹é‡å¤ä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        duplicates = []
        code_blocks = {}

        python_files = list(directory.rglob("*.py"))
        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ç®€å•çš„ä»£ç å—ç›¸ä¼¼åº¦æ£€æµ‹
                lines = content.split('\n')
                for i in range(0, len(lines) - 4):
                    block = '\n'.join(lines[i:i+5])
                    block_hash = hash(block)

                    if block_hash in code_blocks:
                        duplicates.append({
                            'type': 'duplicate_block',
                            'files': [code_blocks[block_hash], str(file_path)],
                            'line_start': i + 1,
                            'message': 'å‘ç°é‡å¤ä»£ç å—',
                            'severity': 'medium'
                        })
                    else:
                        code_blocks[block_hash] = str(file_path)

            except Exception as e:
                self.logger.error(f"æ£€æµ‹é‡å¤ä»£ç å¤±è´¥ {file_path}: {e}")

        return duplicates

    def _detect_long_methods(self, directory: Path) -> List[Dict[str, Any]]:
        """æ£€æµ‹é•¿æ–¹æ³•"""
        long_methods = []

        python_files = list(directory.rglob("*.py"))
        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                tree = ast.parse(''.join(lines))
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        start_line = node.lineno
                        end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line
                        method_length = end_line - start_line + 1

                        if method_length > 50:  # é•¿æ–¹æ³•é˜ˆå€¼
                            long_methods.append({
                                'type': 'long_method',
                                'file': str(file_path),
                                'line': start_line,
                                'method_name': node.name,
                                'length': method_length,
                                'message': f'æ–¹æ³•è¿‡é•¿: {method_length}è¡Œ',
                                'severity': 'medium'
                            })

            except Exception as e:
                self.logger.error(f"æ£€æµ‹é•¿æ–¹æ³•å¤±è´¥ {file_path}: {e}")

        return long_methods

    def _detect_large_classes(self, directory: Path) -> List[Dict[str, Any]]:
        """æ£€æµ‹å¤§ç±»"""
        large_classes = []

        python_files = list(directory.rglob("*.py"))
        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                tree = ast.parse(''.join(lines))
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        start_line = node.lineno
                        end_line = node.end_lineno if hasattr(node, 'end_lineno') else start_line
                        class_length = end_line - start_line + 1

                        method_count = len([n for n in node.body if isinstance(n, ast.FunctionDef)])

                        if class_length > 200 or method_count > 20:  # å¤§ç±»é˜ˆå€¼
                            large_classes.append({
                                'type': 'large_class',
                                'file': str(file_path),
                                'line': start_line,
                                'class_name': node.name,
                                'length': class_length,
                                'method_count': method_count,
                                'message': f'ç±»è¿‡å¤§: {class_length}è¡Œ, {method_count}ä¸ªæ–¹æ³•',
                                'severity': 'high'
                            })

            except Exception as e:
                self.logger.error(f"æ£€æµ‹å¤§ç±»å¤±è´¥ {file_path}: {e}")

        return large_classes

    def _detect_dead_code(self, directory: Path) -> List[Dict[str, Any]]:
        """æ£€æµ‹æ­»ä»£ç """
        dead_code = []

        python_files = list(directory.rglob("*.py"))
        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)
                defined_functions = set()
                called_functions = set()

                # æ”¶é›†å®šä¹‰çš„å‡½æ•°
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        defined_functions.add(node.name)

                # æ”¶é›†è°ƒç”¨çš„å‡½æ•°
                for node in ast.walk(tree):
                    if isinstance(node, ast.Call):
                        if isinstance(node.func, ast.Name):
                            called_functions.add(node.func.id)

                # æ‰¾å‡ºæœªè°ƒç”¨çš„å‡½æ•°ï¼ˆæ’é™¤ç‰¹æ®Šæ–¹æ³•ï¼‰
                unused_functions = defined_functions - called_functions
                for func_name in unused_functions:
                    if not func_name.startswith('_') and not func_name.startswith('test_'):
                        dead_code.append({
                            'type': 'dead_function',
                            'file': str(file_path),
                            'message': f'æœªä½¿ç”¨çš„å‡½æ•°: {func_name}',
                            'severity': 'low'
                        })

            except Exception as e:
                self.logger.error(f"æ£€æµ‹æ­»ä»£ç å¤±è´¥ {file_path}: {e}")

        return dead_code

    def _detect_security_issues(self, directory: Path) -> List[Dict[str, Any]]:
        """æ£€æµ‹å®‰å…¨é—®é¢˜"""
        security_issues = []

        python_files = list(directory.rglob("*.py"))
        for file_path in python_files:
            if '__pycache__' in str(file_path):
                continue

            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()

                for i, line in enumerate(lines, 1):
                    line_content = line.strip().lower()

                    # æ£€æµ‹æ½œåœ¨çš„å®‰å…¨é—®é¢˜
                    security_patterns = {
                        'eval(': 'ä½¿ç”¨äº†å±é™©çš„eval()å‡½æ•°',
                        'exec(': 'ä½¿ç”¨äº†å±é™©çš„exec()å‡½æ•°',
                        'subprocess.call': 'ä½¿ç”¨äº†subprocess.call',
                        'os.system': 'ä½¿ç”¨äº†os.system',
                        'pickle.loads': 'ä½¿ç”¨äº†ä¸å®‰å…¨çš„pickle.loads',
                        'input()': 'ä½¿ç”¨äº†input()ï¼ˆå¯èƒ½å­˜åœ¨æ³¨å…¥é£é™©ï¼‰'
                    }

                    for pattern, message in security_patterns.items():
                        if pattern in line_content:
                            security_issues.append({
                                'type': 'security_issue',
                                'file': str(file_path),
                                'line': i,
                                'message': message,
                                'severity': 'high'
                            })

            except Exception as e:
                self.logger.error(f"æ£€æµ‹å®‰å…¨é—®é¢˜å¤±è´¥ {file_path}: {e}")

        return security_issues

    def _calculate_debt_score(self, debt_metrics: Dict[str, Any]) -> float:
        """è®¡ç®—æŠ€æœ¯å€ºåŠ¡åˆ†æ•°"""
        total_issues = 0
        weighted_sum = 0

        severity_weights = {
            'low': 1,
            'medium': 5,
            'high': 10
        }

        for category, issues in debt_metrics.items():
            if isinstance(issues, list):
                for issue in issues:
                    severity = issue.get('severity', 'medium')
                    weight = severity_weights.get(severity, 5)
                    total_issues += 1
                    weighted_sum += weight

        if total_issues == 0:
            return 100.0  # æ— æŠ€æœ¯å€ºåŠ¡

        # å€ºåŠ¡åˆ†æ•°ï¼š100 - (åŠ æƒå¹³å‡ * 10)
        avg_weight = weighted_sum / total_issues
        debt_score = max(0, 100 - (avg_weight * 2))

        return round(debt_score, 2)


class PerformanceMetricsCollector:
    """æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self):
        self.logger = get_logger(self.__class__.__name__)

    def collect_system_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=1)

            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_info = {
                'total': memory.total,
                'available': memory.available,
                'percent': memory.percent,
                'used': memory.used,
                'free': memory.free
            }

            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            disk = psutil.disk_usage('/')
            disk_info = {
                'total': disk.total,
                'used': disk.used,
                'free': disk.free,
                'percent': (disk.used / disk.total) * 100
            }

            # ç½‘ç»œIO
            network = psutil.net_io_counters()
            network_info = {
                'bytes_sent': network.bytes_sent,
                'bytes_recv': network.bytes_recv,
                'packets_sent': network.packets_sent,
                'packets_recv': network.packets_recv
            }

            # è¿›ç¨‹ä¿¡æ¯
            process_count = len(psutil.pids())

            return {
                'timestamp': datetime.now().isoformat(),
                'cpu_percent': cpu_percent,
                'memory': memory_info,
                'disk': disk_info,
                'network': network_info,
                'process_count': process_count
            }

        except Exception as e:
            self.logger.error(f"æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {}

    def collect_application_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†åº”ç”¨ç¨‹åºæ€§èƒ½æŒ‡æ ‡"""
        try:
            current_process = psutil.Process()

            process_info = {
                'pid': current_process.pid,
                'memory_info': current_process.memory_info()._asdict(),
                'memory_percent': current_process.memory_percent(),
                'cpu_percent': current_process.cpu_percent(),
                'num_threads': current_process.num_threads(),
                'create_time': current_process.create_time(),
                'status': current_process.status()
            }

            return {
                'timestamp': datetime.now().isoformat(),
                'process': process_info
            }

        except Exception as e:
            self.logger.error(f"æ”¶é›†åº”ç”¨æŒ‡æ ‡å¤±è´¥: {e}")
            return {}


class AdvancedMetricsAnalyzer:
    """é«˜çº§åº¦é‡åˆ†æå™¨ä¸»ç±»"""

    def __init__(self):
        self.complexity_analyzer = CodeComplexityAnalyzer()
        self.debt_analyzer = TechnicalDebtAnalyzer()
        self.performance_collector = PerformanceMetricsCollector()
        self.logger = get_logger(self.__class__.__name__)

    def run_full_analysis(self, project_root: Path) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„é«˜çº§åº¦é‡åˆ†æ"""
        self.logger.info("å¼€å§‹é«˜çº§åº¦é‡åˆ†æ...")

        # åªåˆ†æsrcç›®å½•
        src_directory = project_root / "src"
        if not src_directory.exists():
            self.logger.warning(f"srcç›®å½•ä¸å­˜åœ¨: {src_directory}")
            return {}

        analysis_results = {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(project_root),
            'complexity_metrics': self.complexity_analyzer.analyze_directory_complexity(src_directory),
            'technical_debt': self.debt_analyzer.analyze_technical_debt(src_directory),
            'performance_metrics': {
                'system': self.performance_collector.collect_system_metrics(),
                'application': self.performance_collector.collect_application_metrics()
            }
        }

        # è®¡ç®—ç»¼åˆåˆ†æ•°
        overall_score = self._calculate_overall_score(analysis_results)
        analysis_results['overall_advanced_score'] = overall_score

        self.logger.info("é«˜çº§åº¦é‡åˆ†æå®Œæˆ")
        return analysis_results

    def _calculate_overall_score(self, results: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆé«˜çº§åº¦é‡åˆ†æ•°"""
        scores = []

        # å¤æ‚åº¦åˆ†æ•° (å¯ç»´æŠ¤æ€§æŒ‡æ•°)
        complexity = results.get('complexity_metrics', {}).get('summary', {})
        maintainability_index = complexity.get('avg_maintainability_index', 50)
        scores.append(maintainability_index)

        # æŠ€æœ¯å€ºåŠ¡åˆ†æ•°
        debt_score = results.get('technical_debt', {}).get('debt_score', 50)
        scores.append(debt_score)

        # æ€§èƒ½åˆ†æ•° (åŸºäºç³»ç»Ÿèµ„æºä½¿ç”¨)
        system_metrics = results.get('performance_metrics', {}).get('system', {})
        cpu_usage = system_metrics.get('cpu_percent', 50)
        memory_usage = system_metrics.get('memory', {}).get('percent', 50)
        performance_score = max(0, 100 - ((cpu_usage + memory_usage) / 2))
        scores.append(performance_score)

        # è®¡ç®—åŠ æƒå¹³å‡
        if scores:
            return round(sum(scores) / len(scores), 2)
        return 50.0


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    analyzer = AdvancedMetricsAnalyzer()
    project_root = Path(__file__).parent.parent.parent

    results = analyzer.run_full_analysis(project_root)

    print("ğŸ” é«˜çº§åº¦é‡åˆ†æç»“æœ:")
    print(f"ğŸ“Š ç»¼åˆåˆ†æ•°: {results.get('overall_advanced_score', 0):.2f}")

    complexity = results.get('complexity_metrics', {}).get('summary', {})
    print(f"ğŸ§® å¹³å‡åœˆå¤æ‚åº¦: {complexity.get('avg_cyclomatic_complexity', 0):.1f}")
    print(f"ğŸ§  å¹³å‡è®¤çŸ¥å¤æ‚åº¦: {complexity.get('avg_cognitive_complexity', 0):.1f}")
    print(f"ğŸ› ï¸ å¹³å‡å¯ç»´æŠ¤æ€§æŒ‡æ•°: {complexity.get('avg_maintainability_index', 0):.1f}")

    debt = results.get('technical_debt', {})
    print(f"âš ï¸ æŠ€æœ¯å€ºåŠ¡åˆ†æ•°: {debt.get('debt_score', 0):.1f}")

    system = results.get('performance_metrics', {}).get('system', {})
    print(f"ğŸ’» CPUä½¿ç”¨ç‡: {system.get('cpu_percent', 0):.1f}%")
    print(f"ğŸ§  å†…å­˜ä½¿ç”¨ç‡: {system.get('memory', {}).get('percent', 0):.1f}%")


if __name__ == "__main__":
    main()