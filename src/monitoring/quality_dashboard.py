#!/usr/bin/env python3
"""
è´¨é‡ç›‘æ§ä»ªè¡¨æ¿
è‡ªåŠ¨åŒ–è´¨é‡ç›‘æ§å’ŒæŠ¥å‘Šç”Ÿæˆç³»ç»Ÿ
"""

import asyncio
import json
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Any

import psutil

from src.core.logger import get_logger

logger = get_logger(__name__)


class QualityMetrics:
    """è´¨é‡æŒ‡æ ‡æ•°æ®ç±»"""

    def __init__(self):
        self.timestamp = datetime.now()
        self.code_quality_score = 0.0
        self.test_coverage = 0.0
        self.test_pass_rate = 0.0
        self.security_issues = 0
        self.performance_score = 0.0
        self.technical_debt = 0
        self.build_status = "unknown"
        self.metrics = {}


class QualityMonitor:
    """è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = (
            Path(project_root) if project_root else Path(__file__).parent.parent.parent
        )
        self.metrics_history: list[QualityMetrics] = []
        self.reports_dir = self.project_root / "reports" / "quality"
        self.reports_dir.mkdir(parents=True, exist_ok=True)

    async def collect_all_metrics(self) -> QualityMetrics:
        """æ”¶é›†æ‰€æœ‰è´¨é‡æŒ‡æ ‡"""
        metrics = QualityMetrics()

        logger.info("å¼€å§‹æ”¶é›†è´¨é‡æŒ‡æ ‡...")

        # å¹¶è¡Œæ”¶é›†å„é¡¹æŒ‡æ ‡
        tasks = [
            self._collect_code_quality(metrics),
            self._collect_test_metrics(metrics),
            self._collect_security_metrics(metrics),
            self._collect_performance_metrics(metrics),
            self._collect_technical_debt(metrics),
            self._collect_build_status(metrics),
        ]

        await asyncio.gather(*tasks, return_exceptions=True)

        # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        metrics.code_quality_score = self._calculate_overall_score(metrics)

        # ä¿å­˜åˆ°å†å²è®°å½•
        self.metrics_history.append(metrics)

        logger.info(f"è´¨é‡æŒ‡æ ‡æ”¶é›†å®Œæˆï¼Œç»¼åˆè¯„åˆ†: {metrics.code_quality_score:.1f}")
        return metrics

    async def _collect_code_quality(self, metrics: QualityMetrics):
        """æ”¶é›†ä»£ç è´¨é‡æŒ‡æ ‡"""
        try:
            # è¿è¡ŒRuffæ£€æŸ¥
            result = await self._run_command_async(
                ["ruff", "check", "src/", "tests/", "--output-format=json"], timeout=60
            )

            if result and result.returncode == 0:
                ruff_data = json.loads(result.stdout)
                error_count = len(ruff_data.get("results", []))
                warning_count = len(
                    [
                        r
                        for r in ruff_data.get("results", [])
                        if r.get("type") == "warning"
                    ]
                )

                # è®¡ç®—ä»£ç è´¨é‡åˆ†æ•°
                total_issues = error_count + warning_count
                if total_issues == 0:
                    metrics.metrics["code_quality_score"] = 100
                else:
                    # åŸºç¡€åˆ†100åˆ†ï¼Œæ¯ä¸ªé—®é¢˜æ‰£åˆ†
                    base_score = 100
                    deduction = min(90, total_issues * 0.5)  # æœ€å¤šæ‰£90åˆ†
                    metrics.metrics["code_quality_score"] = max(
                        10, base_score - deduction
                    )

                metrics.metrics["ruff_errors"] = error_count
                metrics.metrics["ruff_warnings"] = warning_count
                logger.info(
                    f"ä»£ç è´¨é‡æ£€æŸ¥å®Œæˆ: {error_count} é”™è¯¯, {warning_count} è­¦å‘Š"
                )

        except Exception as e:
            logger.error(f"ä»£ç è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            metrics.metrics["code_quality_score"] = 50

    async def _collect_test_metrics(self, metrics: QualityMetrics):
        """æ”¶é›†æµ‹è¯•æŒ‡æ ‡"""
        try:
            # è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†è¦†ç›–ç‡
            result = await self._run_command_async(
                [
                    "pytest",
                    "tests/unit/",
                    "--cov=src",
                    "--cov-report=json",
                    "--cov-report=term-missing",
                ],
                timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
            )

            if result:
                # è§£ææµ‹è¯•ç»“æœ
                output = result.stdout + result.stderr
                lines = output.split("\n")

                # æŸ¥æ‰¾è¦†ç›–ç‡ä¿¡æ¯
                for line in lines:
                    if "TOTAL" in line and "%" in line:
                        try:
                            parts = line.split()
                            if len(parts) >= 4:
                                coverage_part = parts[3]
                                coverage = float(coverage_part.replace("%", ""))
                                metrics.test_coverage = coverage
                                break
                        except (ValueError, IndexError):
                            continue

                # æŸ¥æ‰¾æµ‹è¯•é€šè¿‡ç‡
                passed = 0
                failed = 0
                total = 0

                for line in lines:
                    if "passed" in line and "failed" in line and "error" in line:
                        try:
                            parts = line.split()
                            for part in parts:
                                if part.isdigit():
                                    total += int(part)
                                elif part.endswith("passed"):
                                    passed += int(part.replace("passed", ""))
                                elif part.endswith("failed"):
                                    failed += int(part.replace("failed", ""))
                                elif part.endswith("error"):
                                    failed += int(part.replace("error", ""))
                        except ValueError:
                            continue

                if total > 0:
                    metrics.test_pass_rate = (passed / total) * 100

                logger.info(
                    f"æµ‹è¯•æŒ‡æ ‡æ”¶é›†å®Œæˆ: è¦†ç›–ç‡ {metrics.test_coverage}%, é€šè¿‡ç‡ {metrics.test_pass_rate}%"
                )

        except Exception as e:
            logger.error(f"æµ‹è¯•æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            metrics.test_coverage = 0
            metrics.test_pass_rate = 0

    async def _collect_security_metrics(self, metrics: QualityMetrics):
        """æ”¶é›†å®‰å…¨æŒ‡æ ‡"""
        try:
            # è¿è¡ŒBanditå®‰å…¨æ‰«æ
            result = await self._run_command_async(
                ["bandit", "-r", "src/", "-f", "json"], timeout=120
            )

            if result and result.returncode == 0:
                bandit_data = json.loads(result.stdout)
                high_severity = bandit_data["metrics"]["_totals"]["SEVERITY.HIGH"]
                medium_severity = bandit_data["metrics"]["_totals"]["SEVERITY.MEDIUM"]
                low_severity = bandit_data["metrics"]["_totals"]["SEVERITY.LOW"]

                metrics.security_issues = high_severity + medium_severity
                metrics.metrics["security_high"] = high_severity
                metrics.metrics["security_medium"] = medium_severity
                metrics.metrics["security_low"] = low_severity

                logger.info(
                    f"å®‰å…¨æ‰«æå®Œæˆ: é«˜å± {high_severity}, ä¸­å± {medium_severity}, ä½å± {low_severity}"
                )

        except Exception as e:
            logger.error(f"å®‰å…¨æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            metrics.security_issues = 0

    async def _collect_performance_metrics(self, metrics: QualityMetrics):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
            result = await self._run_command_async(
                ["python", "src/monitoring/performance_profiler.py"], timeout=180
            )

            if result and result.returncode == 0:
                # è§£ææ€§èƒ½åŸºå‡†ç»“æœ
                output = result.stdout
                if "performance_benchmark_results.json" in output:
                    # å¦‚æœç”Ÿæˆäº†ç»“æœæ–‡ä»¶ï¼Œè§£æå®ƒ
                    pass  # è¿™é‡Œå¯ä»¥æ·»åŠ JSONè§£æé€»è¾‘

                # åŸºäºç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µè®¡ç®—æ€§èƒ½åˆ†æ•°
                cpu_percent = psutil.cpu_percent(interval=1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage("/")

                # æ€§èƒ½åˆ†æ•°è®¡ç®—
                performance_score = 100

                # CPUä½¿ç”¨ç‡å½±å“
                if cpu_percent > 80:
                    performance_score -= 20
                elif cpu_percent > 60:
                    performance_score -= 10

                # å†…å­˜ä½¿ç”¨ç‡å½±å“
                if memory.percent > 85:
                    performance_score -= 20
                elif memory.percent > 70:
                    performance_score -= 10

                # ç£ç›˜ç©ºé—´å½±å“
                disk_percent = (disk.used / disk.total) * 100
                if disk_percent > 90:
                    performance_score -= 15

                metrics.performance_score = max(0, performance_score)
                metrics.metrics["cpu_usage"] = cpu_percent
                metrics.metrics["memory_usage"] = memory.percent
                metrics.metrics["disk_usage"] = disk_percent

                logger.info(
                    f"æ€§èƒ½æŒ‡æ ‡æ”¶é›†å®Œæˆ: CPU {cpu_percent}%, å†…å­˜ {memory.percent}%, ç£ç›˜ {disk_percent:.1f}%"
                )

        except Exception as e:
            logger.error(f"æ€§èƒ½æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
            metrics.performance_score = 70

    async def _collect_technical_debt(self, metrics: QualityMetrics):
        """æ”¶é›†æŠ€æœ¯å€ºåŠ¡æŒ‡æ ‡"""
        try:
            # ä¼°ç®—æŠ€æœ¯å€ºåŠ¡åˆ†æ•°
            debt_score = 0

            # åŸºäºä»£ç è´¨é‡é—®é¢˜çš„å€ºåŠ¡
            if "code_quality_score" in metrics.metrics:
                quality_score = metrics.metrics["code_quality_score"]
                if quality_score < 80:
                    debt_score += (80 - quality_score) * 2

            # åŸºäºæµ‹è¯•è¦†ç›–ç‡çš„å€ºåŠ¡
            if metrics.test_coverage < 70:
                debt_score += (70 - metrics.test_coverage) * 1.5

            # åŸºäºå®‰å…¨é—®é¢˜çš„å€ºåŠ¡
            if metrics.security_issues > 0:
                debt_score += metrics.security_issues * 5

            # åŸºäºå¤æ‚åº¦çš„å€ºåŠ¡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            complexity_score = await self._calculate_complexity_score()
            if complexity_score > 100:
                debt_score += (complexity_score - 100) * 0.5

            metrics.technical_debt = debt_score
            metrics.metrics["complexity_score"] = complexity_score

            logger.info(f"æŠ€æœ¯å€ºåŠ¡ä¼°ç®—å®Œæˆ: {debt_score:.1f} åˆ†")

        except Exception as e:
            logger.error(f"æŠ€æœ¯å€ºåŠ¡æ”¶é›†å¤±è´¥: {e}")
            metrics.technical_debt = 0

    async def _collect_build_status(self, metrics: QualityMetrics):
        """æ”¶é›†æ„å»ºçŠ¶æ€"""
        try:
            # æ£€æŸ¥æœ€è¿‘çš„æ„å»ºçŠ¶æ€
            build_success = await self._check_build_status()
            metrics.build_status = "success" if build_success else "failed"

            logger.info(f"æ„å»ºçŠ¶æ€æ£€æŸ¥å®Œæˆ: {metrics.build_status}")

        except Exception as e:
            logger.error(f"æ„å»ºçŠ¶æ€æ£€æŸ¥å¤±è´¥: {e}")
            metrics.build_status = "unknown"

    async def _run_command_async(
        self, cmd: list[str], timeout: int = 60
    ) -> subprocess.CompletedProcess | None:
        """å¼‚æ­¥è¿è¡Œå‘½ä»¤"""
        try:
            process = await asyncio.create_subprocess_exec(
                *cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.project_root,
            )

            stdout, stderr = await asyncio.wait_for(
                process.communicate(), timeout=timeout
            )

            return subprocess.CompletedProcess(
                args=cmd,
                returncode=process.returncode,
                stdout=stdout.decode("utf-8"),
                stderr=stderr.decode("utf-8"),
            )

        except TimeoutError:
            logger.error(f"å‘½ä»¤æ‰§è¡Œè¶…æ—¶: {' '.join(cmd)}")
            return None
        except Exception as e:
            logger.error(f"å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e}")
            return None

    async def _calculate_complexity_score(self) -> float:
        """è®¡ç®—ä»£ç å¤æ‚åº¦åˆ†æ•°"""
        try:
            # ç®€åŒ–çš„å¤æ‚åº¦è®¡ç®—
            # å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨radonç­‰å·¥å…·
            total_files = 0
            total_lines = 0

            for py_file in self.project_root.rglob("*.py"):
                if "test" not in str(py_file):  # æ’é™¤æµ‹è¯•æ–‡ä»¶
                    total_files += 1
                    try:
                        lines = len(py_file.read_text(encoding="utf-8").split("\n"))
                        total_lines += lines
                    except:
                        pass

            if total_files == 0:
                return 0

            avg_lines_per_file = total_lines / total_files

            # å¤æ‚åº¦è¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆï¼‰
            complexity_score = avg_lines_per_file / 5  # æ¯5è¡Œ1åˆ†
            return min(200, complexity_score)  # é™åˆ¶æœ€é«˜200åˆ†

        except Exception as e:
            logger.error(f"å¤æ‚åº¦è®¡ç®—å¤±è´¥: {e}")
            return 0

    def _calculate_overall_score(self, metrics: QualityMetrics) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        weights = {
            "code_quality": 0.25,
            "test_coverage": 0.20,
            "security": 0.20,
            "performance": 0.15,
            "technical_debt": 0.20,
        }

        # ä»£ç è´¨é‡åˆ†æ•°
        code_quality = metrics.metrics.get("code_quality_score", 50)

        # æµ‹è¯•è¦†ç›–ç‡åˆ†æ•°
        coverage_score = min(100, metrics.test_coverage * 1.25)  # 80%è¦†ç›–ç‡ = 100åˆ†

        # å®‰å…¨åˆ†æ•°ï¼ˆå®‰å…¨é—®é¢˜è¶Šå°‘åˆ†æ•°è¶Šé«˜ï¼‰
        security_issues = metrics.security_issues
        security_score = max(0, 100 - security_issues * 10)

        # æ€§èƒ½åˆ†æ•°
        performance_score = metrics.performance_score

        # æŠ€æœ¯å€ºåŠ¡åˆ†æ•°ï¼ˆå€ºåŠ¡è¶Šå°‘åˆ†æ•°è¶Šé«˜ï¼‰
        debt_score = max(0, 100 - metrics.technical_debt)

        overall_score = (
            code_quality * weights["code_quality"]
            + coverage_score * weights["test_coverage"]
            + security_score * weights["security"]
            + performance_score * weights["performance"]
            + debt_score * weights["technical_debt"]
        )

        return round(overall_score, 1)

    async def generate_quality_report(self) -> dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        metrics = await self.collect_all_metrics()

        report = {
            "timestamp": metrics.timestamp.isoformat(),
            "overall_score": metrics.code_quality_score,
            "status": self._get_quality_status(metrics.code_quality_score),
            "metrics": {
                "code_quality": {
                    "score": metrics.metrics.get("code_quality_score", 0),
                    "errors": metrics.metrics.get("ruff_errors", 0),
                    "warnings": metrics.metrics.get("ruff_warnings", 0),
                },
                "testing": {
                    "coverage": metrics.test_coverage,
                    "pass_rate": metrics.test_pass_rate,
                },
                "security": {
                    "total_issues": metrics.security_issues,
                    "high_severity": metrics.metrics.get("security_high", 0),
                    "medium_severity": metrics.metrics.get("security_medium", 0),
                    "low_severity": metrics.metrics.get("security_low", 0),
                },
                "performance": {
                    "score": metrics.performance_score,
                    "cpu_usage": metrics.metrics.get("cpu_usage", 0),
                    "memory_usage": metrics.metrics.get("memory_usage", 0),
                    "disk_usage": metrics.metrics.get("disk_usage", 0),
                },
                "technical_debt": {
                    "score": metrics.technical_debt,
                    "complexity": metrics.metrics.get("complexity_score", 0),
                },
                "build": {"status": metrics.build_status},
            },
            "recommendations": self._generate_recommendations(metrics),
            "trends": self._analyze_trends(),
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = (
            self.reports_dir
            / f"quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report

    def _get_quality_status(self, score: float) -> str:
        """æ ¹æ®åˆ†æ•°è·å–è´¨é‡çŠ¶æ€"""
        if score >= 90:
            return "excellent"
        elif score >= 80:
            return "good"
        elif score >= 70:
            return "acceptable"
        elif score >= 60:
            return "poor"
        else:
            return "critical"

    def _generate_recommendations(self, metrics: QualityMetrics) -> list[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        if metrics.code_quality_score < 80:
            recommendations.append("å»ºè®®ä¿®å¤ä»£ç è´¨é‡é—®é¢˜ï¼Œè¿è¡Œ 'make fix-code'")

        if metrics.test_coverage < 30:
            recommendations.append("æµ‹è¯•è¦†ç›–ç‡ä¸è¶³30%ï¼Œå»ºè®®å¢åŠ å•å…ƒæµ‹è¯•")

        if metrics.security_issues > 0:
            recommendations.append(
                f"å‘ç° {metrics.security_issues} ä¸ªå®‰å…¨é—®é¢˜ï¼Œå»ºè®®ç«‹å³ä¿®å¤"
            )

        if metrics.performance_score < 70:
            recommendations.append("æ€§èƒ½åˆ†æ•°è¾ƒä½ï¼Œå»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–")

        if metrics.technical_debt > 50:
            recommendations.append("æŠ€æœ¯å€ºåŠ¡è¾ƒé«˜ï¼Œå»ºè®®è¿›è¡Œä»£ç é‡æ„")

        if metrics.build_status != "success":
            recommendations.append("æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥CI/CDé…ç½®")

        if not recommendations:
            recommendations.append("è´¨é‡æŒ‡æ ‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")

        return recommendations

    def _analyze_trends(self) -> dict[str, Any]:
        """åˆ†æè´¨é‡è¶‹åŠ¿"""
        if len(self.metrics_history) < 2:
            return {"message": "æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿"}

        recent_metrics = self.metrics_history[-7:]  # æœ€è¿‘7å¤©
        previous_metrics = (
            self.metrics_history[-14:-7] if len(self.metrics_history) >= 14 else []
        )

        if not previous_metrics:
            return {"message": "å†å²æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¶‹åŠ¿"}

        recent_avg = sum(m.code_quality_score for m in recent_metrics) / len(
            recent_metrics
        )
        previous_avg = sum(m.code_quality_score for m in previous_metrics) / len(
            previous_metrics
        )

        trend = "stable"
        if recent_avg > previous_avg + 5:
            trend = "improving"
        elif recent_avg < previous_avg - 5:
            trend = "declining"

        return {
            "trend": trend,
            "recent_average": round(recent_avg, 1),
            "previous_average": round(previous_avg, 1),
            "change": round(recent_avg - previous_avg, 1),
            "data_points": len(self.metrics_history),
        }

    async def start_monitoring(self, interval_minutes: int = 60):
        """å¯åŠ¨æŒç»­ç›‘æ§"""
        logger.info(f"å¯åŠ¨è´¨é‡ç›‘æ§ï¼Œé—´éš”: {interval_minutes} åˆ†é’Ÿ")

        while True:
            try:
                await self.generate_quality_report()
                await asyncio.sleep(interval_minutes * 60)
            except KeyboardInterrupt:
                logger.info("è´¨é‡ç›‘æ§å·²åœæ­¢")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                await asyncio.sleep(300)  # å‡ºé”™åç­‰å¾…5åˆ†é’Ÿå†é‡è¯•


async def main():
    """ä¸»å‡½æ•°"""
    monitor = QualityMonitor()

    # ç”Ÿæˆä¸€æ¬¡è´¨é‡æŠ¥å‘Š
    report = await monitor.generate_quality_report()

    print("\n" + "=" * 60)
    print("ğŸ—ï¸ é¡¹ç›®è´¨é‡ç›‘æ§æŠ¥å‘Š")
    print("=" * 60)
    print(f"â° æ—¶é—´: {report['timestamp']}")
    print(f"ğŸ“Š ç»¼åˆè¯„åˆ†: {report['overall_score']} ({report['status']})")
    print(f"ğŸ§ª æµ‹è¯•è¦†ç›–ç‡: {report['metrics']['testing']['coverage']:.1f}%")
    print(f"ğŸ”’ å®‰å…¨é—®é¢˜: {report['metrics']['security']['total_issues']}")
    print(f"âš¡ æ€§èƒ½åˆ†æ•°: {report['metrics']['performance']['score']:.1f}")
    print("=" * 60)

    print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    for i, rec in enumerate(report["recommendations"], 1):
        print(f"{i}. {rec}")

    print(f"\nğŸ“ˆ è¶‹åŠ¿åˆ†æ: {report['trends']}")

    # å¯åŠ¨æŒç»­ç›‘æ§ï¼ˆå¯é€‰ï¼‰
    # await monitor.start_monitoring(interval_minutes=60)


if __name__ == "__main__":
    asyncio.run(main())
