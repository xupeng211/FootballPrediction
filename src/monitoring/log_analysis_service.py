"""
å¢å¼ºæ—¥å¿—åˆ†ææœåŠ¡
Enhanced Log Analysis Service

æä¾›å®Œæ•´çš„æ—¥å¿—èšåˆã€åˆ†æå’Œç›‘æ§åŠŸèƒ½ã€‚
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

from .log_aggregator import (
    AccessLogParser,
    ErrorLogParser,
    LogCollector,
    LogLevel,
    LogQuery,
    LogSource,
    PythonLogParser,
)

logger = logging.getLogger(__name__)


class LogAnalysisService:
    """æ—¥å¿—åˆ†ææœåŠ¡"""

    def __init__(self):
        self.collector = LogCollector()
        self.analysis_results: dict[str, Any] = {}
        self._running = False

    async def start(self):
        """å¯åŠ¨æ—¥å¿—åˆ†ææœåŠ¡"""
        if self._running:
            return

        self._running = True

        # æ·»åŠ è§£æå™¨
        self.collector.add_parser(PythonLogParser())
        self.collector.add_parser(AccessLogParser())
        self.collector.add_parser(ErrorLogParser())

        # å¯åŠ¨æ”¶é›†å™¨
        await self.collector.start_collection()

        logger.info("æ—¥å¿—åˆ†ææœåŠ¡å·²å¯åŠ¨")

    async def stop(self):
        """åœæ­¢æ—¥å¿—åˆ†ææœåŠ¡"""
        self._running = False
        await self.collector.stop_collection()
        logger.info("æ—¥å¿—åˆ†ææœåŠ¡å·²åœæ­¢")

    async def analyze_log_file(self, file_path: str, source_type: str = "application"):
        """åˆ†ææ—¥å¿—æ–‡ä»¶"""
        if not Path(file_path).exists():
            logger.warning(f"æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}

        try:
            source_map = {
                "application": LogSource.APPLICATION,
                "access": LogSource.ACCESS,
                "error": LogSource.ERROR,
                "system": LogSource.SYSTEM,
                "audit": LogSource.AUDIT,
            }
            source = source_map.get(source_type.lower(), LogSource.APPLICATION)

            # ç›‘è§†æ–‡ä»¶
            await self.collector.watch_file(file_path, source)

            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ–‡ä»¶å†…å®¹è¢«å¤„ç†
            await asyncio.sleep(2)

            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = await self._generate_analysis_report()

            return report

        except Exception as e:
            logger.error(f"åˆ†ææ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
            return {"error": str(e)}

    async def analyze_log_string(
        self, log_content: str, source_type: str = "application"
    ):
        """åˆ†ææ—¥å¿—å­—ç¬¦ä¸²å†…å®¹"""
        try:
            source_map = {
                "application": LogSource.APPLICATION,
                "access": LogSource.ACCESS,
                "error": LogSource.ERROR,
                "system": LogSource.SYSTEM,
                "audit": LogSource.AUDIT,
            }
            source = source_map.get(source_type.lower(), LogSource.APPLICATION)

            # å¤„ç†æ¯ä¸€è¡Œæ—¥å¿—
            for line in log_content.strip().split("\n"):
                if line.strip():
                    await self.collector.parse_and_store(line, source)

            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = await self._generate_analysis_report()

            return report

        except Exception as e:
            logger.error(f"åˆ†ææ—¥å¿—å†…å®¹å¤±è´¥: {e}")
            return {"error": str(e)}

    async def _generate_analysis_report(self) -> dict[str, Any]:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        logs = list(self.collector.log_buffer)

        if not logs:
            return {"message": "æ²¡æœ‰æ—¥å¿—æ•°æ®å¯åˆ†æ"}

        # åŸºç¡€ç»Ÿè®¡
        total_logs = len(logs)
        error_count = len([log for log in logs if log.level == LogLevel.ERROR])
        warning_count = len([log for log in logs if log.level == LogLevel.WARNING])
        info_count = len([log for log in logs if log.level == LogLevel.INFO])

        # é”™è¯¯åˆ†æ
        error_logs = [log for log in logs if log.level == LogLevel.ERROR]
        common_errors = self._analyze_common_errors(error_logs)

        # æ—¶é—´åˆ†æ
        recent_errors = self._analyze_recent_errors(logs)

        # æºåˆ†æ
        source_analysis = self._analyze_log_sources(logs)

        report = {
            "summary": {
                "total_logs": total_logs,
                "error_count": error_count,
                "warning_count": warning_count,
                "info_count": info_count,
                "error_rate": (
                    f"{(error_count/total_logs*100):.2f}%" if total_logs > 0 else "0%"
                ),
            },
            "error_analysis": {
                "common_errors": common_errors,
                "recent_errors": recent_errors,
                "critical_errors": [
                    log.message for log in error_logs if "CRITICAL" in log.message
                ],
            },
            "source_analysis": source_analysis,
            "recommendations": self._generate_recommendations(
                error_count, warning_count, total_logs
            ),
            "timestamp": datetime.now().isoformat(),
        }

        self.analysis_results = report
        return report

    def _analyze_common_errors(self, error_logs: list) -> dict[str, int]:
        """åˆ†æå¸¸è§é”™è¯¯"""
        error_messages = [log.message for log in error_logs]
        error_counts = {}

        for message in error_messages:
            # æå–é”™è¯¯ç±»å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
            if "ConnectionError" in message:
                error_type = "è¿æ¥é”™è¯¯"
            elif "TimeoutError" in message:
                error_type = "è¶…æ—¶é”™è¯¯"
            elif "ValueError" in message:
                error_type = "å€¼é”™è¯¯"
            elif "KeyError" in message:
                error_type = "é”®é”™è¯¯"
            elif "AttributeError" in message:
                error_type = "å±æ€§é”™è¯¯"
            else:
                error_type = "å…¶ä»–é”™è¯¯"

            error_counts[error_type] = error_counts.get(error_type, 0) + 1

        return error_counts

    def _analyze_recent_errors(self, logs: list) -> dict[str, Any]:
        """åˆ†ææœ€è¿‘çš„é”™è¯¯"""
        now = datetime.now()
        recent_threshold = now - timedelta(hours=1)

        recent_error_logs = [
            log
            for log in logs
            if log.level == LogLevel.ERROR and log.timestamp >= recent_threshold
        ]

        return {
            "count": len(recent_error_logs),
            "rate": f"{len(recent_error_logs)} é”™è¯¯/å°æ—¶",
            "latest": recent_error_logs[-1].message if recent_error_logs else None,
        }

    def _analyze_log_sources(self, logs: list) -> dict[str, Any]:
        """åˆ†ææ—¥å¿—æ¥æº"""
        source_counts = {}

        for log in logs:
            source_name = (
                log.source.value if hasattr(log.source, "value") else str(log.source)
            )
            source_counts[source_name] = source_counts.get(source_name, 0) + 1

        return {
            "sources": source_counts,
            "most_active": (
                max(source_counts, key=source_counts.get) if source_counts else None
            ),
        }

    def _generate_recommendations(
        self, error_count: int, warning_count: int, total_logs: int
    ) -> list[str]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []

        if error_count > 0:
            error_rate = (error_count / total_logs) * 100 if total_logs > 0 else 0
            if error_rate > 10:
                recommendations.append("âŒ é”™è¯¯ç‡è¿‡é«˜ï¼Œéœ€è¦ç«‹å³å…³æ³¨ç³»ç»Ÿç¨³å®šæ€§")
            elif error_rate > 5:
                recommendations.append("âš ï¸ é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤å…³é”®é”™è¯¯")
            else:
                recommendations.append("âœ… é”™è¯¯ç‡åœ¨å¯æ¥å—èŒƒå›´å†…")

        if warning_count > error_count * 2:
            recommendations.append("âš ï¸ è­¦å‘Šæ•°é‡è¾ƒå¤šï¼Œå¯èƒ½å­˜åœ¨æ½œåœ¨é—®é¢˜")

        if total_logs == 0:
            recommendations.append("ğŸ“‹ æ²¡æœ‰æ£€æµ‹åˆ°æ—¥å¿—æ•°æ®ï¼Œè¯·æ£€æŸ¥æ—¥å¿—é…ç½®")

        if len(recommendations) == 0:
            recommendations.append("âœ… ç³»ç»Ÿè¿è¡ŒçŠ¶æ€è‰¯å¥½")

        return recommendations

    def get_logs_by_level(self, level: LogLevel, limit: int = 100) -> list:
        """æŒ‰çº§åˆ«è·å–æ—¥å¿—"""
        query = LogQuery(level=level, limit=limit)
        return self.collector.get_logs(query)

    def get_logs_by_time_range(self, start_time: datetime, end_time: datetime) -> list:
        """æŒ‰æ—¶é—´èŒƒå›´è·å–æ—¥å¿—"""
        query = LogQuery(start_time=start_time, end_time=end_time)
        return self.collector.get_logs(query)

    def export_analysis_report(self, file_path: str):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        if not self.analysis_results:
            logger.warning("æ²¡æœ‰åˆ†ææŠ¥å‘Šå¯å¯¼å‡º")
            return False

        try:
            with open(file_path, "w", encoding="utf-8") as f:
                json.dump(
                    self.analysis_results, f, indent=2, ensure_ascii=False, default=str
                )

            logger.info(f"åˆ†ææŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {file_path}")
            return True

        except Exception as e:
            logger.error(f"å¯¼å‡ºåˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return False


# å…¨å±€æ—¥å¿—åˆ†ææœåŠ¡å®ä¾‹
_log_analysis_service: LogAnalysisService | None = None


async def get_log_analysis_service() -> LogAnalysisService:
    """è·å–å…¨å±€æ—¥å¿—åˆ†ææœåŠ¡å®ä¾‹"""
    global _log_analysis_service

    if _log_analysis_service is None:
        _log_analysis_service = LogAnalysisService()
        await _log_analysis_service.start()

    return _log_analysis_service
