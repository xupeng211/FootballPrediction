"""
Titan007 è‡ªåŠ¨åŒ–è°ƒåº¦å·¥ä½œæµ
Titan007 Automated Scheduling Flow

å®ç°æ™ºèƒ½è°ƒåº¦ç­–ç•¥ï¼š
- å¸¸è§„æ¨¡å¼ï¼šæ¯å¤© 08:00 è¿è¡Œï¼Œè·å–åˆç›˜æ•°æ®
- ä¸´åœºæ¨¡å¼ï¼šç­›é€‰æœªæ¥ 2 å°æ—¶å†…å¼€èµ›çš„æ¯”èµ›ï¼Œæ¯ 10 åˆ†é’Ÿè¿è¡Œä¸€æ¬¡é‡‡é›†
- æ™ºèƒ½é‡è¯•å’Œé”™è¯¯å¤„ç†
- å®Œæ•´çš„æ—¥å¿—è®°å½•å’Œç›‘æ§

ä½¿ç”¨ç¤ºä¾‹:
    # è¿è¡Œå¸¸è§„æ¨¡å¼
    python scripts/deploy_flow.py --run

    # è¿è¡Œä¸´åœºæ¨¡å¼
    python scripts/deploy_flow.py --run --live
"""

from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional

from prefect import flow
from prefect.server.schemas.schedules import CronSchedule

from src.tasks.titan_tasks import (
    fetch_fixtures,
    align_ids,
    batch_collect_odds,
    cleanup_history_data,
)
from src.config.titan_settings import get_titan_settings

# è®¾ç½®æ—¥å¿—
import logging

logger = logging.getLogger(__name__)


def _filter_upcoming_matches(
    fixtures: List[Dict[str, Any]], hours_ahead: int = 2
) -> List[Dict[str, Any]]:
    """
    ç­›é€‰æœªæ¥æŒ‡å®šå°æ—¶å†…å¼€èµ›çš„æ¯”èµ›

    Args:
        fixtures: æ¯”èµ›åˆ—è¡¨
        hours_ahead: å°æ—¶æ•°ï¼ˆé»˜è®¤2å°æ—¶ï¼‰

    Returns:
        List[Dict[str, Any]]: å³å°†å¼€å§‹çš„æ¯”èµ›åˆ—è¡¨
    """
    try:
        now = datetime.now()
        cutoff_time = now + timedelta(hours=hours_ahead)

        upcoming_matches = []
        for fixture in fixtures:
            match_date = fixture.get("match_date")
            if match_date:
                # è§£ææ¯”èµ›æ—¥æœŸ
                if isinstance(match_date, str):
                    match_datetime = datetime.fromisoformat(
                        match_date.replace("Z", "+00:00")
                    )
                else:
                    match_datetime = match_date

                # ç­›é€‰å³å°†å¼€å§‹çš„æ¯”èµ›
                if now <= match_datetime <= cutoff_time:
                    upcoming_matches.append(fixture)

        logger.info(
            "ç­›é€‰å³å°†å¼€å§‹çš„æ¯”èµ›",
            extra={
                "total_fixtures": len(fixtures),
                "upcoming_matches": len(upcoming_matches),
                "hours_ahead": hours_ahead,
                "cutoff_time": cutoff_time.isoformat(),
            },
        )

        return upcoming_matches

    except Exception as e:
        logger.error(
            "ç­›é€‰å³å°†å¼€å§‹çš„æ¯”èµ›å¤±è´¥",
            extra={"error_type": type(e).__name__, "error_message": str(e)},
        )
        return []


@flow(
    name="Titan007 å¸¸è§„æ•°æ®é‡‡é›†",
    description="æ¯å¤©è¿è¡Œä¸€æ¬¡ï¼Œè·å–å½“å¤©æ¯”èµ›çš„åˆç›˜æ•°æ®",
    retries=1,
    retry_delay_seconds=300,
)
async def titan_regular_flow(
    start_date: Optional[str] = None,
    days_ahead: int = 1,
    batch_size: int = 20,
    max_concurrency: int = 15,
) -> Dict[str, Any]:
    """
    Titan007 å¸¸è§„æ•°æ®é‡‡é›†å·¥ä½œæµ

    æµç¨‹ï¼š
    1. è·å–å½“å¤©èµ›ç¨‹
    2. ID å¯¹é½
    3. æ‰¹é‡é‡‡é›†èµ”ç‡æ•°æ®
    4. ï¼ˆå¯é€‰ï¼‰æ¸…ç†å†å²æ•°æ®

    Args:
        start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼ "YYYY-MM-DD"ï¼Œé»˜è®¤ä¸ºä»Šå¤©
        days_ahead: æå‰å¤šå°‘å¤©çš„æ¯”èµ›
        batch_size: IDå¯¹é½çš„æ‰¹å¤„ç†å¤§å°
        max_concurrency: å¹¶å‘é‡‡é›†æ¯”èµ›æ•°

    Returns:
        Dict[str, Any]: æ‰§è¡Œç»“æœç»Ÿè®¡
    """
    run_logger = flow.get_run_logger()
    settings = get_titan_settings()

    run_logger.info(
        "ğŸš€ å¼€å§‹ Titan007 å¸¸è§„æ•°æ®é‡‡é›†å·¥ä½œæµ",
        extra={
            "start_date": start_date,
            "days_ahead": days_ahead,
            "batch_size": batch_size,
            "max_concurrency": max_concurrency,
            "flow_run_id": flow.id,
        },
    )

    try:
        flow_start_time = datetime.now()
        results = {
            "flow_type": "regular",
            "start_time": flow_start_time.isoformat(),
            "fixtures": 0,
            "aligned": 0,
            "collected": 0,
            "errors": 0,
            "total_odds": 0,
        }

        # Step 1: è·å–èµ›ç¨‹
        run_logger.info("ğŸ“‹ Step 1: è·å–å½“å¤©èµ›ç¨‹")
        fixtures = await fetch_fixtures(start_date=start_date, days_ahead=days_ahead)
        results["fixtures"] = len(fixtures)

        if not fixtures:
            run_logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¯”èµ›æ•°æ®ï¼Œæµç¨‹ç»“æŸ")
            return results

        # Step 2: ID å¯¹é½
        run_logger.info("ğŸ”— Step 2: æ‰§è¡Œ ID å¯¹é½")
        aligned_matches = await align_ids(fixtures, batch_size=batch_size)
        results["aligned"] = len(aligned_matches)

        if not aligned_matches:
            run_logger.warning("âš ï¸ æ²¡æœ‰å¯¹é½æˆåŠŸçš„æ¯”èµ›ï¼Œæµç¨‹ç»“æŸ")
            return results

        # Step 3: æ‰¹é‡é‡‡é›†èµ”ç‡
        run_logger.info("ğŸ“Š Step 3: æ‰¹é‡é‡‡é›†èµ”ç‡æ•°æ®")
        collection_results = await batch_collect_odds(
            matches=aligned_matches, max_concurrency=max_concurrency
        )
        results["collected"] = len(collection_results)

        # ç»Ÿè®¡èµ”ç‡é‡‡é›†ç»“æœ
        total_odds = sum(r.get("success_count", 0) for r in collection_results)
        results["total_odds"] = total_odds
        results["errors"] = sum(r.get("error_count", 0) for r in collection_results)

        # Step 4: æ¸…ç†å†å²æ•°æ®ï¼ˆå¯é€‰ï¼‰
        try:
            run_logger.info("ğŸ§¹ Step 4: æ¸…ç†å†å²æ•°æ®")
            cleanup_result = await cleanup_history_data(
                days_to_keep=settings.db_pool.pool_recycle // 3600
            )  # è½¬æ¢ä¸ºå¤©æ•°
            results["cleanup"] = cleanup_result
        except Exception as e:
            run_logger.warning(
                "å†å²æ•°æ®æ¸…ç†å¤±è´¥ï¼Œä½†æµç¨‹ç»§ç»­",
                extra={"error_type": type(e).__name__, "error_message": str(e)},
            )

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        flow_end_time = datetime.now()
        execution_time = (flow_end_time - flow_start_time).total_seconds()

        results.update(
            {
                "end_time": flow_end_time.isoformat(),
                "execution_time_seconds": execution_time,
                "success_rate": results["total_odds"] / (results["collected"] * 3)
                if results["collected"] > 0
                else 0,
            }
        )

        run_logger.info(
            "âœ… Titan007 å¸¸è§„æ•°æ®é‡‡é›†å·¥ä½œæµå®Œæˆ",
            extra={
                "fixtures": results["fixtures"],
                "aligned": results["aligned"],
                "collected": results["collected"],
                "total_odds": results["total_odds"],
                "errors": results["errors"],
                "execution_time_seconds": results["execution_time_seconds"],
                "success_rate": results["success_rate"],
            },
        )

        return results

    except Exception as e:
        run_logger.error(
            "âŒ Titan007 å¸¸è§„æ•°æ®é‡‡é›†å·¥ä½œæµå¤±è´¥",
            extra={"error_type": type(e).__name__, "error_message": str(e)},
        )
        raise


@flow(
    name="Titan007 ä¸´åœºæ•°æ®é‡‡é›†",
    description="æ¯10åˆ†é’Ÿè¿è¡Œä¸€æ¬¡ï¼Œé‡‡é›†å³å°†å¼€èµ›æ¯”èµ›çš„æœ€æ–°èµ”ç‡",
    retries=2,
    retry_delay_seconds=60,
)
async def titan_live_flow(
    hours_ahead: int = 2, batch_size: int = 10, max_concurrency: int = 8
) -> Dict[str, Any]:
    """
    Titan007 ä¸´åœºæ•°æ®é‡‡é›†å·¥ä½œæµ

    æµç¨‹ï¼š
    1. è·å–æœªæ¥å‡ å°æ—¶å†…çš„æ¯”èµ›
    2. ID å¯¹é½
    3. æ‰¹é‡é‡‡é›†èµ”ç‡æ•°æ®
    4. ä¸“æ³¨æœ€æ–°æ•°æ®ï¼ˆä¸æ¸…ç†å†å²æ•°æ®ï¼‰

    Args:
        hours_ahead: æå‰å¤šå°‘å°æ—¶ç­›é€‰æ¯”èµ›
        batch_size: IDå¯¹é½çš„æ‰¹å¤„ç†å¤§å°
        max_concurrency: å¹¶å‘é‡‡é›†æ¯”èµ›æ•°

    Returns:
        Dict[str, Any]: æ‰§è¡Œç»“æœç»Ÿè®¡
    """
    run_logger = flow.get_run_logger()

    run_logger.info(
        "ğŸš€ å¼€å§‹ Titan007 ä¸´åœºæ•°æ®é‡‡é›†å·¥ä½œæµ",
        extra={
            "hours_ahead": hours_ahead,
            "batch_size": batch_size,
            "max_concurrency": max_concurrency,
            "flow_run_id": flow.id,
        },
    )

    try:
        flow_start_time = datetime.now()
        results = {
            "flow_type": "live",
            "start_time": flow_start_time.isoformat(),
            "total_fixtures": 0,
            "upcoming_fixtures": 0,
            "aligned": 0,
            "collected": 0,
            "errors": 0,
            "total_odds": 0,
        }

        # Step 1: è·å–å½“å¤©èµ›ç¨‹
        run_logger.info("ğŸ“‹ Step 1: è·å–å½“å¤©èµ›ç¨‹")
        fixtures = await fetch_fixtures(days_ahead=hours_ahead)
        results["total_fixtures"] = len(fixtures)

        if not fixtures:
            run_logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ¯”èµ›æ•°æ®ï¼Œæµç¨‹ç»“æŸ")
            return results

        # Step 1.5: ç­›é€‰å³å°†å¼€å§‹çš„æ¯”èµ›
        run_logger.info("â° Step 1.5: ç­›é€‰å³å°†å¼€å§‹çš„æ¯”èµ›")
        upcoming_fixtures = _filter_upcoming_matches(fixtures, hours_ahead)
        results["upcoming_fixtures"] = len(upcoming_fixtures)

        if not upcoming_fixtures:
            run_logger.info("âœ… æ²¡æœ‰å³å°†å¼€å§‹çš„æ¯”èµ›ï¼Œæµç¨‹ç»“æŸ")
            return results

        # Step 2: ID å¯¹é½
        run_logger.info("ğŸ”— Step 2: æ‰§è¡Œ ID å¯¹é½")
        aligned_matches = await align_ids(upcoming_fixtures, batch_size=batch_size)
        results["aligned"] = len(aligned_matches)

        if not aligned_matches:
            run_logger.warning("âš ï¸ æ²¡æœ‰å¯¹é½æˆåŠŸçš„æ¯”èµ›ï¼Œæµç¨‹ç»“æŸ")
            return results

        # Step 3: æ‰¹é‡é‡‡é›†èµ”ç‡
        run_logger.info("ğŸ“Š Step 3: æ‰¹é‡é‡‡é›†æœ€æ–°èµ”ç‡æ•°æ®")
        collection_results = await batch_collect_odds(
            matches=aligned_matches, max_concurrency=max_concurrency
        )
        results["collected"] = len(collection_results)

        # ç»Ÿè®¡èµ”ç‡é‡‡é›†ç»“æœ
        total_odds = sum(r.get("success_count", 0) for r in collection_results)
        results["total_odds"] = total_odds
        results["errors"] = sum(r.get("error_count", 0) for r in collection_results)

        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        flow_end_time = datetime.now()
        execution_time = (flow_end_time - flow_start_time).total_seconds()

        results.update(
            {
                "end_time": flow_end_time.isoformat(),
                "execution_time_seconds": execution_time,
                "success_rate": results["total_odds"] / (results["collected"] * 3)
                if results["collected"] > 0
                else 0,
                "odds_per_second": results["total_odds"] / execution_time
                if execution_time > 0
                else 0,
            }
        )

        run_logger.info(
            "âœ… Titan007 ä¸´åœºæ•°æ®é‡‡é›†å·¥ä½œæµå®Œæˆ",
            extra={
                "total_fixtures": results["total_fixtures"],
                "upcoming_fixtures": results["upcoming_fixtures"],
                "aligned": results["aligned"],
                "collected": results["collected"],
                "total_odds": results["total_odds"],
                "errors": results["errors"],
                "execution_time_seconds": results["execution_time_seconds"],
                "success_rate": results["success_rate"],
                "odds_per_second": results["odds_per_second"],
            },
        )

        return results

    except Exception as e:
        run_logger.error(
            "âŒ Titan007 ä¸´åœºæ•°æ®é‡‡é›†å·¥ä½œæµå¤±è´¥",
            extra={"error_type": type(e).__name__, "error_message": str(e)},
        )
        raise


@flow(
    name="Titan007 æ··åˆæ•°æ®é‡‡é›†",
    description="ç»“åˆå¸¸è§„æ¨¡å¼å’Œä¸´åœºæ¨¡å¼çš„æ··åˆè°ƒåº¦",
    retries=1,
    retry_delay_seconds=300,
)
async def titan_hybrid_flow(
    regular_hours_ahead: int = 1,
    live_hours_ahead: int = 2,
    enable_live: bool = True,
    cleanup_days: int = 7,
) -> Dict[str, Any]:
    """
    Titan007 æ··åˆæ•°æ®é‡‡é›†å·¥ä½œæµ

    å…ˆæ‰§è¡Œå¸¸è§„æ¨¡å¼è·å–å…¨å¤©æ•°æ®ï¼Œç„¶åå¯åŠ¨ä¸´åœºæ¨¡å¼è¿›è¡Œå®æ—¶æ›´æ–°ã€‚

    Args:
        regular_hours_ahead: å¸¸è§„æ¨¡å¼çš„æå‰å¤©æ•°
        live_hours_ahead: ä¸´åœºæ¨¡å¼çš„æå‰å°æ—¶æ•°
        enable_live: æ˜¯å¦å¯ç”¨ä¸´åœºæ¨¡å¼
        cleanup_days: å†å²æ•°æ®ä¿ç•™å¤©æ•°

    Returns:
        Dict[str, Any]: æ‰§è¡Œç»“æœç»Ÿè®¡
    """
    run_logger = flow.get_run_logger()

    run_logger.info(
        "ğŸš€ å¼€å§‹ Titan007 æ··åˆæ•°æ®é‡‡é›†å·¥ä½œæµ",
        extra={
            "regular_hours_ahead": regular_hours_ahead,
            "live_hours_ahead": live_hours_ahead,
            "enable_live": enable_live,
            "cleanup_days": cleanup_days,
            "flow_run_id": flow.id,
        },
    )

    try:
        flow_start_time = datetime.now()
        results = {
            "flow_type": "hybrid",
            "start_time": flow_start_time.isoformat(),
            "regular_results": {},
            "live_results": {},
            "cleanup_results": {},
            "total_odds": 0,
        }

        # æ‰§è¡Œå¸¸è§„æ¨¡å¼
        run_logger.info("ğŸ“‹ æ‰§è¡Œå¸¸è§„æ•°æ®é‡‡é›†æ¨¡å¼")
        regular_results = await titan_regular_flow(days_ahead=regular_hours_ahead)
        results["regular_results"] = regular_results
        results["total_odds"] += regular_results.get("total_odds", 0)

        # å¯åŠ¨ä¸´åœºæ¨¡å¼
        if enable_live:
            run_logger.info("âš¡ å¯åŠ¨ä¸´åœºæ•°æ®é‡‡é›†æ¨¡å¼")
            live_results = await titan_live_flow(hours_ahead=live_hours_ahead)
            results["live_results"] = live_results
            results["total_odds"] += live_results.get("total_odds", 0)

        # æ¸…ç†å†å²æ•°æ®
        if cleanup_days > 0:
            run_logger.info("ğŸ§¹ æ¸…ç†å†å²æ•°æ®")
            cleanup_results = await cleanup_history_data(days_to_keep=cleanup_days)
            results["cleanup_results"] = cleanup_results

        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        flow_end_time = datetime.now()
        execution_time = (flow_end_time - flow_start_time).total_seconds()

        results.update(
            {
                "end_time": flow_end_time.isoformat(),
                "execution_time_seconds": execution_time,
                "total_odds": results["total_odds"],
                "regular_odds": regular_results.get("total_odds", 0),
                "live_odds": live_results.get("total_odds", 0) if enable_live else 0,
            }
        )

        run_logger.info(
            "âœ… Titan007 æ··åˆæ•°æ®é‡‡é›†å·¥ä½œæµå®Œæˆ",
            extra={
                "regular_odds": results["regular_odds"],
                "live_odds": results["live_odds"],
                "total_odds": results["total_odds"],
                "execution_time_seconds": results["execution_time_seconds"],
                "cleanup_days": cleanup_days,
            },
        )

        return results

    except Exception as e:
        run_logger.error(
            "âŒ Titan007 æ··åˆæ•°æ®é‡‡é›†å·¥ä½œæµå¤±è´¥",
            extra={"error_type": type(e).__name__, "error_message": str(e)},
        )
        raise


# è°ƒåº¦é…ç½®
titan_regular_schedule = CronSchedule(
    cron="0 8 * * *",  # æ¯å¤©æ—©ä¸Š8ç‚¹
    timezone="Asia/Shanghai",
)

titan_live_schedule = CronSchedule(
    cron="*/10 * * * *",  # æ¯10åˆ†é’Ÿ
    timezone="Asia/Shanghai",
)

# å¢å¼ºå‹è°ƒåº¦é…ç½®
titan_weekend_schedule = CronSchedule(
    cron="0 9 * * 6",  # å‘¨å…­æ—©ä¸Š9ç‚¹
    timezone="Asia/Shanghai",
)

# æ™ºèƒ½è°ƒåº¦ç­–ç•¥ - åŸºäºæ¯”èµ›å¯†åº¦çš„åŠ¨æ€è°ƒåº¦
titan_smart_schedule = CronSchedule(
    cron="0 */2 * * *",  # æ¯2å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    timezone="Asia/Shanghai",
)

# èµ›å­£é«˜å³°æœŸè°ƒåº¦ - é‡è¦æ¯”èµ›çª—å£æœŸ
titan_peak_season_schedule = CronSchedule(
    cron="30 7,12,18 * * 1,5",  # å‘¨ä¸€å’Œå‘¨äº”çš„7:30, 12:30, 18:30
    timezone="Asia/Shanghai",
)

# æ¸…ç†ä»»åŠ¡è°ƒåº¦ - æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹è¿è¡Œ
titan_cleanup_schedule = CronSchedule(
    cron="0 2 * * 0",  # æ¯å‘¨æ—¥å‡Œæ™¨2ç‚¹
    timezone="Asia/Shanghai",
)
