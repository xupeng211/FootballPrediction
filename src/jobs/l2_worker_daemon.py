#!/usr/bin/env python3
"""
L2 Worker å®ˆæŠ¤è¿›ç¨‹ - ä¼ä¸šçº§æŒç»­æ•°æ®é‡‡é›†æœåŠ¡
L2 Worker Daemon - Enterprise-grade Continuous Data Collection Service

ç‰¹æ€§Features:
- æŒç»­è¿è¡Œï¼Œè‡ªåŠ¨é‡å¯
- æ‰¹é‡å¤„ç† + æ™ºèƒ½ä¼‘çœ 
- èµ„æºé™åˆ¶ä¿æŠ¤
- å¥åº·æ£€æŸ¥å’Œé”™è¯¯æ¢å¤
- å¯é…ç½®çš„å¹¶å‘æ§åˆ¶
"""

import asyncio
import logging
import sys
import os
import json
import signal
from pathlib import Path
from typing import Any
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.collectors.fotmob_api_collector import FotMobAPICollector
from src.database.async_manager import initialize_database, get_db_session
from sqlalchemy import text

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)


class L2WorkerDaemon:
    """L2 Worker å®ˆæŠ¤è¿›ç¨‹"""

    def __init__(self):
        self.running = True
        self.stats = {
            "start_time": datetime.now(),
            "total_processed": 0,
            "total_failed": 0,
            "last_batch_time": None,
            "consecutive_failures": 0,
            "batches_processed": 0,
        }
        self.config = self._load_config()
        self.collector = None

        # æ³¨å†Œä¿¡å·å¤„ç†
        signal.signal(signal.SIGTERM, self._signal_handler)
        signal.signal(signal.SIGINT, self._signal_handler)

    def _load_config(self) -> dict[str, Any]:
        """åŠ è½½é…ç½®"""
        return {
            "batch_size": int(os.getenv("L2_WORKER_BATCH_SIZE", "20")),
            "sleep_interval": int(os.getenv("L2_WORKER_SLEEP_INTERVAL", "10")),
            "max_concurrent": int(os.getenv("L2_WORKER_MAX_CONCURRENT", "5")),
            "max_consecutive_failures": int(os.getenv("L2_WORKER_MAX_FAILURES", "10")),
            "health_check_interval": int(
                os.getenv("L2_WORKER_HEALTH_CHECK", "300")
            ),  # 5åˆ†é’Ÿ
            "timeout": int(os.getenv("L2_WORKER_TIMEOUT", "30")),
            "base_delay": float(os.getenv("L2_WORKER_BASE_DELAY", "1.5")),
            "enable_jitter": os.getenv("L2_WORKER_JITTER", "true").lower() == "true",
        }

    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        logger.info(f"ğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡ä¼˜é›…å…³é—­...")
        self.running = False

    async def initialize(self):
        """åˆå§‹åŒ–æœåŠ¡"""
        try:
            logger.info("ğŸš€ åˆå§‹åŒ–L2 Workerå®ˆæŠ¤è¿›ç¨‹")

            # åˆå§‹åŒ–æ•°æ®åº“
            db_url = os.getenv(
                "DATABASE_URL",
                "postgresql+asyncpg://postgres:postgres@db:5432/football_prediction",
            )
            initialize_database(db_url)
            logger.info("âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆ")

            # åˆå§‹åŒ–é‡‡é›†å™¨
            self.collector = FotMobAPICollector(
                max_concurrent=self.config["max_concurrent"],
                timeout=self.config["timeout"],
                base_delay=self.config["base_delay"],
                enable_proxy=False,
                enable_jitter=self.config["enable_jitter"],
            )

            await self.collector.initialize()
            logger.info("âœ… FotMobé‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆ")

            # è¾“å‡ºé…ç½®ä¿¡æ¯
            logger.info("ğŸ“‹ Workeré…ç½®:")
            logger.info(f"   æ‰¹é‡å¤§å°: {self.config['batch_size']}")
            logger.info(f"   ä¼‘çœ é—´éš”: {self.config['sleep_interval']}ç§’")
            logger.info(f"   æœ€å¤§å¹¶å‘: {self.config['max_concurrent']}")
            logger.info(f"   å¥åº·æ£€æŸ¥: {self.config['health_check_interval']}ç§’")

        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def get_pending_matches(self, limit: int) -> list:
        """è·å–å¾…å¤„ç†çš„æ¯”èµ›"""
        async with get_db_session() as session:
            query = text("""
                SELECT fotmob_id, home_team_id, away_team_id
                FROM matches
                WHERE fotmob_id IS NOT NULL
                  AND home_xg IS NULL
                LIMIT :limit
            """)

            result = await session.execute(query, {"limit": limit})
            return result.fetchall()

    async def process_match_batch(self, matches: list) -> dict[str, int]:
        """å¤„ç†ä¸€æ‰¹æ¯”èµ›"""
        if not matches:
            return {"success": 0, "failed": 0}

        success_count = 0
        failed_count = 0

        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡: {len(matches)} åœºæ¯”èµ›")

        for i, (fotmob_id, _home_team_id, _away_team_id) in enumerate(matches, 1):
            if not self.running:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œä¸­æ–­æ‰¹æ¬¡å¤„ç†")
                break

            try:
                logger.info(f"ğŸ“Š [{i}/{len(matches)}] å¤„ç†: {fotmob_id}")

                # é‡‡é›†æ•°æ®
                match_data = await self.collector.collect_match_details(fotmob_id)

                if match_data:
                    # æ›´æ–°æ•°æ®åº“
                    await self._update_match_data(fotmob_id, match_data)

                    logger.info(
                        f"âœ… æˆåŠŸ: {fotmob_id} - xG: {match_data.xg_home} vs {match_data.xg_away}"
                    )
                    success_count += 1
                else:
                    logger.warning(f"âš ï¸ å¤±è´¥: {fotmob_id} - æ•°æ®é‡‡é›†å¤±è´¥")
                    failed_count += 1

            except Exception as e:
                logger.error(f"âŒ å¤„ç†å¤±è´¥ {fotmob_id}: {e}")
                failed_count += 1

            # æ™ºèƒ½å»¶è¿Ÿ
            if i < len(matches):
                delay = self.config["base_delay"] + (i % 3) * 0.3
                await asyncio.sleep(delay)

        self.stats["total_processed"] += success_count
        self.stats["total_failed"] += failed_count
        self.stats["last_batch_time"] = datetime.now()
        self.stats["batches_processed"] += 1

        # æ›´æ–°è¿ç»­å¤±è´¥è®¡æ•°
        if success_count == 0:
            self.stats["consecutive_failures"] += 1
        else:
            self.stats["consecutive_failures"] = 0

        logger.info(f"ğŸ“Š æ‰¹æ¬¡å®Œæˆ: æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")

        return {"success": success_count, "failed": failed_count}

    async def _update_match_data(self, fotmob_id: str, match_data):
        """æ›´æ–°æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“"""
        async with get_db_session() as session:
            update_query = text("""
                UPDATE matches SET
                    home_xg = :home_xg,
                    away_xg = :away_xg,
                    home_score = :home_score,
                    away_score = :away_score,
                    status = :status,
                    venue = :venue,
                    referee = :referee,
                    stats_json = :stats_json,
                    lineups_json = :lineups_json,
                    environment_json = :environment_json,
                    data_completeness = 'complete',
                    updated_at = NOW()
                WHERE fotmob_id = :fotmob_id
            """)

            await session.execute(
                update_query,
                {
                    "fotmob_id": fotmob_id,
                    "home_xg": match_data.xg_home,
                    "away_xg": match_data.xg_away,
                    "home_score": match_data.home_score,
                    "away_score": match_data.away_score,
                    "status": match_data.status,
                    "venue": match_data.venue,
                    "referee": match_data.referee,
                    "stats_json": json.dumps(match_data.stats_json)
                    if match_data.stats_json
                    else None,
                    "lineups_json": json.dumps(match_data.lineups_json)
                    if match_data.lineups_json
                    else None,
                    "environment_json": json.dumps(match_data.environment_json)
                    if match_data.environment_json
                    else None,
                },
            )

            await session.commit()

    async def health_check(self) -> bool:
        """å¥åº·æ£€æŸ¥"""
        try:
            async with get_db_session() as session:
                result = await session.execute(text("SELECT 1"))
                return result.scalar() == 1
        except Exception as e:
            logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False

    async def get_database_stats(self) -> dict[str, Any]:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        async with get_db_session() as session:
            # æ€»ä½“ç»Ÿè®¡
            total_query = text("""
                SELECT
                    COUNT(*) as total_matches,
                    COUNT(CASE WHEN home_xg IS NOT NULL THEN 1 END) as l2_completed,
                    COUNT(CASE WHEN home_xg IS NULL THEN 1 END) as l2_pending
                FROM matches
                WHERE fotmob_id IS NOT NULL
            """)

            result = await session.execute(total_query)
            stats = result.fetchone()

            return {
                "total_matches": stats.total_matches,
                "l2_completed": stats.l2_completed,
                "l2_pending": stats.l2_pending,
                "completion_rate": round(
                    stats.l2_completed * 100.0 / stats.total_matches, 2
                )
                if stats.total_matches > 0
                else 0,
            }

    def print_status(self):
        """æ‰“å°çŠ¶æ€ä¿¡æ¯"""
        uptime = datetime.now() - self.stats["start_time"]

        logger.info("=" * 60)
        logger.info("ğŸ“Š L2 Worker å®ˆæŠ¤è¿›ç¨‹çŠ¶æ€")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  è¿è¡Œæ—¶é—´: {uptime}")
        logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡: {self.stats['batches_processed']}")
        logger.info(f"âœ… æ€»æˆåŠŸ: {self.stats['total_processed']}")
        logger.info(f"âŒ æ€»å¤±è´¥: {self.stats['total_failed']}")
        logger.info(f"ğŸ”„ è¿ç»­å¤±è´¥: {self.stats['consecutive_failures']}")

        if self.stats["last_batch_time"]:
            time_since_batch = datetime.now() - self.stats["last_batch_time"]
            logger.info(f"â° ä¸Šæ¬¡æ‰¹æ¬¡: {time_since_batch} ago")

    async def run(self):
        """ä¸»è¿è¡Œå¾ªç¯"""
        logger.info("ğŸš€ L2 Workerå®ˆæŠ¤è¿›ç¨‹å¯åŠ¨")

        last_health_check = datetime.now()

        try:
            while self.running:
                try:
                    # å¥åº·æ£€æŸ¥
                    if (datetime.now() - last_health_check).seconds > self.config[
                        "health_check_interval"
                    ]:
                        if not await self.health_check():
                            logger.error("âŒ å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œç­‰å¾…é‡è¯•...")
                            await asyncio.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿåé‡è¯•
                            continue
                        last_health_check = datetime.now()

                    # æ£€æŸ¥è¿ç»­å¤±è´¥æ¬¡æ•°
                    if (
                        self.stats["consecutive_failures"]
                        >= self.config["max_consecutive_failures"]
                    ):
                        logger.error(
                            f"âŒ è¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤š ({self.stats['consecutive_failures']})ï¼Œå»¶é•¿ç­‰å¾…æ—¶é—´"
                        )
                        await asyncio.sleep(300)  # ç­‰å¾…5åˆ†é’Ÿ
                        continue

                    # è·å–å¾…å¤„ç†æ¯”èµ›
                    matches = await self.get_pending_matches(self.config["batch_size"])

                    if matches:
                        # å¤„ç†æ‰¹æ¬¡
                        await self.process_match_batch(matches)

                        # æ‰“å°è¿›åº¦
                        db_stats = await self.get_database_stats()
                        logger.info(
                            f"ğŸ“ˆ æ•°æ®åº“è¿›åº¦: {db_stats['l2_completed']}/{db_stats['total_matches']} "
                            f"({db_stats['completion_rate']}%)"
                        )

                    else:
                        logger.info("ğŸ¯ æ²¡æœ‰å¾…å¤„ç†çš„æ¯”èµ›ï¼Œä¼‘çœ ...")

                    # ä¼‘çœ 
                    await asyncio.sleep(self.config["sleep_interval"])

                except Exception as e:
                    logger.error(f"âŒ è¿è¡Œå¾ªç¯é”™è¯¯: {e}")
                    self.stats["consecutive_failures"] += 1
                    await asyncio.sleep(60)  # é”™è¯¯åç­‰å¾…1åˆ†é’Ÿ

        except KeyboardInterrupt:
            logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            logger.error(f"ğŸ’¥ è‡´å‘½é”™è¯¯: {e}")
            raise
        finally:
            await self.cleanup()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")

        if self.collector:
            await self.collector.close()

        # æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š
        self.print_status()

        logger.info("âœ… L2 Workerå®ˆæŠ¤è¿›ç¨‹å·²åœæ­¢")


async def main():
    """ä¸»å‡½æ•°"""
    daemon = L2WorkerDaemon()

    try:
        await daemon.initialize()
        await daemon.run()
    except Exception as e:
        logger.error(f"ğŸ’¥ å®ˆæŠ¤è¿›ç¨‹å¤±è´¥: {e}")
        sys.exit(1)

    return 0


if __name__ == "__main__":
    try:
        exit_code = asyncio.run(main())
        sys.exit(exit_code)
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        sys.exit(1)
