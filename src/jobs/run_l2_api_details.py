#!/usr/bin/env python3
"""
L2 è¯¦æƒ…é‡‡é›†ä»»åŠ¡ - APIç‰ˆæœ¬
L2 Details Collection Job - API Version

ä½¿ç”¨æ–°çš„FotMob APIè¿›è¡Œé«˜æ€§èƒ½è¯¦æƒ…æ•°æ®é‡‡é›†
"""

import asyncio
import logging
import os
import sys
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from prefect import flow, task, get_run_logger
from prefect.client.orchestration import PrefectClient

from src.collectors.fotmob_api_collector import FotMobAPICollector
from src.services.l2_data_service import L2DataService

logger = logging.getLogger(__name__)


@task(
    name="è·å–å¾…å¤„ç†æ¯”èµ›ID",
    retries=3,
    retry_delay_seconds=30,
    cache_key_fn=lambda: "pending_matches",
    cache_expiration=timedelta(minutes=30)
)
async def get_pending_matches(limit: int = 10000) -> list[str]:
    """è·å–å¾…å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨"""
    log = get_run_logger()
    log.info(f"ğŸ” è·å–å¾…å¤„ç†æ¯”èµ›åˆ—è¡¨ï¼Œé™åˆ¶: {limit}")

    service = L2DataService()
    matches = await service.get_pending_matches(limit)

    log.info(f"ğŸ“Š æ‰¾åˆ° {len(matches)} åœºå¾…å¤„ç†æ¯”èµ›")
    return matches


@task(
    name="APIæ•°æ®é‡‡é›†",
    retries=2,
    retry_delay_seconds=60
)
async def collect_match_details_batch(
    fotmob_ids: list[str],
    batch_size: int = 50,
    max_concurrent: int = 10
) -> dict[str, Any]:
    """æ‰¹é‡é‡‡é›†æ¯”èµ›è¯¦æƒ…æ•°æ®"""
    log = get_run_logger()
    log.info(f"ğŸš€ å¼€å§‹æ‰¹é‡é‡‡é›† {len(fotmob_ids)} åœºæ¯”èµ›è¯¦æƒ…")

    collector = FotMobAPICollector(
        max_concurrent=max_concurrent,
        timeout=30,
        max_retries=3,
        base_delay=1.0,
        enable_proxy=True,
        enable_jitter=True
    )

    try:
        await collector.initialize()

        # åˆ†æ‰¹å¤„ç†
        all_results = []
        total_batches = (len(fotmob_ids) + batch_size - 1) // batch_size

        for i in range(0, len(fotmob_ids), batch_size):
            batch_ids = fotmob_ids[i:i + batch_size]
            batch_num = i // batch_size + 1

            log.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch_ids)} åœºæ¯”èµ›)")

            batch_results = await collector.collect_batch(batch_ids)
            all_results.extend(batch_results)

            # æ‰¹æ¬¡é—´æš‚åœ
            if batch_num < total_batches:
                await asyncio.sleep(2)

        # è·å–é‡‡é›†ç»Ÿè®¡
        stats = collector.get_stats()
        log.info(f"ğŸ“Š é‡‡é›†å®Œæˆ: æˆåŠŸ {stats['matches_collected']}/{len(fotmob_ids)} åœº")
        log.info(f"ğŸ“ˆ è¯·æ±‚ç»Ÿè®¡: æˆåŠŸ {stats['successful_requests']}, å¤±è´¥ {stats['failed_requests']}")

        return {
            "results": all_results,
            "stats": stats,
            "total_requested": len(fotmob_ids),
            "success_count": len(all_results),
            "success_rate": len(all_results) / len(fotmob_ids) * 100 if fotmob_ids else 0
        }

    finally:
        await collector.close()


@task(
    name="æ•°æ®åº“å†™å…¥",
    retries=3,
    retry_delay_seconds=30
)
async def save_match_details_to_db(match_details_data: dict[str, Any]) -> dict[str, Any]:
    """å°†é‡‡é›†çš„è¯¦æƒ…æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“"""
    log = get_run_logger()

    service = L2DataService()
    results = await service.save_batch_match_details(match_details_data["results"])

    log.info(f"ğŸ’¾ æ•°æ®åº“å†™å…¥å®Œæˆ: {results}")

    return {
        "db_results": results,
        "collection_stats": match_details_data["stats"],
        "total_success": results["success"],
        "total_failed": results["failed"]
    }


@task(
    name="æ›´æ–°æ•°æ®çŠ¶æ€",
    retries=2,
    retry_delay_seconds=10
)
async def update_data_completeness(
    fotmob_ids: list[str],
    success_count: int,
    failed_count: int
) -> dict[str, Any]:
    """æ›´æ–°æ•°æ®å®Œæ•´åº¦çŠ¶æ€"""
    log = get_run_logger()

    service = L2DataService()

    # æ›´æ–°æˆåŠŸçš„è®°å½•ä¸ºcomplete
    completed_ids = fotmob_ids[:success_count]  # ç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åŸºäºå…·ä½“æˆåŠŸçš„ID
    updated_complete = await service.update_data_completeness_status(
        completed_ids, "complete"
    )

    # æ›´æ–°å¤±è´¥çš„è®°å½•ä¸ºfailed
    if failed_count > 0:
        failed_ids = fotmob_ids[success_count:]
        updated_failed = await service.update_data_completeness_status(
            failed_ids, "failed"
        )
    else:
        updated_failed = 0

    log.info(f"âœ… çŠ¶æ€æ›´æ–°å®Œæˆ: complete={updated_complete}, failed={updated_failed}")

    return {
        "updated_complete": updated_complete,
        "updated_failed": updated_failed,
        "total_processed": len(fotmob_ids)
    }


@flow(
    name="L2 APIè¯¦æƒ…é‡‡é›†æµç¨‹",
    description="ä½¿ç”¨FotMob APIè¿›è¡ŒL2è¯¦æƒ…æ•°æ®é‡‡é›†",
    log_prints=True
)
async def run_l2_api_details(
    limit: int = 10000,
    batch_size: int = 50,
    max_concurrent: int = 10,
    dry_run: bool = False
) -> dict[str, Any]:
    """
    L2è¯¦æƒ…æ•°æ®é‡‡é›†ä¸»æµç¨‹

    Args:
        limit: æœ€å¤§å¤„ç†æ¯”èµ›æ•°é‡
        batch_size: æ‰¹å¤„ç†å¤§å°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œï¼ˆåªé‡‡é›†ä¸å†™å…¥æ•°æ®åº“ï¼‰

    Returns:
        é‡‡é›†ç»“æœç»Ÿè®¡ä¿¡æ¯
    """
    log = get_run_logger()
    start_time = datetime.now()

    log.info("ğŸ¯ å¼€å§‹L2 APIè¯¦æƒ…é‡‡é›†æµç¨‹")
    log.info(f"ğŸ“‹ å‚æ•°: limit={limit}, batch_size={batch_size}, max_concurrent={max_concurrent}")
    log.info(f"ğŸ”§ æ¨¡å¼: {'è¯•è¿è¡Œ' if dry_run else 'æ­£å¼è¿è¡Œ'}")

    try:
        # 1. è·å–å¾…å¤„ç†çš„æ¯”èµ›ID
        fotmob_ids = await get_pending_matches(limit)

        if not fotmob_ids:
            log.info("ğŸ“ æ²¡æœ‰å¾…å¤„ç†çš„æ¯”èµ›ï¼Œæµç¨‹ç»“æŸ")
            return {
                "status": "completed",
                "message": "æ²¡æœ‰å¾…å¤„ç†çš„æ¯”èµ›",
                "processed_count": 0,
                "duration": (datetime.now() - start_time).total_seconds()
            }

        # 2. æ‰¹é‡é‡‡é›†è¯¦æƒ…æ•°æ®
        collection_result = await collect_match_details_batch(
            fotmob_ids, batch_size, max_concurrent
        )

        # 3. å¦‚æœä¸æ˜¯è¯•è¿è¡Œï¼Œä¿å­˜åˆ°æ•°æ®åº“
        if not dry_run and collection_result["results"]:
            save_result = await save_match_details_to_db(collection_result)

            # 4. æ›´æ–°æ•°æ®å®Œæ•´åº¦çŠ¶æ€
            await update_data_completeness(
                fotmob_ids[:collection_result["success_count"]],  # ç®€åŒ–å¤„ç†
                save_result["total_success"],
                save_result["total_failed"]
            )
        else:
            log.info("ğŸ§ª è¯•è¿è¡Œæ¨¡å¼ï¼Œè·³è¿‡æ•°æ®åº“å†™å…¥")
            save_result = {"db_results": {"success": collection_result["success_count"]}}

        # 5. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        duration = (datetime.now() - start_time).total_seconds()

        final_report = {
            "status": "completed",
            "start_time": start_time.isoformat(),
            "duration_seconds": duration,
            "total_requested": len(fotmob_ids),
            "collection_success": collection_result["success_count"],
            "collection_success_rate": collection_result["success_rate"],
            "db_success": save_result["db_results"].get("success", 0),
            "db_failed": save_result["db_results"].get("failed", 0),
            "collection_stats": collection_result["stats"],
            "dry_run": dry_run
        }

        log.info("ğŸ‰ L2è¯¦æƒ…é‡‡é›†æµç¨‹å®Œæˆ!")
        log.info(f"ğŸ“Š é‡‡é›†ç»Ÿè®¡: {collection_result['success_count']}/{len(fotmob_ids)} ({collection_result['success_rate']:.1f}%)")
        log.info(f"â±ï¸ æ€»è€—æ—¶: {duration:.1f}ç§’")

        return final_report

    except Exception as e:
        log.error(f"âŒ L2è¯¦æƒ…é‡‡é›†æµç¨‹å¤±è´¥: {e}")
        duration = (datetime.now() - start_time).total_seconds()

        return {
            "status": "failed",
            "error": str(e),
            "duration_seconds": duration,
            "start_time": start_time.isoformat()
        }


@flow(
    name="L2å¢é‡å›å¡«æµç¨‹",
    description="å¯¹ç‰¹å®šæ—¥æœŸèŒƒå›´çš„å¤±è´¥æ¯”èµ›è¿›è¡Œå¢é‡å›å¡«",
    log_prints=True
)
async def run_l2_incremental_backfill(
    days_back: int = 7,
    batch_size: int = 50,
    max_concurrent: int = 5
) -> dict[str, Any]:
    """
    L2å¢é‡å›å¡«æµç¨‹

    Args:
        days_back: å›æº¯å¤©æ•°
        batch_size: æ‰¹å¤„ç†å¤§å°
        max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆå›å¡«æ—¶æ›´ä¿å®ˆï¼‰

    Returns:
        å›å¡«ç»“æœç»Ÿè®¡ä¿¡æ¯
    """
    log = get_run_logger()
    start_time = datetime.now()

    log.info("ğŸ”„ å¼€å§‹L2å¢é‡å›å¡«æµç¨‹")
    log.info(f"ğŸ“‹ å‚æ•°: days_back={days_back}, batch_size={batch_size}")

    try:
        # è·å–å¤±è´¥çš„æ¯”èµ›
        service = L2DataService()

        # è¿™é‡Œåº”è¯¥å®ç°è·å–å¤±è´¥æ¯”èµ›IDçš„é€»è¾‘
        # æš‚æ—¶ä½¿ç”¨ç®€åŒ–çš„æ–¹å¼è·å–æ‰€æœ‰partialçŠ¶æ€çš„æ¯”èµ›
        failed_ids = await service.get_pending_matches(limit=5000)

        if not failed_ids:
            log.info("ğŸ“ æ²¡æœ‰éœ€è¦å›å¡«çš„æ¯”èµ›ï¼Œæµç¨‹ç»“æŸ")
            return {
                "status": "completed",
                "message": "æ²¡æœ‰éœ€è¦å›å¡«çš„æ¯”èµ›",
                "processed_count": 0
            }

        log.info(f"ğŸ”„ æ‰¾åˆ° {len(failed_ids)} åœºéœ€è¦å›å¡«çš„æ¯”èµ›")

        # ä½¿ç”¨æ›´ä¿å®ˆçš„å¹¶å‘è®¾ç½®è¿›è¡Œå›å¡«
        result = await run_l2_api_details(
            limit=len(failed_ids),
            batch_size=batch_size,
            max_concurrent=max_concurrent,
            dry_run=False
        )

        result["backfill_type"] = "incremental"
        result["days_back"] = days_back

        return result

    except Exception as e:
        log.error(f"âŒ L2å¢é‡å›å¡«æµç¨‹å¤±è´¥: {e}")
        return {
            "status": "failed",
            "error": str(e),
            "backfill_type": "incremental",
            "days_back": days_back
        }


# CLIå…¥å£ç‚¹
if __name__ == "__main__":
    import asyncio
    import sys

    async def main():
        if len(sys.argv) < 2:
            print("ç”¨æ³•:")
            print("  python src/jobs/run_l2_api_details.py full           # å®Œæ•´é‡‡é›†")
            print("  python src/jobs/run_l2_api_details.py backfill       # å¢é‡å›å¡«")
            print("  python src/jobs/run_l2_api_details.py dry-run        # è¯•è¿è¡Œ")
            print("")
            print("ç¯å¢ƒå˜é‡:")
            print("  LIMIT=10000         # å¤„ç†æ•°é‡é™åˆ¶")
            print("  BATCH_SIZE=50       # æ‰¹å¤„ç†å¤§å°")
            print("  MAX_CONCURRENT=10   # æœ€å¤§å¹¶å‘æ•°")
            sys.exit(1)

        command = sys.argv[1]

        # ä»ç¯å¢ƒå˜é‡è·å–å‚æ•°
        limit = int(os.getenv("LIMIT", "10000"))
        batch_size = int(os.getenv("BATCH_SIZE", "50"))
        max_concurrent = int(os.getenv("MAX_CONCURRENT", "10"))

        if command == "full":
            result = await run_l2_api_details(
                limit=limit,
                batch_size=batch_size,
                max_concurrent=max_concurrent,
                dry_run=False
            )
        elif command == "backfill":
            result = await run_l2_incremental_backfill(
                days_back=7,
                batch_size=batch_size,
                max_concurrent=max_concurrent // 2  # å›å¡«æ—¶ä½¿ç”¨æ›´ä¿å®ˆçš„å¹¶å‘
            )
        elif command == "dry-run":
            result = await run_l2_api_details(
                limit=limit,
                batch_size=batch_size,
                max_concurrent=max_concurrent,
                dry_run=True
            )
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {command}")
            sys.exit(1)

        # è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ¯ L2 APIè¯¦æƒ…é‡‡é›†ç»“æœ")
        print("="*60)
        print(f"çŠ¶æ€: {result.get('status', 'unknown')}")
        print(f"æ€»è€—æ—¶: {result.get('duration_seconds', 0):.1f}ç§’")

        if result.get("status") == "completed":
            print(f"è¯·æ±‚æ€»æ•°: {result.get('total_requested', 0)}")
            print(f"é‡‡é›†æˆåŠŸ: {result.get('collection_success', 0)} ({result.get('collection_success_rate', 0):.1f}%)")
            print(f"å†™å…¥æˆåŠŸ: {result.get('db_success', 0)}")
            print(f"å†™å…¥å¤±è´¥: {result.get('db_failed', 0)}")

            # æ˜¾ç¤ºé‡‡é›†ç»Ÿè®¡
            if "collection_stats" in result:
                stats = result["collection_stats"]
                print("\nğŸ“Š é‡‡é›†ç»Ÿè®¡:")
                print(f"  APIè¯·æ±‚: {stats.get('requests_made', 0)}")
                print(f"  æˆåŠŸè¯·æ±‚: {stats.get('successful_requests', 0)}")
                print(f"  å¤±è´¥è¯·æ±‚: {stats.get('failed_requests', 0)}")
                print(f"  é€Ÿç‡é™åˆ¶: {stats.get('rate_limited', 0)}")
                print(f"  æ•°æ®å¤§å°: {stats.get('total_data_size', 0) / 1024:.1f}KB")
        else:
            print(f"é”™è¯¯: {result.get('error', 'unknown error')}")

        print("="*60)

    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())
