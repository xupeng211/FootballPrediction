#!/usr/bin/env python3
"""
å¼‚æ­¥I/Oæ€§èƒ½ä¼˜åŒ–æ¨¡å—
æä¾›é«˜æ€§èƒ½å¼‚æ­¥æ“ä½œã€è¿æ¥æ± ä¼˜åŒ–ã€æ‰¹é‡å¤„ç†ç­‰åŠŸèƒ½
"""

import asyncio
import time
from collections.abc import Callable
from contextlib import asynccontextmanager
from dataclasses import dataclass
from typing import Any, TypeVar

import aiofiles
import aiofiles.os
from sqlalchemy.sql import text

from src.core.logger import get_logger
from src.database.connection import DatabaseManager

logger = get_logger(__name__)

T = TypeVar("T")


@dataclass
class AsyncPerformanceMetrics:
    """å¼‚æ­¥æ€§èƒ½æŒ‡æ ‡"""

    operation_count: int = 0
    total_time: float = 0.0
    avg_time: float = 0.0
    peak_time: float = 0.0
    errors_count: int = 0
    last_reset: float = time.time()


class AsyncConnectionPool:
    """å¼‚æ­¥è¿æ¥æ± ä¼˜åŒ–å™¨"""

    def __init__(
        self,
        db_manager: DatabaseManager,
        min_size: int = 5,
        max_size: int = 20,
        timeout: float = 30.0,
    ):
        self.db_manager = db_manager
        self.min_size = min_size
        self.max_size = max_size
        self.timeout = timeout
        self._pool_size = min_size
        self._active_connections = 0
        self._connection_requests = 0

    @asynccontextmanager
    async def get_connection(self):
        """è·å–è¿æ¥æ± ä¸­çš„è¿æ¥"""
        start_time = time.time()

        try:
            # ç­‰å¾…å¯ç”¨è¿æ¥
            while self._active_connections >= self.max_size:
                await asyncio.sleep(0.01)
                if time.time() - start_time > self.timeout:
                    raise TimeoutError("è¿æ¥æ± è¶…æ—¶")

            self._active_connections += 1
            self._connection_requests += 1

            async with self.db_manager.get_async_session() as session:
                yield session

        finally:
            self._active_connections -= 1

    def get_pool_stats(self) -> dict[str, Any]:
        """è·å–è¿æ¥æ± ç»Ÿè®¡"""
        return {
            "active_connections": self._active_connections,
            "pool_size": self._pool_size,
            "total_requests": self._connection_requests,
            "utilization": self._active_connections / self.max_size,
        }


class AsyncBatchProcessor:
    """å¼‚æ­¥æ‰¹é‡å¤„ç†å™¨"""

    def __init__(
        self,
        batch_size: int = 100,
        max_concurrent_batches: int = 5,
        timeout: float = 30.0,
    ):
        self.batch_size = batch_size
        self.max_concurrent_batches = max_concurrent_batches
        self.timeout = timeout
        self.metrics = AsyncPerformanceMetrics()

    async def process_batch(
        self,
        items: list[T],
        processor: Callable[[list[T]], Any],
        progress_callback: Callable[[int, int], None] | None = None,
    ) -> list[Any]:
        """
        æ‰¹é‡å¤„ç†æ•°æ®

        Args:
            items: è¦å¤„ç†çš„æ•°æ®åˆ—è¡¨
            processor: æ‰¹é‡å¤„ç†å‡½æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        start_time = time.time()
        results = []

        # åˆ†æ‰¹å¤„ç†
        batches = [
            items[i : i + self.batch_size]
            for i in range(0, len(items), self.batch_size)
        ]

        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ‰¹æ¬¡æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent_batches)

        async def process_single_batch(batch: list[T], batch_index: int) -> Any:
            async with semaphore:
                try:
                    batch_start = time.time()
                    result = await processor(batch)
                    batch_time = time.time() - batch_start

                    # æ›´æ–°æŒ‡æ ‡
                    self.metrics.operation_count += 1
                    self.metrics.total_time += batch_time
                    self.metrics.avg_time = (
                        self.metrics.total_time / self.metrics.operation_count
                    )
                    self.metrics.peak_time = max(self.metrics.peak_time, batch_time)

                    # è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(batch_index + 1, len(batches))

                    return result

                except Exception as e:
                    self.metrics.errors_count += 1
                    logger.error(f"æ‰¹æ¬¡ {batch_index} å¤„ç†å¤±è´¥: {e}")
                    raise

        # å¹¶å‘å¤„ç†æ‰¹æ¬¡
        tasks = [process_single_batch(batch, i) for i, batch in enumerate(batches)]

        try:
            batch_results = await asyncio.wait_for(
                asyncio.gather(*tasks, return_exceptions=True), timeout=self.timeout
            )

            # å¤„ç†ç»“æœå’Œå¼‚å¸¸
            for result in batch_results:
                if isinstance(result, Exception):
                    logger.error(f"æ‰¹æ¬¡å¤„ç†å¼‚å¸¸: {result}")
                    results.append(None)
                else:
                    results.append(result)

        except TimeoutError:
            logger.error(f"æ‰¹é‡å¤„ç†è¶…æ—¶: {self.timeout}ç§’")
            raise

        total_time = time.time() - start_time
        logger.info(
            f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(items)}é¡¹æ•°æ®, {len(batches)}ä¸ªæ‰¹æ¬¡, "
            f"è€—æ—¶ {total_time:.3f}ç§’"
        )

        return results


class AsyncQueryOptimizer:
    """å¼‚æ­¥æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.query_cache = {}
        self.metrics = AsyncPerformanceMetrics()

    async def execute_optimized_query(
        self,
        query: str,
        params: dict[str, Any] | None = None,
        use_cache: bool = True,
        fetch_mode: str = "all",  # all, one, many
    ) -> Any:
        """
        æ‰§è¡Œä¼˜åŒ–çš„æŸ¥è¯¢

        Args:
            query: SQLæŸ¥è¯¢è¯­å¥
            params: æŸ¥è¯¢å‚æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜
            fetch_mode: è·å–æ¨¡å¼

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()

        # æŸ¥è¯¢ç¼“å­˜é”®
        cache_key = f"{query}_{str(params)}" if use_cache else None

        try:
            # æ£€æŸ¥ç¼“å­˜
            if cache_key and cache_key in self.query_cache:
                logger.debug(f"ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜: {cache_key[:50]}...")
                return self.query_cache[cache_key]

            async with self.db_manager.get_async_session() as session:
                # æ‰§è¡ŒæŸ¥è¯¢
                stmt = text(query)
                result = await session.execute(stmt, params or {})

                # æ ¹æ®æ¨¡å¼è·å–ç»“æœ
                if fetch_mode == "one":
                    data = result.scalar_one_or_none()
                elif fetch_mode == "many":
                    data = result.scalars().many()
                else:  # all
                    data = result.scalars().all()

                # ç¼“å­˜ç»“æœï¼ˆä»…å¯¹å°æ•°æ®é›†ï¼‰
                if cache_key and isinstance(data, (list, tuple)) and len(data) < 1000:
                    self.query_cache[cache_key] = data

                # æ›´æ–°æŒ‡æ ‡
                query_time = time.time() - start_time
                self.metrics.operation_count += 1
                self.metrics.total_time += query_time
                self.metrics.avg_time = (
                    self.metrics.total_time / self.metrics.operation_count
                )
                self.metrics.peak_time = max(self.metrics.peak_time, query_time)

                logger.debug(f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆ: {query_time:.3f}ç§’")

                return data

        except Exception as e:
            self.metrics.errors_count += 1
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            raise

    async def execute_batch_queries(
        self,
        queries: list[dict[str, Any]],  # [{"query": "...", "params": {...}}, ...]
        max_concurrent: int = 10,
    ) -> list[Any]:
        """
        æ‰¹é‡æ‰§è¡ŒæŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def execute_single_query(query_data: dict[str, Any]) -> Any:
            async with semaphore:
                return await self.execute_optimized_query(
                    query_data["query"],
                    query_data.get("params"),
                    query_data.get("use_cache", True),
                    query_data.get("fetch_mode", "all"),
                )

        # å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢
        tasks = [execute_single_query(q) for q in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æŸ¥è¯¢ä¸­çš„å¼‚å¸¸: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)

        return processed_results

    def clear_cache(self):
        """æ¸…ç©ºæŸ¥è¯¢ç¼“å­˜"""
        self.query_cache.clear()
        logger.info("æŸ¥è¯¢ç¼“å­˜å·²æ¸…ç©º")

    def get_cache_stats(self) -> dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        return {
            "cache_size": len(self.query_cache),
            "operation_count": self.metrics.operation_count,
            "avg_query_time": self.metrics.avg_time,
            "error_rate": (
                self.metrics.errors_count / max(1, self.metrics.operation_count)
            ),
        }


class AsyncFileOptimizer:
    """å¼‚æ­¥æ–‡ä»¶æ“ä½œä¼˜åŒ–å™¨"""

    def __init__(self, chunk_size: int = 8192):
        self.chunk_size = chunk_size
        self.metrics = AsyncPerformanceMetrics()

    async def read_file_chunks(
        self,
        file_path: str,
        processor: Callable[[bytes], Any],
        progress_callback: Callable[[int, int], None] | None = None,
    ) -> Any:
        """
        åˆ†å—è¯»å–æ–‡ä»¶å¹¶å¤„ç†

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            processor: æ•°æ®å¤„ç†å‡½æ•°
            progress_callback: è¿›åº¦å›è°ƒ

        Returns:
            å¤„ç†ç»“æœ
        """
        start_time = time.time()

        try:
            # è·å–æ–‡ä»¶å¤§å°
            file_size = (await aiofiles.os.stat(file_path)).st_size
            processed_bytes = 0

            async with aiofiles.open(file_path, "rb") as file:
                while True:
                    chunk = await file.read(self.chunk_size)
                    if not chunk:
                        break

                    # å¤„ç†æ•°æ®å—
                    await processor(chunk)
                    processed_bytes += len(chunk)

                    # è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(processed_bytes, file_size)

            total_time = time.time() - start_time
            logger.info(
                f"å¼‚æ­¥æ–‡ä»¶è¯»å–å®Œæˆ: {file_path}, "
                f"å¤§å° {file_size} bytes, è€—æ—¶ {total_time:.3f}ç§’"
            )

        except Exception as e:
            self.metrics.errors_count += 1
            logger.error(f"å¼‚æ­¥æ–‡ä»¶è¯»å–å¤±è´¥: {e}")
            raise

    async def write_file_chunks(
        self,
        file_path: str,
        data_chunks: list[bytes],
        progress_callback: Callable[[int, int], None] | None = None,
    ) -> None:
        """
        åˆ†å—å†™å…¥æ–‡ä»¶

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            data_chunks: æ•°æ®å—åˆ—è¡¨
            progress_callback: è¿›åº¦å›è°ƒ
        """
        start_time = time.time()
        total_chunks = len(data_chunks)

        try:
            async with aiofiles.open(file_path, "wb") as file:
                for i, chunk in enumerate(data_chunks):
                    await file.write(chunk)

                    # è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(i + 1, total_chunks)

            total_time = time.time() - start_time
            total_size = sum(len(chunk) for chunk in data_chunks)

            logger.info(
                f"å¼‚æ­¥æ–‡ä»¶å†™å…¥å®Œæˆ: {file_path}, "
                f"å¤§å° {total_size} bytes, è€—æ—¶ {total_time:.3f}ç§’"
            )

        except Exception as e:
            self.metrics.errors_count += 1
            logger.error(f"å¼‚æ­¥æ–‡ä»¶å†™å…¥å¤±è´¥: {e}")
            raise

    def get_performance_stats(self) -> dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            "operation_count": self.metrics.operation_count,
            "avg_time": self.metrics.avg_time,
            "peak_time": self.metrics.peak_time,
            "error_count": self.metrics.errors_count,
            "error_rate": (
                self.metrics.errors_count / max(1, self.metrics.operation_count)
            ),
        }


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_connection_pool: AsyncConnectionPool | None = None
_global_batch_processor: AsyncBatchProcessor | None = None
_global_query_optimizer: AsyncQueryOptimizer | None = None
_global_file_optimizer: AsyncFileOptimizer | None = None


def get_connection_pool() -> AsyncConnectionPool:
    """è·å–å…¨å±€è¿æ¥æ± """
    global _global_connection_pool
    if _global_connection_pool is None:
        _global_connection_pool = AsyncConnectionPool(DatabaseManager())
    return _global_connection_pool


def get_batch_processor() -> AsyncBatchProcessor:
    """è·å–å…¨å±€æ‰¹é‡å¤„ç†å™¨"""
    global _global_batch_processor
    if _global_batch_processor is None:
        _global_batch_processor = AsyncBatchProcessor()
    return _global_batch_processor


def get_query_optimizer() -> AsyncQueryOptimizer:
    """è·å–å…¨å±€æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    global _global_query_optimizer
    if _global_query_optimizer is None:
        _global_query_optimizer = AsyncQueryOptimizer(DatabaseManager())
    return _global_query_optimizer


def get_file_optimizer() -> AsyncFileOptimizer:
    """è·å–å…¨å±€æ–‡ä»¶ä¼˜åŒ–å™¨"""
    global _global_file_optimizer
    if _global_file_optimizer is None:
        _global_file_optimizer = AsyncFileOptimizer()
    return _global_file_optimizer


# ä¾¿æ·è£…é¥°å™¨
def async_performance_track(func: Callable) -> Callable:
    """å¼‚æ­¥æ€§èƒ½è·Ÿè¸ªè£…é¥°å™¨"""

    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            execution_time = time.time() - start_time

            logger.info(f"å¼‚æ­¥å‡½æ•° {func.__name__} æ‰§è¡Œå®Œæˆ: {execution_time:.3f}ç§’")

            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(
                f"å¼‚æ­¥å‡½æ•° {func.__name__} æ‰§è¡Œå¤±è´¥: {e}, è€—æ—¶: {execution_time:.3f}ç§’"
            )
            raise

    return wrapper


if __name__ == "__main__":

    async def demo_async_optimization():
        """æ¼”ç¤ºå¼‚æ­¥ä¼˜åŒ–åŠŸèƒ½"""
        print("ğŸš€ æ¼”ç¤ºå¼‚æ­¥I/Oæ€§èƒ½ä¼˜åŒ–")

        # æ‰¹é‡å¤„ç†æ¼”ç¤º
        processor = get_batch_processor()

        # æ¨¡æ‹Ÿæ•°æ®
        data = list(range(1000))

        def process_batch(batch: list[int]) -> list[int]:
            return [x * 2 for x in batch]

        def progress_callback(current: int, total: int):
            print(f"è¿›åº¦: {current}/{total} ({current/total*100:.1f}%)")

        results = await processor.process_batch(data, process_batch, progress_callback)

        print(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {len(results)} ä¸ªç»“æœ")

        # æŸ¥è¯¢ä¼˜åŒ–æ¼”ç¤º
        query_optimizer = get_query_optimizer()

        # æ¨¡æ‹ŸæŸ¥è¯¢
        test_queries = [
            {"query": "SELECT 1 as test", "params": None},
            {"query": "SELECT 2 as test", "params": None},
        ]

        query_results = await query_optimizer.execute_batch_queries(test_queries)
        print(f"âœ… æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼Œæ‰§è¡Œäº† {len(query_results)} ä¸ªæŸ¥è¯¢")

        # æ€§èƒ½ç»Ÿè®¡
        print(f"ğŸ“Š æ‰¹é‡å¤„ç†ç»Ÿè®¡: {processor.metrics.__dict__}")
        print(f"ğŸ“Š æŸ¥è¯¢ä¼˜åŒ–ç»Ÿè®¡: {query_optimizer.get_cache_stats()}")

    asyncio.run(demo_async_optimization())
