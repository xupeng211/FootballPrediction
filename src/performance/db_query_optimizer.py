#!/usr/bin/env python3
"""
æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–æ¨¡å—
æä¾›æ™ºèƒ½æŸ¥è¯¢ä¼˜åŒ–ã€ç´¢å¼•å»ºè®®ã€æ‰§è¡Œè®¡åˆ’åˆ†æç­‰åŠŸèƒ½
"""

import asyncio
import re
import time
from dataclasses import dataclass
from typing import Any

from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

from src.core.logger import get_logger
from src.database.connection import DatabaseManager

logger = get_logger(__name__)


@dataclass
class QueryPerformanceMetrics:
    """æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡"""

    query_hash: str
    execution_count: int = 0
    total_time: float = 0.0
    avg_time: float = 0.0
    min_time: float = float("inf")
    max_time: float = 0.0
    last_executed: float = 0.0
    result_count: int = 0
    error_count: int = 0


@dataclass
class IndexRecommendation:
    """ç´¢å¼•æ¨è"""

    table_name: str
    column_names: list[str]
    index_type: str  # btree, hash, gin, gist
    estimated_improvement: float
    reason: str
    priority: str  # high, medium, low


class QueryAnalyzer:
    """SQLæŸ¥è¯¢åˆ†æå™¨"""

    def __init__(self):
        self.query_patterns = {
            "slow_patterns": [
                r"SELECT\s+.*\s+FROM\s+\w+\s+WHERE\s+.*\s+LIKE\s+",  # LIKEæŸ¥è¯¢
                r"SELECT\s+.*\s+FROM\s+\w+\s+ORDER\s+BY\s+.+\s+LIMIT\s+\d+",  # ORDER BY + LIMIT
                r"SELECT\s+.*\s+FROM\s+\w+\s+WHERE\s+.+\s+IN\s+\([^)]+\)",  # INæŸ¥è¯¢
                r"SELECT\s+COUNT\(.*\)\s+FROM\s+\w+",  # COUNTæŸ¥è¯¢
                r"SELECT\s+.*\s+FROM\s+\w+\s+JOIN\s+.+\s+JOIN\s+",  # å¤šJOIN
            ],
            "optimization_opportunities": [
                r"SELECT\s+\*\s+FROM",  # SELECT *
                r"WHERE\s+.*\s*=\s*.*\s+OR\s+.*\s*=\s*",  # ORæ¡ä»¶
                r"WHERE\s+.*\s+NOT\s+LIKE",  # NOT LIKE
                r"ORDER\s+BY\s+.+\s+DESC",  # DESCæ’åº
            ],
        }

    def analyze_query(self, query: str) -> dict[str, Any]:
        """
        åˆ†æSQLæŸ¥è¯¢

        Args:
            query: SQLæŸ¥è¯¢è¯­å¥

        Returns:
            åˆ†æç»“æœ
        """
        analysis = {
            "query_type": self._detect_query_type(query),
            "complexity": self._estimate_complexity(query),
            "slow_patterns": [],
            "optimization_opportunities": [],
            "suggested_indexes": [],
            "estimated_cost": self._estimate_execution_cost(query),
        }

        # æ£€æŸ¥æ…¢æŸ¥è¯¢æ¨¡å¼
        for pattern_name, pattern_list in self.query_patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, query, re.IGNORECASE | re.MULTILINE)
                if matches:
                    if pattern_name == "slow_patterns":
                        analysis["slow_patterns"].extend(matches)
                    elif pattern_name == "optimization_opportunities":
                        analysis["optimization_opportunities"].extend(matches)

        # ç”Ÿæˆç´¢å¼•å»ºè®®
        analysis["suggested_indexes"] = self._suggest_indexes(query)

        return analysis

    def _detect_query_type(self, query: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢ç±»å‹"""
        query_upper = query.strip().upper()
        if query_upper.startswith("SELECT"):
            if "JOIN" in query_upper:
                return "SELECT_JOIN"
            elif "GROUP BY" in query_upper:
                return "SELECT_AGGREGATE"
            elif "ORDER BY" in query_upper:
                return "SELECT_ORDERED"
            else:
                return "SELECT_SIMPLE"
        elif query_upper.startswith("INSERT"):
            return "INSERT"
        elif query_upper.startswith("UPDATE"):
            return "UPDATE"
        elif query_upper.startswith("DELETE"):
            return "DELETE"
        else:
            return "OTHER"

    def _estimate_complexity(self, query: str) -> str:
        """ä¼°ç®—æŸ¥è¯¢å¤æ‚åº¦"""
        complexity_score = 0
        query_upper = query.upper()

        # åŸºç¡€åˆ†æ•°
        if "JOIN" in query_upper:
            complexity_score += query_upper.count("JOIN") * 2
        if "SUBQUERY" in query_upper or "(" in query:
            complexity_score += 2
        if "GROUP BY" in query_upper:
            complexity_score += 1
        if "ORDER BY" in query_upper:
            complexity_score += 1
        if "UNION" in query_upper:
            complexity_score += 2
        if "HAVING" in query_upper:
            complexity_score += 1

        if complexity_score <= 2:
            return "LOW"
        elif complexity_score <= 5:
            return "MEDIUM"
        else:
            return "HIGH"

    def _estimate_execution_cost(self, query: str) -> float:
        """ä¼°ç®—æ‰§è¡Œæˆæœ¬ï¼ˆç›¸å¯¹å€¼ï¼‰"""
        base_cost = 1.0
        query_upper = query.upper()

        # å¤æ‚åº¦è°ƒæ•´
        if "JOIN" in query_upper:
            base_cost *= 1.5 * query_upper.count("JOIN")
        if "LIKE" in query_upper:
            base_cost *= 2.0
        if "ORDER BY" in query_upper:
            base_cost *= 1.3
        if "GROUP BY" in query_upper:
            base_cost *= 1.4
        if query_upper.count("SELECT") > 1:
            base_cost *= 1.2

        return round(base_cost, 2)

    def _suggest_indexes(self, query: str) -> list[IndexRecommendation]:
        """å»ºè®®ç´¢å¼•"""
        recommendations = []

        # æå–WHEREæ¡ä»¶ä¸­çš„å­—æ®µ
        where_match = re.search(
            r"WHERE\s+(.+?)(?:\s+GROUP\s+BY|\s+ORDER\s+BY|\s+LIMIT|$)",
            query,
            re.IGNORECASE | re.MULTILINE,
        )
        if where_match:
            where_clause = where_match.group(1)
            # æå–å­—æ®µå
            column_pattern = r"(\w+)\s*(?:=|>|<|LIKE|IN)"
            columns = re.findall(column_pattern, where_clause, re.IGNORECASE)

            # ä¸ºæ¯ä¸ªå­—æ®µå»ºè®®ç´¢å¼•
            for column in columns:
                if column.lower() not in ["id", "created_at", "updated_at"]:
                    recommendations.append(
                        IndexRecommendation(
                            table_name="unknown",  # éœ€è¦ä»FROMå­å¥æå–
                            column_names=[column],
                            index_type="btree",
                            estimated_improvement=0.7,
                            reason=f"WHEREæ¡ä»¶ä¸­ä½¿ç”¨ {column}",
                            priority="high",
                        )
                    )

        # æå–ORDER BYå­—æ®µ
        order_match = re.search(
            r"ORDER\s+BY\s+(.+?)(?:\s+LIMIT|$)", query, re.IGNORECASE | re.MULTILINE
        )
        if order_match:
            order_clause = order_match.group(1)
            columns = [col.strip() for col in order_clause.split(",")]

            for column in columns:
                column = column.split()[0]  # ç§»é™¤ASC/DESC
                if column.lower() not in ["id", "created_at", "updated_at"]:
                    recommendations.append(
                        IndexRecommendation(
                            table_name="unknown",
                            column_names=[column],
                            index_type="btree",
                            estimated_improvement=0.5,
                            reason=f"ORDER BYä½¿ç”¨ {column}",
                            priority="medium",
                        )
                    )

        return recommendations


class QueryOptimizer:
    """æŸ¥è¯¢ä¼˜åŒ–å™¨"""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.query_metrics: dict[str, QueryPerformanceMetrics] = {}
        self.analyzer = QueryAnalyzer()
        self._slow_query_threshold = 1.0  # 1ç§’

    async def execute_optimized_query(
        self,
        query: str,
        params: dict[str, Any] | None = None,
        analyze: bool = True,
        auto_explain: bool = False,
    ) -> Any:
        """
        æ‰§è¡Œä¼˜åŒ–çš„æŸ¥è¯¢

        Args:
            query: SQLæŸ¥è¯¢
            params: æŸ¥è¯¢å‚æ•°
            analyze: æ˜¯å¦åˆ†ææŸ¥è¯¢
            auto_explain: æ˜¯å¦è‡ªåŠ¨ç”Ÿæˆæ‰§è¡Œè®¡åˆ’

        Returns:
            æŸ¥è¯¢ç»“æœ
        """
        start_time = time.time()
        query_hash = self._generate_query_hash(query)

        # è·å–æˆ–åˆ›å»ºæ€§èƒ½æŒ‡æ ‡
        if query_hash not in self.query_metrics:
            self.query_metrics[query_hash] = QueryPerformanceMetrics(
                query_hash=query_hash
            )
        metrics = self.query_metrics[query_hash]

        try:
            async with self.db_manager.get_async_session() as session:
                # æŸ¥è¯¢åˆ†æ
                if analyze:
                    analysis = self.analyzer.analyze_query(query)
                    if analysis["complexity"] == "HIGH":
                        logger.warning(f"å¤æ‚æŸ¥è¯¢æ£€æµ‹: {query[:100]}...")

                # è‡ªåŠ¨EXPLAINï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if auto_explain and self._is_select_query(query):
                    explain_result = await self._explain_query(session, query, params)
                    logger.debug(f"æ‰§è¡Œè®¡åˆ’: {explain_result}")

                # æ‰§è¡ŒæŸ¥è¯¢
                stmt = text(query)
                result = await session.execute(stmt, params or {})

                # è·å–ç»“æœ
                if self._is_single_result_query(query):
                    data = result.scalar_one_or_none()
                else:
                    data = result.scalars().all()

                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                execution_time = time.time() - start_time
                self._update_metrics(
                    metrics, execution_time, len(data) if isinstance(data, list) else 1
                )

                # æ…¢æŸ¥è¯¢è­¦å‘Š
                if execution_time > self._slow_query_threshold:
                    logger.warning(
                        f"æ…¢æŸ¥è¯¢æ£€æµ‹: {execution_time:.3f}ç§’, {query[:100]}..."
                    )

                return data

        except Exception as e:
            execution_time = time.time() - start_time
            metrics.error_count += 1
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}, è€—æ—¶: {execution_time:.3f}ç§’")
            raise

    async def execute_batch_with_optimization(
        self,
        queries: list[dict[str, Any]],
        max_concurrent: int = 10,
    ) -> list[Any]:
        """
        æ‰¹é‡æ‰§è¡Œä¼˜åŒ–çš„æŸ¥è¯¢

        Args:
            queries: æŸ¥è¯¢åˆ—è¡¨ [{"query": "...", "params": {...}}, ...]
            max_concurrent: æœ€å¤§å¹¶å‘æ•°

        Returns:
            æŸ¥è¯¢ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrent)

        async def execute_single(query_data: dict[str, Any]) -> Any:
            async with semaphore:
                return await self.execute_optimized_query(
                    query_data["query"],
                    query_data.get("params"),
                    analyze=True,
                    auto_explain=False,
                )

        # æŒ‰å¤æ‚åº¦æ’åºï¼Œå…ˆæ‰§è¡Œç®€å•æŸ¥è¯¢
        sorted_queries = sorted(
            queries, key=lambda q: self.analyzer._estimate_execution_cost(q["query"])
        )

        tasks = [execute_single(q) for q in sorted_queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"æ‰¹é‡æŸ¥è¯¢å¼‚å¸¸: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)

        return processed_results

    async def _explain_query(
        self, session: AsyncSession, query: str, params: dict[str, Any] | None
    ) -> str:
        """ç”ŸæˆæŸ¥è¯¢æ‰§è¡Œè®¡åˆ’"""
        try:
            explain_query = f"EXPLAIN (ANALYZE, BUFFERS) {query}"
            stmt = text(explain_query)
            result = await session.execute(stmt, params)
            explain_plan = result.fetchall()
            return "\n".join(str(row[0]) for row in explain_plan)
        except Exception as e:
            logger.warning(f"æ‰§è¡Œè®¡åˆ’ç”Ÿæˆå¤±è´¥: {e}")
            return ""

    def _generate_query_hash(self, query: str) -> str:
        """ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ"""
        import hashlib

        normalized_query = re.sub(r"\s+", " ", query.strip().lower())
        return hashlib.md5(normalized_query.encode()).hexdigest()[:16]

    def _is_select_query(self, query: str) -> bool:
        """æ˜¯å¦ä¸ºSELECTæŸ¥è¯¢"""
        return query.strip().upper().startswith("SELECT")

    def _is_single_result_query(self, query: str) -> bool:
        """æ˜¯å¦ä¸ºå•ç»“æœæŸ¥è¯¢"""
        query_upper = query.upper()
        return (
            "LIMIT 1" in query_upper
            or "WHERE" in query_upper
            and "id" in query_upper.lower()
        )

    def _update_metrics(
        self, metrics: QueryPerformanceMetrics, execution_time: float, result_count: int
    ):
        """æ›´æ–°æ€§èƒ½æŒ‡æ ‡"""
        metrics.execution_count += 1
        metrics.total_time += execution_time
        metrics.avg_time = metrics.total_time / metrics.execution_count
        metrics.min_time = min(metrics.min_time, execution_time)
        metrics.max_time = max(metrics.max_time, execution_time)
        metrics.last_executed = time.time()
        metrics.result_count = result_count

    def get_performance_report(self) -> dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        if not self.query_metrics:
            return {"message": "æ²¡æœ‰æŸ¥è¯¢æ€§èƒ½æ•°æ®"}

        # ç»Ÿè®¡ä¿¡æ¯
        total_queries = sum(m.execution_count for m in self.query_metrics.values())
        total_time = sum(m.total_time for m in self.query_metrics.values())
        total_errors = sum(m.error_count for m in self.query_metrics.values())

        # æ…¢æŸ¥è¯¢
        slow_queries = [
            m
            for m in self.query_metrics.values()
            if m.avg_time > self._slow_query_threshold
        ]

        # é«˜é¢‘æŸ¥è¯¢
        high_frequency_queries = sorted(
            self.query_metrics.values(), key=lambda m: m.execution_count, reverse=True
        )[:10]

        return {
            "summary": {
                "total_queries": total_queries,
                "total_time": round(total_time, 3),
                "avg_query_time": round(total_time / max(1, total_queries), 3),
                "error_rate": round(total_errors / max(1, total_queries) * 100, 2),
                "unique_queries": len(self.query_metrics),
                "slow_queries_count": len(slow_queries),
            },
            "slow_queries": [
                {
                    "query_hash": m.query_hash,
                    "execution_count": m.execution_count,
                    "avg_time": round(m.avg_time, 3),
                    "max_time": round(m.max_time, 3),
                }
                for m in slow_queries
            ],
            "top_queries": [
                {
                    "query_hash": m.query_hash,
                    "execution_count": m.execution_count,
                    "avg_time": round(m.avg_time, 3),
                    "total_time": round(m.total_time, 3),
                    "result_count": m.result_count,
                }
                for m in high_frequency_queries
            ],
            "recommendations": self._generate_recommendations(),
        }

    def _generate_recommendations(self) -> list[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # åˆ†ææ…¢æŸ¥è¯¢
        slow_queries = [
            m
            for m in self.query_metrics.values()
            if m.avg_time > self._slow_query_threshold
        ]

        if slow_queries:
            recommendations.append(
                f"å‘ç° {len(slow_queries)} ä¸ªæ…¢æŸ¥è¯¢ï¼Œå»ºè®®ä¼˜åŒ–æŸ¥è¯¢æˆ–æ·»åŠ ç´¢å¼•"
            )

        # åˆ†æé”™è¯¯ç‡
        total_errors = sum(m.error_count for m in self.query_metrics.values())
        total_queries = sum(m.execution_count for m in self.query_metrics.values())
        error_rate = total_errors / max(1, total_queries)

        if error_rate > 0.05:  # 5%
            recommendations.append(
                f"æŸ¥è¯¢é”™è¯¯ç‡è¿‡é«˜ ({error_rate:.1%})ï¼Œå»ºè®®æ£€æŸ¥æŸ¥è¯¢è¯­æ³•å’Œæ•°æ®"
            )

        # åˆ†æé«˜é¢‘æŸ¥è¯¢
        high_freq = [m for m in self.query_metrics.values() if m.execution_count > 100]

        if high_freq:
            recommendations.append(
                f"å‘ç° {len(high_freq)} ä¸ªé«˜é¢‘æŸ¥è¯¢ï¼Œå»ºè®®æ·»åŠ ç¼“å­˜æˆ–ä¼˜åŒ–ç´¢å¼•"
            )

        return recommendations

    def clear_metrics(self):
        """æ¸…ç©ºæ€§èƒ½æŒ‡æ ‡"""
        self.query_metrics.clear()
        logger.info("æŸ¥è¯¢æ€§èƒ½æŒ‡æ ‡å·²æ¸…ç©º")


class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨"""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.query_optimizer = QueryOptimizer(db_manager)

    async def analyze_table_performance(self, table_names: list[str]) -> dict[str, Any]:
        """åˆ†æè¡¨æ€§èƒ½"""
        async with self.db_manager.get_async_session() as session:
            analysis = {}

            for table_name in table_names:
                try:
                    # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
                    stats_query = """
                    SELECT
                        schemaname,
                        tablename,
                        n_tup_ins as inserts,
                        n_tup_upd as updates,
                        n_tup_del as deletes,
                        n_live_tup as live_rows,
                        n_dead_tup as dead_rows,
                        last_vacuum,
                        last_autovacuum,
                        last_analyze,
                        last_autoanalyze
                    FROM pg_stat_user_tables
                    WHERE tablename = :table_name
                    """
                    stmt = text(stats_query)
                    result = await session.execute(stmt, {"table_name": table_name})
                    stats = result.fetchone()

                    # è·å–ç´¢å¼•ä¿¡æ¯
                    index_query = """
                    SELECT
                        indexname,
                        indexdef
                    FROM pg_indexes
                    WHERE tablename = :table_name
                    """
                    stmt = text(index_query)
                    index_result = await session.execute(
                        stmt, {"table_name": table_name}
                    )
                    indexes = index_result.fetchall()

                    analysis[table_name] = {
                        "table_stats": dict(stats._mapping) if stats else None,
                        "indexes": [
                            {"name": row[0], "definition": row[1]} for row in indexes
                        ],
                        "recommendations": self._generate_table_recommendations(
                            table_name, stats, indexes
                        ),
                    }

                except Exception as e:
                    logger.error(f"è¡¨ {table_name} æ€§èƒ½åˆ†æå¤±è´¥: {e}")
                    analysis[table_name] = {"error": str(e)}

            return analysis

    def _generate_table_recommendations(
        self, table_name: str, stats: Any, indexes: list
    ) -> list[str]:
        """ç”Ÿæˆè¡¨ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        if not stats:
            return ["æ— æ³•è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"]

        # æ£€æŸ¥æ­»è¡Œæ¯”ä¾‹
        total_rows = stats.n_live_tup + stats.n_dead_tup
        if total_rows > 0:
            dead_ratio = stats.n_dead_tup / total_rows
            if dead_ratio > 0.2:  # 20%
                recommendations.append(
                    f"æ­»è¡Œæ¯”ä¾‹è¿‡é«˜ ({dead_ratio:.1%})ï¼Œå»ºè®®æ‰§è¡ŒVACUUM"
                )

        # æ£€æŸ¥ç´¢å¼•æ•°é‡
        if len(indexes) > 10:
            recommendations.append(f"ç´¢å¼•æ•°é‡è¿‡å¤š ({len(indexes)})ï¼Œå¯èƒ½å½±å“å†™å…¥æ€§èƒ½")

        # æ£€æŸ¥æœ€ååˆ†ææ—¶é—´
        if not stats.last_analyze and not stats.last_autoanalyze:
            recommendations.append("è¡¨ä»æœªè¢«åˆ†æï¼Œå»ºè®®æ‰§è¡ŒANALYZE")

        return recommendations

    async def generate_optimization_report(self) -> dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        # è·å–æ‰€æœ‰ç”¨æˆ·è¡¨
        async with self.db_manager.get_async_session() as session:
            tables_query = """
            SELECT tablename
            FROM pg_tables
            WHERE schemaname = 'public'
            """
            stmt = text(tables_query)
            result = await session.execute(stmt)
            table_names = [row[0] for row in result.fetchall()]

        # åˆ†æè¡¨æ€§èƒ½
        table_analysis = await self.analyze_table_performance(table_names)

        # è·å–æŸ¥è¯¢æ€§èƒ½æŠ¥å‘Š
        query_report = self.query_optimizer.get_performance_report()

        return {
            "timestamp": time.time(),
            "tables": table_analysis,
            "queries": query_report,
            "overall_recommendations": self._generate_overall_recommendations(
                table_analysis, query_report
            ),
        }

    def _generate_overall_recommendations(
        self, table_analysis: dict[str, Any], query_report: dict[str, Any]
    ) -> list[str]:
        """ç”Ÿæˆæ•´ä½“ä¼˜åŒ–å»ºè®®"""
        recommendations = []

        # è¡¨ä¼˜åŒ–å»ºè®®
        vacuum_needed = [
            name
            for name, analysis in table_analysis.items()
            if analysis.get("table_stats")
            and any("VACUUM" in rec for rec in analysis.get("recommendations", []))
        ]

        if vacuum_needed:
            recommendations.append(f"å»ºè®®å¯¹è¡¨ {', '.join(vacuum_needed)} æ‰§è¡ŒVACUUM")

        # æŸ¥è¯¢ä¼˜åŒ–å»ºè®®
        if query_report.get("summary", {}).get("slow_queries_count", 0) > 0:
            recommendations.append("å‘ç°æ…¢æŸ¥è¯¢ï¼Œå»ºè®®ä¼˜åŒ–SQLè¯­å¥æˆ–æ·»åŠ ç´¢å¼•")

        # ç»¼åˆå»ºè®®
        recommendations.extend(
            [
                "å®šæœŸæ‰§è¡ŒANALYZEæ›´æ–°ç»Ÿè®¡ä¿¡æ¯",
                "ç›‘æ§æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡",
                "è€ƒè™‘ä½¿ç”¨è¿æ¥æ± ä¼˜åŒ–è¿æ¥ç®¡ç†",
                "ä¸ºé«˜é¢‘æŸ¥è¯¢æ·»åŠ é€‚å½“çš„ç´¢å¼•",
            ]
        )

        return recommendations


# å…¨å±€ä¼˜åŒ–å™¨å®ä¾‹
_global_query_optimizer: QueryOptimizer | None = None
_global_db_optimizer: DatabaseOptimizer | None = None


def get_query_optimizer() -> QueryOptimizer:
    """è·å–å…¨å±€æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    global _global_query_optimizer
    if _global_query_optimizer is None:
        _global_query_optimizer = QueryOptimizer(DatabaseManager())
    return _global_query_optimizer


def get_database_optimizer() -> DatabaseOptimizer:
    """è·å–å…¨å±€æ•°æ®åº“ä¼˜åŒ–å™¨"""
    global _global_db_optimizer
    if _global_db_optimizer is None:
        _global_db_optimizer = DatabaseOptimizer(DatabaseManager())
    return _global_db_optimizer


if __name__ == "__main__":

    async def demo_database_optimization():
        """æ¼”ç¤ºæ•°æ®åº“ä¼˜åŒ–åŠŸèƒ½"""
        print("ğŸš€ æ¼”ç¤ºæ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–")

        optimizer = get_query_optimizer()
        db_optimizer = get_database_optimizer()

        # æ¨¡æ‹ŸæŸ¥è¯¢æ‰§è¡Œ
        test_queries = [
            {"query": "SELECT 1 as test", "params": None},
            {"query": "SELECT 2 as test", "params": None},
        ]

        # æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢
        results = await optimizer.execute_batch_with_optimization(test_queries)
        print(f"âœ… æ‰§è¡Œäº† {len(results)} ä¸ªä¼˜åŒ–æŸ¥è¯¢")

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        report = optimizer.get_performance_report()
        print(f"ğŸ“Š æŸ¥è¯¢æ€§èƒ½æŠ¥å‘Š: {report}")

        # ç”Ÿæˆæ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Š
        db_report = await db_optimizer.generate_optimization_report()
        print("ğŸ“ˆ æ•°æ®åº“ä¼˜åŒ–æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

    asyncio.run(demo_database_optimization())
