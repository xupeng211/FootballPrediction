#!/usr/bin/env python3
"""
L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆå™¨ - ETL Pipeline Extract Phase
Generate Backfill Queue for L2 Data Collection

åŸºäº ETL (Extract-Transform-Load) æ¨¡å¼çš„é«˜æ€§èƒ½æ•°æ®åº“å¯¼å‡ºå·¥å…·ã€‚
ä¸“é—¨ç”¨äºä» PostgreSQL æ•°æ®åº“ä¸­ç­›é€‰å·²ç»“æŸä¸”è¯¦æƒ…ç¼ºå¤±çš„æ¯”èµ›ï¼Œ
ä¸º L2 å›å¡«è„šæœ¬æä¾›æ ‡å‡†åŒ–çš„è¾“å…¥é˜Ÿåˆ—ã€‚

Architecture:
    Extract: ä»æ•°æ®åº“ç­›é€‰æ¯”èµ› (PostgreSQL + Async SQLAlchemy)
    Transform: æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
    Load: å¯¼å‡ºä¸º CSV æ–‡ä»¶

ä½¿ç”¨ç¤ºä¾‹:
    python src/scripts/generate_backfill_queue.py

è¾“å‡ºæ–‡ä»¶:
    data/l2_backfill_queue.csv (æ ‡å‡†æ ¼å¼ï¼ŒåŒ…å« match_id åˆ—è¡¨)

Author: Database Expert
Version: 2.0 (ETL Optimized)
"""

import sys
import csv
import asyncio
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# é¡¹ç›®æ¨¡å—å¯¼å…¥
from src.database.async_manager import initialize_database, fetch_all
from sqlalchemy import text
from tqdm import tqdm

# å°è¯•å¯¼å…¥ tqdmï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç®€å•è¿›åº¦æ¡
try:
    from tqdm import tqdm

    HAS_TQDM = True
except ImportError:
    HAS_TQDM = False
    logging.warning("tqdm not installed, using simple progress indicator")

# é…ç½®ä¸“ä¸šæ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("logs/backfill_queue_generation.log", mode="a"),
    ],
)
logger = logging.getLogger(__name__)


@dataclass
class BackfillConfig:
    """å›å¡«é…ç½®å‚æ•°"""

    output_dir: str = "data"
    output_filename: str = "l2_backfill_queue.csv"
    batch_size: int = 1000  # æ‰¹é‡å¤„ç†å¤§å°ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨
    show_progress: bool = True
    dry_run: bool = False  # ä»…æŸ¥è¯¢ï¼Œä¸å¯¼å‡ºæ–‡ä»¶

    # æ¯”èµ›çŠ¶æ€é…ç½® (åŸºäº FotMob API å®é™…çŠ¶æ€)
    finished_statuses = [
        "FT",
        "AET",
        "PEN",
        "finished",
    ]  # Full Time, After Extra Time, Penalties

    # æ•°æ®å®Œæ•´åº¦é…ç½®
    incomplete_completeness = [
        "NULL",
        "partial",
        "basic",
        "detailed",
    ]  # éœ€è¦å›å¡«çš„å®Œæ•´åº¦çŠ¶æ€


class BackfillQueueGenerator:
    """
    é«˜æ€§èƒ½ L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆå™¨

    ä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†å¤§è§„æ¨¡æ¯”èµ›æ•°æ®çš„ ETL æå–å·¥å…·ã€‚
    é‡‡ç”¨å¼‚æ­¥æ•°æ®åº“æ“ä½œå’Œæ‰¹é‡å¤„ç†ä¼˜åŒ–ï¼Œç¡®ä¿åœ¨å¤§æ•°æ®é‡åœºæ™¯ä¸‹çš„ç¨³å®šæ€§èƒ½ã€‚
    """

    def __init__(self, config: BackfillConfig):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨

        Args:
            config: å›å¡«é…ç½®å‚æ•°
        """
        self.config = config
        self.output_dir = Path(config.output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)
        self.output_file = self.output_dir / config.output_filename

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_queried": 0,
            "valid_matches": 0,
            "by_season": {},
            "by_completeness": {},
            "by_status": {},
            "exported": 0,
        }

        logger.info("ğŸ”§ åˆå§‹åŒ–å›å¡«é˜Ÿåˆ—ç”Ÿæˆå™¨")
        logger.info(f"   è¾“å‡ºæ–‡ä»¶: {self.output_file}")
        logger.info(f"   æ‰¹é‡å¤§å°: {config.batch_size}")
        logger.info(f"   å·²ç»“æŸçŠ¶æ€: {config.finished_statuses}")
        logger.info(f"   ä¸å®Œæ•´çŠ¶æ€: {config.incomplete_completeness}")

    async def _initialize_database(self) -> None:
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            logger.info("ğŸ”Œ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...")
            initialize_database()
            logger.info("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def _build_sql_query(self) -> text:
        """
        æ„å»ºä¼˜åŒ–çš„ SQL æŸ¥è¯¢

        Returns:
            SQLAlchemy Text å¯¹è±¡
        """
        # æ„å»ºçŠ¶æ€æ¡ä»¶
        status_conditions = ", ".join(
            [f"'{status}'" for status in self.config.finished_statuses]
        )

        # æ„å»ºå®Œæ•´åº¦æ¡ä»¶
        completeness_conditions = []
        if "NULL" in self.config.incomplete_completeness:
            completeness_conditions.append("data_completeness IS NULL")

        for completeness in self.config.incomplete_completeness:
            if completeness != "NULL":
                completeness_conditions.append(f"data_completeness = '{completeness}'")

        completeness_clause = " OR ".join(completeness_conditions)

        # æ„å»ºå®Œæ•´æŸ¥è¯¢è¯­å¥
        query = f"""
            SELECT
                fotmob_id as match_id,
                status,
                data_completeness,
                match_date,
                home_team_name,
                away_team_name,
                season,
                league_id,
                created_at,
                updated_at
            FROM matches
            WHERE fotmob_id IS NOT NULL
              AND status IN ({status_conditions})
              AND ({completeness_clause})
            ORDER BY match_date DESC, fotmob_id DESC
        """

        logger.info(
            f"ğŸ“‹ æ„å»ºæŸ¥è¯¢: å·²ç»“æŸçŠ¶æ€ {len(self.config.finished_statuses)} ä¸ªï¼Œä¸å®Œæ•´çŠ¶æ€ {len(self.config.incomplete_completeness)} ä¸ª"
        )
        return text(query)

    async def extract_pending_matches(self) -> List[Dict[str, Any]]:
        """
        æå–éœ€è¦ L2 å›å¡«çš„æ¯”èµ›æ•°æ® (ETL Extract Phase)

        Returns:
            å¾…å›å¡«çš„æ¯”èµ›æ•°æ®åˆ—è¡¨
        """
        logger.info("ğŸ” å¼€å§‹æå–éœ€è¦ L2 å›å¡«çš„æ¯”èµ›...")

        query = self._build_sql_query()

        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            matches = await fetch_all(query)
            self.stats["total_queried"] = len(matches)

            logger.info(f"ğŸ“Š æŸ¥è¯¢ç»“æœ: å…± {len(matches)} åœºæ¯”èµ›")

            # æ•°æ®éªŒè¯å’Œç»Ÿè®¡
            valid_matches = []
            for match in matches:
                # åŸºæœ¬æ•°æ®éªŒè¯
                if not match.get("match_id"):
                    logger.warning("âš ï¸ è·³è¿‡æ— æ•ˆæ¯”èµ›è®°å½•: ç¼ºå°‘ match_id")
                    continue

                # ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
                season = match.get("season", "Unknown")
                self.stats["by_season"][season] = (
                    self.stats["by_season"].get(season, 0) + 1
                )

                completeness = match.get("data_completeness", "NULL")
                self.stats["by_completeness"][completeness] = (
                    self.stats["by_completeness"].get(completeness, 0) + 1
                )

                status = match.get("status", "Unknown")
                self.stats["by_status"][status] = (
                    self.stats["by_status"].get(status, 0) + 1
                )

                valid_matches.append(match)

            self.stats["valid_matches"] = len(valid_matches)
            logger.info(f"âœ… æœ‰æ•ˆæ¯”èµ›æ•°æ®: {len(valid_matches)} åœº")

            return valid_matches

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {e}", exc_info=True)
            raise

    def _prepare_csv_data(self, matches: List[Dict[str, Any]]) -> List[Dict[str, str]]:
        """
        æ•°æ®è½¬æ¢å’Œæ ‡å‡†åŒ– (ETL Transform Phase)

        Args:
            matches: åŸå§‹æ¯”èµ›æ•°æ®

        Returns:
            æ ‡å‡†åŒ–çš„ CSV æ•°æ®
        """
        logger.info("ğŸ”„ å¼€å§‹æ•°æ®è½¬æ¢å’Œæ ‡å‡†åŒ–...")

        csv_data = []
        for match in matches:
            csv_row = {
                "match_id": str(match["match_id"]),  # ç¡®ä¿ match_id ä¸ºå­—ç¬¦ä¸²
                "status": match.get("status", ""),
                "data_completeness": match.get("data_completeness", "NULL"),
                "match_date": self._format_datetime(match.get("match_date")),
                "home_team": match.get("home_team_name", ""),
                "away_team": match.get("away_team_name", ""),
                "season": match.get("season", ""),
                "league_id": str(match.get("league_id", "")),
                "created_at": self._format_datetime(match.get("created_at")),
                "updated_at": self._format_datetime(match.get("updated_at")),
            }
            csv_data.append(csv_row)

        logger.info(f"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {len(csv_data)} æ¡è®°å½•")
        return csv_data

    def _format_datetime(self, dt_obj: Optional[datetime]) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸæ—¶é—´å¯¹è±¡"""
        if dt_obj is None:
            return ""
        if isinstance(dt_obj, str):
            return dt_obj
        return dt_obj.strftime("%Y-%m-%d %H:%M:%S")

    def _write_csv_file(self, csv_data: List[Dict[str, str]]) -> None:
        """
        å†™å…¥ CSV æ–‡ä»¶ (ETL Load Phase)

        Args:
            csv_data: æ ‡å‡†åŒ–çš„ CSV æ•°æ®
        """
        if not csv_data:
            logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®éœ€è¦å¯¼å‡º")
            return

        if self.config.dry_run:
            logger.info(
                f"ğŸ” DRY RUN: å°†å¯¼å‡º {len(csv_data)} æ¡è®°å½•åˆ° {self.output_file}"
            )
            return

        logger.info(f"ğŸ’¾ å¼€å§‹å¯¼å‡º {len(csv_data)} æ¡è®°å½•åˆ° {self.output_file}")

        try:
            # åˆ›å»ºè¿›åº¦æ¡
            if self.config.show_progress and HAS_TQDM:
                pbar = tqdm(total=len(csv_data), desc="å¯¼å‡ºè¿›åº¦", unit="records")
            else:
                pbar = None
                logger.info("ğŸ“ å†™å…¥ CSV æ–‡ä»¶...")

            with open(self.output_file, "w", newline="", encoding="utf-8") as csvfile:
                # CSV å­—æ®µå®šä¹‰
                fieldnames = [
                    "match_id",
                    "status",
                    "data_completeness",
                    "match_date",
                    "home_team",
                    "away_team",
                    "season",
                    "league_id",
                    "created_at",
                    "updated_at",
                ]

                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()

                # æ‰¹é‡å†™å…¥ä»¥æé«˜æ€§èƒ½
                for i in range(0, len(csv_data), self.config.batch_size):
                    batch = csv_data[i : i + self.config.batch_size]
                    writer.writerows(batch)

                    if pbar:
                        pbar.update(len(batch))
                    else:
                        # ç®€å•è¿›åº¦æ˜¾ç¤º
                        progress = min(i + self.config.batch_size, len(csv_data))
                        percentage = (progress / len(csv_data)) * 100
                        logger.info(
                            f"   è¿›åº¦: {progress}/{len(csv_data)} ({percentage:.1f}%)"
                        )

            if pbar:
                pbar.close()

            self.stats["exported"] = len(csv_data)
            logger.info(f"âœ… CSV å¯¼å‡ºå®Œæˆ: {self.output_file}")

            # æ–‡ä»¶å¤§å°ä¿¡æ¯
            file_size = self.output_file.stat().st_size
            logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size / 1024:.1f} KB")

        except Exception as e:
            logger.error(f"âŒ CSV å¯¼å‡ºå¤±è´¥: {e}", exc_info=True)
            raise

    def _print_detailed_summary(self) -> None:
        """æ‰“å°è¯¦ç»†çš„ç»Ÿè®¡æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ¯ L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆå®Œæˆ - ETL Pipeline Summary")
        print("=" * 80)

        # åŸºæœ¬ç»Ÿè®¡
        print("ğŸ“Š æ•°æ®æå–ç»Ÿè®¡:")
        print(f"   æ•°æ®åº“æŸ¥è¯¢æ€»æ•°: {self.stats['total_queried']}")
        print(f"   æœ‰æ•ˆæ¯”èµ›æ•°é‡: {self.stats['valid_matches']}")
        print(f"   æˆåŠŸå¯¼å‡ºè®°å½•: {self.stats['exported']}")
        print(f"   è¾“å‡ºæ–‡ä»¶è·¯å¾„: {self.output_file}")
        print()

        # æŒ‰èµ›å­£åˆ†å¸ƒ
        if self.stats["by_season"]:
            print("ğŸ“… æŒ‰èµ›å­£åˆ†å¸ƒ:")
            for season, count in sorted(self.stats["by_season"].items(), reverse=True):
                percentage = (count / self.stats["valid_matches"]) * 100
                print(f"   {season:>12}: {count:>4} åœº ({percentage:>5.1f}%)")
            print()

        # æŒ‰æ•°æ®å®Œæ•´åº¦åˆ†å¸ƒ
        if self.stats["by_completeness"]:
            print("ğŸ“ˆ æŒ‰æ•°æ®å®Œæ•´åº¦åˆ†å¸ƒ:")
            for completeness, count in sorted(
                self.stats["by_completeness"].items(), key=lambda x: x[1], reverse=True
            ):
                percentage = (count / self.stats["valid_matches"]) * 100
                print(f"   {completeness:>12}: {count:>4} åœº ({percentage:>5.1f}%)")
            print()

        # æŒ‰æ¯”èµ›çŠ¶æ€åˆ†å¸ƒ
        if self.stats["by_status"]:
            print("ğŸ æŒ‰æ¯”èµ›çŠ¶æ€åˆ†å¸ƒ:")
            for status, count in sorted(
                self.stats["by_status"].items(), key=lambda x: x[1], reverse=True
            ):
                percentage = (count / self.stats["valid_matches"]) * 100
                print(f"   {status:>12}: {count:>4} åœº ({percentage:>5.1f}%)")
            print()

        print("ğŸš€ ä½¿ç”¨å»ºè®®:")
        print(
            f"   python src/scripts/backfill_l2_matches.py --input {self.output_file}"
        )
        print("=" * 80)

    async def run(self) -> None:
        """
        è¿è¡Œå®Œæ•´çš„ ETL æµç¨‹
        """
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹ L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆ ETL æµç¨‹...")

        try:
            # 1. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            await self._initialize_database()

            # 2. Extract: ä»æ•°æ®åº“æå–æ•°æ®
            matches = await self.extract_pending_matches()

            if not matches:
                logger.info("âœ… æ‰€æœ‰æ¯”èµ›éƒ½å·²å®Œæˆ L2 æ•°æ®é‡‡é›†ï¼Œæ— éœ€å›å¡«")
                return

            # 3. Transform: æ•°æ®è½¬æ¢å’Œæ ‡å‡†åŒ–
            csv_data = self._prepare_csv_data(matches)

            # 4. Load: å¯¼å‡ºåˆ° CSV æ–‡ä»¶
            self._write_csv_file(csv_data)

            # 5. ç”Ÿæˆè¯¦ç»†æ‘˜è¦
            self._print_detailed_summary()

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            logger.info(f"ğŸ‰ L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆå®Œæˆï¼è€—æ—¶: {execution_time:.2f} ç§’")

        except Exception as e:
            logger.error(f"âŒ L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆå¤±è´¥: {e}", exc_info=True)
            sys.exit(1)


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="L2 å›å¡«é˜Ÿåˆ—ç”Ÿæˆå™¨ - ETL Pipeline Extract Phase",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s                                    # ä½¿ç”¨é»˜è®¤é…ç½®
  %(prog)s --output-dir custom_data           # è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  %(prog)s --batch-size 500                   # è®¾ç½®æ‰¹é‡å¤§å°
  %(prog)s --dry-run                          # ä»…æŸ¥è¯¢ï¼Œä¸å¯¼å‡ºæ–‡ä»¶
  %(prog)s --no-progress                      # ç¦ç”¨è¿›åº¦æ˜¾ç¤º
        """,
    )

    parser.add_argument(
        "--output-dir", "-o", default="data", help="è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: data)"
    )

    parser.add_argument(
        "--output-filename",
        "-f",
        default="l2_backfill_queue.csv",
        help="è¾“å‡ºæ–‡ä»¶å (é»˜è®¤: l2_backfill_queue.csv)",
    )

    parser.add_argument(
        "--batch-size",
        "-b",
        type=int,
        default=1000,
        help="æ‰¹é‡å¤„ç†å¤§å°ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨ (é»˜è®¤: 1000)",
    )

    parser.add_argument(
        "--dry-run", "-d", action="store_true", help="ä»…æŸ¥è¯¢ç»Ÿè®¡ï¼Œä¸å¯¼å‡ºæ–‡ä»¶"
    )

    parser.add_argument(
        "--no-progress", "-np", action="store_true", help="ç¦ç”¨è¿›åº¦æ˜¾ç¤º"
    )

    return parser.parse_args()


async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    Path("logs").mkdir(exist_ok=True)

    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_arguments()

    # æ„å»ºé…ç½®
    config = BackfillConfig(
        output_dir=args.output_dir,
        output_filename=args.output_filename,
        batch_size=args.batch_size,
        show_progress=not args.no_progress,
        dry_run=args.dry_run,
    )

    # åˆ›å»ºç”Ÿæˆå™¨å¹¶è¿è¡Œ
    generator = BackfillQueueGenerator(config)
    await generator.run()


if __name__ == "__main__":
    # è¿è¡Œ ETL æµç¨‹
    asyncio.run(main())
