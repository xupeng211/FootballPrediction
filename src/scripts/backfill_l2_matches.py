#!/usr/bin/env python3
"""
ç”Ÿäº§çº§L2æ•°æ®å›å¡«è„šæœ¬ - Production Release
Production-Grade L2 Data Backfill Script

ç”¨äºæ‰¹é‡æŠ“å–å†å²æ¯”èµ›L2è¯¦æƒ…æ•°æ®ï¼Œæ”¯æŒï¼š
- æµå¼å¤„ç†é¿å…å†…å­˜æº¢å‡º
- å¹¶å‘æ§åˆ¶é˜²æ­¢æœåŠ¡å™¨è¿‡è½½
- å®æ—¶è¿›åº¦ç›‘æ§å’Œæ—¥å¿—è®°å½•
- ä¼˜é›…åœæœºå’Œæ–­ç‚¹ç»­ä¼ 
- å¤±è´¥é‡è¯•å’Œé”™è¯¯éš”ç¦»
- HTTPå‹ç¼©æ•°æ®å¤„ç†
- JSONåºåˆ—åŒ–é”™è¯¯ä¿®å¤

ä½œè€…: L2é‡æ„å›¢é˜Ÿ
åˆ›å»ºæ—¶é—´: 2025-12-10
ç‰ˆæœ¬: 1.0.0 (Production Release)
"""

import asyncio
import csv
import logging
import signal
import sys
from datetime import datetime
from pathlib import Path
from typing import AsyncGenerator, Optional, Set, Tuple
from dataclasses import dataclass, field

import aiofiles
import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.collectors.l2_fetcher import L2Fetcher, L2FetchError
from src.collectors.l2_parser import L2Parser
from src.schemas.l2_schemas import L2MatchData


@dataclass
class BackfillConfig:
    """å›å¡«é…ç½®"""

    # å¹¶å‘æ§åˆ¶
    max_concurrent: int = 15
    batch_size: int = 1000  # æ¯æ‰¹è¯»å–çš„match_idsæ•°é‡
    request_timeout: float = 30.0
    max_retries: int = 5

    # æ–‡ä»¶è·¯å¾„
    input_file: str = "data/l2_backfill_queue.csv"
    output_dir: str = "data/l2_output"
    log_file: str = "logs/backfill.log"
    failed_ids_file: str = "logs/failed_ids.txt"

    # æ§åˆ¶å‚æ•°
    enable_progress_bar: bool = True
    save_interval: int = 10  # æ¯10ä¸ªæˆåŠŸä¿å­˜ä¸€æ¬¡è¿›åº¦
    rate_limit_delay: float = 0.1  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰


@dataclass
class BackfillStats:
    """å›å¡«ç»Ÿè®¡ä¿¡æ¯"""

    total_processed: int = 0
    successful: int = 0
    failed: int = 0
    skipped: int = 0
    start_time: datetime = field(default_factory=datetime.now)

    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        return (self.successful / max(self.total_processed, 1)) * 100

    @property
    def elapsed_time(self) -> float:
        """å·²ç”¨æ—¶é—´ï¼ˆç§’ï¼‰"""
        return (datetime.now() - self.start_time).total_seconds()

    @property
    def processing_rate(self) -> float:
        """å¤„ç†é€Ÿç‡ï¼ˆä¸ª/ç§’ï¼‰"""
        return self.total_processed / max(self.elapsed_time, 1)


class BackfillManager:
    """
    L2æ•°æ®å›å¡«ç®¡ç†å™¨ - ç”Ÿäº§ç‰ˆæœ¬

    è´Ÿè´£æ‰¹é‡å¤„ç†æ¯”èµ›æ•°æ®çš„L2è¯¦æƒ…æŠ“å–ï¼Œå…·å¤‡æµå¼å¤„ç†ã€
    å¹¶å‘æ§åˆ¶ã€é”™è¯¯æ¢å¤å’Œè¿›åº¦ç›‘æ§ç­‰åŠŸèƒ½ã€‚
    """

    def __init__(self, config: Optional[BackfillConfig] = None):
        """
        åˆå§‹åŒ–å›å¡«ç®¡ç†å™¨

        Args:
            config: å›å¡«é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        """
        self.config = config or BackfillConfig()
        self.stats = BackfillStats()

        # è¿è¡Œæ—¶çŠ¶æ€
        self._running = False
        self._shutdown_requested = False
        self._processed_ids: Set[str] = set()
        self._failed_ids: Set[str] = set()

        # ç»„ä»¶åˆå§‹åŒ–
        self._setup_logging()
        self._setup_directories()

        # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆåœ¨asyncç¯å¢ƒä¸­ï¼‰
        self._fetcher: Optional[L2Fetcher] = None
        self._parser: Optional[L2Parser] = None
        self._semaphore: Optional[asyncio.Semaphore] = None

        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        self._setup_signal_handlers()

        self.logger.info("BackfillManager initialized with config: %s", self.config)

    def _setup_logging(self) -> None:
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = Path(self.config.log_file).parent
        log_dir.mkdir(parents=True, exist_ok=True)

        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(self.config.log_file, encoding="utf-8"),
                logging.StreamHandler(sys.stdout),
            ],
        )

        self.logger = logging.getLogger(__name__)

        # å‡å°‘ç¬¬ä¸‰æ–¹åº“æ—¥å¿—å™ªéŸ³
        logging.getLogger("httpx").setLevel(logging.WARNING)
        logging.getLogger("tenacity").setLevel(logging.WARNING)

    def _setup_directories(self) -> None:
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        Path(self.config.output_dir).mkdir(parents=True, exist_ok=True)
        Path(self.config.failed_ids_file).parent.mkdir(parents=True, exist_ok=True)

    def _setup_signal_handlers(self) -> None:
        """è®¾ç½®ä¿¡å·å¤„ç†å™¨ï¼Œæ”¯æŒä¼˜é›…åœæœº"""

        def signal_handler(signum, frame):
            self.logger.info(
                "Received signal %d, initiating graceful shutdown...", signum
            )
            self._shutdown_requested = True
            self._running = False

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

    async def _initialize_components(self) -> None:
        """åœ¨å¼‚æ­¥ç¯å¢ƒä¸­åˆå§‹åŒ–ç»„ä»¶"""
        if not self._fetcher:
            from src.collectors.rate_limiter import RateLimiter

            # åˆ›å»ºä¾èµ–ç»„ä»¶ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦å…·ä½“é…ç½®ï¼‰
            rate_limiter = RateLimiter()

            self._fetcher = L2Fetcher(
                timeout=self.config.request_timeout,
                max_retries=self.config.max_retries,
                rate_limiter=rate_limiter,
            )

            # L2Fetcher ä½¿ç”¨å»¶è¿Ÿåˆå§‹åŒ–ï¼Œæ— éœ€æ˜¾å¼è°ƒç”¨ initialize()

        if not self._parser:
            self._parser = L2Parser(strict_mode=False)

        if not self._semaphore:
            self._semaphore = asyncio.Semaphore(self.config.max_concurrent)

    async def _read_match_ids_stream(self) -> AsyncGenerator[str, None]:
        """
        æµå¼è¯»å–match_idsï¼Œé¿å…å†…å­˜æº¢å‡º

        Yields:
            str: match_id
        """
        self.logger.info("Reading match IDs from %s", self.config.input_file)

        try:
            async with aiofiles.open(
                self.config.input_file, "r", encoding="utf-8"
            ) as file:
                # è¯»å–å¹¶è·³è¿‡æ ‡é¢˜è¡Œ
                header_line = await file.readline()
                self.logger.debug("CSV header: %s", header_line.strip())

                batch_count = 0
                async for line in file:
                    if self._shutdown_requested:
                        break

                    line = line.strip()
                    if not line:
                        continue

                    # æ‰‹åŠ¨è§£æCSVï¼ˆç®€å•æƒ…å†µï¼Œåªæœ‰ä¸€åˆ—ï¼‰
                    match_id = line.split(",")[0].strip().strip("\"'")

                    if match_id and match_id not in self._processed_ids:
                        yield match_id
                        batch_count += 1

                        # æ¯æ‰¹è®°å½•æ—¥å¿—
                        if batch_count % self.config.batch_size == 0:
                            self.logger.info(
                                "Read %d match IDs so far (batch %d)",
                                batch_count,
                                batch_count // self.config.batch_size,
                            )

        except FileNotFoundError:
            self.logger.error("Input file not found: %s", self.config.input_file)
            raise
        except Exception as e:
            self.logger.error("Error reading match IDs: %s", e)
            raise

    async def _process_single_match(
        self, match_id: str
    ) -> Tuple[bool, Optional[L2MatchData]]:
        """
        å¤„ç†å•ä¸ªæ¯”èµ›æ•°æ®

        Args:
            match_id: æ¯”èµ›ID

        Returns:
            Tuple[bool, Optional[L2MatchData]]: (æˆåŠŸæ ‡å¿—, è§£æåçš„æ•°æ®)
        """
        async with self._semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            if self._shutdown_requested:
                return False, None

            try:
                # è·å–åŸå§‹æ•°æ®
                raw_data = await self._fetcher.fetch_match_details(match_id)

                # è§£ææ•°æ®
                result = self._parser.parse_match_data(raw_data)

                if result.success:
                    self.logger.debug("Successfully processed match %s", match_id)
                    return True, result.data
                else:
                    self.logger.warning(
                        "Failed to parse match %s: %s", match_id, result.error_message
                    )
                    return False, None

            except L2FetchError as e:
                self.logger.error(
                    "Fetch error for match %s (status=%s): %s",
                    match_id,
                    e.status_code,
                    e.message,
                )
                return False, None
            except Exception as e:
                self.logger.error(
                    "Unexpected error processing match %s: %s", match_id, e
                )
                return False, None

    async def _save_match_data(self, match_id: str, match_data: L2MatchData) -> None:
        """
        ä¿å­˜æ¯”èµ›æ•°æ®åˆ°æ–‡ä»¶

        Args:
            match_id: æ¯”èµ›ID
            match_data: æ¯”èµ›æ•°æ®
        """
        filename = f"{match_id}.json"
        filepath = Path(self.config.output_dir) / filename

        try:
            import json

            data_dict = match_data.to_dict()

            # å¤„ç†datetimeåºåˆ—åŒ–é—®é¢˜
            def datetime_handler(obj):
                if hasattr(obj, "isoformat"):
                    return obj.isoformat()
                raise TypeError(repr(obj) + " is not JSON serializable")

            async with aiofiles.open(filepath, "w", encoding="utf-8") as file:
                await file.write(
                    json.dumps(
                        data_dict,
                        ensure_ascii=False,
                        indent=2,
                        default=datetime_handler,
                    )
                )

            self.logger.debug("Saved match data to %s", filepath)

        except Exception as e:
            self.logger.error("Failed to save match data for %s: %s", match_id, e)
            raise

    async def _log_failed_id(self, match_id: str) -> None:
        """è®°å½•å¤±è´¥çš„match_id"""
        try:
            async with aiofiles.open(
                self.config.failed_ids_file, "a", encoding="utf-8"
            ) as file:
                await file.write(f"{match_id}\n")
        except Exception as e:
            self.logger.error("Failed to log failed ID %s: %s", match_id, e)

    async def _save_progress(self) -> None:
        """ä¿å­˜å¤„ç†è¿›åº¦"""
        progress_file = Path(self.config.output_dir) / "progress.txt"

        try:
            progress_data = {
                "processed_count": self.stats.total_processed,
                "successful_count": self.stats.successful,
                "failed_count": self.stats.failed,
                "processed_ids": list(self._processed_ids),
                "last_update": datetime.now().isoformat(),
            }

            import json

            async with aiofiles.open(progress_file, "w", encoding="utf-8") as file:
                await file.write(json.dumps(progress_data, indent=2))

        except Exception as e:
            self.logger.error("Failed to save progress: %s", e)

    async def run_backfill(self) -> BackfillStats:
        """
        è¿è¡Œæ•°æ®å›å¡«ä¸»æµç¨‹

        Returns:
            BackfillStats: å›å¡«ç»Ÿè®¡ä¿¡æ¯
        """
        self.logger.info("Starting L2 data backfill...")
        self._running = True
        self.stats.start_time = datetime.now()

        try:
            await self._initialize_components()

            # åˆ›å»ºè¿›åº¦æ¡
            progress_bar = tqdm.tqdm(
                desc="Processing matches",
                unit="matches",
                disable=not self.config.enable_progress_bar,
            )

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡é˜Ÿåˆ—
            tasks = set()
            save_counter = 0

            # æµå¼å¤„ç†match_ids
            async for match_id in self._read_match_ids_stream():
                if self._shutdown_requested:
                    self.logger.info("Shutdown requested, stopping new tasks...")
                    break

                # åˆ›å»ºå¤„ç†ä»»åŠ¡
                task = asyncio.create_task(self._process_single_match(match_id))
                tasks.add(task)

                # æ§åˆ¶å¹¶å‘é˜Ÿåˆ—å¤§å°
                if len(tasks) >= self.config.max_concurrent:
                    done, pending = await asyncio.wait(
                        tasks, return_when=asyncio.FIRST_COMPLETED
                    )

                    for completed_task in done:
                        try:
                            success, match_data = completed_task.result()
                            self.stats.total_processed += 1

                            if success and match_data:
                                # ä¿å­˜æ•°æ®
                                await self._save_match_data(match_id, match_data)
                                self.stats.successful += 1
                                self._processed_ids.add(match_id)

                            else:
                                # è®°å½•å¤±è´¥
                                self.stats.failed += 1
                                self._failed_ids.add(match_id)
                                await self._log_failed_id(match_id)

                            save_counter += 1

                            # å®šæœŸä¿å­˜è¿›åº¦
                            if save_counter >= self.config.save_interval:
                                await self._save_progress()
                                save_counter = 0

                        except Exception as e:
                            self.logger.error("Error in task result: %s", e)

                    tasks = pending

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.update(1)

                # è¯·æ±‚é—´å»¶è¿Ÿ
                if self.config.rate_limit_delay > 0:
                    await asyncio.sleep(self.config.rate_limit_delay)

            # ç­‰å¾…å‰©ä½™ä»»åŠ¡å®Œæˆ - ä¿®å¤ AsyncIO è­¦å‘Šçš„å…³é”®éƒ¨åˆ†
            if tasks:
                self.logger.info(
                    "Waiting for %d remaining tasks to complete...", len(tasks)
                )

                # ä½¿ç”¨ asyncio.gather ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œé¿å… as_completed çš„è­¦å‘Š
                try:
                    results = await asyncio.gather(*tasks, return_exceptions=True)

                    for i, result in enumerate(results):
                        if isinstance(result, Exception):
                            self.logger.error(
                                "Task %d failed with exception: %s", i, result
                            )
                            self.stats.failed += 1
                            continue

                        try:
                            success, match_data = result
                            self.stats.total_processed += 1

                            if success and match_data:
                                await self._save_match_data(match_id, match_data)
                                self.stats.successful += 1
                            else:
                                self.stats.failed += 1
                                await self._log_failed_id(match_id)

                        except Exception as e:
                            self.logger.error(
                                "Error processing final task result %d: %s", i, e
                            )
                            self.stats.failed += 1

                except Exception as e:
                    self.logger.error("Error waiting for final tasks: %s", e)

                    # å¦‚æœ gather å¤±è´¥ï¼Œå°è¯•å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
                    for task in tasks:
                        if not task.done():
                            task.cancel()

                    # ç­‰å¾…å–æ¶ˆå®Œæˆ
                    try:
                        await asyncio.gather(*tasks, return_exceptions=True)
                    except Exception as e:
                        pass

            progress_bar.close()

        except Exception as e:
            self.logger.error("Fatal error in backfill process: %s", e)
            raise
        finally:
            self._running = False

            # æ¸…ç†èµ„æº - ç¡®ä¿ fetcher æ­£ç¡®å…³é—­
            try:
                if self._fetcher:
                    await self._fetcher.close()
            except Exception as e:
                self.logger.error("Error closing fetcher: %s", e)

            # æœ€ç»ˆä¿å­˜è¿›åº¦
            try:
                await self._save_progress()
            except Exception as e:
                self.logger.error("Error saving final progress: %s", e)

            self.logger.info(
                "Backfill completed. Total: %d, Success: %d, Failed: %d, Success Rate: %.2f%%",
                self.stats.total_processed,
                self.stats.successful,
                self.stats.failed,
                self.stats.success_rate,
            )

        return self.stats


def create_sample_input_file(file_path: str, num_ids: int = 1000) -> None:
    """
    åˆ›å»ºç¤ºä¾‹è¾“å…¥æ–‡ä»¶ç”¨äºæµ‹è¯•

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        num_ids: match_idsæ•°é‡
    """
    import random

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path(file_path).parent.mkdir(parents=True, exist_ok=True)

    with open(file_path, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["match_id"])  # æ ‡é¢˜è¡Œ

        # ç”Ÿæˆç¤ºä¾‹match_ids
        for i in range(num_ids):
            match_id = f"{random.randint(1000000, 9999999)}"
            writer.writerow([match_id])

    print(f"Created sample input file with {num_ids} match_ids: {file_path}")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="L2æ•°æ®å›å¡«è„šæœ¬")
    parser.add_argument(
        "--input", "-i", default="data/l2_backfill_queue.csv", help="è¾“å…¥CSVæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument("--output", "-o", default="data/l2_output", help="è¾“å‡ºç›®å½•è·¯å¾„")
    parser.add_argument("--concurrent", "-c", type=int, default=15, help="æœ€å¤§å¹¶å‘æ•°")
    parser.add_argument("--create-sample", action="store_true", help="åˆ›å»ºç¤ºä¾‹è¾“å…¥æ–‡ä»¶")

    args = parser.parse_args()

    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if args.create_sample:
        create_sample_input_file(args.input, 100)
        return

    # é…ç½®å›å¡«å‚æ•°
    config = BackfillConfig(
        input_file=args.input,
        output_dir=args.output,
        max_concurrent=args.concurrent,
        enable_progress_bar=True,
    )

    # è¿è¡Œå›å¡«
    manager = BackfillManager(config)

    try:
        stats = await manager.run_backfill()
        print("\nğŸ‰ Backfill completed successfully!")
        print("ğŸ“Š Statistics:")
        print(f"   Total processed: {stats.total_processed}")
        print(f"   Successful: {stats.successful}")
        print(f"   Failed: {stats.failed}")
        print(f"   Success rate: {stats.success_rate:.2f}%")
        print(f"   Processing rate: {stats.processing_rate:.2f} matches/sec")
        print(f"   Elapsed time: {stats.elapsed_time:.2f} seconds")

    except KeyboardInterrupt:
        print("\nâ¹ï¸ Backfill interrupted by user")
    except Exception as e:
        print(f"\nâŒ Backfill failed: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
