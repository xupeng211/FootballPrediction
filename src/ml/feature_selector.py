#!/usr/bin/env python3
"""
é«˜çº§ç‰¹å¾é€‰æ‹©å™¨ - åŸºäºæ¨¡å‹é‡è¦æ€§ã€å…±çº¿æ€§æ£€æµ‹å’Œå¤šé‡ç­–ç•¥çš„è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©å·¥å…·ã€‚
æ”¯æŒåˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œæä¾›è¯¦ç»†çš„ç‰¹å¾åˆ†ææŠ¥å‘Šã€‚
"""

import logging
from typing import Any, , , Optional, , Union

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.feature_selection import (
    RFE,
    SelectKBest,
    f_classif,
    f_regression,
    mutual_info_classif,
    mutual_info_regression,
)
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns

logger = logging.getLogger(__name__)


class AdvancedFeatureSelector:
    """
    é«˜çº§ç‰¹å¾é€‰æ‹©å™¨ - åŸºäºæ¨¡å‹é‡è¦æ€§ã€å…±çº¿æ€§æ£€æµ‹å’Œå¤šé‡ç­–ç•¥çš„è‡ªåŠ¨åŒ–ç‰¹å¾é€‰æ‹©å·¥å…·ã€‚
    æ”¯æŒåˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œæä¾›è¯¦ç»†çš„ç‰¹å¾åˆ†ææŠ¥å‘Šã€‚
    """

    def __init__(
        self,
        task_type: str = "classification",  # "classification" æˆ– "regression"
        correlation_threshold: float = 0.95,
        min_features: int = 5,
        max_features: int = 100,
        random_state: int = 42,
        n_jobs: int = -1,
    ):
        """
        åˆå§‹åŒ–ç‰¹å¾é€‰æ‹©å™¨.

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œ'classification' æˆ– 'regression'
            correlation_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼çš„ç‰¹å¾ä¼šè¢«ç§»é™¤
            min_features: æœ€å°‘ä¿ç•™ç‰¹å¾æ•°
            max_features: æœ€å¤šä¿ç•™ç‰¹å¾æ•°
            random_state: éšæœºç§å­
            n_jobs: å¹¶è¡Œä½œä¸šæ•°
        """
        self.task_type = task_type
        self.correlation_threshold = correlation_threshold
        self.min_features = min_features
        self.max_features = max_features
        self.random_state = random_state
        self.n_jobs = n_jobs

        # å­˜å‚¨ç»“æœ
        self.selected_features: list[str] = []
        self.feature_importance: dict[str, float] = {}
        self.correlation_matrix: Optional[pd.DataFrame] = None
        self.removed_features: list[str] = []
        self.selection_report: dict[str, Any] = {}

        # åˆå§‹åŒ–åŸºç¡€æ¨¡å‹
        self._init_base_models()

    def _init_base_models(self):
        """åˆå§‹åŒ–åŸºç¡€æ¨¡å‹ç”¨äºç‰¹å¾é‡è¦æ€§è¯„ä¼°."""
        # éšæœºæ£®æ—æ¨¡å‹
        self.rf_model = (
            RandomForestClassifier(
                n_estimators=100,
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                max_depth=10,
            )
            if self.task_type == "classification"
            else RandomForestRegressor(
                n_estimators=100,
                random_state=self.random_state,
                n_jobs=self.n_jobs,
                max_depth=10,
            )
        )

        # XGBoostæ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        self.xgb_model = None
        try:
            import xgboost as xgb

            self.xgb_model = (
                xgb.XGBClassifier(
                    n_estimators=100,
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                    max_depth=6,
                    learning_rate=0.1,
                )
                if self.task_type == "classification"
                else xgb.XGBRegressor(
                    n_estimators=100,
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                    max_depth=6,
                    learning_rate=0.1,
                )
            )
        except ImportError:
            logger.warning("XGBoost not available, using only RandomForest")

    def fit(self, X: pd.DataFrame, y: pd.Series) -> "AdvancedFeatureSelector":
        """
        æ‰§è¡Œç‰¹å¾é€‰æ‹©æµç¨‹.

        Args:
            X: ç‰¹å¾çŸ©é˜µ
            y: ç›®æ ‡å˜é‡

        Returns:
            self: è¿”å›è‡ªèº«å®ä¾‹
        """
        logger.info(f"ğŸ” å¼€å§‹ç‰¹å¾é€‰æ‹©æµç¨‹ - ä»»åŠ¡ç±»å‹: {self.task_type}")
        logger.info(f"ğŸ“Š è¾“å…¥ç‰¹å¾æ•°é‡: {X.shape[1]}")

        # 1. æ•°æ®é¢„å¤„ç†
        X_clean = self._preprocess_data(X)

        # 2. ç§»é™¤å¸¸æ•°ç‰¹å¾
        X_clean, constant_removed = self._remove_constant_features(X_clean)
        logger.info(f"ğŸ“Š ç§»é™¤å¸¸æ•°ç‰¹å¾å: {X_clean.shape[1]}")

        # 3. ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾
        X_clean, correlation_removed = self._remove_high_correlation_features(X_clean)
        logger.info(f"ğŸ“Š ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾å: {X_clean.shape[1]}")

        # 4. åŸºäºæ¨¡å‹é‡è¦æ€§çš„ç‰¹å¾é€‰æ‹©
        X_clean, model_removed = self._select_by_model_importance(X_clean, y)
        logger.info(f"ğŸ“Š æ¨¡å‹é‡è¦æ€§é€‰æ‹©å: {X_clean.shape[1]}")

        # 5. åŸºäºç»Ÿè®¡æµ‹è¯•çš„ç‰¹å¾é€‰æ‹©
        X_clean, statistical_removed = self._select_by_statistical_tests(X_clean, y)
        logger.info(f"ğŸ“Š ç»Ÿè®¡æµ‹è¯•é€‰æ‹©å: {X_clean.shape[1]}")

        # 6. é€’å½’ç‰¹å¾æ¶ˆé™¤
        X_clean, rfe_removed = self._recursive_feature_elimination(X_clean, y)
        logger.info(f"ğŸ“Š é€’å½’ç‰¹å¾æ¶ˆé™¤å: {X_clean.shape[1]}")

        # 7. æœ€ç»ˆç‰¹å¾æ•°é‡è°ƒæ•´
        X_clean = self._adjust_feature_count(X_clean, y)
        logger.info(f"ğŸ“Š æœ€ç»ˆç‰¹å¾æ•°é‡: {X_clean.shape[1]}")

        # å­˜å‚¨ç»“æœ
        self.selected_features = X_clean.columns.tolist()
        self.selection_report = {
            "original_features": X.shape[1],
            "constant_removed": constant_removed,
            "correlation_removed": correlation_removed,
            "model_removed": model_removed,
            "statistical_removed": statistical_removed,
            "rfe_removed": rfe_removed,
            "final_features": len(self.selected_features),
        }

        logger.info(f"âœ… ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä¿ç•™ {len(self.selected_features)} ä¸ªç‰¹å¾")

        return self

    def _preprocess_data(self, X: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†."""
        # å¤„ç†ç¼ºå¤±å€¼
        X_clean = X.copy()

        # æ•°å€¼å‹ç‰¹å¾ç”¨ä¸­ä½æ•°å¡«å……
        numeric_cols = X_clean.select_dtypes(include=[np.number]).columns
        X_clean[numeric_cols] = X_clean[numeric_cols].fillna(
            X_clean[numeric_cols].median()
        )

        # åˆ†ç±»ç‰¹å¾ç”¨ä¼—æ•°å¡«å……
        categorical_cols = X_clean.select_dtypes(include=["object"]).columns
        for col in categorical_cols:
            X_clean[col] = X_clean[col].fillna(
                X_clean[col].mode()[0] if len(X_clean[col].mode()) > 0 else "Unknown"
            )

        return X_clean

    def _remove_constant_features(
        self, X: pd.DataFrame
    ) -> tuple[pd.DataFrame, list[str]]:
        """ç§»é™¤å¸¸æ•°ç‰¹å¾."""
        constant_features = []

        for col in X.columns:
            unique_values = X[col].nunique()
            if unique_values <= 1:
                constant_features.append(col)

        X_clean = X.drop(columns=constant_features)
        self.removed_features.extend(
            [f"Constant feature: {col}" for col in constant_features]
        )

        return X_clean, constant_features

    def _remove_high_correlation_features(
        self, X: pd.DataFrame
    ) -> tuple[pd.DataFrame, list[str]]:
        """ç§»é™¤é«˜ç›¸å…³æ€§ç‰¹å¾."""
        # åªå¯¹æ•°å€¼å‹ç‰¹å¾è®¡ç®—ç›¸å…³æ€§
        numeric_cols = X.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) <= 1:
            return X, []

        correlation_matrix = X[numeric_cols].corr().abs()

        # æ‰¾åˆ°é«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
        high_corr_features = set()
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                if correlation_matrix.iloc[i, j] > self.correlation_threshold:
                    col_i = correlation_matrix.columns[i]
                    col_j = correlation_matrix.columns[j]
                    # ç§»é™¤ç›¸å…³æ€§è¾ƒé«˜çš„ç‰¹å¾ï¼ˆä¿ç•™åœ¨å­—å…¸ä¸­é¡ºåºé åçš„ï¼‰
                    if correlation_matrix.columns.get_loc(
                        col_i
                    ) < correlation_matrix.columns.get_loc(col_j):
                        high_corr_features.add(col_i)
                        self.removed_features.append(
                            f"High correlation: {col_i} with {col_j}"
                        )

        self.correlation_matrix = correlation_matrix
        X_clean = X.drop(columns=list(high_corr_features))

        return X_clean, list(high_corr_features)

    def _select_by_model_importance(
        self, X: pd.DataFrame, y: pd.Series
    ) -> tuple[pd.DataFrame, list[str]]:
        """åŸºäºæ¨¡å‹é‡è¦æ€§é€‰æ‹©ç‰¹å¾."""
        # éšæœºæ£®æ—é‡è¦æ€§
        rf_scores = self._get_rf_importance(X, y)

        # XGBoosté‡è¦æ€§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        xgb_scores = {}
        if self.xgb_model is not None:
            xgb_scores = self._get_xgb_importance(X, y)

        # åˆå¹¶é‡è¦æ€§å¾—åˆ†
        feature_scores = {}
        for feature in X.columns:
            score = rf_scores.get(feature, 0)
            if xgb_scores:
                score = (score + xgb_scores.get(feature, 0)) / 2
            feature_scores[feature] = score

        # æŒ‰é‡è¦æ€§æ’åºå¹¶é€‰æ‹©
        sorted_features = sorted(
            feature_scores.items(), key=lambda x: x[1], reverse=True
        )
        self.feature_importance = dict(sorted_features)

        # ä¿ç•™é‡è¦æ€§è¾ƒé«˜çš„ç‰¹å¾
        importance_threshold = np.percentile(
            list(feature_scores.values()), 50
        )  # ä¸­ä½æ•°ä½œä¸ºé˜ˆå€¼
        selected_features = [f for f, s in sorted_features if s >= importance_threshold]

        if len(selected_features) < self.min_features:
            selected_features = [f for f, s in sorted_features[: self.min_features]]

        removed_features = [f for f in X.columns if f not in selected_features]
        self.removed_features.extend([f"Low importance: {f}" for f in removed_features])

        return X[selected_features], removed_features

    def _get_rf_importance(self, X: pd.DataFrame, y: pd.Series) -> dict[str, float]:
        """è·å–éšæœºæ£®æ—ç‰¹å¾é‡è¦æ€§."""
        self.rf_model.fit(X, y)
        return dict(zip(X.columns, self.rf_model.feature_importances_, strict=False))

    def _get_xgb_importance(self, X: pd.DataFrame, y: pd.Series) -> dict[str, float]:
        """è·å–XGBoostç‰¹å¾é‡è¦æ€§."""
        self.xgb_model.fit(X, y)
        if hasattr(self.xgb_model, "feature_importances_"):
            return dict(
                zip(X.columns, self.xgb_model.feature_importances_, strict=False)
            )
        return {}

    def _select_by_statistical_tests(
        self, X: pd.DataFrame, y: pd.Series
    ) -> tuple[pd.DataFrame, list[str]]:
        """åŸºäºç»Ÿè®¡æµ‹è¯•é€‰æ‹©ç‰¹å¾."""
        if self.task_type == "classification":
            selector = SelectKBest(f_classif, k=min(len(X.columns), 50))
        else:
            selector = SelectKBest(f_regression, k=min(len(X.columns), 50))

        selector.fit(X, y)
        X.columns[selector.get_support()].tolist()

        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ç‰¹å¾
        scores = selector.scores_
        feature_scores = dict(zip(X.columns, scores, strict=False))
        sorted_features = sorted(
            feature_scores.items(), key=lambda x: x[1], reverse=True
        )

        # ä¿ç•™åˆ†æ•°è¾ƒé«˜çš„ç‰¹å¾
        score_threshold = np.percentile(scores[~np.isnan(scores)], 50)
        final_selected = [f for f, s in sorted_features if s >= score_threshold]

        if len(final_selected) < self.min_features:
            final_selected = [f for f, s in sorted_features[: self.min_features]]

        removed_features = [f for f in X.columns if f not in final_selected]
        self.removed_features.extend(
            [f"Low statistical score: {f}" for f in removed_features]
        )

        return X[final_selected], removed_features

    def _recursive_feature_elimination(
        self, X: pd.DataFrame, y: pd.Series
    ) -> tuple[pd.DataFrame, list[str]]:
        """é€’å½’ç‰¹å¾æ¶ˆé™¤."""
        n_features_to_select = min(len(X.columns), max(self.min_features, 20))

        rfe = RFE(
            estimator=self.rf_model,
            n_features_to_select=n_features_to_select,
            step=1,
        )

        rfe.fit(X, y)
        selected_features = X.columns[rfe.support_].tolist()
        removed_features = [f for f in X.columns if f not in selected_features]
        self.removed_features.extend([f"RFE eliminated: {f}" for f in removed_features])

        return X[selected_features], removed_features

    def _adjust_feature_count(self, X: pd.DataFrame, y: pd.Series) -> pd.DataFrame:
        """è°ƒæ•´æœ€ç»ˆç‰¹å¾æ•°é‡."""
        current_features = len(X.columns)

        if current_features > self.max_features:
            # ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³ç‰¹å¾æ•°é‡
            best_features = self._select_best_features_by_cv(X, y, self.max_features)
            return X[best_features]
        elif current_features < self.min_features:
            # å¦‚æœç‰¹å¾å¤ªå°‘ï¼Œè¿”å›æ‰€æœ‰ç‰¹å¾
            logger.warning(
                f"ç‰¹å¾æ•°é‡({current_features})å°‘äºæœ€å°è¦æ±‚({self.min_features})"
            )

        return X

    def _select_best_features_by_cv(
        self, X: pd.DataFrame, y: pd.Series, max_features: int
    ) -> list[str]:
        """ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³ç‰¹å¾ç»„åˆ."""
        feature_scores = {}

        for feature in X.columns:
            # å•ç‰¹å¾äº¤å‰éªŒè¯
            single_feature = X[[feature]]
            scores = cross_val_score(
                self.rf_model,
                single_feature,
                y,
                cv=5,
                scoring=(
                    "f1"
                    if self.task_type == "classification"
                    else "neg_mean_squared_error"
                ),
            )
            feature_scores[feature] = np.mean(scores)

        # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©æœ€ä½³ç‰¹å¾
        sorted_features = sorted(
            feature_scores.items(), key=lambda x: x[1], reverse=True
        )
        return [f for f, s in sorted_features[:max_features]]

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """è½¬æ¢æ•°æ®ï¼Œåªä¿ç•™é€‰æ‹©çš„ç‰¹å¾."""
        missing_features = [f for f in self.selected_features if f not in X.columns]
        if missing_features:
            logger.warning(f"è¾“å…¥æ•°æ®ç¼ºå°‘ç‰¹å¾: {missing_features}")

        # åªè¿”å›å­˜åœ¨çš„é€‰æ‹©ç‰¹å¾
        available_features = [f for f in self.selected_features if f in X.columns]
        return X[available_features]

    def get_support(self) -> list[bool]:
        """
        è·å–ç‰¹å¾æ©ç .

        Returns:
            boolåˆ—è¡¨ï¼Œé•¿åº¦ç­‰äºè¾“å…¥ç‰¹å¾æ•°ï¼ŒTrueè¡¨ç¤ºè¢«é€‰æ‹©
        """
        if not hasattr(self, "_input_feature_names"):
            return []

        support = [
            feature in self.selected_features for feature in self._input_feature_names
        ]
        return support

    def get_feature_importance_ranking(self) -> list[tuple[str, float]]:
        """è·å–ç‰¹å¾é‡è¦æ€§æ’å."""
        if not self.feature_importance:
            return []

        return sorted(self.feature_importance.items(), key=lambda x: x[1], reverse=True)

    def plot_feature_importance(
        self, top_n: int = 20, figsize: tuple[int, int] = (12, 8)
    ):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾è¡¨."""
        if not self.feature_importance:
            logger.warning("æ²¡æœ‰ç‰¹å¾é‡è¦æ€§æ•°æ®å¯ç»˜åˆ¶")
            return

        # è·å–top_nç‰¹å¾
        top_features = self.get_feature_importance_ranking()[:top_n]
        features, importance = zip(*top_features, strict=False)

        plt.figure(figsize=figsize)
        sns.barplot(x=list(importance), y=list(features))
        plt.title(f"Top {top_n} Feature Importance")
        plt.xlabel("Importance Score")
        plt.ylabel("Features")

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, v in enumerate(importance):
            plt.text(v + 0.001, i, f"{v:.4f}", va="center")

        plt.tight_layout()
        plt.show()

    def plot_correlation_matrix(self, figsize: tuple[int, int] = (12, 10)):
        """ç»˜åˆ¶ç›¸å…³æ€§çŸ©é˜µçƒ­å›¾."""
        if self.correlation_matrix is None:
            logger.warning("æ²¡æœ‰ç›¸å…³æ€§çŸ©é˜µæ•°æ®å¯ç»˜åˆ¶")
            return

        plt.figure(figsize=figsize)
        sns.heatmap(
            self.correlation_matrix,
            annot=True,
            cmap="coolwarm",
            center=0,
            square=True,
            fmt=".2f",
        )
        plt.title("Feature Correlation Matrix")
        plt.tight_layout()
        plt.show()

    def generate_report(self) -> dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†çš„ç‰¹å¾é€‰æ‹©æŠ¥å‘Š."""
        if not self.selection_report:
            logger.warning("è¯·å…ˆè¿è¡Œfitæ–¹æ³•")
            return {}

        report = {
            **self.selection_report,
            "selected_features": self.selected_features,
            "removed_features_count": len(self.removed_features),
            "removed_features": self.removed_features,
            "feature_importance": self.feature_importance,
            "correlation_threshold": self.correlation_threshold,
            "min_features": self.min_features,
            "max_features": self.max_features,
        }

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        if self.feature_importance:
            importance_scores = list(self.feature_importance.values())
            report["importance_stats"] = {
                "mean": np.mean(importance_scores),
                "std": np.std(importance_scores),
                "min": np.min(importance_scores),
                "max": np.max(importance_scores),
            }

        return report

    def save_report(self, filepath: str):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶."""
        report = self.generate_report()

        if filepath.endswith(".json"):
            import json

            with open(filepath, "w") as f:
                json.dump(report, f, indent=2, default=str)
        else:
            with open(filepath, "w") as f:
                f.write("Feature Selection Report\n")
                f.write("=" * 50 + "\n\n")

                for key, value in report.items():
                    f.write(f"{key}:\n")
                    if isinstance(value, list):
                        for item in value:
                            f.write(f"  - {item}\n")
                    else:
                        f.write(f"  {value}\n")
                    f.write("\n")

        logger.info(f"ç‰¹å¾é€‰æ‹©æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filepath}")


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹."""
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    n_features = 20

    X = pd.DataFrame(
        np.random.randn(n_samples, n_features),
        columns=[f"feature_{i}" for i in range(n_features)],
    )

    # æ·»åŠ ä¸€äº›ç›¸å…³çš„ç‰¹å¾
    X["feature_21"] = X["feature_1"] * 0.9 + np.random.randn(n_samples) * 0.1
    X["feature_22"] = X["feature_2"] * 0.8 + np.random.randn(n_samples) * 0.2

    # åˆ›å»ºåˆ†ç±»ç›®æ ‡
    y = pd.Series(np.random.choice([0, 1], n_samples))

    # åˆå§‹åŒ–ç‰¹å¾é€‰æ‹©å™¨
    selector = AdvancedFeatureSelector(
        task_type="classification",
        correlation_threshold=0.9,
        min_features=5,
        max_features=15,
    )

    # æ‰§è¡Œç‰¹å¾é€‰æ‹©
    selector.fit(X, y)

    # è·å–æŠ¥å‘Š
    report = selector.generate_report()
    print(f"åŸå§‹ç‰¹å¾æ•°: {report['original_features']}")
    print(f"æœ€ç»ˆç‰¹å¾æ•°: {report['final_features']}")
    print(f"é€‰æ‹©çš„ç‰¹å¾: {report['selected_features']}")

    # è½¬æ¢æ•°æ®
    X_transformed = selector.transform(X)
    print(f"è½¬æ¢åæ•°æ®å½¢çŠ¶: {X_transformed.shape}")


if __name__ == "__main__":
    example_usage()
