"""
MLflowå®éªŒè·Ÿè¸ªæ¨¡å—
MLflow Experiment Tracking Module

è¿™ä¸ªæ¨¡å—æä¾›äº†ä¸MLflowé›†æˆçš„å®éªŒè·Ÿè¸ªåŠŸèƒ½ï¼Œç”¨äºè®°å½•
æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‚æ•°ã€æŒ‡æ ‡å’Œå·¥ä»¶ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
- å®éªŒç®¡ç†å’Œåˆ›å»º
- å‚æ•°è®°å½•
- æŒ‡æ ‡è·Ÿè¸ª
- æ¨¡å‹ç‰ˆæœ¬æ§åˆ¶
- å·¥ä»¶ç®¡ç†
"""

import os
import sys
import time
import json
import logging
from typing import Dict, Any, Optional, Union
from datetime import datetime

try:
    import mlflow
    import mlflow.sklearn
    import mlflow.pytorch
    import numpy as np
    from mlflow.entities import RunStatus

    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    logging.warning("MLflow not available. Experiment tracking will be disabled.")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class MLflowExperimentTracker:
    """MLflowå®éªŒè·Ÿè¸ªå™¨ç±»"""

    def __init__(self, experiment_name: str = "football-prediction"):
        """
        åˆå§‹åŒ–å®éªŒè·Ÿè¸ªå™¨

        Args:
            experiment_name: å®éªŒåç§°
        """
        self.experiment_name = experiment_name
        self.tracking_uri = self._get_tracking_uri()
        self.experiment_id = None

        if MLFLOW_AVAILABLE:
            self._setup_mlflow()
        else:
            logger.warning("MLflow is not available. Tracking will be disabled.")

    def _get_tracking_uri(self) -> str:
        """è·å–MLflowè·Ÿè¸ªURI"""
        # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡
        if "MLFLOW_TRACKING_URI" in os.environ:
            return os.environ["MLFLOW_TRACKING_URI"]

        # ä½¿ç”¨æœ¬åœ°é»˜è®¤ä½ç½®
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        mlruns_dir = os.path.join(project_root, "mlruns")

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(mlruns_dir, exist_ok=True)

        return f"file://{mlruns_dir}"

    def _setup_mlflow(self):
        """è®¾ç½®MLflowé…ç½®"""
        try:
            mlflow.set_tracking_uri(self.tracking_uri)
            logger.info(f"MLflow tracking URI: {self.tracking_uri}")

            # åˆ›å»ºæˆ–è·å–å®éªŒ
            experiment = mlflow.get_experiment_by_name(self.experiment_name)
            if experiment is None:
                experiment_id = mlflow.create_experiment(self.experiment_name)
                logger.info(
                    f"Created new experiment: {self.experiment_name} (ID: {experiment_id})"
                )
            else:
                experiment_id = experiment.experiment_id
                logger.info(
                    f"Using existing experiment: {self.experiment_name} (ID: {experiment_id})"
                )

            self.experiment_id = experiment_id

        except Exception as e:
            logger.error(f"Failed to setup MLflow: {e}")
            raise

    def start_run(self, run_name: str | None = None) -> str | None:
        """
        å¼€å§‹æ–°çš„å®éªŒè¿è¡Œ

        Args:
            run_name: è¿è¡Œåç§°

        Returns:
            run_id: è¿è¡ŒID
        """
        if not MLFLOW_AVAILABLE:
            logger.warning("MLflow not available, skipping run start")
            return None

        try:
            if run_name is None:
                run_name = f"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

            run = mlflow.start_run(
                run_name=run_name,
                experiment_id=self.experiment_id,
                tags={
                    "project": "football-prediction",
                    "version": "1.0.0",
                    "author": "ml-team",
                },
            )

            logger.info(f"Started MLflow run: {run.info.run_id}")
            return run.info.run_id

        except Exception as e:
            logger.error(f"Failed to start MLflow run: {e}")
            return None

    def log_params(self, params: dict[str, Any]):
        """
        è®°å½•å‚æ•°

        Args:
            params: å‚æ•°å­—å…¸
        """
        if not MLFLOW_AVAILABLE:
            return

        try:
            for key, value in params.items():
                mlflow.log_param(key, value)
            logger.info(f"Logged {len(params)} parameters")
        except Exception as e:
            logger.error(f"Failed to log parameters: {e}")

    def log_metrics(
        self, metrics: dict[str, int | float], step: int | None = None
    ):
        """
        è®°å½•æŒ‡æ ‡

        Args:
            metrics: æŒ‡æ ‡å­—å…¸
            step: æ­¥éª¤æ•°ï¼ˆå¯é€‰ï¼‰
        """
        if not MLFLOW_AVAILABLE:
            return

        try:
            for key, value in metrics.items():
                mlflow.log_metric(key, value, step=step)
            logger.info(f"Logged {len(metrics)} metrics at step {step}")
        except Exception as e:
            logger.error(f"Failed to log metrics: {e}")

    def log_artifact(self, local_path: str, artifact_path: str | None = None):
        """
        è®°å½•å·¥ä»¶

        Args:
            local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            artifact_path: å·¥ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        if not MLFLOW_AVAILABLE:
            return

        try:
            mlflow.log_artifact(local_path, artifact_path)
            logger.info(f"Logged artifact: {local_path}")
        except Exception as e:
            logger.error(f"Failed to log artifact: {e}")

    def log_model(self, model: Any, artifact_path: str = "model"):
        """
        è®°å½•æ¨¡å‹

        Args:
            model: æ¨¡å‹å¯¹è±¡
            artifact_path: å·¥ä»¶è·¯å¾„
        """
        if not MLFLOW_AVAILABLE:
            return

        try:
            # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„æ—¥å¿—è®°å½•æ–¹æ³•
            model_type = type(model).__name__

            if "sklearn" in str(type(model)).lower():
                mlflow.sklearn.log_model(model, artifact_path)
            elif "torch" in str(type(model)).lower():
                mlflow.pytorch.log_model(model, artifact_path)
            else:
                # é€šç”¨æ–¹æ³•ï¼šå…ˆä¿å­˜æ¨¡å‹ï¼Œç„¶åè®°å½•
                import joblib

                model_path = f"{artifact_path}.pkl"
                joblib.dump(model, model_path)
                mlflow.log_artifact(model_path, artifact_path)

            logger.info(f"Logged {model_type} model as {artifact_path}")

        except Exception as e:
            logger.error(f"Failed to log model: {e}")

    def set_tags(self, tags: dict[str, str]):
        """
        è®¾ç½®æ ‡ç­¾

        Args:
            tags: æ ‡ç­¾å­—å…¸
        """
        if not MLFLOW_AVAILABLE:
            return

        try:
            mlflow.set_tags(tags)
            logger.info(f"Set {len(tags)} tags")
        except Exception as e:
            logger.error(f"Failed to set tags: {e}")

    def end_run(self, status: str = "FINISHED"):
        """
        ç»“æŸè¿è¡Œ

        Args:
            status: è¿è¡ŒçŠ¶æ€
        """
        if not MLFLOW_AVAILABLE:
            return

        try:
            mlflow.end_run()
            logger.info(f"Ended MLflow run with status: {status}")
        except Exception as e:
            logger.error(f"Failed to end run: {e}")

    def run_experiment(self, experiment_func, params: dict[str, Any] = None):
        """
        è¿è¡Œå®éªŒçš„ä¾¿æ·æ–¹æ³•

        Args:
            experiment_func: å®éªŒå‡½æ•°
            params: å®éªŒå‚æ•°

        Returns:
            å®éªŒç»“æœ
        """
        if not MLFLOW_AVAILABLE:
            logger.warning("MLflow not available, running without tracking")
            return experiment_func(params or {})

        run_id = self.start_run()
        if run_id is None:
            return experiment_func(params or {})

        try:
            # è®°å½•å‚æ•°
            if params:
                self.log_params(params)

            # è®¾ç½®æ ‡ç­¾
            self.set_tags(
                {"start_time": datetime.now().isoformat(), "environment": "development"}
            )

            # è¿è¡Œå®éªŒ
            start_time = time.time()
            result = experiment_func(params or {})
            execution_time = time.time() - start_time

            # è®°å½•æ‰§è¡Œæ—¶é—´
            self.log_metrics({"execution_time": execution_time})

            # è®°å½•ç»“æœæ‘˜è¦
            if isinstance(result, dict):
                self.log_metrics(
                    {
                        "result_size": len(result) if hasattr(result, "__len__") else 0,
                        "success": True,
                    }
                )

            return result

        except Exception as e:
            # è®°å½•é”™è¯¯
            self.log_metrics({"error": 1, "success": False})
            logger.error(f"Experiment failed: {e}")
            raise

        finally:
            self.end_run()


def dummy_football_prediction_experiment(params: dict[str, Any]) -> dict[str, Any]:
    """
    æ¨¡æ‹Ÿè¶³çƒé¢„æµ‹å®éªŒ

    Args:
        params: å®éªŒå‚æ•°

    Returns:
        å®éªŒç»“æœ
    """
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    import random

    learning_rate = params.get("learning_rate", 0.001)
    epochs = params.get("epochs", 100)
    model_type = params.get("model_type", "lstm")

    # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡
    train_losses = []
    val_accuracies = []

    for epoch in range(epochs):
        # æ¨¡æ‹Ÿè®­ç»ƒæŸå¤±ï¼ˆé€’å‡ï¼‰
        train_loss = 1.0 * (0.95**epoch) + random.uniform(-0.05, 0.05)
        train_losses.append(train_loss)

        # æ¨¡æ‹ŸéªŒè¯å‡†ç¡®ç‡ï¼ˆé€’å¢ï¼‰
        val_acc = 0.5 + 0.4 * (1 - 0.99**epoch) + random.uniform(-0.02, 0.02)
        val_accuracies.append(min(val_acc, 1.0))

    # è¿”å›ç»“æœ
    return {
        "final_train_loss": train_losses[-1],
        "final_val_accuracy": val_accuracies[-1],
        "best_val_accuracy": max(val_accuracies),
        "train_losses": train_losses,
        "val_accuracies": val_accuracies,
        "epochs_trained": epochs,
        "model_parameters": f"{model_type}_lr_{learning_rate}",
    }


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºMLflowé›†æˆ"""
    print("ğŸš€ Starting MLflow Integration Test")
    print("=" * 50)

    # åˆ›å»ºå®éªŒè·Ÿè¸ªå™¨
    try:
        tracker = MLflowExperimentTracker("football-prediction-demo")
        print("âœ… MLflow experiment tracker initialized")
        print(f"ğŸ“Š Tracking URI: {tracker.tracking_uri}")
        print(f"ğŸ·ï¸  Experiment: {tracker.experiment_name}")

        # å®šä¹‰å®éªŒå‚æ•°
        experiment_params = {
            "learning_rate": 0.001,
            "epochs": 50,
            "batch_size": 32,
            "model_type": "lstm",
            "dropout_rate": 0.2,
            "optimizer": "adam",
        }

        print("\nğŸ¯ Running experiment with parameters:")
        for key, value in experiment_params.items():
            print(f"  {key}: {value}")

        # è¿è¡Œå®éªŒ
        print("\nğŸ§ª Running experiment...")
        result = tracker.run_experiment(
            dummy_football_prediction_experiment, experiment_params
        )

        # è®°å½•ç»“æœæŒ‡æ ‡
        metrics = {
            "final_train_loss": result["final_train_loss"],
            "final_val_accuracy": result["final_val_accuracy"],
            "best_val_accuracy": result["best_val_accuracy"],
        }

        tracker.log_metrics(metrics)

        # è®°å½•é¢å¤–å·¥ä»¶
        results_json = {
            "experiment_id": tracker.experiment_id,
            "parameters": experiment_params,
            "results": result,
            "timestamp": datetime.now().isoformat(),
        }

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶å¹¶è®°å½•ä¸ºå·¥ä»¶
        import json

        results_file = "experiment_results.json"
        with open(results_file, "w") as f:
            json.dump(results_json, f, indent=2)

        tracker.log_artifact(results_file, "results")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(results_file)

        print("\nâœ… Experiment completed successfully!")
        print(f"ğŸ“Š Final validation accuracy: {metrics['final_val_accuracy']:.4f}")
        print(f"ğŸ† Best validation accuracy: {metrics['best_val_accuracy']:.4f}")
        print("ğŸ“ Results logged to MLflow experiment")

        return result

    except Exception as e:
        print(f"âŒ Failed to run experiment: {e}")
        return None


if __name__ == "__main__":
    # ç›´æ¥è¿è¡Œæ¼”ç¤º
    main()
