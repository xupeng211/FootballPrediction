"""
P0-4 ML Pipeline ç›‘æ§å’Œæ—¥å¿—æ¨¡å—
æä¾›ç»Ÿä¸€çš„ç›‘æ§ã€æ—¥å¿—å’ŒæŒ‡æ ‡æ”¶é›†åŠŸèƒ½
"""

from __future__ import annotations

import logging
import time
import json
from datetime import datetime
from typing import Any, Dict, List, Optional
from pathlib import Path
from dataclasses import dataclass, asdict

import pandas as pd

from .config import PipelineConfig


@dataclass
class TrainingMetrics:
    """è®­ç»ƒæŒ‡æ ‡æ•°æ®ç»“æ„"""

    algorithm: str
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    training_time: float
    model_size_mb: float
    sample_count: int
    feature_count: int
    timestamp: datetime
    status: str = "completed"


@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡æ•°æ®ç»“æ„"""

    cpu_usage: float
    memory_usage_mb: float
    disk_usage_gb: float
    timestamp: datetime


class PipelineLogger:
    """Pipelineä¸“ç”¨æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, config: PipelineConfig):
        """åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨"""
        self.config = config
        self.setup_logging()

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        log_dir = Path(self.config.project_root) / self.config.training.log_dir
        log_dir.mkdir(exist_ok=True)

        # é…ç½®æ—¥å¿—æ ¼å¼
        log_format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

        # é…ç½®æ–‡ä»¶å¤„ç†å™¨
        log_file = log_dir / f"pipeline_{datetime.now().strftime('%Y%m%d')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(logging.Formatter(log_format))

        # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(
            logging.DEBUG if self.config.debug_mode else logging.INFO
        )
        console_handler.setFormatter(logging.Formatter(log_format))

        # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
        root_logger = logging.getLogger("pipeline")
        root_logger.setLevel(logging.DEBUG if self.config.debug_mode else logging.INFO)
        root_logger.addHandler(file_handler)
        root_logger.addHandler(console_handler)

        # é˜²æ­¢é‡å¤æ·»åŠ 
        root_logger.propagate = False

        self.logger = root_logger

    def log_training_start(self, algorithm: str, sample_count: int, feature_count: int):
        """è®°å½•è®­ç»ƒå¼€å§‹"""
        self.logger.info(
            f"ğŸš€ è®­ç»ƒå¼€å§‹ - ç®—æ³•: {algorithm}, "
            f"æ ·æœ¬æ•°: {sample_count}, ç‰¹å¾æ•°: {feature_count}"
        )

    def log_training_complete(self, metrics: TrainingMetrics):
        """è®°å½•è®­ç»ƒå®Œæˆ"""
        self.logger.info(
            f"âœ… è®­ç»ƒå®Œæˆ - ç®—æ³•: {metrics.algorithm}, "
            f"å‡†ç¡®ç‡: {metrics.accuracy:.3f}, "
            f"è®­ç»ƒæ—¶é—´: {metrics.training_time:.2f}s"
        )

    def log_model_saved(self, model_name: str, model_path: str, size_mb: float):
        """è®°å½•æ¨¡å‹ä¿å­˜"""
        self.logger.info(
            f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜ - åç§°: {model_name}, "
            f"è·¯å¾„: {model_path}, å¤§å°: {size_mb:.2f}MB"
        )

    def log_error(self, error: Exception, context: str = ""):
        """è®°å½•é”™è¯¯"""
        self.logger.error(f"âŒ é”™è¯¯å‘ç”Ÿ - {context}: {str(error)}", exc_info=True)

    def log_warning(self, message: str):
        """è®°å½•è­¦å‘Š"""
        self.logger.warning(f"âš ï¸ {message}")

    def log_info(self, message: str):
        """è®°å½•ä¿¡æ¯"""
        self.logger.info(f"â„¹ï¸ {message}")

    def log_debug(self, message: str):
        """è®°å½•è°ƒè¯•ä¿¡æ¯"""
        self.logger.debug(f"ğŸ” {message}")


class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""

    def __init__(self, config: PipelineConfig):
        """åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨"""
        self.config = config
        self.metrics_dir = Path(config.project_root) / "artifacts" / "metrics"
        self.metrics_dir.mkdir(parents=True, exist_ok=True)

        self.training_metrics: list[TrainingMetrics] = []
        self.system_metrics: list[SystemMetrics] = []

    def record_training_metrics(self, metrics: TrainingMetrics):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        self.training_metrics.append(metrics)
        self._save_training_metrics()

    def record_system_metrics(self):
        """è®°å½•ç³»ç»ŸæŒ‡æ ‡"""
        try:
            import psutil

            metrics = SystemMetrics(
                cpu_usage=psutil.cpu_percent(),
                memory_usage_mb=psutil.virtual_memory().used / 1024 / 1024,
                disk_usage_gb=psutil.disk_usage("/").used / 1024 / 1024 / 1024,
                timestamp=datetime.now(),
            )

            self.system_metrics.append(metrics)

            # ä¿æŒæœ€è¿‘1000æ¡è®°å½•
            if len(self.system_metrics) > 1000:
                self.system_metrics = self.system_metrics[-1000:]

        except ImportError:
            # psutilæœªå®‰è£…ï¼Œè·³è¿‡ç³»ç»ŸæŒ‡æ ‡æ”¶é›†
            pass

    def _save_training_metrics(self):
        """ä¿å­˜è®­ç»ƒæŒ‡æ ‡åˆ°æ–‡ä»¶"""
        metrics_file = self.metrics_dir / "training_metrics.json"

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_metrics = []
        for metric in self.training_metrics:
            metric_dict = asdict(metric)
            metric_dict["timestamp"] = metric.timestamp.isoformat()
            serializable_metrics.append(metric_dict)

        with open(metrics_file, "w", encoding="utf-8") as f:
            json.dump(serializable_metrics, f, indent=2, ensure_ascii=False)

    def get_training_summary(self) -> dict[str, Any]:
        """è·å–è®­ç»ƒæŒ‡æ ‡æ‘˜è¦"""
        if not self.training_metrics:
            return {}

        df = pd.DataFrame([asdict(m) for m in self.training_metrics])

        summary = {
            "total_trainings": len(df),
            "algorithms": df["algorithm"].unique().tolist(),
            "average_accuracy": df["accuracy"].mean(),
            "best_accuracy": df["accuracy"].max(),
            "average_training_time": df["training_time"].mean(),
            "total_training_time": df["training_time"].sum(),
            "latest_training": (
                df["timestamp"].max().isoformat() if not df.empty else None
            ),
        }

        # æŒ‰ç®—æ³•åˆ†ç»„ç»Ÿè®¡
        algorithm_stats = (
            df.groupby("algorithm")
            .agg({"accuracy": ["mean", "max", "count"], "training_time": "mean"})
            .round(4)
        )

        summary["algorithm_performance"] = algorithm_stats.to_dict()

        return summary

    def export_metrics_csv(self, output_path: Optional[str] = None):
        """å¯¼å‡ºæŒ‡æ ‡ä¸ºCSVæ ¼å¼"""
        if not self.training_metrics:
            return None

        if output_path is None:
            output_path = self.metrics_dir / "training_metrics.csv"

        df = pd.DataFrame([asdict(m) for m in self.training_metrics])
        df.to_csv(output_path, index=False)

        return output_path


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, config: PipelineConfig):
        """åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨"""
        self.config = config
        self.logger = PipelineLogger(config)
        self.metrics_collector = MetricsCollector(config)

    def monitor_training(self, algorithm: str):
        """è®­ç»ƒè¿‡ç¨‹ç›‘æ§ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return TrainingContext(self, algorithm)

    def record_training_result(
        self,
        algorithm: str,
        result: dict[str, Any],
        training_time: float,
        sample_count: int,
        feature_count: int,
        model_size_mb: float,
    ):
        """è®°å½•è®­ç»ƒç»“æœ"""
        metrics = TrainingMetrics(
            algorithm=algorithm,
            accuracy=result.get("accuracy", 0.0),
            precision=result.get("precision", 0.0),
            recall=result.get("recall", 0.0),
            f1_score=result.get("f1_score", 0.0),
            training_time=training_time,
            model_size_mb=model_size_mb,
            sample_count=sample_count,
            feature_count=feature_count,
            timestamp=datetime.now(),
        )

        self.metrics_collector.record_training_metrics(metrics)
        self.logger.log_training_complete(metrics)

    def get_performance_dashboard(self) -> dict[str, Any]:
        """è·å–æ€§èƒ½ä»ªè¡¨æ¿æ•°æ®"""
        summary = self.metrics_collector.get_training_summary()

        dashboard = {
            "timestamp": datetime.now().isoformat(),
            "pipeline_version": "P0-4",
            "training_summary": summary,
            "system_health": self._get_system_health(),
        }

        return dashboard

    def _get_system_health(self) -> dict[str, str]:
        """è·å–ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            import psutil

            cpu_percent = psutil.cpu_percent()
            memory_percent = psutil.virtual_memory().percent
            disk_percent = psutil.disk_usage("/").percent

            # ç®€å•çš„å¥åº·è¯„ä¼°
            if cpu_percent < 70 and memory_percent < 80 and disk_percent < 90:
                health_status = "healthy"
            elif cpu_percent < 90 and memory_percent < 90 and disk_percent < 95:
                health_status = "warning"
            else:
                health_status = "critical"

            return {
                "status": health_status,
                "cpu_usage": f"{cpu_percent:.1f}%",
                "memory_usage": f"{memory_percent:.1f}%",
                "disk_usage": f"{disk_percent:.1f}%",
            }

        except ImportError:
            return {
                "status": "unknown",
                "cpu_usage": "N/A",
                "memory_usage": "N/A",
                "disk_usage": "N/A",
            }


class TrainingContext:
    """è®­ç»ƒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""

    def __init__(self, monitor: PerformanceMonitor, algorithm: str):
        """åˆå§‹åŒ–è®­ç»ƒä¸Šä¸‹æ–‡"""
        self.monitor = monitor
        self.algorithm = algorithm
        self.start_time = None
        self.sample_count = 0
        self.feature_count = 0

    def __enter__(self):
        """è¿›å…¥è®­ç»ƒä¸Šä¸‹æ–‡"""
        self.start_time = time.time()
        self.monitor.logger.log_training_start(
            self.algorithm, self.sample_count, self.feature_count
        )

        # è®°å½•ç³»ç»ŸæŒ‡æ ‡
        self.monitor.metrics_collector.record_system_metrics()

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºè®­ç»ƒä¸Šä¸‹æ–‡"""
        training_time = time.time() - self.start_time

        if exc_type is not None:
            self.monitor.logger.log_error(exc_val, f"è®­ç»ƒå¤±è´¥ - ç®—æ³•: {self.algorithm}")
        else:
            self.monitor.logger.log_info(
                f"è®­ç»ƒä¸Šä¸‹æ–‡ç»“æŸ - ç®—æ³•: {self.algorithm}, "
                f"è€—æ—¶: {training_time:.2f}s"
            )

        # è®°å½•ç³»ç»ŸæŒ‡æ ‡
        self.monitor.metrics_collector.record_system_metrics()


# ä¾¿æ·å‡½æ•°
def create_monitor(config: PipelineConfig) -> PerformanceMonitor:
    """åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨"""
    return PerformanceMonitor(config)


def get_logger(config: PipelineConfig) -> PipelineLogger:
    """è·å–Pipelineæ—¥å¿—è®°å½•å™¨"""
    return PipelineLogger(config)
