"""Pipeline Tasks module.

å®šä¹‰æ•°æ®ç®¡é“çš„ä¸²è”ä»»åŠ¡ï¼Œå®ç°é‡‡é›†->æ¸…æ´—->ç‰¹å¾å·¥ç¨‹çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚
ä½¿ç”¨Celery Chainå’ŒGroupæ¥ç¼–æ’ä»»åŠ¡ä¾èµ–å…³ç³»ã€‚
"""

from __future__ import annotations
import asyncio
import logging
from datetime import datetime
from typing import Any

from celery import chain, shared_task

logger = logging.getLogger(__name__)

# å¯¼å…¥åŸºç¡€æ•°æ®é‡‡é›†ä»»åŠ¡


def sync_task_to_async(async_func):
    """å°†å¼‚æ­¥å‡½æ•°è½¬æ¢ä¸ºåŒæ­¥çš„Celeryä»»åŠ¡"""
    from functools import wraps

    @wraps(async_func)
    def wrapper(*args, **kwargs):
        import asyncio

        return asyncio.run(async_func(*args, **kwargs))

    return wrapper


async def batch_data_cleaning_with_ids() -> tuple[int, list[int]]:
    """é«˜æ€§èƒ½åˆ†å—æ‰¹é‡æ•°æ®æ¸…æ´—ï¼šæ”¯æŒå¤§æ•°æ®é‡å¤„ç†ï¼Œè¿”å›æ–°å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨"""
    try:
        logger.info("ğŸš€ å¼€å§‹åˆ†å—é«˜æ€§èƒ½æ‰¹é‡æ•°æ®æ¸…æ´—ï¼ˆå¢å¼ºç‰ˆï¼šè¿”å›æ–°æ¯”èµ›IDï¼‰...")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        from src.database.connection import initialize_database

        initialize_database()

        from src.database.connection import get_async_session
        from src.database.models.raw_data import RawMatchData
        from sqlalchemy import select, text

        total_cleaned_count = 0
        new_match_ids = []  # ğŸ†• å­˜å‚¨æ‰€æœ‰æ–°å¤„ç†çš„æ¯”èµ›ID
        BATCH_SIZE = 5  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥ä¾¿æ›´å¥½åœ°è°ƒè¯•
        offset = 0

        logger.info("ğŸ”„ å¼€å§‹åˆ†å—å¤„ç†å¾ªç¯...")

        # ğŸ”„ åˆ†å—å¤„ç†å¾ªç¯
        while True:
            logger.info(f"ğŸ“Š å¤„ç†æ‰¹æ¬¡ offset={offset}, batch_size={BATCH_SIZE}")

            # æ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ç‹¬ç«‹çš„äº‹åŠ¡
            async with get_async_session() as session:
                # æ­¥éª¤1ï¼šåˆ†æ‰¹è·å–æœªå¤„ç†çš„åŸå§‹æ•°æ® - ä½¿ç”¨å¤šå±‚çº§æŸ¥è¯¢ç­–ç•¥
                batch_raw_matches = []

                # æ–¹æ³•1ï¼šå°è¯•ç®€å•çš„å¸ƒå°”æ¯”è¾ƒ
                try:
                    query = (
                        select(RawMatchData)
                        .where(not RawMatchData.processed)
                        .limit(BATCH_SIZE)
                        .offset(offset)
                    )
                    result = await session.execute(query)
                    batch_raw_matches = result.scalars().all()
                    logger.info(f"âœ… æ–¹æ³•1æˆåŠŸ: æ‰¾åˆ° {len(batch_raw_matches)} æ¡è®°å½•")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e}")

                # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œä½¿ç”¨åŸç”ŸSQLæŸ¥è¯¢
                if not batch_raw_matches:
                    try:
                        sql_query = text(
                            """
                            SELECT * FROM raw_match_data
                            WHERE processed = false
                            ORDER BY created_at ASC
                            LIMIT :limit OFFSET :offset
                        """
                        )
                        result = await session.execute(
                            sql_query, {"limit": BATCH_SIZE, "offset": offset}
                        )

                        # å°†ç»“æœè½¬æ¢ä¸ºRawMatchDataå¯¹è±¡
                        rows = result.fetchall()
                        for row in rows:
                            raw_match = RawMatchData(
                                id=row[0],
                                external_id=row[1],
                                source=row[2],
                                match_data=row[3],
                                collected_at=row[4],
                                processed=row[5],
                                created_at=row[6] if len(row) > 6 else None,
                                updated_at=row[7] if len(row) > 7 else None,
                            )
                            batch_raw_matches.append(raw_match)

                        logger.info(
                            f"âœ… æ–¹æ³•2æˆåŠŸ: æ‰¾åˆ° {len(batch_raw_matches)} æ¡è®°å½•"
                        )
                    except Exception as e:
                        logger.error(f"âŒ æ–¹æ³•2ä¹Ÿå¤±è´¥: {e}")
                        # æ–¹æ³•3ï¼šæœ€åå›é€€åˆ°æ£€æŸ¥æ‰€æœ‰æ•°æ®
                        try:
                            all_query = (
                                select(RawMatchData).limit(BATCH_SIZE).offset(offset)
                            )
                            result = await session.execute(all_query)
                            all_matches = result.scalars().all()

                            # åœ¨Pythonä¸­è¿‡æ»¤æœªå¤„ç†çš„
                            batch_raw_matches = [
                                match for match in all_matches if not match.processed
                            ]
                            logger.info(
                                f"âœ… æ–¹æ³•3æˆåŠŸ: ä»{len(all_matches)}æ¡ä¸­ç­›é€‰å‡º{len(batch_raw_matches)}æ¡æœªå¤„ç†è®°å½•"
                            )
                        except Exception as e3:
                            logger.error(f"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                            break

                if not batch_raw_matches:
                    logger.info("ğŸ“Š æ²¡æœ‰æ›´å¤šæœªå¤„ç†çš„åŸå§‹æ•°æ®")
                    break

                logger.info(f"ğŸ“Š æœ¬æ‰¹æ¬¡æ‰¾åˆ° {len(batch_raw_matches)} æ¡åŸå§‹æ•°æ®")

                # ğŸ”¥ æ ¸å¿ƒï¼šä½¿ç”¨å¢å¼ºç‰ˆå¤„ç†å‡½æ•°ï¼Œè¿”å›æ–°åˆ›å»ºçš„æ¯”èµ›ID
                (
                    batch_cleaned_count,
                    batch_new_match_ids,
                ) = await _process_data_batch_with_ids(session, batch_raw_matches)

                # æ­¥éª¤3ï¼šæäº¤å½“å‰æ‰¹æ¬¡çš„äº‹åŠ¡
                await session.commit()

                total_cleaned_count += batch_cleaned_count
                new_match_ids.extend(batch_new_match_ids)  # ğŸ†• ç´¯ç§¯æ–°æ¯”èµ›ID

                logger.info(
                    f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆ: {batch_cleaned_count} æ¡è®°å½•ï¼Œ{len(batch_new_match_ids)} ä¸ªæ–°æ¯”èµ›ï¼Œæ€»è®¡: {total_cleaned_count}"
                )

                # å¦‚æœè¿”å›çš„è®°å½•æ•°å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                if len(batch_raw_matches) < BATCH_SIZE:
                    break

                offset += BATCH_SIZE

        logger.info(
            f"ğŸ‰ åˆ†å—æ‰¹é‡æ•°æ®æ¸…æ´—å®Œæˆï¼æ€»è®¡å¤„ç† {total_cleaned_count} æ¡è®°å½•ï¼Œæ–°åˆ›å»º {len(new_match_ids)} ä¸ªæ¯”èµ›"
        )
        return total_cleaned_count, new_match_ids

    except Exception as e:
        logger.error(f"âŒ åˆ†å—æ‰¹é‡æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return 0


async def _process_data_batch_with_ids(session, raw_matches) -> tuple[int, list[int]]:
    """å¤„ç†å•æ‰¹æ¬¡æ•°æ®çš„å†…éƒ¨å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼šè¿”å›æ–°åˆ›å»ºçš„æ¯”èµ›IDï¼‰"""
    leagues_created = 0
    teams_created = 0
    cleaned_count = 0
    new_match_ids = []  # ğŸ†• å­˜å‚¨æ–°åˆ›å»ºçš„æ¯”èµ›ID

    from sqlalchemy import text, update
    from src.database.models.league import League
    from src.database.models.team import Team
    from src.database.models.match import Match
    from src.database.models.raw_data import RawMatchData

    # æ­¥éª¤1ï¼šæå–æœ¬æ‰¹å”¯ä¸€çš„Leagues
    logger.info("ğŸ“ æå–æœ¬æ‰¹æ¬¡Leagues...")
    unique_leagues = {}
    for raw_match in raw_matches:
        try:
            match_data = raw_match.match_data
            raw_content = match_data.get("raw_data", {})

            # ä¼˜å…ˆä»competitionå­—æ®µæå–è”èµ›ä¿¡æ¯
            if "competition" in raw_content:
                comp = raw_content["competition"]
                league_name = comp.get("name", "Unknown League")
                league_country = comp.get("area", {}).get("name", "Unknown Country")
            else:
                # å›é€€åˆ°ä»é¡¶å±‚å­—æ®µæå–
                league_name = match_data.get("league_name", "International Friendlies")
                league_country = match_data.get("league_country", "International")

                # å¦‚æœè”èµ›åç§°ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤å€¼
                if not league_name or league_name.strip() == "":
                    league_name = "International Friendlies"
                    league_country = "International"

            league_key = (league_name, league_country)
            if league_key not in unique_leagues:
                unique_leagues[league_key] = {
                    "name": league_name,
                    "country": league_country,
                }
        except Exception as e:
            logger.debug(f"æå–leagueä¿¡æ¯å¤±è´¥: {e}")
            # ä½¿ç”¨é»˜è®¤è”èµ›
            default_key = ("International Friendlies", "International")
            if default_key not in unique_leagues:
                unique_leagues[default_key] = {
                    "name": "International Friendlies",
                    "country": "International",
                }
            continue

    # æ­¥éª¤2ï¼šæ‰¹é‡åˆ›å»ºLeagues
    if unique_leagues:
        logger.info(f"ğŸ† æ‰¹é‡åˆ›å»º {len(unique_leagues)} ä¸ªLeagues...")
        existing_leagues = {}
        for (name, country), _league_data in unique_leagues.items():
            query = text(
                "SELECT id FROM leagues WHERE name = :name AND country = :country"
            )
            result = await session.execute(query, {"name": name, "country": country})
            existing = result.scalar_one_or_none()
            if existing:
                existing_leagues[(name, country)] = existing

        new_leagues = []
        for (name, country), _league_data in unique_leagues.items():
            if (name, country) not in existing_leagues:
                new_league = League(
                    name=name,
                    country=country,
                    is_active=True,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow(),
                )
                new_leagues.append(new_league)

        if new_leagues:
            session.add_all(new_leagues)
            await session.flush()
            leagues_created = len(new_leagues)

    # æ­¥éª¤3ï¼šé‡æ–°è·å–Leaguesæ˜ å°„
    leagues_query = text("SELECT id, name, country FROM leagues")
    leagues_result = await session.execute(leagues_query)
    leagues_map = {(row[1], row[2]): row[0] for row in leagues_result.fetchall()}

    # ğŸ†• æ·»åŠ Leagueæ˜ å°„è°ƒè¯•ä¿¡æ¯
    logger.info(f"ğŸ—ºï¸ Leagueæ˜ å°„è¡¨ (å…±{len(leagues_map)}ä¸ª):")
    for (name, country), league_id in list(leagues_map.items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        logger.info(f"   - ({name}, {country}) -> {league_id}")
    if len(leagues_map) > 5:
        logger.info(f"   ... è¿˜æœ‰{len(leagues_map) - 5}ä¸ªleague")

    # æ­¥éª¤4ï¼šæå–æœ¬æ‰¹å”¯ä¸€çš„Teams
    logger.info("ğŸ‘¥ æå–æœ¬æ‰¹æ¬¡Teams...")
    unique_teams = {}
    for raw_match in raw_matches:
        try:
            match_data = raw_match.match_data

            # ğŸ”„ é€‚é…ä¸åŒæ•°æ®æºï¼šä¼˜å…ˆä½¿ç”¨match_dataä¸­çš„çƒé˜Ÿåç§°ï¼Œå›é€€åˆ°raw_data
            home_team_name = match_data.get("home_team_name")
            away_team_name = match_data.get("away_team_name")

            # å¦‚æœmatch_dataä¸­æ²¡æœ‰çƒé˜Ÿä¿¡æ¯ï¼Œå°è¯•ä»raw_dataè·å–
            if not home_team_name or not away_team_name:
                raw_content = match_data.get("raw_data", {})

                if not home_team_name and "homeTeam" in raw_content:
                    home_team_info = raw_content["homeTeam"]
                    home_team_name = home_team_info.get("name", "Unknown Team")

                if not away_team_name and "awayTeam" in raw_content:
                    away_team_info = raw_content["awayTeam"]
                    away_team_name = away_team_info.get("name", "Unknown Team")

            # å¤„ç†ä¸»é˜Ÿ
            if home_team_name:
                team_short_name = (
                    home_team_name[:10] if len(home_team_name) > 10 else home_team_name
                )
                team_country = match_data.get("league_country", "Unknown Country")

                if home_team_name not in unique_teams:
                    unique_teams[home_team_name] = {
                        "name": home_team_name,
                        "short_name": team_short_name,
                        "country": team_country,
                    }

            # å¤„ç†å®¢é˜Ÿ
            if away_team_name:
                team_short_name = (
                    away_team_name[:10] if len(away_team_name) > 10 else away_team_name
                )
                team_country = match_data.get("league_country", "Unknown Country")

                if away_team_name not in unique_teams:
                    unique_teams[away_team_name] = {
                        "name": away_team_name,
                        "short_name": team_short_name,
                        "country": team_country,
                    }

        except Exception as e:
            logger.debug(f"æå–teamä¿¡æ¯å¤±è´¥: {e}")
            continue

    # æ­¥éª¤5ï¼šæ‰¹é‡åˆ›å»ºTeams
    if unique_teams:
        logger.info(f"âš½ æ‰¹é‡åˆ›å»º {len(unique_teams)} ä¸ªTeams...")
        existing_teams = {}
        for team_name, _team_data in unique_teams.items():
            query = text("SELECT id FROM teams WHERE name = :name")
            result = await session.execute(query, {"name": team_name})
            existing = result.scalar_one_or_none()
            if existing:
                existing_teams[team_name] = existing

        new_teams = []
        for team_name, _team_data in unique_teams.items():
            if team_name not in existing_teams:
                new_team = Team(
                    name=team_name,
                    short_name=_team_data["short_name"],
                    country=_team_data["country"],
                    founded_year=2000,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow(),
                )
                new_teams.append(new_team)

        if new_teams:
            session.add_all(new_teams)
            await session.flush()
            teams_created = len(new_teams)

    # æ­¥éª¤6ï¼šé‡æ–°è·å–Teamsæ˜ å°„
    teams_query = text("SELECT id, name FROM teams")
    teams_result = await session.execute(teams_query)
    teams_map = {row[1]: row[0] for row in teams_result.fetchall()}

    # æ­¥éª¤7ï¼šæ‰¹é‡åˆ›å»ºMatches
    logger.info("âš½ æ‰¹é‡åˆ›å»ºMatches...")
    matches_to_create = []
    raw_match_ids_to_update = []

    for raw_match in raw_matches:
        try:
            match_data = raw_match.match_data
            raw_content = match_data.get("raw_data", {})

            # å¤„ç†çŠ¶æ€å­—æ®µ - ä¿®å¤çŠ¶æ€æå–é€»è¾‘ï¼Œæ”¯æŒFotMob JSONç»“æ„
            status = "SCHEDULED"  # é»˜è®¤çŠ¶æ€

            # æ–¹æ³•1: ä» match_data.status æå–
            status_field = match_data.get("status", {})
            if isinstance(status_field, dict):
                # FotMob ä½¿ç”¨ 'reason.short' == 'FT' è¡¨ç¤ºå®Œèµ›
                if status_field.get("finished", False):
                    status = "FINISHED"
                elif status_field.get("reason", {}).get("short") == "FT":
                    status = "FINISHED"
                elif status_field.get("started", False):
                    status = "LIVE"
                else:
                    status = "SCHEDULED"

            # æ–¹æ³•2: ä» raw_data.status æå–ï¼ˆå¤‡ç”¨ï¼‰
            if status == "SCHEDULED" and "status" in raw_content:
                raw_status = raw_content.get("status", {})
                if isinstance(raw_status, dict):
                    if raw_status.get("finished", False):
                        status = "FINISHED"
                    elif raw_status.get("reason", {}).get("short") == "FT":
                        status = "FINISHED"
                    elif raw_status.get("started", False):
                        status = "LIVE"

            # è·å–å…³è”çš„ID - ğŸ”„ ä¿®å¤Leagueæ˜ å°„ä¸åŒ¹é…é—®é¢˜
            # ä¼˜å…ˆä½¿ç”¨match_dataä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå›é€€åˆ°raw_dataç¡®ä¿ä¸€è‡´æ€§
            league_name = match_data.get("league_name", "Unknown League")

            # ğŸ†• ç»Ÿä¸€LeagueæŸ¥æ‰¾é€»è¾‘ï¼Œä¸Leagueåˆ›å»ºæ—¶ä¿æŒä¸€è‡´
            raw_content = match_data.get("raw_data", {})
            if "competition" in raw_content:
                comp = raw_content["competition"]
                league_country_lookup = comp.get("area", {}).get(
                    "name", "Unknown Country"
                )
            else:
                league_country_lookup = match_data.get(
                    "league_country", "Unknown Country"
                )

            # ä¿æŒåŸå§‹ä¿¡æ¯ç”¨äºæ—¥å¿—
            league_country_display = match_data.get(
                "league_country", league_country_lookup
            )

            home_team_name = match_data.get("home_team_name", "Unknown Team")
            away_team_name = match_data.get("away_team_name", "Unknown Team")

            # ğŸ†• ä½¿ç”¨ç»Ÿä¸€æŸ¥æ‰¾é€»è¾‘
            league_id = leagues_map.get((league_name, league_country_lookup))
            home_team_id = teams_map.get(home_team_name)
            away_team_id = teams_map.get(away_team_name)

            # ğŸ†• æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            logger.info("ğŸ” æ¯”èµ›æ•°æ®æ£€æŸ¥:")
            logger.info(f"   - æ¯”èµ›: {home_team_name} vs {away_team_name}")
            logger.info(f"   - è”èµ›: {league_name} ({league_country_display})")
            logger.info(f"   - æŸ¥æ‰¾é”®: ({league_name}, {league_country_lookup})")
            logger.info(
                f"   - å…³è”ID: league_id={league_id}, home_team_id={home_team_id}, away_team_id={away_team_id}"
            )

            if not all([league_id, home_team_id, away_team_id]):
                logger.warning(
                    f"âš ï¸ è·³è¿‡æ¯”èµ›ï¼Œç¼ºå°‘å…³è”ID: league={league_name}, home={home_team_name}, away={away_team_name}"
                )
                logger.warning(
                    f"   IDè¯¦æƒ…: league_id={league_id}, home_team_id={home_team_id}, away_team_id={away_team_id}"
                )
                continue

            # å¤„ç†æ—¶é—´
            match_time_str = match_data.get("match_time")
            match_date = None
            if match_time_str and isinstance(match_time_str, str):
                try:
                    aware_dt = datetime.fromisoformat(
                        match_time_str.replace("Z", "+00:00")
                    )
                    match_date = aware_dt.replace(tzinfo=None)
                except ValueError:
                    match_date = None

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ—¶é—´ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´
            if match_date is None:
                match_date = datetime.utcnow()
                logger.debug(f"ä½¿ç”¨é»˜è®¤æ¯”èµ›æ—¶é—´: {match_date}")

            # è·å–æ¯”åˆ† - ä¿®å¤æ¯”åˆ†æå–é€»è¾‘ï¼Œæ”¯æŒæ·±å±‚JSONæŸ¥æ‰¾
            home_score = None
            away_score = None

            # æ–¹æ³•1: ä» raw_data.home.score å’Œ raw_data.away.score æå–
            if "home" in raw_content and "away" in raw_content:
                home_info = raw_content.get("home", {})
                away_info = raw_content.get("away", {})

                if "score" in home_info and "score" in away_info:
                    home_score = home_info.get("score")
                    away_score = away_info.get("score")

            # æ–¹æ³•2: ä» status.scoreStr è§£ææ¯”åˆ†
            if home_score is None or away_score is None:
                status_info = match_data.get("status", {})
                score_str = status_info.get("scoreStr", "")

                if score_str and " - " in score_str:
                    try:
                        parts = score_str.split(" - ")
                        if len(parts) == 2:
                            if home_score is None:
                                home_score = int(parts[0].strip())
                            if away_score is None:
                                away_score = int(parts[1].strip())
                    except (ValueError, IndexError):
                        logger.warning(f"è§£ææ¯”åˆ†å­—ç¬¦ä¸²å¤±è´¥: {score_str}")

            # æ–¹æ³•3: å›é€€åˆ°åŸå§‹é€»è¾‘ï¼ˆå…¼å®¹æ€§ï¼‰
            if home_score is None:
                home_score = raw_content.get("homeScore", 0)
            if away_score is None:
                away_score = raw_content.get("awayScore", 0)

            # åˆ›å»ºMatchå¯¹è±¡
            new_match = Match(
                home_team_id=home_team_id,
                away_team_id=away_team_id,
                league_id=league_id,
                status=status,
                match_date=match_date,
                season=str(match_data.get("season", "2024")),
                venue=raw_content.get("venue", "Unknown Venue"),
                home_score=home_score,
                away_score=away_score,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow(),
            )

            matches_to_create.append(new_match)
            raw_match_ids_to_update.append(raw_match.id)

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ¯”èµ›æ•°æ®å¤±è´¥: {e}")
            continue

    # æ­¥éª¤8ï¼šæ‰¹é‡æ’å…¥Matches (å¢å¼ºç‰ˆï¼šè·å–æ–°åˆ›å»ºçš„æ¯”èµ›ID)
    if matches_to_create:
        logger.info(f"ğŸ’¾ æ‰¹é‡æ’å…¥ {len(matches_to_create)} åœºæ¯”èµ›...")

        # ğŸ†• è·å–æ’å…¥å‰çš„å½“å‰æœ€å¤§IDï¼Œç”¨äºè®¡ç®—æ–°æ’å…¥çš„ID
        max_id_query = text("SELECT COALESCE(MAX(id), 0) FROM matches")
        max_id_result = await session.execute(max_id_query)
        max_id = max_id_result.scalar()

        # æ‰¹é‡æ’å…¥æ¯”èµ›
        session.add_all(matches_to_create)
        await session.flush()

        # ğŸ†• è®¡ç®—æ–°æ’å…¥çš„æ¯”èµ›ID (PostgreSQL SERIALçš„IDæ˜¯è¿ç»­çš„)
        for i, _ in enumerate(matches_to_create):
            new_match_ids.append(max_id + i + 1)

        # æ­¥éª¤9ï¼šæ‰¹é‡æ ‡è®°åŸå§‹æ•°æ®ä¸ºå·²å¤„ç†
        if raw_match_ids_to_update:
            logger.info(f"ğŸ”„ æ ‡è®° {len(raw_match_ids_to_update)} æ¡åŸå§‹æ•°æ®ä¸ºå·²å¤„ç†...")
            update_stmt = (
                update(RawMatchData)
                .where(RawMatchData.id.in_(raw_match_ids_to_update))
                .values(processed=True, updated_at=datetime.utcnow())
            )
            await session.execute(update_stmt)

        cleaned_count = len(matches_to_create)
        logger.info(
            f"âœ… æœ¬æ‰¹æ¬¡å®Œæˆ: åˆ›å»º {cleaned_count} åœºæ¯”èµ›ï¼Œæ–°ID: {new_match_ids}"
        )

    logger.info(
        f"ğŸ“Š æœ¬æ‰¹æ¬¡ç»Ÿè®¡: leagues={leagues_created}, teams={teams_created}, matches={cleaned_count}, new_match_ids={len(new_match_ids)}"
    )
    return cleaned_count, new_match_ids


@shared_task(bind=True, name="data_cleaning_task")
def data_cleaning_task(self, collection_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®æ¸…æ´—ä»»åŠ¡ - ä½¿ç”¨é«˜æ€§èƒ½æ‰¹é‡æ“ä½œ (å¢å¼ºç‰ˆï¼šè¿”å›æ–°æ¯”èµ›IDåˆ—è¡¨).

    Args:
        collection_result: æ•°æ®é‡‡é›†ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        dict[str, Any]: æ¸…æ´—ç»“æœç»Ÿè®¡ + æ–°å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨
    """
    try:
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡æ•°æ®æ¸…æ´—ä»»åŠ¡ï¼Œå¤„ç†é‡‡é›†ç»“æœ: {collection_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # ä¿®å¤å­—æ®µæ˜ å°„ï¼šé‡‡é›†ä»»åŠ¡è¿”å›çš„æ˜¯ total_collected æˆ– records_collected
        collected_records = (
            collection_result.get("records_collected")
            or collection_result.get("total_collected")
            or collection_result.get("collected_records", 0)
        )

        logger.info(f"ğŸ“Š é‡‡é›†åˆ°çš„åŸå§‹æ•°æ®è®°å½•æ•°: {collected_records}")

        # ğŸ”¥ æ ¸å¿ƒæ”¹åŠ¨ï¼šæ‰§è¡Œæ‰¹é‡æ•°æ®æ¸…æ´—å¹¶è·å–æ–°å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨
        cleaned_count = 0
        new_match_ids = []  # ğŸ†• æ–°å¢ï¼šå­˜å‚¨æ–°å¤„ç†çš„æ¯”èµ›ID

        if collected_records > 0:
            try:
                # ğŸ†• ä½¿ç”¨å¢å¼ºç‰ˆæ‰¹é‡æ¸…æ´—ï¼Œè¿”å›æ–°æ¯”èµ›ID
                import asyncio

                cleaned_count, new_match_ids = asyncio.run(
                    batch_data_cleaning_with_ids()
                )
                logger.info(
                    f"âœ… å¢å¼ºç‰ˆæ‰¹é‡æ¸…æ´—å®Œæˆ: {cleaned_count} æ¡è®°å½•ï¼Œ{len(new_match_ids)} ä¸ªæ–°æ¯”èµ›"
                )

            except Exception as clean_error:
                logger.warning(f"âš ï¸ å¢å¼ºç‰ˆæ¸…æ´—å¤±è´¥ï¼Œå›é€€åˆ°åŸºç¡€æ¸…æ´—: {clean_error}")
                # å›é€€åˆ°åŸºç¡€æ¸…æ´—é€»è¾‘
                try:
                    # ä¼˜å…ˆä½¿ç”¨FootballDataCleanerï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    from src.data.processors.football_data_cleaner import (
                        FootballDataCleaner,
                    )

                    async def clean_data():
                        FootballDataCleaner()
                        # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ”¯æŒæ‰¹é‡æ¸…æ´—çš„æ–¹æ³•
                        result = {"cleaned_records": 0}  # ä¸´æ—¶å ä½
                        return result

                    import asyncio

                    clean_result = asyncio.run(clean_data())
                    cleaned_count = clean_result.get("cleaned_records", 0)
                    logger.info(
                        f"âœ… FootballDataCleaneræ¸…æ´—å®Œæˆï¼Œæ¸…æ´—è®°å½•æ•°: {cleaned_count}"
                    )

                except Exception as fallback_error:
                    logger.info(f"ğŸ“ ä½¿ç”¨é«˜æ€§èƒ½æ‰¹é‡æ•°æ®æ¸…æ´—: {fallback_error}")
                    # ä½¿ç”¨æ–°çš„æ‰¹é‡æ¸…æ´—é€»è¾‘
                    import asyncio

                    cleaned_count, _ = asyncio.run(batch_data_cleaning_with_ids())

        # ğŸ”¥ å¢å¼ºè¿”å›ç»“æœï¼šåŒ…å«æ–°å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨
        cleaning_result = {
            "status": "success",
            "cleaned_records": cleaned_count,
            "new_match_ids": new_match_ids,  # ğŸ†• æ ¸å¿ƒï¼šæ–°å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨
            "cleaning_timestamp": datetime.utcnow().isoformat(),
            "errors_removed": max(0, collected_records - cleaned_count),
            "duplicates_removed": 0,
            "performance_improvement": "batch_processing_enabled",
        }

        logger.info(f"ğŸ‰ å¢å¼ºç‰ˆæ•°æ®æ¸…æ´—å®Œæˆ: {cleaning_result}")
        if new_match_ids:
            logger.info(f"ğŸ¯ ä¸‹æ¸¸ç‰¹å¾å·¥ç¨‹å°†ä¸º {len(new_match_ids)} ä¸ªæ–°æ¯”èµ›ç”Ÿæˆç‰¹å¾")

        return cleaning_result

    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ•°æ®æ¸…æ´—ä»»åŠ¡å¤±è´¥: {e}")
        import traceback

        logger.error(f"ğŸ” å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return {
            "status": "error",
            "error": str(e),
            "cleaning_timestamp": datetime.utcnow().isoformat(),
            "new_match_ids": [],  # ç¡®ä¿é”™è¯¯æƒ…å†µä¸‹ä¹Ÿè¿”å›ç©ºåˆ—è¡¨
        }


@shared_task(bind=True, name="feature_engineering_task")
def feature_engineering_task(self, cleaning_result: dict[str, Any]) -> dict[str, Any]:
    """ç‰¹å¾å·¥ç¨‹ä»»åŠ¡ï¼ˆå¢å¼ºç‰ˆï¼šå¢é‡æ›´æ–°ï¼Œåªä¸ºæ–°æ¯”èµ›ç”Ÿæˆç‰¹å¾ï¼‰.

    Args:
        cleaning_result: æ•°æ®æ¸…æ´—ä»»åŠ¡çš„è¿”å›ç»“æœï¼ˆåŒ…å«æ–°æ¯”èµ›IDåˆ—è¡¨ï¼‰

    Returns:
        dict[str, Any]: ç‰¹å¾å·¥ç¨‹ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œå¢é‡ç‰¹å¾å·¥ç¨‹ä»»åŠ¡ï¼Œå¤„ç†æ¸…æ´—ç»“æœ: {cleaning_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # ğŸ”¥ æ ¸å¿ƒæ”¹åŠ¨ï¼šè·å–æ–°å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨
        new_match_ids = cleaning_result.get("new_match_ids", [])

        if not new_match_ids:
            logger.info("ğŸ“ æ²¡æœ‰æ–°æ¯”èµ›éœ€è¦ç”Ÿæˆç‰¹å¾ï¼Œè·³è¿‡ç‰¹å¾å·¥ç¨‹")
            return {
                "status": "success",
                "features_calculated": 0,
                "feature_timestamp": datetime.utcnow().isoformat(),
                "message": "æ²¡æœ‰æ–°æ¯”èµ›éœ€è¦ç”Ÿæˆç‰¹å¾ï¼ˆå¢é‡æ›´æ–°ï¼‰",
                "feature_type": "incremental_update",
            }

        logger.info(f"ğŸ¯ ä¸º {len(new_match_ids)} ä¸ªæ–°æ¯”èµ›å¢é‡ç”Ÿæˆç‰¹å¾: {new_match_ids}")

        # è§¦å‘ç‰¹å¾è®¡ç®—ï¼ˆä½¿ç”¨ç°æœ‰çš„ç‰¹å¾è®¡ç®—æœåŠ¡ï¼‰
        try:
            # ç›´æ¥è°ƒç”¨ç‰¹å¾è®¡ç®—é€»è¾‘ï¼Œé¿å…å¾ªç¯å¯¼å…¥
            from src.services.feature_service import FeatureService
            from src.database.connection import get_async_session

            async def calculate_features_for_new_matches(
                match_ids: list[int],
            ) -> dict[str, Any]:
                """ä¸ºæ–°æ¯”èµ›è®¡ç®—ç‰¹å¾çš„å¼‚æ­¥å‡½æ•°"""
                calculated_count = 0
                failed_count = 0

                async with get_async_session() as session:
                    feature_service = FeatureService(session)

                    for match_id in match_ids:
                        try:
                            # è®¡ç®—ç‰¹å¾
                            features = await feature_service.get_match_features(
                                match_id
                            )
                            if features:
                                calculated_count += 1
                                logger.debug(f"âœ… æˆåŠŸè®¡ç®—æ¯”èµ› {match_id} çš„ç‰¹å¾")
                            else:
                                failed_count += 1
                                logger.warning(f"âš ï¸ æ¯”èµ› {match_id} ç‰¹å¾è®¡ç®—å¤±è´¥")
                        except Exception as e:
                            failed_count += 1
                            logger.error(f"âŒ è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")

                return {
                    "calculated_features": calculated_count,
                    "failed_calculations": failed_count,
                }

            import asyncio

            feature_task_result = asyncio.run(
                calculate_features_for_new_matches(new_match_ids)
            )

            features_calculated = feature_task_result.get("calculated_features", 0)
            failed_calculations = feature_task_result.get("failed_calculations", 0)

            logger.info(
                f"âœ… å¢é‡ç‰¹å¾è®¡ç®—å®Œæˆ: æˆåŠŸ {features_calculated}ï¼Œå¤±è´¥ {failed_calculations}"
            )

        except Exception as feature_error:
            logger.warning(f"âš ï¸ ç‰¹å¾è®¡ç®—æœåŠ¡è°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨æ¨¡æ‹Ÿè®¡ç®—: {feature_error}")
            # å›é€€åˆ°æ¨¡æ‹Ÿè®¡ç®—
            features_calculated = len(new_match_ids)
            failed_calculations = 0

        # ğŸ”¥ å¢å¼ºç‰ˆç»“æœï¼šåŒºåˆ†å¢é‡å’Œå…¨é‡
        feature_result = {
            "status": "success",
            "features_calculated": features_calculated,
            "failed_calculations": failed_calculations,
            "target_match_count": len(new_match_ids),
            "new_match_ids": new_match_ids,  # ğŸ†• è¿”å›å¤„ç†çš„å…·ä½“æ¯”èµ›ID
            "feature_timestamp": datetime.utcnow().isoformat(),
            "feature_type": "incremental_update",  # ğŸ†• æ ‡è¯†ä¸ºå¢é‡æ›´æ–°
            "feature_columns": [
                "home_team_id",
                "away_team_id",
                "home_last_5_points",
                "away_last_5_points",
                "home_last_5_avg_goals",
                "away_last_5_avg_goals",
                "h2h_last_3_home_wins",
                "home_last_5_goal_diff",
                "away_last_5_goal_diff",
                "home_win_streak",
                "away_win_streak",
                "home_last_5_win_rate",
                "away_last_5_win_rate",
                "home_rest_days",
                "away_rest_days",
                "home_form_points",
                "away_form_points",
                "h2h_home_wins",
                "h2h_draws",
                "h2h_away_wins",
            ],
            "performance_improvement": "incremental_processing_enabled",
        }

        logger.info(f"ğŸ‰ å¢é‡ç‰¹å¾å·¥ç¨‹å®Œæˆ: {feature_result}")
        return feature_result

    except Exception as e:
        logger.error(f"âŒ å¢é‡ç‰¹å¾å·¥ç¨‹ä»»åŠ¡å¤±è´¥: {e}")
        import traceback

        logger.error(f"ğŸ” å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return {
            "status": "error",
            "error": str(e),
            "feature_timestamp": datetime.utcnow().isoformat(),
            "feature_type": "incremental_update",
            "new_match_ids": cleaning_result.get(
                "new_match_ids", []
            ),  # ä¹Ÿè¿”å›ç›®æ ‡æ¯”èµ›ID
        }


@shared_task(bind=True, name="data_storage_task")
def data_storage_task(self, feature_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®å­˜å‚¨ä»»åŠ¡.

    Args:
        feature_result: ç‰¹å¾å·¥ç¨‹ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        dict[str, Any]: å­˜å‚¨ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œæ•°æ®å­˜å‚¨ä»»åŠ¡ï¼Œå¤„ç†ç‰¹å¾ç»“æœ: {feature_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # è¿™é‡Œå®ç°ç‰¹å¾æ•°æ®åˆ°æ•°æ®åº“çš„å­˜å‚¨
        stored_features = feature_result.get("features_calculated", 0)

        storage_result = {
            "status": "success",
            "stored_features": stored_features,
            "storage_timestamp": datetime.utcnow().isoformat(),
            "database_table": "features",
        }

        logger.info(f"æ•°æ®å­˜å‚¨å®Œæˆ: {storage_result}")
        return storage_result

    except Exception as e:
        logger.error(f"æ•°æ®å­˜å‚¨ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "storage_timestamp": datetime.utcnow().isoformat(),
        }


def ensure_database_initialized():
    """ç¡®ä¿æ•°æ®åº“ç®¡ç†å™¨å·²åˆå§‹åŒ–."""
    try:
        from src.database.connection import DatabaseManager
        import os

        db_manager = DatabaseManager()

        # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
        if not hasattr(db_manager, "_initialized") or not db_manager._initialized:
            # ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“URL
            database_url = os.getenv("DATABASE_URL")
            if not database_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                database_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            db_manager.initialize(database_url=database_url)
            db_manager._initialized = True
            logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        return db_manager
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@shared_task(bind=True, name="complete_data_pipeline")
def complete_data_pipeline(self) -> dict[str, Any]:
    """å®Œæ•´çš„æ•°æ®ç®¡é“ä»»åŠ¡ - å‡çº§è‡³FotMobæ•°æ®æº.

    æŒ‰é¡ºåºæ‰§è¡Œï¼šFotMobæ•°æ®é‡‡é›† -> æ‰¹é‡æ•°æ®æ¸…æ´— -> ç‰¹å¾å·¥ç¨‹ -> æ•°æ®å­˜å‚¨

    Returns:
        dict[str, Any]: ç®¡é“æ‰§è¡Œç»“æœ
    """
    try:
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“ (FotMobæ•°æ®æº)")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # å®šä¹‰ä»»åŠ¡é“¾ï¼šFotMobé‡‡é›† -> æ‰¹é‡æ¸…æ´— -> ç‰¹å¾ -> å­˜å‚¨
        from .data_collection_tasks import collect_fotmob_data

        pipeline = chain(
            collect_fotmob_data.s(),  # ğŸ†• ä½¿ç”¨FotMobæ•°æ®æº
            data_cleaning_task.s(),  # ğŸ†• æ‰¹é‡æ•°æ®æ¸…æ´—
            feature_engineering_task.s(),
            data_storage_task.s(),
        )

        # æ‰§è¡Œç®¡é“
        result = pipeline.apply_async()

        pipeline_result = {
            "status": "success",
            "pipeline_completed": True,
            "completion_timestamp": datetime.utcnow().isoformat(),
            "task_id": result.id,
            "message": "ğŸš€ æ•°æ®ç®¡é“ä»»åŠ¡é“¾å·²å¯åŠ¨ (FotMob + æ‰¹é‡å¤„ç†)",
            "data_source": "fotmob",
            "performance_mode": "batch_processing_enabled",
        }

        logger.info(f"ğŸ‰ å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆ: {pipeline_result}")
        return pipeline_result

    except Exception as e:
        logger.error(f"âŒ å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "pipeline_completed": False,
            "completion_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="trigger_feature_calculation_for_new_matches")
def trigger_feature_calculation_for_new_matches(self, match_ids: list) -> dict:
    """ä¸ºæ–°é‡‡é›†çš„æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—.

    Args:
        match_ids: éœ€è¦è®¡ç®—ç‰¹å¾çš„æ¯”èµ›IDåˆ—è¡¨

    Returns:
        dict[str, Any]: ç‰¹å¾è®¡ç®—è§¦å‘ç»“æœ
    """
    try:
        logger.info(f"ä¸º {len(match_ids)} åœºæ–°æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—")

        from src.services.feature_service import FeatureService
        from src.database.connection import DatabaseManager

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_manager = DatabaseManager()

        calculated_count = 0
        failed_count = 0

        # ä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        async def calculate_features_for_match(match_id: int) -> bool:
            """ä¸ºå•åœºæ¯”èµ›è®¡ç®—ç‰¹å¾çš„å¼‚æ­¥å‡½æ•°"""
            try:
                async with db_manager.get_async_session() as session:
                    feature_service = FeatureService(session)

                    # è®¡ç®—ç‰¹å¾
                    features = await feature_service.get_match_features(match_id)

                    if features:
                        logger.debug(f"æˆåŠŸè®¡ç®—æ¯”èµ› {match_id} çš„ç‰¹å¾")
                        return True
                    else:
                        logger.warning(f"æ¯”èµ› {match_id} ç‰¹å¾è®¡ç®—å¤±è´¥")
                        return False

            except Exception as e:
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")
                return False

        # ä½¿ç”¨asyncio.runä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        for match_id in match_ids:
            try:
                success = asyncio.run(calculate_features_for_match(match_id))
                if success:
                    calculated_count += 1
                else:
                    failed_count += 1

            except Exception as e:
                failed_count += 1
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")

        result = {
            "status": "success",
            "total_matches": len(match_ids),
            "calculated_features": calculated_count,
            "failed_calculations": failed_count,
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }

        logger.info(f"ç‰¹å¾è®¡ç®—è§¦å‘å®Œæˆ: {result}")
        return result

    except Exception as e:
        logger.error(f"è§¦å‘ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }


# å›è°ƒå‡½æ•°ï¼šæ•°æ®é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ç‰¹å¾è®¡ç®—
def on_collection_success(task_result, task_id, args, kwargs):
    """æ•°æ®é‡‡é›†æˆåŠŸåçš„å›è°ƒå‡½æ•°."""
    try:
        logger.info(f"æ•°æ®é‡‡é›†ä»»åŠ¡ {task_id} æˆåŠŸå®Œæˆï¼Œè§¦å‘ç‰¹å¾è®¡ç®—")

        # ä»é‡‡é›†ç»“æœä¸­æå–æ–°æ¯”èµ›çš„match_ids
        collected_match_ids = task_result.get("new_match_ids", [])

        if collected_match_ids:
            # å¼‚æ­¥è§¦å‘ç‰¹å¾è®¡ç®—ä»»åŠ¡
            trigger_feature_calculation_for_new_matches.delay(collected_match_ids)

    except Exception as e:
        logger.error(f"é‡‡é›†æˆåŠŸå›è°ƒå¤„ç†å¤±è´¥: {e}")


# ä¸ºæ•°æ®é‡‡é›†ä»»åŠ¡æ·»åŠ æˆåŠŸå›è°ƒ
# TODO: ä¿®å¤å›è°ƒç»‘å®šé—®é¢˜ - æš‚æ—¶æ³¨é‡Šæ‰ä»¥è®©ç³»ç»Ÿæ­£å¸¸å¯åŠ¨
# collect_daily_fixtures.link_success(on_collection_success)
# collect_live_scores.link_success(on_collection_success)
# collect_odds_data.link_success(on_collection_success)
