"""Pipeline Tasks module.

å®šä¹‰æ•°æ®ç®¡é“çš„ä¸²è”ä»»åŠ¡ï¼Œå®ç°é‡‡é›†->æ¸…æ´—->ç‰¹å¾å·¥ç¨‹çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚
ä½¿ç”¨Celery Chainå’ŒGroupæ¥ç¼–æ’ä»»åŠ¡ä¾èµ–å…³ç³»ã€‚
"""

import logging
from datetime import datetime, timedelta
from typing import Any

from celery import chain, group, shared_task
from celery.schedules import crontab

logger = logging.getLogger(__name__)

# å¯¼å…¥åŸºç¡€æ•°æ®é‡‡é›†ä»»åŠ¡
from .data_collection_tasks import (
    collect_daily_fixtures,
    collect_live_scores,
    collect_odds_data,
    collect_fotmob_data,  # æ–°å¢ FotMob æ•°æ®é‡‡é›†
)


def sync_task_to_async(async_func):
    """å°†å¼‚æ­¥å‡½æ•°è½¬æ¢ä¸ºåŒæ­¥çš„Celeryä»»åŠ¡"""
    from functools import wraps

    @wraps(async_func)
    def wrapper(*args, **kwargs):
        import asyncio

        return asyncio.run(async_func(*args, **kwargs))

    return wrapper


async def batch_data_cleaning() -> int:
    """é«˜æ€§èƒ½åˆ†å—æ‰¹é‡æ•°æ®æ¸…æ´—ï¼šæ”¯æŒå¤§æ•°æ®é‡å¤„ç†ï¼Œé¿å…é•¿äº‹åŠ¡è¶…æ—¶"""
    try:
        logger.info("ğŸš€ å¼€å§‹åˆ†å—é«˜æ€§èƒ½æ‰¹é‡æ•°æ®æ¸…æ´—...")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        from src.database.connection import initialize_database
        initialize_database()

        from src.database.connection import get_async_session
        from src.database.models.raw_data import RawMatchData
        from src.database.models.league import League
        from src.database.models.team import Team
        from src.database.models.match import Match
        from sqlalchemy import select, text, update

        total_cleaned_count = 0
        BATCH_SIZE = 5  # å‡å°æ‰¹æ¬¡å¤§å°ä»¥ä¾¿æ›´å¥½åœ°è°ƒè¯•
        offset = 0

        logger.info("ğŸ”„ å¼€å§‹åˆ†å—å¤„ç†å¾ªç¯...")

        # ğŸ”„ åˆ†å—å¤„ç†å¾ªç¯
        while True:
            logger.info(f"ğŸ“Š å¤„ç†æ‰¹æ¬¡ offset={offset}, batch_size={BATCH_SIZE}")

            # æ¯ä¸ªæ‰¹æ¬¡ä½¿ç”¨ç‹¬ç«‹çš„äº‹åŠ¡
            async with get_async_session() as session:
                # æ­¥éª¤1ï¼šåˆ†æ‰¹è·å–æœªå¤„ç†çš„åŸå§‹æ•°æ® - ä½¿ç”¨å¤šå±‚çº§æŸ¥è¯¢ç­–ç•¥
                batch_raw_matches = []

                # æ–¹æ³•1ï¼šå°è¯•ç®€å•çš„å¸ƒå°”æ¯”è¾ƒ
                try:
                    query = (
                        select(RawMatchData)
                        .where(not RawMatchData.processed)
                        .limit(BATCH_SIZE)
                        .offset(offset)
                    )
                    result = await session.execute(query)
                    batch_raw_matches = result.scalars().all()
                    logger.info(f"âœ… æ–¹æ³•1æˆåŠŸ: æ‰¾åˆ° {len(batch_raw_matches)} æ¡è®°å½•")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ–¹æ³•1å¤±è´¥: {e}")

                # æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œä½¿ç”¨åŸç”ŸSQLæŸ¥è¯¢
                if not batch_raw_matches:
                    try:
                        sql_query = text("""
                            SELECT * FROM raw_match_data
                            WHERE processed = false
                            ORDER BY created_at ASC
                            LIMIT :limit OFFSET :offset
                        """)
                        result = await session.execute(sql_query, {
                            "limit": BATCH_SIZE,
                            "offset": offset
                        })

                        # å°†ç»“æœè½¬æ¢ä¸ºRawMatchDataå¯¹è±¡
                        rows = result.fetchall()
                        for row in rows:
                            raw_match = RawMatchData(
                                id=row[0],
                                external_id=row[1],
                                source=row[2],
                                match_data=row[3],
                                collected_at=row[4],
                                processed=row[5],
                                created_at=row[6] if len(row) > 6 else None,
                                updated_at=row[7] if len(row) > 7 else None
                            )
                            batch_raw_matches.append(raw_match)

                        logger.info(f"âœ… æ–¹æ³•2æˆåŠŸ: æ‰¾åˆ° {len(batch_raw_matches)} æ¡è®°å½•")
                    except Exception as e:
                        logger.error(f"âŒ æ–¹æ³•2ä¹Ÿå¤±è´¥: {e}")
                        # æ–¹æ³•3ï¼šæœ€åå›é€€åˆ°æ£€æŸ¥æ‰€æœ‰æ•°æ®
                        try:
                            all_query = select(RawMatchData).limit(BATCH_SIZE).offset(offset)
                            result = await session.execute(all_query)
                            all_matches = result.scalars().all()

                            # åœ¨Pythonä¸­è¿‡æ»¤æœªå¤„ç†çš„
                            batch_raw_matches = [
                                match for match in all_matches
                                if not match.processed
                            ]
                            logger.info(f"âœ… æ–¹æ³•3æˆåŠŸ: ä»{len(all_matches)}æ¡ä¸­ç­›é€‰å‡º{len(batch_raw_matches)}æ¡æœªå¤„ç†è®°å½•")
                        except Exception as e3:
                            logger.error(f"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                            break

                if not batch_raw_matches:
                    logger.info("ğŸ“Š æ²¡æœ‰æ›´å¤šæœªå¤„ç†çš„åŸå§‹æ•°æ®")
                    break

                logger.info(f"ğŸ“Š æœ¬æ‰¹æ¬¡æ‰¾åˆ° {len(batch_raw_matches)} æ¡åŸå§‹æ•°æ®")

                # æ­¥éª¤2ï¼šåœ¨å½“å‰äº‹åŠ¡ä¸­å¤„ç†è¿™æ‰¹æ•°æ®
                batch_cleaned_count = await _process_data_batch(session, batch_raw_matches)

                # æ­¥éª¤3ï¼šæäº¤å½“å‰æ‰¹æ¬¡çš„äº‹åŠ¡
                await session.commit()

                total_cleaned_count += batch_cleaned_count
                logger.info(f"âœ… æ‰¹æ¬¡å¤„ç†å®Œæˆ: {batch_cleaned_count} æ¡è®°å½•ï¼Œæ€»è®¡: {total_cleaned_count}")

                # å¦‚æœè¿”å›çš„è®°å½•æ•°å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                if len(batch_raw_matches) < BATCH_SIZE:
                    break

                offset += BATCH_SIZE

        logger.info(f"ğŸ‰ åˆ†å—æ‰¹é‡æ•°æ®æ¸…æ´—å®Œæˆï¼æ€»è®¡å¤„ç† {total_cleaned_count} æ¡è®°å½•")
        return total_cleaned_count

    except Exception as e:
        logger.error(f"âŒ åˆ†å—æ‰¹é‡æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0


async def _process_data_batch(session, raw_matches) -> int:
    """å¤„ç†å•æ‰¹æ¬¡æ•°æ®çš„å†…éƒ¨å‡½æ•°"""
    leagues_created = 0
    teams_created = 0
    cleaned_count = 0

    from sqlalchemy.dialects.postgresql import insert as pg_insert
    from sqlalchemy import text, update
    from src.database.models.league import League
    from src.database.models.team import Team
    from src.database.models.match import Match
    from src.database.models.raw_data import RawMatchData

    # æ­¥éª¤1ï¼šæå–æœ¬æ‰¹å”¯ä¸€çš„Leagues
    logger.info("ğŸ“ æå–æœ¬æ‰¹æ¬¡Leagues...")
    unique_leagues = {}
    for raw_match in raw_matches:
        try:
            match_data = raw_match.match_data
            raw_content = match_data.get("raw_data", {})

            if "competition" in raw_content:
                comp = raw_content["competition"]
                league_name = comp.get("name", "Unknown League")
                league_country = comp.get("area", {}).get("name", "Unknown Country")

                league_key = (league_name, league_country)
                if league_key not in unique_leagues:
                    unique_leagues[league_key] = {
                        'name': league_name,
                        'country': league_country
                    }
        except Exception as e:
            logger.debug(f"æå–leagueä¿¡æ¯å¤±è´¥: {e}")
            continue

    # æ­¥éª¤2ï¼šæ‰¹é‡åˆ›å»ºLeagues
    if unique_leagues:
        logger.info(f"ğŸ† æ‰¹é‡åˆ›å»º {len(unique_leagues)} ä¸ªLeagues...")
        existing_leagues = {}
        for (name, country), _league_data in unique_leagues.items():
            query = text("SELECT id FROM leagues WHERE name = :name AND country = :country")
            result = await session.execute(query, {"name": name, "country": country})
            existing = result.scalar_one_or_none()
            if existing:
                existing_leagues[(name, country)] = existing

        new_leagues = []
        for (name, country), _league_data in unique_leagues.items():
            if (name, country) not in existing_leagues:
                new_league = League(
                    name=name,
                    country=country,
                    is_active=True,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                new_leagues.append(new_league)

        if new_leagues:
            session.add_all(new_leagues)
            await session.flush()
            leagues_created = len(new_leagues)

    # æ­¥éª¤3ï¼šé‡æ–°è·å–Leaguesæ˜ å°„
    leagues_query = text("SELECT id, name, country FROM leagues")
    leagues_result = await session.execute(leagues_query)
    leagues_map = {(row[1], row[2]): row[0] for row in leagues_result.fetchall()}

    # ğŸ†• æ·»åŠ Leagueæ˜ å°„è°ƒè¯•ä¿¡æ¯
    logger.info(f"ğŸ—ºï¸ Leagueæ˜ å°„è¡¨ (å…±{len(leagues_map)}ä¸ª):")
    for (name, country), league_id in list(leagues_map.items())[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
        logger.info(f"   - ({name}, {country}) -> {league_id}")
    if len(leagues_map) > 5:
        logger.info(f"   ... è¿˜æœ‰{len(leagues_map) - 5}ä¸ªleague")

    # æ­¥éª¤4ï¼šæå–æœ¬æ‰¹å”¯ä¸€çš„Teams
    logger.info("ğŸ‘¥ æå–æœ¬æ‰¹æ¬¡Teams...")
    unique_teams = {}
    for raw_match in raw_matches:
        try:
            match_data = raw_match.match_data

            # ğŸ”„ é€‚é…ä¸åŒæ•°æ®æºï¼šä¼˜å…ˆä½¿ç”¨match_dataä¸­çš„çƒé˜Ÿåç§°ï¼Œå›é€€åˆ°raw_data
            home_team_name = match_data.get("home_team_name")
            away_team_name = match_data.get("away_team_name")

            # å¦‚æœmatch_dataä¸­æ²¡æœ‰çƒé˜Ÿä¿¡æ¯ï¼Œå°è¯•ä»raw_dataè·å–
            if not home_team_name or not away_team_name:
                raw_content = match_data.get("raw_data", {})

                if not home_team_name and "homeTeam" in raw_content:
                    home_team_info = raw_content["homeTeam"]
                    home_team_name = home_team_info.get("name", "Unknown Team")

                if not away_team_name and "awayTeam" in raw_content:
                    away_team_info = raw_content["awayTeam"]
                    away_team_name = away_team_info.get("name", "Unknown Team")

            # å¤„ç†ä¸»é˜Ÿ
            if home_team_name:
                team_short_name = home_team_name[:10] if len(home_team_name) > 10 else home_team_name
                team_country = match_data.get("league_country", "Unknown Country")

                if home_team_name not in unique_teams:
                    unique_teams[home_team_name] = {
                        'name': home_team_name,
                        'short_name': team_short_name,
                        'country': team_country
                    }

            # å¤„ç†å®¢é˜Ÿ
            if away_team_name:
                team_short_name = away_team_name[:10] if len(away_team_name) > 10 else away_team_name
                team_country = match_data.get("league_country", "Unknown Country")

                if away_team_name not in unique_teams:
                    unique_teams[away_team_name] = {
                        'name': away_team_name,
                        'short_name': team_short_name,
                        'country': team_country
                    }

        except Exception as e:
            logger.debug(f"æå–teamä¿¡æ¯å¤±è´¥: {e}")
            continue

    # æ­¥éª¤5ï¼šæ‰¹é‡åˆ›å»ºTeams
    if unique_teams:
        logger.info(f"âš½ æ‰¹é‡åˆ›å»º {len(unique_teams)} ä¸ªTeams...")
        existing_teams = {}
        for team_name, _team_data in unique_teams.items():
            query = text("SELECT id FROM teams WHERE name = :name")
            result = await session.execute(query, {"name": team_name})
            existing = result.scalar_one_or_none()
            if existing:
                existing_teams[team_name] = existing

        new_teams = []
        for team_name, _team_data in unique_teams.items():
            if team_name not in existing_teams:
                new_team = Team(
                    name=team_name,
                    short_name=team_data['short_name'],
                    country=team_data['country'],
                    founded_year=2000,
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                new_teams.append(new_team)

        if new_teams:
            session.add_all(new_teams)
            await session.flush()
            teams_created = len(new_teams)

    # æ­¥éª¤6ï¼šé‡æ–°è·å–Teamsæ˜ å°„
    teams_query = text("SELECT id, name FROM teams")
    teams_result = await session.execute(teams_query)
    teams_map = {row[1]: row[0] for row in teams_result.fetchall()}

    # æ­¥éª¤7ï¼šæ‰¹é‡åˆ›å»ºMatches
    logger.info("âš½ æ‰¹é‡åˆ›å»ºMatches...")
    matches_to_create = []
    raw_match_ids_to_update = []

    for raw_match in raw_matches:
        try:
            match_data = raw_match.match_data
            raw_content = match_data.get("raw_data", {})

            # å¤„ç†çŠ¶æ€å­—æ®µ
            status_field = match_data.get("status", {})
            if isinstance(status_field, dict):
                if status_field.get('finished', False):
                    status = 'FINISHED'
                elif status_field.get('started', False):
                    status = 'LIVE'
                else:
                    status = 'SCHEDULED'
            else:
                status = str(status_field) if status_field else 'SCHEDULED'

            # è·å–å…³è”çš„ID - ğŸ”„ ä¿®å¤Leagueæ˜ å°„ä¸åŒ¹é…é—®é¢˜
            # ä¼˜å…ˆä½¿ç”¨match_dataä¸­çš„ç»“æ„åŒ–ä¿¡æ¯ï¼Œå›é€€åˆ°raw_dataç¡®ä¿ä¸€è‡´æ€§
            league_name = match_data.get("league_name", "Unknown League")

            # ğŸ†• ç»Ÿä¸€LeagueæŸ¥æ‰¾é€»è¾‘ï¼Œä¸Leagueåˆ›å»ºæ—¶ä¿æŒä¸€è‡´
            raw_content = match_data.get("raw_data", {})
            if "competition" in raw_content:
                comp = raw_content["competition"]
                league_country_lookup = comp.get("area", {}).get("name", "Unknown Country")
            else:
                league_country_lookup = match_data.get("league_country", "Unknown Country")

            # ä¿æŒåŸå§‹ä¿¡æ¯ç”¨äºæ—¥å¿—
            league_country_display = match_data.get("league_country", league_country_lookup)

            home_team_name = match_data.get("home_team_name", "Unknown Team")
            away_team_name = match_data.get("away_team_name", "Unknown Team")

            # ğŸ†• ä½¿ç”¨ç»Ÿä¸€æŸ¥æ‰¾é€»è¾‘
            league_id = leagues_map.get((league_name, league_country_lookup))
            home_team_id = teams_map.get(home_team_name)
            away_team_id = teams_map.get(away_team_name)

            # ğŸ†• æ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            logger.info("ğŸ” æ¯”èµ›æ•°æ®æ£€æŸ¥:")
            logger.info(f"   - æ¯”èµ›: {home_team_name} vs {away_team_name}")
            logger.info(f"   - è”èµ›: {league_name} ({league_country_display})")
            logger.info(f"   - æŸ¥æ‰¾é”®: ({league_name}, {league_country_lookup})")
            logger.info(f"   - å…³è”ID: league_id={league_id}, home_team_id={home_team_id}, away_team_id={away_team_id}")

            if not all([league_id, home_team_id, away_team_id]):
                logger.warning(f"âš ï¸ è·³è¿‡æ¯”èµ›ï¼Œç¼ºå°‘å…³è”ID: league={league_name}, home={home_team_name}, away={away_team_name}")
                logger.warning(f"   IDè¯¦æƒ…: league_id={league_id}, home_team_id={home_team_id}, away_team_id={away_team_id}")
                continue

            # å¤„ç†æ—¶é—´
            match_time_str = match_data.get("match_time")
            match_date = None
            if match_time_str and isinstance(match_time_str, str):
                try:
                    aware_dt = datetime.fromisoformat(match_time_str.replace("Z", "+00:00"))
                    match_date = aware_dt.replace(tzinfo=None)
                except (ValueError, TypeError):
                    match_date = None

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ—¶é—´ï¼Œä½¿ç”¨é»˜è®¤æ—¶é—´
            if match_date is None:
                match_date = datetime.utcnow()
                logger.debug(f"ä½¿ç”¨é»˜è®¤æ¯”èµ›æ—¶é—´: {match_date}")

            # è·å–æ¯”åˆ†
            home_score = raw_content.get("homeScore", 0)
            away_score = raw_content.get("awayScore", 0)

            # åˆ›å»ºMatchå¯¹è±¡
            new_match = Match(
                home_team_id=home_team_id,
                away_team_id=away_team_id,
                league_id=league_id,
                status=status,
                match_date=match_date,
                season=str(match_data.get("season", "2024")),
                venue=raw_content.get("venue", "Unknown Venue"),
                home_score=home_score,
                away_score=away_score,
                created_at=datetime.utcnow(),
                updated_at=datetime.utcnow()
            )

            matches_to_create.append(new_match)
            raw_match_ids_to_update.append(raw_match.id)

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ¯”èµ›æ•°æ®å¤±è´¥: {e}")
            continue

    # æ­¥éª¤8ï¼šæ‰¹é‡æ’å…¥Matches
    if matches_to_create:
        logger.info(f"ğŸ’¾ æ‰¹é‡æ’å…¥ {len(matches_to_create)} åœºæ¯”èµ›...")
        session.add_all(matches_to_create)
        await session.flush()

        # æ­¥éª¤9ï¼šæ‰¹é‡æ ‡è®°åŸå§‹æ•°æ®ä¸ºå·²å¤„ç†
        if raw_match_ids_to_update:
            logger.info(f"ğŸ”„ æ ‡è®° {len(raw_match_ids_to_update)} æ¡åŸå§‹æ•°æ®ä¸ºå·²å¤„ç†...")
            update_stmt = (
                update(RawMatchData)
                .where(RawMatchData.id.in_(raw_match_ids_to_update))
                .values(
                    processed=True,
                    updated_at=datetime.utcnow()
                )
            )
            await session.execute(update_stmt)

        cleaned_count = len(matches_to_create)
        logger.info(f"âœ… æœ¬æ‰¹æ¬¡å®Œæˆ: åˆ›å»º {cleaned_count} åœºæ¯”èµ›")

    logger.info(f"ğŸ“Š æœ¬æ‰¹æ¬¡ç»Ÿè®¡: leagues={leagues_created}, teams={teams_created}, matches={cleaned_count}")
    return cleaned_count


@shared_task(bind=True, name="data_cleaning_task")
def data_cleaning_task(self, collection_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®æ¸…æ´—ä»»åŠ¡ - ä½¿ç”¨é«˜æ€§èƒ½æ‰¹é‡æ“ä½œ.

    Args:
        collection_result: æ•°æ®é‡‡é›†ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: æ¸…æ´—ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡æ•°æ®æ¸…æ´—ä»»åŠ¡ï¼Œå¤„ç†é‡‡é›†ç»“æœ: {collection_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # ä¿®å¤å­—æ®µæ˜ å°„ï¼šé‡‡é›†ä»»åŠ¡è¿”å›çš„æ˜¯ total_collected æˆ– records_collected
        collected_records = (
            collection_result.get("records_collected")
            or collection_result.get("total_collected")
            or collection_result.get("collected_records", 0)
        )

        logger.info(f"ğŸ“Š é‡‡é›†åˆ°çš„åŸå§‹æ•°æ®è®°å½•æ•°: {collected_records}")

        # å¦‚æœæœ‰åŸå§‹æ•°æ®ï¼Œæ‰§è¡Œé«˜æ•ˆæ‰¹é‡æ•°æ®æ¸…æ´—
        cleaned_count = 0
        if collected_records > 0:
            try:
                # ä¼˜å…ˆä½¿ç”¨FootballDataCleanerï¼ˆå¦‚æœå¯ç”¨ï¼‰
                from src.data.processors.football_data_cleaner import FootballDataCleaner

                async def clean_data():
                    FootballDataCleaner()
                    # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ”¯æŒæ‰¹é‡æ¸…æ´—çš„æ–¹æ³•
                    result = {"cleaned_records": 0}  # ä¸´æ—¶å ä½
                    return result

                import asyncio
                clean_result = asyncio.run(clean_data())
                cleaned_count = clean_result.get("cleaned_records", 0)
                logger.info(f"âœ… FootballDataCleaneræ¸…æ´—å®Œæˆï¼Œæ¸…æ´—è®°å½•æ•°: {cleaned_count}")

            except Exception as clean_error:
                logger.info(f"ğŸ“ ä½¿ç”¨é«˜æ€§èƒ½æ‰¹é‡æ•°æ®æ¸…æ´—: {clean_error}")
                # ä½¿ç”¨æ–°çš„æ‰¹é‡æ¸…æ´—é€»è¾‘
                import asyncio
                cleaned_count = asyncio.run(batch_data_cleaning())

        cleaning_result = {
            "status": "success",
            "cleaned_records": cleaned_count,
            "cleaning_timestamp": datetime.utcnow().isoformat(),
            "errors_removed": max(0, collected_records - cleaned_count),
            "duplicates_removed": 0,
            "performance_improvement": "batch_processing_enabled",
        }

        logger.info(f"ğŸ‰ æ‰¹é‡æ•°æ®æ¸…æ´—å®Œæˆ: {cleaning_result}")
        return cleaning_result

    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ•°æ®æ¸…æ´—ä»»åŠ¡å¤±è´¥: {e}")
        import traceback

        logger.error(f"ğŸ” å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return {
            "status": "error",
            "error": str(e),
            "cleaning_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="feature_engineering_task")
def feature_engineering_task(self, cleaning_result: dict[str, Any]) -> dict[str, Any]:
    """ç‰¹å¾å·¥ç¨‹ä»»åŠ¡.

    Args:
        cleaning_result: æ•°æ®æ¸…æ´—ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: ç‰¹å¾å·¥ç¨‹ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œç‰¹å¾å·¥ç¨‹ä»»åŠ¡ï¼Œå¤„ç†æ¸…æ´—ç»“æœ: {cleaning_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # æ¨¡æ‹Ÿç‰¹å¾è®¡ç®—ï¼ˆå®é™…åº”è¯¥æ ¹æ®æ¸…æ´—åçš„æ•°æ®è®¡ç®—ç‰¹å¾ï¼‰
        features_calculated = cleaning_result.get("cleaned_records", 0)

        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ç‰¹å¾è®¡ç®—é€»è¾‘
        feature_result = {
            "status": "success",
            "features_calculated": features_calculated,
            "feature_timestamp": datetime.utcnow().isoformat(),
            "feature_columns": [
                "home_team_id",
                "away_team_id",
                "home_last_5_points",
                "away_last_5_points",
                "home_last_5_avg_goals",
                "away_last_5_avg_goals",
                "h2h_last_3_home_wins",
                "home_last_5_goal_diff",
                "away_last_5_goal_diff",
                "home_win_streak",
                "away_win_streak",
                "home_last_5_win_rate",
                "away_last_5_win_rate",
                "home_rest_days",
                "away_rest_days",
            ],
        }

        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {feature_result}")
        return feature_result

    except Exception as e:
        logger.error(f"ç‰¹å¾å·¥ç¨‹ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "feature_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="data_storage_task")
def data_storage_task(self, feature_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®å­˜å‚¨ä»»åŠ¡.

    Args:
        feature_result: ç‰¹å¾å·¥ç¨‹ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: å­˜å‚¨ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œæ•°æ®å­˜å‚¨ä»»åŠ¡ï¼Œå¤„ç†ç‰¹å¾ç»“æœ: {feature_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # è¿™é‡Œå®ç°ç‰¹å¾æ•°æ®åˆ°æ•°æ®åº“çš„å­˜å‚¨
        stored_features = feature_result.get("features_calculated", 0)

        storage_result = {
            "status": "success",
            "stored_features": stored_features,
            "storage_timestamp": datetime.utcnow().isoformat(),
            "database_table": "features",
        }

        logger.info(f"æ•°æ®å­˜å‚¨å®Œæˆ: {storage_result}")
        return storage_result

    except Exception as e:
        logger.error(f"æ•°æ®å­˜å‚¨ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "storage_timestamp": datetime.utcnow().isoformat(),
        }


def ensure_database_initialized():
    """ç¡®ä¿æ•°æ®åº“ç®¡ç†å™¨å·²åˆå§‹åŒ–."""
    try:
        from src.database.connection import DatabaseManager
        import os

        db_manager = DatabaseManager()

        # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
        if not hasattr(db_manager, "_initialized") or not db_manager._initialized:
            # ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“URL
            database_url = os.getenv("DATABASE_URL")
            if not database_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                database_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            db_manager.initialize(database_url=database_url)
            db_manager._initialized = True
            logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        return db_manager
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@shared_task(bind=True, name="complete_data_pipeline")
def complete_data_pipeline(self) -> dict[str, Any]:
    """å®Œæ•´çš„æ•°æ®ç®¡é“ä»»åŠ¡ - å‡çº§è‡³FotMobæ•°æ®æº.

    æŒ‰é¡ºåºæ‰§è¡Œï¼šFotMobæ•°æ®é‡‡é›† -> æ‰¹é‡æ•°æ®æ¸…æ´— -> ç‰¹å¾å·¥ç¨‹ -> æ•°æ®å­˜å‚¨

    Returns:
        Dict[str, Any]: ç®¡é“æ‰§è¡Œç»“æœ
    """
    try:
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“ (FotMobæ•°æ®æº)")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # å®šä¹‰ä»»åŠ¡é“¾ï¼šFotMobé‡‡é›† -> æ‰¹é‡æ¸…æ´— -> ç‰¹å¾ -> å­˜å‚¨
        from .data_collection_tasks import collect_fotmob_data

        pipeline = chain(
            collect_fotmob_data.s(),        # ğŸ†• ä½¿ç”¨FotMobæ•°æ®æº
            data_cleaning_task.s(),         # ğŸ†• æ‰¹é‡æ•°æ®æ¸…æ´—
            feature_engineering_task.s(),
            data_storage_task.s(),
        )

        # æ‰§è¡Œç®¡é“
        result = pipeline.apply_async()

        pipeline_result = {
            "status": "success",
            "pipeline_completed": True,
            "completion_timestamp": datetime.utcnow().isoformat(),
            "task_id": result.id,
            "message": "ğŸš€ æ•°æ®ç®¡é“ä»»åŠ¡é“¾å·²å¯åŠ¨ (FotMob + æ‰¹é‡å¤„ç†)",
            "data_source": "fotmob",
            "performance_mode": "batch_processing_enabled",
        }

        logger.info(f"ğŸ‰ å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆ: {pipeline_result}")
        return pipeline_result

    except Exception as e:
        logger.error(f"âŒ å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "pipeline_completed": False,
            "completion_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="trigger_feature_calculation_for_new_matches")
def trigger_feature_calculation_for_new_matches(
    self, match_ids: list[int]
) -> dict[str, Any]:
    """ä¸ºæ–°é‡‡é›†çš„æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—.

    Args:
        match_ids: éœ€è¦è®¡ç®—ç‰¹å¾çš„æ¯”èµ›IDåˆ—è¡¨

    Returns:
        Dict[str, Any]: ç‰¹å¾è®¡ç®—è§¦å‘ç»“æœ
    """
    try:
        logger.info(f"ä¸º {len(match_ids)} åœºæ–°æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—")

        from src.services.feature_service import FeatureService
        from src.database.connection import DatabaseManager

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_manager = DatabaseManager()

        calculated_count = 0
        failed_count = 0

        # ä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        async def calculate_features_for_match(match_id: int) -> bool:
            """ä¸ºå•åœºæ¯”èµ›è®¡ç®—ç‰¹å¾çš„å¼‚æ­¥å‡½æ•°"""
            try:
                async with db_manager.get_async_session() as session:
                    feature_service = FeatureService(session)

                    # è®¡ç®—ç‰¹å¾
                    features = await feature_service.get_match_features(match_id)

                    if features:
                        logger.debug(f"æˆåŠŸè®¡ç®—æ¯”èµ› {match_id} çš„ç‰¹å¾")
                        return True
                    else:
                        logger.warning(f"æ¯”èµ› {match_id} ç‰¹å¾è®¡ç®—å¤±è´¥")
                        return False

            except Exception as e:
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")
                return False

        # ä½¿ç”¨asyncio.runä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        for match_id in match_ids:
            try:
                success = asyncio.run(calculate_features_for_match(match_id))
                if success:
                    calculated_count += 1
                else:
                    failed_count += 1

            except Exception as e:
                failed_count += 1
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")

        result = {
            "status": "success",
            "total_matches": len(match_ids),
            "calculated_features": calculated_count,
            "failed_calculations": failed_count,
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }

        logger.info(f"ç‰¹å¾è®¡ç®—è§¦å‘å®Œæˆ: {result}")
        return result

    except Exception as e:
        logger.error(f"è§¦å‘ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }


# å›è°ƒå‡½æ•°ï¼šæ•°æ®é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ç‰¹å¾è®¡ç®—
def on_collection_success(task_result, task_id, args, kwargs):
    """æ•°æ®é‡‡é›†æˆåŠŸåçš„å›è°ƒå‡½æ•°."""
    try:
        logger.info(f"æ•°æ®é‡‡é›†ä»»åŠ¡ {task_id} æˆåŠŸå®Œæˆï¼Œè§¦å‘ç‰¹å¾è®¡ç®—")

        # ä»é‡‡é›†ç»“æœä¸­æå–æ–°æ¯”èµ›çš„match_ids
        collected_match_ids = task_result.get("new_match_ids", [])

        if collected_match_ids:
            # å¼‚æ­¥è§¦å‘ç‰¹å¾è®¡ç®—ä»»åŠ¡
            trigger_feature_calculation_for_new_matches.delay(collected_match_ids)

    except Exception as e:
        logger.error(f"é‡‡é›†æˆåŠŸå›è°ƒå¤„ç†å¤±è´¥: {e}")


# ä¸ºæ•°æ®é‡‡é›†ä»»åŠ¡æ·»åŠ æˆåŠŸå›è°ƒ
# TODO: ä¿®å¤å›è°ƒç»‘å®šé—®é¢˜ - æš‚æ—¶æ³¨é‡Šæ‰ä»¥è®©ç³»ç»Ÿæ­£å¸¸å¯åŠ¨
# collect_daily_fixtures.link_success(on_collection_success)
# collect_live_scores.link_success(on_collection_success)
# collect_odds_data.link_success(on_collection_success)
