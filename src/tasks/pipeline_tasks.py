"""Pipeline Tasks module.

å®šä¹‰æ•°æ®ç®¡é“çš„ä¸²è”ä»»åŠ¡ï¼Œå®ç°é‡‡é›†->æ¸…æ´—->ç‰¹å¾å·¥ç¨‹çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚
ä½¿ç”¨Celery Chainå’ŒGroupæ¥ç¼–æ’ä»»åŠ¡ä¾èµ–å…³ç³»ã€‚
"""

import logging
from datetime import datetime, timedelta
from typing import Any

from celery import chain, group, shared_task
from celery.schedules import crontab

logger = logging.getLogger(__name__)

# å¯¼å…¥åŸºç¡€æ•°æ®é‡‡é›†ä»»åŠ¡
from .data_collection_tasks import (
    collect_daily_fixtures,
    collect_live_scores,
    collect_odds_data,
    collect_fotmob_data,  # æ–°å¢ FotMob æ•°æ®é‡‡é›†
)


def sync_task_to_async(async_func):
    """å°†å¼‚æ­¥å‡½æ•°è½¬æ¢ä¸ºåŒæ­¥çš„Celeryä»»åŠ¡"""
    from functools import wraps

    @wraps(async_func)
    def wrapper(*args, **kwargs):
        import asyncio

        return asyncio.run(async_func(*args, **kwargs))

    return wrapper


async def batch_data_cleaning() -> int:
    """æ‰¹é‡æ•°æ®æ¸…æ´—ï¼šä½¿ç”¨é«˜æ•ˆçš„æ‰¹é‡æ“ä½œå¤„ç†leaguesã€teamså’Œmatches"""
    try:
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡æ•°æ®æ¸…æ´—...")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        from src.database.connection import get_async_session
        from src.database.models.raw_data import RawMatchData
        from src.database.models.league import League
        from src.database.models.team import Team
        from src.database.models.match import Match
        from sqlalchemy import select, text, insert, update
        from sqlalchemy.dialects.postgresql import insert as pg_insert
        import pandas as pd

        cleaned_count = 0

        async with get_async_session() as session:
            # è·å–æ‰€æœ‰æœªå¤„ç†çš„åŸå§‹æ•°æ®
            query = select(RawMatchData).where(RawMatchData.processed.is_(False))
            result = await session.execute(query)
            raw_matches = result.scalars().all()

            if not raw_matches:
                logger.info("ğŸ“Š æ²¡æœ‰æœªå¤„ç†çš„åŸå§‹æ•°æ®")
                return 0

            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œæ‰¹é‡å¤„ç†
            raw_data_list = []
            for raw_match in raw_matches:
                raw_data_list.append({
                    'id': raw_match.id,
                    'external_id': raw_match.external_id,
                    'match_data': dict(raw_match.match_data),
                    'source': raw_match.source
                })

            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(raw_data_list)} æ¡æœªå¤„ç†çš„åŸå§‹æ¯”èµ›æ•°æ®")

            # æ­¥éª¤1ï¼šæ‰¹é‡æå–å’Œåˆ›å»ºleagues
            logger.info("ğŸ“ æ­¥éª¤1ï¼šæ‰¹é‡åˆ›å»ºleaguesè®°å½•...")
            leagues_data = []
            league_external_id_map = {}  # external_id -> league_name + country

            for raw_match_data in raw_data_list:
                try:
                    raw_data = raw_match_data['match_data'].get("raw_data", {})
                    if "competition" in raw_data:
                        comp = raw_data["competition"]
                        external_id = str(comp.get("id"))
                        league_name = comp.get("name", "Unknown League")
                        country = comp.get("area", {}).get("name", "Unknown Country")

                        if external_id not in league_external_id_map:
                            league_external_id_map[external_id] = {
                                'name': league_name,
                                'country': country
                            }
                            leagues_data.append({
                                'external_id': external_id,
                                'name': league_name,
                                'country': country,
                                'is_active': True
                            })
                except Exception as e:
                    logger.debug(f"æå–leagueä¿¡æ¯å¤±è´¥: {e}")
                    continue

            # æ‰¹é‡æ’å…¥leagues (ä½¿ç”¨ON CONFLICTé¿å…é‡å¤)
            league_count = 0
            if leagues_data:
                try:
                    # æŸ¥è¯¢å·²å­˜åœ¨çš„leagues
                    existing_leagues_query = text("""
                        SELECT external_id, id FROM leagues
                        WHERE external_id = ANY(:external_ids)
                    """)
                    result = await session.execute(
                        existing_leagues_query,
                        {"external_ids": [league['external_id'] for league in leagues_data]}
                    )
                    existing_leagues = {row[0]: row[1] for row in result.fetchall()}

                    # åªæ’å…¥ä¸å­˜åœ¨çš„leagues
                    new_leagues = [
                        league for league in leagues_data
                        if league['external_id'] not in existing_leagues
                    ]

                    if new_leagues:
                        # ä½¿ç”¨æ‰¹é‡æ’å…¥
                        leagues_df = pd.DataFrame(new_leagues)
                        leagues_df['created_at'] = datetime.utcnow()
                        leagues_df['updated_at'] = datetime.utcnow()

                        # ç§»é™¤external_idå­—æ®µï¼ˆè¡¨ä¸­å¯èƒ½æ²¡æœ‰ï¼‰
                        if 'external_id' in leagues_df.columns:
                            leagues_df = leagues_df.drop(columns=['external_id'])

                        # æ‰¹é‡æ’å…¥
                        await session.execute(
                            pg_insert(League).returning(League.id),
                            leagues_df.to_dict('records')
                        )
                        await session.flush()
                        league_count = len(new_leagues)

                    logger.info(f"âœ… æ‰¹é‡åˆ›å»ºleagueså®Œæˆï¼Œæ–°å¢ {league_count} ä¸ªè”èµ›")

                except Exception as e:
                    logger.error(f"æ‰¹é‡åˆ›å»ºleagueså¤±è´¥: {e}")

            # æ­¥éª¤2ï¼šæ‰¹é‡æå–å’Œåˆ›å»ºteams
            logger.info("ğŸ‘¥ æ­¥éª¤2ï¼šæ‰¹é‡åˆ›å»ºteamsè®°å½•...")
            teams_data = []
            team_external_id_map = {}  # external_id -> team info

            for raw_match_data in raw_data_list:
                try:
                    raw_data = raw_match_data['match_data'].get("raw_data", {})

                    # å¤„ç†ä¸»é˜Ÿå’Œå®¢é˜Ÿ
                    for team_type in ["homeTeam", "awayTeam"]:
                        if team_type in raw_data:
                            team_info = raw_data[team_type]
                            external_id = str(team_info.get("id"))
                            team_name = team_info.get("name", "Unknown Team")
                            short_name = team_info.get("shortName")
                            country = raw_data.get("area", {}).get("name", "Unknown Country")

                            if external_id not in team_external_id_map:
                                team_external_id_map[external_id] = {
                                    'name': team_name,
                                    'short_name': short_name,
                                    'country': country
                                }
                                teams_data.append({
                                    'external_id': external_id,
                                    'name': team_name,
                                    'short_name': short_name,
                                    'country': country,
                                    'founded_year': 1870  # é»˜è®¤å€¼
                                })
                except Exception as e:
                    logger.debug(f"æå–teamä¿¡æ¯å¤±è´¥: {e}")
                    continue

            # æ‰¹é‡æ’å…¥teams
            team_count = 0
            if teams_data:
                try:
                    # æŸ¥è¯¢å·²å­˜åœ¨çš„teams
                    existing_teams_query = text("""
                        SELECT external_id, id FROM teams
                        WHERE external_id = ANY(:external_ids)
                    """)
                    result = await session.execute(
                        existing_teams_query,
                        {"external_ids": [team['external_id'] for team in teams_data]}
                    )
                    existing_teams = {row[0]: row[1] for row in result.fetchall()}

                    # åªæ’å…¥ä¸å­˜åœ¨çš„teams
                    new_teams = [
                        team for team in teams_data
                        if team['external_id'] not in existing_teams
                    ]

                    if new_teams:
                        # ä½¿ç”¨æ‰¹é‡æ’å…¥
                        teams_df = pd.DataFrame(new_teams)
                        teams_df['created_at'] = datetime.utcnow()
                        teams_df['updated_at'] = datetime.utcnow()

                        # ç§»é™¤external_idå­—æ®µï¼ˆå¦‚æœè¡¨ä¸­æ²¡æœ‰ï¼‰
                        if 'external_id' in teams_df.columns:
                            teams_df = teams_df.drop(columns=['external_id'])

                        await session.execute(
                            pg_insert(Team).returning(Team.id),
                            teams_df.to_dict('records')
                        )
                        await session.flush()
                        team_count = len(new_teams)

                    logger.info(f"âœ… æ‰¹é‡åˆ›å»ºteamså®Œæˆï¼Œæ–°å¢ {team_count} ä¸ªçƒé˜Ÿ")

                except Exception as e:
                    logger.error(f"æ‰¹é‡åˆ›å»ºteamså¤±è´¥: {e}")

            # æ­¥éª¤3ï¼šæ‰¹é‡åˆ›å»ºmatchesè®°å½•
            logger.info("âš½ æ­¥éª¤3ï¼šæ‰¹é‡åˆ›å»ºmatchesè®°å½•...")

            # é‡æ–°è·å–æ‰€æœ‰leagueså’Œteamsçš„IDæ˜ å°„
            leagues_query = text("SELECT id, name, country FROM leagues")
            teams_query = text("SELECT id, name FROM teams")

            leagues_result = await session.execute(leagues_query)
            teams_result = await session.execute(teams_query)

            leagues_map = {(row[1], row[2]): row[0] for row in leagues_result.fetchall()}  # (name, country) -> id
            teams_map = {row[1]: row[0] for row in teams_result.fetchall()}  # name -> id

            matches_data = []
            raw_match_ids = []

            for raw_match_data in raw_data_list:
                try:
                    match_data = raw_match_data['match_data']
                    raw_match_data_content = raw_match_data['match_data'].get("raw_data", {})

                    # è·å–å…³è”çš„ID
                    league_name = match_data.get("league_name", "Unknown League")
                    league_country = match_data.get("league_country", "Unknown Country")
                    home_team_name = match_data.get("home_team_name", "Unknown Team")
                    away_team_name = match_data.get("away_team_name", "Unknown Team")

                    league_id = leagues_map.get((league_name, league_country))
                    home_team_id = teams_map.get(home_team_name)
                    away_team_id = teams_map.get(away_team_name)

                    if not all([league_id, home_team_id, away_team_id]):
                        logger.warning(f"è·³è¿‡æ¯”èµ›ï¼Œç¼ºå°‘å…³è”ID: league={league_name}, home={home_team_name}, away={away_team_name}")
                        continue

                    # å¤„ç†æ—¶é—´
                    match_time_str = match_data.get("match_time")
                    match_date = None
                    if match_time_str and isinstance(match_time_str, str):
                        try:
                            aware_dt = datetime.fromisoformat(match_time_str.replace("Z", "+00:00"))
                            match_date = aware_dt.replace(tzinfo=None)
                        except (ValueError, TypeError):
                            match_date = None

                    # å‡†å¤‡matchæ•°æ®
                    match_record = {
                        'home_team_id': home_team_id,
                        'away_team_id': away_team_id,
                        'league_id': league_id,
                        'status': match_data.get("status", "scheduled"),
                        'match_date': match_date,
                        'season': str(match_data.get("season", "")),
                        'venue': raw_match_data_content.get("area", {}).get("name"),
                        'home_score': raw_match_data_content.get("score", {}).get("fullTime", {}).get("home", 0),
                        'away_score': raw_match_data_content.get("score", {}).get("fullTime", {}).get("away", 0),
                        'created_at': datetime.utcnow(),
                        'updated_at': datetime.utcnow()
                    }

                    matches_data.append(match_record)
                    raw_match_ids.append(raw_match_data['id'])

                except Exception as e:
                    logger.error(f"å¤„ç†æ¯”èµ›æ•°æ®å¤±è´¥: {e}")
                    continue

            # æ‰¹é‡æ’å…¥matches
            if matches_data:
                try:
                    matches_df = pd.DataFrame(matches_data)

                    # ä½¿ç”¨DataFrameçš„to_sqlè¿›è¡Œæ‰¹é‡æ’å…¥
                    from sqlalchemy import create_engine
                    import os

                    # è·å–æ•°æ®åº“URL
                    db_url = os.getenv("DATABASE_URL")
                    if db_url and "+asyncpg" in db_url:
                        db_url = db_url.replace("+asyncpg", "")

                    engine = create_engine(db_url)

                    # æ‰¹é‡æ’å…¥matches
                    matches_df.to_sql("matches", engine, if_exists="append", index=False, method="multi")

                    cleaned_count = len(matches_data)
                    logger.info(f"âœ… æ‰¹é‡åˆ›å»ºmatcheså®Œæˆï¼Œæ–°å¢ {cleaned_count} åœºæ¯”èµ›")

                except Exception as e:
                    logger.error(f"æ‰¹é‡åˆ›å»ºmatcheså¤±è´¥: {e}")

            # æ­¥éª¤4ï¼šæ‰¹é‡æ ‡è®°åŸå§‹æ•°æ®ä¸ºå·²å¤„ç†
            if raw_match_ids:
                try:
                    update_stmt = (
                        update(RawMatchData)
                        .where(RawMatchData.id.in_(raw_match_ids))
                        .values(processed=True, updated_at=datetime.utcnow())
                    )
                    await session.execute(update_stmt)
                    await session.commit()

                except Exception as e:
                    logger.error(f"æ ‡è®°åŸå§‹æ•°æ®å¤±è´¥: {e}")

        logger.info("ğŸ‰ æ‰¹é‡æ•°æ®æ¸…æ´—å®Œæˆï¼")
        logger.info(f"   - æ–°å¢leagues: {league_count}")
        logger.info(f"   - æ–°å¢teams: {team_count}")
        logger.info(f"   - æ–°å¢matches: {cleaned_count}")

        return cleaned_count

    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0


@shared_task(bind=True, name="data_cleaning_task")
def data_cleaning_task(self, collection_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®æ¸…æ´—ä»»åŠ¡ - ä½¿ç”¨é«˜æ€§èƒ½æ‰¹é‡æ“ä½œ.

    Args:
        collection_result: æ•°æ®é‡‡é›†ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: æ¸…æ´—ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡Œæ‰¹é‡æ•°æ®æ¸…æ´—ä»»åŠ¡ï¼Œå¤„ç†é‡‡é›†ç»“æœ: {collection_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # ä¿®å¤å­—æ®µæ˜ å°„ï¼šé‡‡é›†ä»»åŠ¡è¿”å›çš„æ˜¯ total_collected æˆ– records_collected
        collected_records = (
            collection_result.get("records_collected")
            or collection_result.get("total_collected")
            or collection_result.get("collected_records", 0)
        )

        logger.info(f"ğŸ“Š é‡‡é›†åˆ°çš„åŸå§‹æ•°æ®è®°å½•æ•°: {collected_records}")

        # å¦‚æœæœ‰åŸå§‹æ•°æ®ï¼Œæ‰§è¡Œé«˜æ•ˆæ‰¹é‡æ•°æ®æ¸…æ´—
        cleaned_count = 0
        if collected_records > 0:
            try:
                # ä¼˜å…ˆä½¿ç”¨FootballDataCleanerï¼ˆå¦‚æœå¯ç”¨ï¼‰
                from src.data.processors.football_data_cleaner import FootballDataCleaner

                async def clean_data():
                    cleaner = FootballDataCleaner()
                    # è¿™é‡Œå¯ä»¥æ‰©å±•ä¸ºæ”¯æŒæ‰¹é‡æ¸…æ´—çš„æ–¹æ³•
                    result = {"cleaned_records": 0}  # ä¸´æ—¶å ä½
                    return result

                import asyncio
                clean_result = asyncio.run(clean_data())
                cleaned_count = clean_result.get("cleaned_records", 0)
                logger.info(f"âœ… FootballDataCleaneræ¸…æ´—å®Œæˆï¼Œæ¸…æ´—è®°å½•æ•°: {cleaned_count}")

            except Exception as clean_error:
                logger.info(f"ğŸ“ ä½¿ç”¨é«˜æ€§èƒ½æ‰¹é‡æ•°æ®æ¸…æ´—: {clean_error}")
                # ä½¿ç”¨æ–°çš„æ‰¹é‡æ¸…æ´—é€»è¾‘
                import asyncio
                cleaned_count = asyncio.run(batch_data_cleaning())

        cleaning_result = {
            "status": "success",
            "cleaned_records": cleaned_count,
            "cleaning_timestamp": datetime.utcnow().isoformat(),
            "errors_removed": max(0, collected_records - cleaned_count),
            "duplicates_removed": 0,
            "performance_improvement": "batch_processing_enabled",
        }

        logger.info(f"ğŸ‰ æ‰¹é‡æ•°æ®æ¸…æ´—å®Œæˆ: {cleaning_result}")
        return cleaning_result

    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡æ•°æ®æ¸…æ´—ä»»åŠ¡å¤±è´¥: {e}")
        import traceback

        logger.error(f"ğŸ” å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return {
            "status": "error",
            "error": str(e),
            "cleaning_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="feature_engineering_task")
def feature_engineering_task(self, cleaning_result: dict[str, Any]) -> dict[str, Any]:
    """ç‰¹å¾å·¥ç¨‹ä»»åŠ¡.

    Args:
        cleaning_result: æ•°æ®æ¸…æ´—ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: ç‰¹å¾å·¥ç¨‹ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œç‰¹å¾å·¥ç¨‹ä»»åŠ¡ï¼Œå¤„ç†æ¸…æ´—ç»“æœ: {cleaning_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # æ¨¡æ‹Ÿç‰¹å¾è®¡ç®—ï¼ˆå®é™…åº”è¯¥æ ¹æ®æ¸…æ´—åçš„æ•°æ®è®¡ç®—ç‰¹å¾ï¼‰
        features_calculated = cleaning_result.get("cleaned_records", 0)

        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ç‰¹å¾è®¡ç®—é€»è¾‘
        feature_result = {
            "status": "success",
            "features_calculated": features_calculated,
            "feature_timestamp": datetime.utcnow().isoformat(),
            "feature_columns": [
                "home_team_id",
                "away_team_id",
                "home_last_5_points",
                "away_last_5_points",
                "home_last_5_avg_goals",
                "away_last_5_avg_goals",
                "h2h_last_3_home_wins",
                "home_last_5_goal_diff",
                "away_last_5_goal_diff",
                "home_win_streak",
                "away_win_streak",
                "home_last_5_win_rate",
                "away_last_5_win_rate",
                "home_rest_days",
                "away_rest_days",
            ],
        }

        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {feature_result}")
        return feature_result

    except Exception as e:
        logger.error(f"ç‰¹å¾å·¥ç¨‹ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "feature_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="data_storage_task")
def data_storage_task(self, feature_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®å­˜å‚¨ä»»åŠ¡.

    Args:
        feature_result: ç‰¹å¾å·¥ç¨‹ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: å­˜å‚¨ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œæ•°æ®å­˜å‚¨ä»»åŠ¡ï¼Œå¤„ç†ç‰¹å¾ç»“æœ: {feature_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # è¿™é‡Œå®ç°ç‰¹å¾æ•°æ®åˆ°æ•°æ®åº“çš„å­˜å‚¨
        stored_features = feature_result.get("features_calculated", 0)

        storage_result = {
            "status": "success",
            "stored_features": stored_features,
            "storage_timestamp": datetime.utcnow().isoformat(),
            "database_table": "features",
        }

        logger.info(f"æ•°æ®å­˜å‚¨å®Œæˆ: {storage_result}")
        return storage_result

    except Exception as e:
        logger.error(f"æ•°æ®å­˜å‚¨ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "storage_timestamp": datetime.utcnow().isoformat(),
        }


def ensure_database_initialized():
    """ç¡®ä¿æ•°æ®åº“ç®¡ç†å™¨å·²åˆå§‹åŒ–."""
    try:
        from src.database.connection import DatabaseManager
        import os

        db_manager = DatabaseManager()

        # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
        if not hasattr(db_manager, "_initialized") or not db_manager._initialized:
            # ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“URL
            database_url = os.getenv("DATABASE_URL")
            if not database_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                database_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            db_manager.initialize(database_url=database_url)
            db_manager._initialized = True
            logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        return db_manager
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@shared_task(bind=True, name="complete_data_pipeline")
def complete_data_pipeline(self) -> dict[str, Any]:
    """å®Œæ•´çš„æ•°æ®ç®¡é“ä»»åŠ¡ - å‡çº§è‡³FotMobæ•°æ®æº.

    æŒ‰é¡ºåºæ‰§è¡Œï¼šFotMobæ•°æ®é‡‡é›† -> æ‰¹é‡æ•°æ®æ¸…æ´— -> ç‰¹å¾å·¥ç¨‹ -> æ•°æ®å­˜å‚¨

    Returns:
        Dict[str, Any]: ç®¡é“æ‰§è¡Œç»“æœ
    """
    try:
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“ (FotMobæ•°æ®æº)")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # å®šä¹‰ä»»åŠ¡é“¾ï¼šFotMobé‡‡é›† -> æ‰¹é‡æ¸…æ´— -> ç‰¹å¾ -> å­˜å‚¨
        from .data_collection_tasks import collect_fotmob_data

        pipeline = chain(
            collect_fotmob_data.s(),        # ğŸ†• ä½¿ç”¨FotMobæ•°æ®æº
            data_cleaning_task.s(),         # ğŸ†• æ‰¹é‡æ•°æ®æ¸…æ´—
            feature_engineering_task.s(),
            data_storage_task.s(),
        )

        # æ‰§è¡Œç®¡é“
        result = pipeline.apply_async()

        pipeline_result = {
            "status": "success",
            "pipeline_completed": True,
            "completion_timestamp": datetime.utcnow().isoformat(),
            "task_id": result.id,
            "message": "ğŸš€ æ•°æ®ç®¡é“ä»»åŠ¡é“¾å·²å¯åŠ¨ (FotMob + æ‰¹é‡å¤„ç†)",
            "data_source": "fotmob",
            "performance_mode": "batch_processing_enabled",
        }

        logger.info(f"ğŸ‰ å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆ: {pipeline_result}")
        return pipeline_result

    except Exception as e:
        logger.error(f"âŒ å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "pipeline_completed": False,
            "completion_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="trigger_feature_calculation_for_new_matches")
def trigger_feature_calculation_for_new_matches(
    self, match_ids: list[int]
) -> dict[str, Any]:
    """ä¸ºæ–°é‡‡é›†çš„æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—.

    Args:
        match_ids: éœ€è¦è®¡ç®—ç‰¹å¾çš„æ¯”èµ›IDåˆ—è¡¨

    Returns:
        Dict[str, Any]: ç‰¹å¾è®¡ç®—è§¦å‘ç»“æœ
    """
    try:
        logger.info(f"ä¸º {len(match_ids)} åœºæ–°æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—")

        from src.services.feature_service import FeatureService
        from src.database.connection import DatabaseManager

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_manager = DatabaseManager()

        calculated_count = 0
        failed_count = 0

        # ä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        async def calculate_features_for_match(match_id: int) -> bool:
            """ä¸ºå•åœºæ¯”èµ›è®¡ç®—ç‰¹å¾çš„å¼‚æ­¥å‡½æ•°"""
            try:
                async with db_manager.get_async_session() as session:
                    feature_service = FeatureService(session)

                    # è®¡ç®—ç‰¹å¾
                    features = await feature_service.get_match_features(match_id)

                    if features:
                        logger.debug(f"æˆåŠŸè®¡ç®—æ¯”èµ› {match_id} çš„ç‰¹å¾")
                        return True
                    else:
                        logger.warning(f"æ¯”èµ› {match_id} ç‰¹å¾è®¡ç®—å¤±è´¥")
                        return False

            except Exception as e:
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")
                return False

        # ä½¿ç”¨asyncio.runä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        for match_id in match_ids:
            try:
                success = asyncio.run(calculate_features_for_match(match_id))
                if success:
                    calculated_count += 1
                else:
                    failed_count += 1

            except Exception as e:
                failed_count += 1
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")

        result = {
            "status": "success",
            "total_matches": len(match_ids),
            "calculated_features": calculated_count,
            "failed_calculations": failed_count,
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }

        logger.info(f"ç‰¹å¾è®¡ç®—è§¦å‘å®Œæˆ: {result}")
        return result

    except Exception as e:
        logger.error(f"è§¦å‘ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }


# å›è°ƒå‡½æ•°ï¼šæ•°æ®é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ç‰¹å¾è®¡ç®—
def on_collection_success(task_result, task_id, args, kwargs):
    """æ•°æ®é‡‡é›†æˆåŠŸåçš„å›è°ƒå‡½æ•°."""
    try:
        logger.info(f"æ•°æ®é‡‡é›†ä»»åŠ¡ {task_id} æˆåŠŸå®Œæˆï¼Œè§¦å‘ç‰¹å¾è®¡ç®—")

        # ä»é‡‡é›†ç»“æœä¸­æå–æ–°æ¯”èµ›çš„match_ids
        collected_match_ids = task_result.get("new_match_ids", [])

        if collected_match_ids:
            # å¼‚æ­¥è§¦å‘ç‰¹å¾è®¡ç®—ä»»åŠ¡
            trigger_feature_calculation_for_new_matches.delay(collected_match_ids)

    except Exception as e:
        logger.error(f"é‡‡é›†æˆåŠŸå›è°ƒå¤„ç†å¤±è´¥: {e}")


# ä¸ºæ•°æ®é‡‡é›†ä»»åŠ¡æ·»åŠ æˆåŠŸå›è°ƒ
# TODO: ä¿®å¤å›è°ƒç»‘å®šé—®é¢˜ - æš‚æ—¶æ³¨é‡Šæ‰ä»¥è®©ç³»ç»Ÿæ­£å¸¸å¯åŠ¨
# collect_daily_fixtures.link_success(on_collection_success)
# collect_live_scores.link_success(on_collection_success)
# collect_odds_data.link_success(on_collection_success)
