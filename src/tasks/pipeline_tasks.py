"""Pipeline Tasks module.

å®šä¹‰æ•°æ®ç®¡é“çš„ä¸²è”ä»»åŠ¡ï¼Œå®ç°é‡‡é›†->æ¸…æ´—->ç‰¹å¾å·¥ç¨‹çš„è‡ªåŠ¨åŒ–æµç¨‹ã€‚
ä½¿ç”¨Celery Chainå’ŒGroupæ¥ç¼–æ’ä»»åŠ¡ä¾èµ–å…³ç³»ã€‚
"""

import logging
from datetime import datetime, timedelta
from typing import Any

from celery import chain, group, shared_task
from celery.schedules import crontab

logger = logging.getLogger(__name__)

# å¯¼å…¥åŸºç¡€æ•°æ®é‡‡é›†ä»»åŠ¡
from .data_collection_tasks import (
    collect_daily_fixtures,
    collect_live_scores,
    collect_odds_data,
)


def sync_task_to_async(async_func):
    """å°†å¼‚æ­¥å‡½æ•°è½¬æ¢ä¸ºåŒæ­¥çš„Celeryä»»åŠ¡"""
    from functools import wraps

    @wraps(async_func)
    def wrapper(*args, **kwargs):
        import asyncio

        return asyncio.run(async_func(*args, **kwargs))

    return wrapper


async def manual_data_cleaning() -> int:
    """æ‰‹åŠ¨æ•°æ®æ¸…æ´—ï¼šçº§è”åˆ›å»ºleaguesã€teamså’Œmatches"""
    try:
        logger.info("ğŸ”§ å¼€å§‹çº§è”æ•°æ®æ¸…æ´—...")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        from src.database.connection import get_async_session
        from src.database.models.raw_data import RawMatchData
        from src.database.models.league import League
        from src.database.models.team import Team
        from src.database.models.match import Match
        from sqlalchemy import select, text

        cleaned_count = 0

        # ç”¨äºè·Ÿè¸ªå·²åˆ›å»ºçš„IDæ˜ å°„
        league_id_map = {}
        team_id_map = {}

        async with get_async_session() as session:
            # è·å–æ‰€æœ‰æœªå¤„ç†çš„åŸå§‹æ•°æ®ï¼Œä½¿ç”¨optionsæ¥é¢„åŠ è½½å…³è”æ•°æ®
            from sqlalchemy.orm import selectinload
            query = select(RawMatchData).where(RawMatchData.processed.is_(False))
            result = await session.execute(query)
            raw_matches = result.scalars().all()

            # ç«‹å³åŠ è½½æ‰€æœ‰match_dataä»¥é¿å…åç»­çš„lazy loadingé—®é¢˜
            raw_data_list = []
            for raw_match in raw_matches:
                raw_data_list.append({
                    'id': raw_match.id,
                    'external_id': raw_match.external_id,
                    'match_data': dict(raw_match.match_data),  # è½¬æ¢ä¸ºdictä»¥é¿å…lazy loading
                    'source': raw_match.source
                })

            logger.info(f"ğŸ“Š æ‰¾åˆ° {len(raw_data_list)} æ¡æœªå¤„ç†çš„åŸå§‹æ¯”èµ›æ•°æ®")

            # æ­¥éª¤1ï¼šä»raw_dataä¸­æå–å¹¶åˆ›å»ºæ‰€æœ‰å”¯ä¸€çš„leagues
            logger.info("ğŸ“ æ­¥éª¤1ï¼šåˆ›å»ºleaguesè®°å½•...")
            league_count = 0
            for raw_match_data in raw_data_list:
                try:
                    raw_data = raw_match_data['match_data'].get("raw_data", {})
                    if "competition" in raw_data:
                        comp = raw_data["competition"]
                        league_name = comp.get("name", "Unknown League")
                        country = comp.get("area", {}).get("name", "Unknown Country")

                        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                        existing_query = text(
                            "SELECT id FROM leagues WHERE name = :name AND country = :country"
                        )
                        result = await session.execute(
                            existing_query, {"name": league_name, "country": country}
                        )
                        existing_league = result.scalar_one_or_none()

                        if not existing_league:
                            # åˆ›å»ºæ–°league
                            new_league = League(
                                name=league_name, country=country, is_active=True
                            )
                            session.add(new_league)
                            await session.flush()  # è·å–ç”Ÿæˆçš„ID
                            league_id_map[str(comp.get("id"))] = new_league.id
                            league_count += 1
                            logger.debug(
                                f"âœ… åˆ›å»ºè”èµ›: {league_name} (ID: {new_league.id})"
                            )
                        else:
                            league_id_map[str(comp.get("id"))] = existing_league
                            logger.debug(
                                f"â„¹ï¸  è”èµ›å·²å­˜åœ¨: {league_name} (ID: {existing_league})"
                            )

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†leagueå¤±è´¥: {e}")
                    continue

            logger.info(f"ğŸ“ leaguesåˆ›å»ºå®Œæˆï¼Œå…± {league_count} ä¸ªæ–°è”èµ›")

            # æ­¥éª¤2ï¼šä»raw_dataä¸­æå–å¹¶åˆ›å»ºæ‰€æœ‰å”¯ä¸€çš„teams
            logger.info("ğŸ‘¥ æ­¥éª¤2ï¼šåˆ›å»ºteamsè®°å½•...")
            team_count = 0
            for raw_match_data in raw_data_list:
                try:
                    raw_data = raw_match_data['match_data'].get("raw_data", {})

                    # å¤„ç†ä¸»é˜Ÿ
                    if "homeTeam" in raw_data:
                        home_team = raw_data["homeTeam"]
                        team_name = home_team.get("name", "Unknown Team")
                        country = raw_data.get("area", {}).get(
                            "name", "Unknown Country"
                        )
                        team_id = str(home_team.get("id"))

                        if team_id not in team_id_map:
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                            existing_query = text(
                                "SELECT id FROM teams WHERE name = :name"
                            )
                            result = await session.execute(
                                existing_query, {"name": team_name}
                            )
                            existing_team = result.scalar_one_or_none()

                            if not existing_team:
                                new_team = Team(
                                    name=team_name,
                                    short_name=home_team.get("shortName"),
                                    country=country,
                                    founded_year=1870,  # é»˜è®¤å€¼
                                )
                                session.add(new_team)
                                await session.flush()
                                team_id_map[team_id] = new_team.id
                                team_count += 1
                                logger.debug(
                                    f"âœ… åˆ›å»ºçƒé˜Ÿ: {team_name} (ID: {new_team.id})"
                                )
                            else:
                                team_id_map[team_id] = existing_team
                                logger.debug(
                                    f"â„¹ï¸  çƒé˜Ÿå·²å­˜åœ¨: {team_name} (ID: {existing_team})"
                                )

                    # å¤„ç†å®¢é˜Ÿ
                    if "awayTeam" in raw_data:
                        away_team = raw_data["awayTeam"]
                        team_name = away_team.get("name", "Unknown Team")
                        country = raw_data.get("area", {}).get(
                            "name", "Unknown Country"
                        )
                        team_id = str(away_team.get("id"))

                        if team_id not in team_id_map:
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                            existing_query = text(
                                "SELECT id FROM teams WHERE name = :name"
                            )
                            result = await session.execute(
                                existing_query, {"name": team_name}
                            )
                            existing_team = result.scalar_one_or_none()

                            if not existing_team:
                                new_team = Team(
                                    name=team_name,
                                    short_name=away_team.get("shortName"),
                                    country=country,
                                    founded_year=1870,  # é»˜è®¤å€¼
                                )
                                session.add(new_team)
                                await session.flush()
                                team_id_map[team_id] = new_team.id
                                team_count += 1
                                logger.debug(
                                    f"âœ… åˆ›å»ºçƒé˜Ÿ: {team_name} (ID: {new_team.id})"
                                )
                            else:
                                team_id_map[team_id] = existing_team
                                logger.debug(
                                    f"â„¹ï¸  çƒé˜Ÿå·²å­˜åœ¨: {team_name} (ID: {existing_team})"
                                )

                except Exception as e:
                    logger.error(f"âŒ å¤„ç†teamå¤±è´¥: {e}")
                    continue

            logger.info(f"ğŸ‘¥ teamsåˆ›å»ºå®Œæˆï¼Œå…± {team_count} ä¸ªæ–°çƒé˜Ÿ")

            # æ­¥éª¤3ï¼šåˆ›å»ºmatchesè®°å½•ï¼Œä½¿ç”¨å†…éƒ¨ID
            logger.info("âš½ æ­¥éª¤3ï¼šåˆ›å»ºmatchesè®°å½•...")
            for raw_match_data in raw_data_list:
                try:
                    match_data = raw_match_data['match_data']
                    raw_match_data_content = raw_match_data['match_data'].get("raw_data", {})

                    # è·å–å¯¹åº”çš„å†…éƒ¨ID
                    external_league_id = str(match_data.get("external_league_id", 0))
                    external_home_team_id = str(
                        match_data.get("external_home_team_id", 0)
                    )
                    external_away_team_id = str(
                        match_data.get("external_away_team_id", 0)
                    )

                    # æ˜ å°„åˆ°å†…éƒ¨ID
                    league_internal_id = league_id_map.get(external_league_id)
                    home_team_internal_id = team_id_map.get(external_home_team_id)
                    away_team_internal_id = team_id_map.get(external_away_team_id)

                    # éªŒè¯æ‰€æœ‰å¿…éœ€çš„IDéƒ½å­˜åœ¨
                    if not all(
                        [
                            league_internal_id,
                            home_team_internal_id,
                            away_team_internal_id,
                        ]
                    ):
                        logger.warning(
                            f"âš ï¸  è·³è¿‡æ¯”èµ› {match_data.get('external_match_id')}ï¼Œç¼ºå°‘å…³è”çš„ID"
                        )
                        continue

                    # ç¡®ä¿match_dateæ˜¯datetimeå¯¹è±¡
                    match_time_str = match_data.get("match_time")
                    match_date = None
                    if match_time_str:
                        if isinstance(match_time_str, str):
                            try:
                                from datetime import datetime

                                # è§£æISOæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²å¹¶è½¬æ¢ä¸ºnaive datetime
                                aware_dt = datetime.fromisoformat(
                                    match_time_str.replace("Z", "+00:00")
                                )
                                match_date = aware_dt.replace(tzinfo=None)
                            except (ValueError, TypeError):
                                logger.warning(f"æ— æ³•è§£æmatch_time: {match_time_str}")
                                match_date = None
                        else:
                            match_date = match_time_str

                    # åˆ›å»ºMatchå®ä¾‹
                    match = Match(
                        # ä½¿ç”¨å†…éƒ¨ID
                        home_team_id=home_team_internal_id,
                        away_team_id=away_team_internal_id,
                        league_id=league_internal_id,
                        status=match_data.get("status", "scheduled"),
                        match_date=match_date,
                        season=str(match_data.get("season", "")),
                        venue=raw_match_data_content.get("area", {}).get("name"),
                        # æ¯”åˆ†ä¿¡æ¯
                        home_score=raw_match_data_content.get("score", {})
                        .get("fullTime", {})
                        .get("home", 0),
                        away_score=raw_match_data_content.get("score", {})
                        .get("fullTime", {})
                        .get("away", 0),
                    )

                    session.add(match)

                    # æ ‡è®°åŸå§‹æ•°æ®ä¸ºå·²å¤„ç† - éœ€è¦ä»æ•°æ®åº“é‡æ–°è·å–è®°å½•
                    from sqlalchemy import update
                    update_stmt = (
                        update(RawMatchData)
                        .where(RawMatchData.id == raw_match_data['id'])
                        .values(processed=True, updated_at=datetime.utcnow())
                    )
                    await session.execute(update_stmt)

                    cleaned_count += 1

                    # æ¯100æ¡è®°å½•æäº¤ä¸€æ¬¡
                    if cleaned_count % 100 == 0:
                        await session.commit()
                        logger.info(f"âœ… å·²å¤„ç† {cleaned_count} æ¡matchè®°å½•")

                except Exception as match_error:
                    logger.error(
                        f"âŒ å¤„ç†æ¯”èµ› {raw_match_data['external_id']} å¤±è´¥: {match_error}"
                    )
                    continue

            # æäº¤å‰©ä½™çš„äº‹åŠ¡
            await session.commit()

        logger.info("ğŸ‰ çº§è”æ•°æ®æ¸…æ´—å®Œæˆï¼")
        logger.info(f"   - æ–°å¢leagues: {league_count}")
        logger.info(f"   - æ–°å¢teams: {team_count}")
        logger.info(f"   - æ–°å¢matches: {cleaned_count}")

        return cleaned_count

    except Exception as e:
        logger.error(f"âŒ çº§è”æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return 0


@shared_task(bind=True, name="data_cleaning_task")
def data_cleaning_task(self, collection_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®æ¸…æ´—ä»»åŠ¡.

    Args:
        collection_result: æ•°æ®é‡‡é›†ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: æ¸…æ´—ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œæ•°æ®æ¸…æ´—ä»»åŠ¡ï¼Œå¤„ç†é‡‡é›†ç»“æœ: {collection_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # ä¿®å¤å­—æ®µæ˜ å°„ï¼šé‡‡é›†ä»»åŠ¡è¿”å›çš„æ˜¯ total_collected æˆ– records_collected
        collected_records = (
            collection_result.get("records_collected")
            or collection_result.get("total_collected")
            or collection_result.get("collected_records", 0)
        )

        logger.info(f"é‡‡é›†åˆ°çš„åŸå§‹æ•°æ®è®°å½•æ•°: {collected_records}")

        # å¦‚æœæœ‰åŸå§‹æ•°æ®ï¼Œæ‰§è¡ŒçœŸæ­£çš„æ•°æ®æ¸…æ´—
        cleaned_count = 0
        if collected_records > 0:
            try:
                # å¯¼å…¥å¹¶æ‰§è¡Œæ•°æ®æ¸…æ´—é€»è¾‘
                from src.data.processors.data_cleaner import FootballDataCleaner

                async def clean_data():
                    cleaner = FootballDataCleaner()
                    result = await cleaner.clean_all_raw_data()
                    return result

                import asyncio

                clean_result = asyncio.run(clean_data())
                cleaned_count = clean_result.get("cleaned_records", 0)
                logger.info(f"âœ… çœŸå®æ•°æ®æ¸…æ´—å®Œæˆï¼Œæ¸…æ´—è®°å½•æ•°: {cleaned_count}")

            except Exception as clean_error:
                logger.warning(f"âš ï¸ æ•°æ®æ¸…æ´—å™¨æ‰§è¡Œå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨æ¸…æ´—: {clean_error}")
                # æ‰‹åŠ¨æ¸…æ´—é€»è¾‘ï¼šå°†raw_match_dataä¸­çš„æ•°æ®å¯¼å…¥åˆ°matchesè¡¨
                import asyncio

                cleaned_count = asyncio.run(manual_data_cleaning())

        cleaning_result = {
            "status": "success",
            "cleaned_records": cleaned_count,
            "cleaning_timestamp": datetime.utcnow().isoformat(),
            "errors_removed": max(0, collected_records - cleaned_count),
            "duplicates_removed": 0,
        }

        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {cleaning_result}")
        return cleaning_result

    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—ä»»åŠ¡å¤±è´¥: {e}")
        import traceback

        logger.error(f"ğŸ” å®Œæ•´é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return {
            "status": "error",
            "error": str(e),
            "cleaning_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="feature_engineering_task")
def feature_engineering_task(self, cleaning_result: dict[str, Any]) -> dict[str, Any]:
    """ç‰¹å¾å·¥ç¨‹ä»»åŠ¡.

    Args:
        cleaning_result: æ•°æ®æ¸…æ´—ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: ç‰¹å¾å·¥ç¨‹ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œç‰¹å¾å·¥ç¨‹ä»»åŠ¡ï¼Œå¤„ç†æ¸…æ´—ç»“æœ: {cleaning_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # æ¨¡æ‹Ÿç‰¹å¾è®¡ç®—ï¼ˆå®é™…åº”è¯¥æ ¹æ®æ¸…æ´—åçš„æ•°æ®è®¡ç®—ç‰¹å¾ï¼‰
        features_calculated = cleaning_result.get("cleaned_records", 0)

        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„ç‰¹å¾è®¡ç®—é€»è¾‘
        feature_result = {
            "status": "success",
            "features_calculated": features_calculated,
            "feature_timestamp": datetime.utcnow().isoformat(),
            "feature_columns": [
                "home_team_id",
                "away_team_id",
                "home_last_5_points",
                "away_last_5_points",
                "home_last_5_avg_goals",
                "away_last_5_avg_goals",
                "h2h_last_3_home_wins",
                "home_last_5_goal_diff",
                "away_last_5_goal_diff",
                "home_win_streak",
                "away_win_streak",
                "home_last_5_win_rate",
                "away_last_5_win_rate",
                "home_rest_days",
                "away_rest_days",
            ],
        }

        logger.info(f"ç‰¹å¾å·¥ç¨‹å®Œæˆ: {feature_result}")
        return feature_result

    except Exception as e:
        logger.error(f"ç‰¹å¾å·¥ç¨‹ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "feature_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="data_storage_task")
def data_storage_task(self, feature_result: dict[str, Any]) -> dict[str, Any]:
    """æ•°æ®å­˜å‚¨ä»»åŠ¡.

    Args:
        feature_result: ç‰¹å¾å·¥ç¨‹ä»»åŠ¡çš„è¿”å›ç»“æœ

    Returns:
        Dict[str, Any]: å­˜å‚¨ç»“æœç»Ÿè®¡
    """
    try:
        logger.info(f"å¼€å§‹æ‰§è¡Œæ•°æ®å­˜å‚¨ä»»åŠ¡ï¼Œå¤„ç†ç‰¹å¾ç»“æœ: {feature_result}")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # è¿™é‡Œå®ç°ç‰¹å¾æ•°æ®åˆ°æ•°æ®åº“çš„å­˜å‚¨
        stored_features = feature_result.get("features_calculated", 0)

        storage_result = {
            "status": "success",
            "stored_features": stored_features,
            "storage_timestamp": datetime.utcnow().isoformat(),
            "database_table": "features",
        }

        logger.info(f"æ•°æ®å­˜å‚¨å®Œæˆ: {storage_result}")
        return storage_result

    except Exception as e:
        logger.error(f"æ•°æ®å­˜å‚¨ä»»åŠ¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "storage_timestamp": datetime.utcnow().isoformat(),
        }


def ensure_database_initialized():
    """ç¡®ä¿æ•°æ®åº“ç®¡ç†å™¨å·²åˆå§‹åŒ–."""
    try:
        from src.database.connection import DatabaseManager
        import os

        db_manager = DatabaseManager()

        # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
        if not hasattr(db_manager, "_initialized") or not db_manager._initialized:
            # ä½¿ç”¨ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“URL
            database_url = os.getenv("DATABASE_URL")
            if not database_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                database_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            db_manager.initialize(database_url=database_url)
            db_manager._initialized = True
            logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")

        return db_manager
    except Exception as e:
        logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        raise


@shared_task(bind=True, name="complete_data_pipeline")
def complete_data_pipeline(self) -> dict[str, Any]:
    """å®Œæ•´çš„æ•°æ®ç®¡é“ä»»åŠ¡.

    æŒ‰é¡ºåºæ‰§è¡Œï¼šæ•°æ®é‡‡é›† -> æ•°æ®æ¸…æ´— -> ç‰¹å¾å·¥ç¨‹ -> æ•°æ®å­˜å‚¨

    Returns:
        Dict[str, Any]: ç®¡é“æ‰§è¡Œç»“æœ
    """
    try:
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´æ•°æ®ç®¡é“")

        # ç¡®ä¿æ•°æ®åº“å·²åˆå§‹åŒ–
        ensure_database_initialized()

        # å®šä¹‰ä»»åŠ¡é“¾ï¼šé‡‡é›† -> æ¸…æ´— -> ç‰¹å¾ -> å­˜å‚¨
        # ä½¿ç”¨æ­£ç¡®çš„ Celery chain è¯­æ³•ï¼Œå¯¼å…¥å®é™…ä»»åŠ¡å‡½æ•°
        from .data_collection_tasks import collect_daily_fixtures

        pipeline = chain(
            collect_daily_fixtures.s(),
            data_cleaning_task.s(),
            feature_engineering_task.s(),
            data_storage_task.s(),
        )

        # æ‰§è¡Œç®¡é“
        result = pipeline.apply_async()

        pipeline_result = {
            "status": "success",
            "pipeline_completed": True,
            "completion_timestamp": datetime.utcnow().isoformat(),
            "task_id": result.id,
            "message": "æ•°æ®ç®¡é“ä»»åŠ¡é“¾å·²å¯åŠ¨",
        }

        logger.info(f"å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆ: {pipeline_result}")
        return pipeline_result

    except Exception as e:
        logger.error(f"å®Œæ•´æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "pipeline_completed": False,
            "completion_timestamp": datetime.utcnow().isoformat(),
        }


@shared_task(bind=True, name="trigger_feature_calculation_for_new_matches")
def trigger_feature_calculation_for_new_matches(
    self, match_ids: list[int]
) -> dict[str, Any]:
    """ä¸ºæ–°é‡‡é›†çš„æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—.

    Args:
        match_ids: éœ€è¦è®¡ç®—ç‰¹å¾çš„æ¯”èµ›IDåˆ—è¡¨

    Returns:
        Dict[str, Any]: ç‰¹å¾è®¡ç®—è§¦å‘ç»“æœ
    """
    try:
        logger.info(f"ä¸º {len(match_ids)} åœºæ–°æ¯”èµ›è§¦å‘ç‰¹å¾è®¡ç®—")

        from src.services.feature_service import FeatureService
        from src.database.connection import DatabaseManager

        # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
        db_manager = DatabaseManager()

        calculated_count = 0
        failed_count = 0

        # ä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        async def calculate_features_for_match(match_id: int) -> bool:
            """ä¸ºå•åœºæ¯”èµ›è®¡ç®—ç‰¹å¾çš„å¼‚æ­¥å‡½æ•°"""
            try:
                async with db_manager.get_async_session() as session:
                    feature_service = FeatureService(session)

                    # è®¡ç®—ç‰¹å¾
                    features = await feature_service.get_match_features(match_id)

                    if features:
                        logger.debug(f"æˆåŠŸè®¡ç®—æ¯”èµ› {match_id} çš„ç‰¹å¾")
                        return True
                    else:
                        logger.warning(f"æ¯”èµ› {match_id} ç‰¹å¾è®¡ç®—å¤±è´¥")
                        return False

            except Exception as e:
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")
                return False

        # ä½¿ç”¨asyncio.runä¸ºæ¯åœºæ¯”èµ›è®¡ç®—ç‰¹å¾
        for match_id in match_ids:
            try:
                success = asyncio.run(calculate_features_for_match(match_id))
                if success:
                    calculated_count += 1
                else:
                    failed_count += 1

            except Exception as e:
                failed_count += 1
                logger.error(f"è®¡ç®—æ¯”èµ› {match_id} ç‰¹å¾æ—¶å‡ºé”™: {e}")

        result = {
            "status": "success",
            "total_matches": len(match_ids),
            "calculated_features": calculated_count,
            "failed_calculations": failed_count,
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }

        logger.info(f"ç‰¹å¾è®¡ç®—è§¦å‘å®Œæˆ: {result}")
        return result

    except Exception as e:
        logger.error(f"è§¦å‘ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
        return {
            "status": "error",
            "error": str(e),
            "calculation_timestamp": datetime.utcnow().isoformat(),
        }


# å›è°ƒå‡½æ•°ï¼šæ•°æ®é‡‡é›†å®Œæˆåè‡ªåŠ¨è§¦å‘ç‰¹å¾è®¡ç®—
def on_collection_success(task_result, task_id, args, kwargs):
    """æ•°æ®é‡‡é›†æˆåŠŸåçš„å›è°ƒå‡½æ•°."""
    try:
        logger.info(f"æ•°æ®é‡‡é›†ä»»åŠ¡ {task_id} æˆåŠŸå®Œæˆï¼Œè§¦å‘ç‰¹å¾è®¡ç®—")

        # ä»é‡‡é›†ç»“æœä¸­æå–æ–°æ¯”èµ›çš„match_ids
        collected_match_ids = task_result.get("new_match_ids", [])

        if collected_match_ids:
            # å¼‚æ­¥è§¦å‘ç‰¹å¾è®¡ç®—ä»»åŠ¡
            trigger_feature_calculation_for_new_matches.delay(collected_match_ids)

    except Exception as e:
        logger.error(f"é‡‡é›†æˆåŠŸå›è°ƒå¤„ç†å¤±è´¥: {e}")


# ä¸ºæ•°æ®é‡‡é›†ä»»åŠ¡æ·»åŠ æˆåŠŸå›è°ƒ
# TODO: ä¿®å¤å›è°ƒç»‘å®šé—®é¢˜ - æš‚æ—¶æ³¨é‡Šæ‰ä»¥è®©ç³»ç»Ÿæ­£å¸¸å¯åŠ¨
# collect_daily_fixtures.link_success(on_collection_success)
# collect_live_scores.link_success(on_collection_success)
# collect_odds_data.link_success(on_collection_success)
