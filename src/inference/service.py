"""
FastAPIæ¨ç†æœåŠ¡
æä¾›è¶³çƒæ¯”èµ›é¢„æµ‹çš„REST APIç«¯ç‚¹

åŠŸèƒ½:
1. å•åœºæ¯”èµ›é¢„æµ‹
2. æ¨¡å‹ä¿¡æ¯æŸ¥è¯¢
3. å¥åº·æ£€æŸ¥
4. é”™è¯¯å¤„ç†

ä½œè€…: Backend Engineer
åˆ›å»ºæ—¶é—´: 2025-12-10
ç‰ˆæœ¬: 1.0.0 - Phase 3 Inference
"""

import logging
import time
from datetime import datetime
from typing import Dict, Any, List

from fastapi import APIRouter, HTTPException, status
from fastapi.responses import JSONResponse

from .model_loader import model_loader
from .schemas import (
    PredictionRequest,
    PredictionResponse,
    ModelInfoResponse,
    ErrorResponse
)

logger = logging.getLogger(__name__)

# åˆ›å»ºæ¨ç†APIè·¯ç”±
router = APIRouter(
    prefix="/api/v1/inference",
    tags=["inference"],
    responses={
        404: {"model": ErrorResponse, "description": "Not Found"},
        422: {"model": ErrorResponse, "description": "Validation Error"},
        500: {"model": ErrorResponse, "description": "Internal Server Error"}
    }
)


@router.post("/predict", response_model=PredictionResponse, status_code=status.HTTP_200_OK)
async def predict_match(request: PredictionRequest) -> PredictionResponse:
    """
    å¯¹å•åœºæ¯”èµ›è¿›è¡Œé¢„æµ‹

    Args:
        request: é¢„æµ‹è¯·æ±‚æ•°æ®

    Returns:
        é¢„æµ‹ç»“æœ

    Raises:
        HTTPException: å½“é¢„æµ‹å¤±è´¥æ—¶
    """
    start_time = time.time()

    try:
        # è®°å½•è¯·æ±‚
        logger.info(f"ğŸ¯ æ”¶åˆ°é¢„æµ‹è¯·æ±‚: æ¯”èµ›ID {request.match_id}, "
                   f"{request.home_team_name} vs {request.away_team_name}")

        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
        if not model_loader.is_loaded():
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Model not loaded. Please check server logs."
            )

        # è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
        match_data = {
            "id": request.match_id,
            "home_team_name": request.home_team_name,
            "away_team_name": request.away_team_name,
            "match_date": request.match_date.isoformat(),
            "home_score": request.home_score or 0,
            "away_score": request.away_score or 0,
            "home_xg": request.home_xg,
            "away_xg": request.away_xg,
            "home_total_shots": request.home_total_shots,
            "away_total_shots": request.away_total_shots,
            "home_shots_on_target": request.home_shots_on_target,
            "away_shots_on_target": request.away_shots_on_target,
            "league_id": request.league_id,
            "league_name": request.league_name,
            "stats_json": request.stats_json or {}
        }

        # æ‰§è¡Œé¢„æµ‹
        prediction_result = model_loader.predict(match_data)

        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time_ms = (time.time() - start_time) * 1000

        # æ„å»ºå“åº”
        response = PredictionResponse(
            match_id=request.match_id,
            prediction=prediction_result["prediction"],
            probabilities=prediction_result["probabilities"],
            confidence=prediction_result["confidence"],
            model_version=model_loader.model_metadata.get("version", "v1.0.0") if model_loader.model_metadata else "v1.0.0",
            timestamp=datetime.now(),
            feature_count=prediction_result["feature_count"],
            missing_features=prediction_result["missing_features"],
            processing_time_ms=round(processing_time_ms, 2)
        )

        logger.info(f"âœ… é¢„æµ‹å®Œæˆ: {response.prediction} "
                   f"(ç½®ä¿¡åº¦: {response.confidence:.3f}, "
                   f"å¤„ç†æ—¶é—´: {processing_time_ms:.1f}ms)")

        return response

    except HTTPException:
        # é‡æ–°æŠ›å‡ºHTTPå¼‚å¸¸
        raise
    except Exception as e:
        logger.error(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Prediction failed: {str(e)}"
        )


@router.get("/model/info", response_model=ModelInfoResponse)
async def get_model_info() -> ModelInfoResponse:
    """
    è·å–æ¨¡å‹ä¿¡æ¯

    Returns:
        æ¨¡å‹è¯¦ç»†ä¿¡æ¯
    """
    try:
        model_info = model_loader.get_model_info()

        if model_info["status"] == "not_loaded":
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Model not loaded"
            )

        response = ModelInfoResponse(
            status=model_info["status"],
            model_type=model_info["model_type"],
            model_version=model_info["model_metadata"].get("version", "v1.0.0") if model_info.get("model_metadata") else "v1.0.0",
            feature_count=model_info["feature_count"],
            target_classes=model_info["target_classes"],
            performance_metrics=model_info["model_metadata"].get("performance", {}) if model_info.get("model_metadata") else None,
            loaded_at=datetime.now()  # è¿™é‡Œåº”è¯¥ä»æ¨¡å‹åŠ è½½å™¨è·å–å®é™…åŠ è½½æ—¶é—´
        )

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ è·å–æ¨¡å‹ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get model info: {str(e)}"
        )


@router.get("/health")
async def health_check() -> Dict[str, Any]:
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹

    Returns:
        æœåŠ¡çŠ¶æ€ä¿¡æ¯
    """
    try:
        model_status = "loaded" if model_loader.is_loaded() else "not_loaded"

        return {
            "status": "healthy",
            "service": "football-prediction-inference",
            "model_status": model_status,
            "timestamp": datetime.now().isoformat(),
            "version": "v1.0.0"
        }
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return JSONResponse(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            content={
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
        )


@router.post("/predict/batch")
async def predict_batch(matches: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ‰¹é‡é¢„æµ‹ç«¯ç‚¹ (ç®€åŒ–ç‰ˆæœ¬)

    Args:
        matches: æ¯”èµ›æ•°æ®åˆ—è¡¨

    Returns:
        æ‰¹é‡é¢„æµ‹ç»“æœ
    """
    if not model_loader.is_loaded():
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail="Model not loaded"
        )

    if len(matches) > 10:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Maximum 10 matches allowed per batch request"
        )

    start_time = time.time()
    predictions = []

    try:
        for i, match_data in enumerate(matches):
            logger.info(f"å¤„ç†ç¬¬ {i+1}/{len(matches)} åœºæ¯”èµ›")

            # ç¡®ä¿å¿…è¦çš„å­—æ®µå­˜åœ¨
            if "match_id" not in match_data or "home_team_name" not in match_data or "away_team_name" not in match_data:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=f"Match {i+1} missing required fields"
                )

            # æ‰§è¡Œé¢„æµ‹
            prediction_result = model_loader.predict(match_data)

            predictions.append({
                "match_id": match_data["match_id"],
                "prediction": prediction_result["prediction"],
                "probabilities": prediction_result["probabilities"],
                "confidence": prediction_result["confidence"]
            })

        # è®¡ç®—å¤„ç†æ—¶é—´
        processing_time_ms = (time.time() - start_time) * 1000

        # ç»Ÿè®¡é¢„æµ‹åˆ†å¸ƒ
        prediction_counts = {}
        for pred in predictions:
            result = pred["prediction"]
            prediction_counts[result] = prediction_counts.get(result, 0) + 1

        result = {
            "predictions": predictions,
            "total_matches": len(matches),
            "processing_time_ms": round(processing_time_ms, 2),
            "prediction_distribution": prediction_counts
        }

        logger.info(f"âœ… æ‰¹é‡é¢„æµ‹å®Œæˆ: {len(matches)} åœºæ¯”èµ›, "
                   f"å¤„ç†æ—¶é—´: {processing_time_ms:.1f}ms")

        return result

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Batch prediction failed: {str(e)}"
        )


def create_inference_router() -> APIRouter:
    """
    åˆ›å»ºæ¨ç†APIè·¯ç”±å™¨

    Returns:
        é…ç½®å¥½çš„APIRouterå®ä¾‹
    """
    return router


# å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹
async def startup_load_model():
    """åº”ç”¨å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹"""
    try:
        logger.info("ğŸš€ å¯åŠ¨æ¨ç†æœåŠ¡ï¼Œå¼€å§‹åŠ è½½æ¨¡å‹...")
        success = model_loader.load_model_artifacts()

        if success:
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œæ¨ç†æœåŠ¡å°±ç»ª")
        else:
            logger.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œæ¨ç†æœåŠ¡å°†æ— æ³•æ­£å¸¸å·¥ä½œ")

    except Exception as e:
        logger.error(f"âŒ å¯åŠ¨æ—¶åŠ è½½æ¨¡å‹å¤±è´¥: {e}")


# ä¼˜é›…å…³é—­æ—¶çš„æ¸…ç†
async def shutdown_cleanup():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†å·¥ä½œ"""
    logger.info("ğŸ”„ æ¨ç†æœåŠ¡æ­£åœ¨å…³é—­...")
    # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘
    logger.info("âœ… æ¨ç†æœåŠ¡å·²å®‰å…¨å…³é—­")