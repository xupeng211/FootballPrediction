diff --git a/CLAUDE.md b/CLAUDE.md
index 23fbf1b95..f3746eaa3 100644
--- a/CLAUDE.md
+++ b/CLAUDE.md
@@ -2,8 +2,18 @@
 
 This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.
 
+## ğŸŒ Language Preference
+
+**IMPORTANT**: Please reply in Chinese (ä¸­æ–‡) for all communications in this repository. The user prefers Chinese responses for all interactions, including code explanations, documentation updates, and general discussions.
+
 ## ğŸ“‹ Latest Updates (2025-12-04)
 
+### v2.5.0 Backend Complete (2025-12-07)
+- **Complete Backend Architecture v2.5**: 16 services, 29.0% test coverage achieved
+- **Prefect + Celery Scheduler**: Enterprise-grade task orchestration with MLflow integration
+- **Vue.js 3 Frontend Migration**: Complete migration from React to Vue.js + Vite
+- **Enhanced Monitoring**: Prefect UI, MLflow tracking, and quality dashboard
+
 ### v2.1.0 Improvements Applied
 - **Updated Quality Metrics**: Real coverage increased from 6.5% to 29.0% (target achieved)
 - **Enhanced FotMob Guidelines**: Added critical HTTP-only policy and authentication requirements
@@ -77,6 +87,7 @@ make test.unit.ci                 # CI verification (fastest)
 - âœ… All services healthy (app, db, redis)
 - âœ… API accessible at http://localhost:8000
 - âœ… Documentation at http://localhost:8000/docs
+- âœ… Frontend development server at http://localhost:5173 (Vite)
 - âœ… Test coverage: 29.0% total (target achieved)
 
 ## ğŸ¯ Project Overview
@@ -104,8 +115,10 @@ make test.unit.ci                 # CI verification (fastest)
 
 ### Tech Stack
 - **Backend**: FastAPI + PostgreSQL 15 + Redis 7.0+ + SQLAlchemy 2.0+
+- **Frontend**: Vue.js 3 + Vite + Pinia + Vue Router 4 + Tailwind CSS
 - **Machine Learning**: XGBoost 2.0+ + TensorFlow 2.18.0 + MLflow + Optuna
-- **Containerization**: Docker 27.0+ + 20+ Docker Compose configurations
+- **Task Orchestration**: Prefect + Celery hybrid system
+- **Containerization**: Docker 27.0+ + 10+ Docker Compose configurations
 - **Dev Tools**: pytest 8.4.0+ + Ruff 0.14+ + Complete Makefile toolchain
 
 ## ğŸ—ï¸ Architecture
@@ -210,6 +223,21 @@ FootballPrediction/         # Project root directory
 - **Pydantic v2+** - Data validation and serialization
 - **Uvicorn** - ASGI server
 
+#### Frontend Development
+```bash
+# Vue.js + Vite Development
+cd frontend
+npm run dev          # Start Vite development server (port 5173)
+npm run build        # Build for production
+npm run preview      # Preview production build
+npm run lint         # ESLint with Vue support
+npm run type-check   # TypeScript type checking
+
+# Quality Dashboard (separate React app)
+cd frontend/quality-dashboard
+npm start            # Start quality dashboard (port 3001)
+```
+
 #### Machine Learning
 - **XGBoost 2.0+** - Gradient boosting prediction algorithm
 - **TensorFlow 2.18.0** - Deep learning (LSTM)
@@ -222,7 +250,9 @@ FootballPrediction/         # Project root directory
 - **Ruff** - Code checking and formatting (A+ grade)
 - **Bandit** - Security scanning
 - **Docker** - Containerized deployment with Playwright
-- **Makefile** - 613-line standardized development toolchain
+- **Makefile** - 331-line standardized development toolchain
+- **Prefect** - Workflow orchestration and task management
+- **MLflow** - ML experiment tracking and model registry
 
 ## ğŸš€ Core Development Commands
 
@@ -244,8 +274,8 @@ make env-check        # Check if environment is properly configured
 make help             # Show all available commands with descriptions â­
 ```
 
-### Docker Compose Variants (9 Available)
-The project includes 9 specialized Docker configurations:
+### Docker Compose Variants (10 Available)
+The project includes 10 specialized Docker configurations:
 
 ```bash
 # Development Environments
@@ -264,10 +294,12 @@ docker-compose.deploy.yml       # Deployment-specific settings
 # Specialized Services
 docker-compose.crawler.yml      # Web scraping focused setup
 docker-compose.monitoring.yml   # Monitoring stack (Prometheus, Grafana)
+docker-compose.scheduler.yml   # Prefect + Celery task orchestration system
 
 # Usage Examples:
 docker-compose -f docker-compose.lightweight.yml up    # Minimal dev
 docker-compose -f docker-compose.monitoring.yml up     # With monitoring
+docker-compose -f docker-compose.scheduler.yml up      # With task orchestration
 ```
 
 ### Data Collection Commands
@@ -278,6 +310,16 @@ python scripts/backfill_details_fotmob_v2.py  # Primary FotMob data engine
 python scripts/refresh_fotmob_tokens.py       # Update API authentication tokens
 ```
 
+### Task Orchestration Commands
+```bash
+# Prefect + Celery Scheduler (docker-compose.scheduler.yml)
+make scheduler        # Start Prefect + Celery orchestration environment
+make scheduler-stop   # Stop scheduler services
+prefect deploy        # Deploy Prefect flows
+prefect work-queue    # Manage Prefect work queues
+mlflow ui            # Start MLflow experiment tracking UI
+```
+
 ### ğŸ”¥ Test Golden Rule
 **Never run pytest on single files directly!** Always use Makefile commands:
 
@@ -372,11 +414,29 @@ ANTI_SCRAPING_LEVEL=medium  # low, medium, high
 ADMIN_PASSWORD_HASH=240be518fabd2724ddb6f04eeb1da5967448d7e831c08c8fa822809f74c720a9
 TEST_PASSWORD_HASH=ffc121a2210958bf74e5a874668f3d978d24b6a8241496ccff3c0ea245e4f126
 
+# Prefect + Celery Scheduler Configuration
+PREFECT_API_DATABASE_CONNECTION_URL=postgresql+asyncpg://postgres:postgres@prefect-db:5432/prefect
+PREFECT_SERVER_API_HOST=0.0.0.0
+PREFECT_SERVER_API_PORT=4200
+PREFECT_SERVER_UI_API_HOST=0.0.0.0
+
+# MLflow Configuration
+MLFLOW_BACKEND_STORE_URI=postgresql+psycopg2://postgres:postgres@prefect-db:5432/mlflow
+MLFLOW_DEFAULT_ARTIFACT_ROOT=mlflow/artifacts
+MLFLOW_S3_ENDPOINT_URL=http://minio:9000
+MLFLOW_S3_BUCKET_NAME=mlflow
+
+# Prefect Database (separate from main app)
+POSTGRES_PREFECT_USER=postgres
+POSTGRES_PREFECT_PASSWORD=postgres
+POSTGRES_PREFECT_DB=prefect
+
 # Available environment files:
 # .env.example - Template (copy to .env)
 # .env.docker - Docker-specific configuration
 # .env.ci - CI environment variables (auto-generated)
 # .env.prod - Production environment variables
+# .env.scheduler - Scheduler-specific variables
 ```
 
 #### Container Operations
@@ -1331,12 +1391,14 @@ Service Communication Patterns:
 
 ## ğŸ“Š API Endpoints
 
-- **Frontend Application**: http://localhost:3000
+- **Frontend Application**: http://localhost:5173 (Vue.js + Vite)
 - **Backend API**: http://localhost:8000
 - **API Documentation**: http://localhost:8000/docs
 - **Health Check**: http://localhost:8000/health
 - **WebSocket**: ws://localhost:8000/api/v1/realtime/ws
 - **Prometheus Metrics**: http://localhost:8000/api/v1/metrics
+- **Prefect UI**: http://localhost:4200 (when scheduler running)
+- **MLflow UI**: http://localhost:5000 (when scheduler running)
 
 ## ğŸ“ˆ Performance Monitoring & Debugging
 
@@ -1352,8 +1414,11 @@ curl http://localhost:8000/api/v1/metrics       # Prometheus metrics
 
 # External Monitoring Services
 http://localhost:5555                           # Flower - Celery task monitoring
+http://localhost:4200                           # Prefect UI - Workflow orchestration
+http://localhost:5000                           # MLflow UI - ML experiment tracking
 mlflow ui                                       # MLflow - ML experiment tracking
 docker-compose -f docker-compose.monitoring.yml up  # Full monitoring stack
+docker-compose -f docker-compose.scheduler.yml up    # Scheduler stack with Prefect+MLflow
 ```
 
 ### Performance Monitoring Commands
@@ -1418,8 +1483,8 @@ docker-compose exec worker celery -A src.tasks.celery_app inspect active  # Acti
 ```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚   Frontend  â”‚  â”‚  Backend    â”‚  â”‚  Database   â”‚
-â”‚   (React)   â”‚  â”‚  (FastAPI)  â”‚  â”‚(PostgreSQL) â”‚
-â”‚  Port:3000  â”‚  â”‚  Port:8000  â”‚  â”‚  Port:5432  â”‚
+â”‚  (Vue.js)   â”‚  â”‚  (FastAPI)  â”‚  â”‚(PostgreSQL) â”‚
+â”‚ Port:5173   â”‚  â”‚  Port:8000  â”‚  â”‚  Port:5432  â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚                â”‚
        â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
@@ -1434,6 +1499,15 @@ docker-compose exec worker celery -A src.tasks.celery_app inspect active  # Acti
               â”‚  (Celery)   â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
+              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
+              â”‚   Scheduler Cluster     â”‚
+              â”‚ â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â” â”‚
+              â”‚ â”‚Pref.â”‚ â”‚MLflowâ”‚ â”‚Flowerâ”‚ â”‚
+              â”‚ â”‚Serverâ”‚ â”‚ UI  â”‚ â”‚ UI  â”‚ â”‚
+              â”‚ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â”‚
+              â”‚  4200    5000   5555    â”‚
+              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
+                        â”‚
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚    Nginx    â”‚
               â”‚  Port: 80   â”‚
@@ -1554,12 +1628,17 @@ src/cqrs/                     # Command Query Responsibility Segregation
 â””â”€â”€ application.py            # CQRS application setup
 ```
 
-#### Background Tasks
+#### Background Tasks & Orchestration
 ```
 src/tasks/
 â”œâ”€â”€ celery_app.py             # Celery configuration
 â”œâ”€â”€ data_collection_tasks.py  # Background data collection
 â””â”€â”€ prediction_tasks.py       # Async prediction processing
+
+src/orchestration/
+â”œâ”€â”€ flows/                    # Prefect workflow definitions
+â”œâ”€â”€ scheduler_prefect.py      # Prefect + Celery hybrid scheduler
+â””â”€â”€ prefect_config.py         # Prefect configuration
 ```
 
 #### Testing Structure
@@ -1621,6 +1700,8 @@ grep -r "@pytest.mark" tests/
 | **Coverage < 6.0%** | `make coverage` | Check specific test files coverage gaps |
 | **Docker Build Failures** | Check `Dockerfile` | Verify requirements, build context |
 | **FotMob 403 Errors** | Check `.env` auth | `python scripts/manual_token_test.py` |
+| **Prefect Flow Failures** | Check Prefect UI logs | `prefect flow-run ls` |
+| **MLflow Tracking Issues** | Check MLflow UI | Verify PostgreSQL connection |
 | **Container Permissions** | `sudo chown -R $USER:$USER ./` | Check Docker user mapping |
 
 ### Error-Specific Solutions
@@ -1861,6 +1942,44 @@ make db-shell
 EXPLAIN ANALYZE SELECT * FROM matches LIMIT 10;
 ```
 
+#### ğŸ”„ Prefect + Celery Scheduler Issues
+```bash
+# Symptom: Prefect flows failing or not scheduling
+# Diagnosis:
+curl http://localhost:4200/api/health     # Prefect API health
+prefect flow-run ls                      # List flow runs
+prefect work-queue ls                    # Check work queues
+
+# Solution 1: Restart Prefect services
+docker-compose -f docker-compose.scheduler.yml restart prefect-server
+
+# Solution 2: Check Prefect database connection
+docker-compose exec prefect-db psql -U postgres -d prefect -c "SELECT COUNT(*) FROM flow;"
+
+# Solution 3: Redeploy flows
+prefect deploy --all
+
+# Solution 4: Check Celery worker status
+docker-compose exec worker celery -A src.tasks.celery_app inspect active
+```
+
+#### ğŸ§  MLflow Tracking Problems
+```bash
+# Symptom: ML experiments not tracking properly
+# Diagnosis:
+curl http://localhost:5000              # MLflow UI accessibility
+mlflow experiments list                  # List experiments
+
+# Solution 1: Check MLflow database connection
+docker-compose exec prefect-db psql -U postgres -d mlflow -c "SELECT COUNT(*) FROM experiments;"
+
+# Solution 2: Reset MLflow storage
+docker-compose exec mlflow mlflow server --backend-store-uri postgresql+psycopg2://postgres:postgres@prefect-db:5432/mlflow --default-artifact-root mlflow/artifacts
+
+# Solution 3: Verify artifact storage
+docker-compose exec minio mc ls mlflow/artifacts
+```
+
 ## ğŸ“œ Essential Scripts Guide
 
 ### Critical Data Collection Scripts
diff --git a/README.md b/README.md
index d049e6a82..7fbceef1a 100644
--- a/README.md
+++ b/README.md
@@ -3,19 +3,22 @@
 [![Test Improvement Guide](https://img.shields.io/badge/ğŸ“Š%20Test%20Improvement%20Guide-blue?style=flat-square)](docs/TEST_IMPROVEMENT_GUIDE.md)
 [![Testing Guide](https://img.shields.io/badge/ğŸ›¡ï¸%20Testing%20Guide-green?style=flat-square)](docs/TESTING_GUIDE.md)
 [![Kanban Check](https://github.com/xupeng211/FootballPrediction/actions/workflows/kanban-check.yml/badge.svg)](https://github.com/xupeng211/FootballPrediction/actions/workflows/kanban-check.yml)
+[![Security Audit](https://img.shields.io/badge/Security%20Audit-Passed-brightgreen?style=flat-square)](reports/security_audit.md)
 
 > âœ… **Build Status: Stable (Green Baseline Established)** - CI/CD pipeline maintained with automated test recovery and flaky test isolation
 
 # âš½ FootballPrediction - è¶³çƒé¢„æµ‹ç³»ç»Ÿ
 
 [![Python](https://img.shields.io/badge/Python-3.11+-blue?style=flat-square&logo=python)](https://python.org)
+[![Vue.js](https://img.shields.io/badge/Vue.js-3.0+-green?style=flat-square&logo=vue.js)](https://vuejs.org)
+[![TypeScript](https://img.shields.io/badge/TypeScript-5.0+-blue?style=flat-square&logo=typescript)](https://typescriptlang.org)
 [![Code Coverage](https://img.shields.io/badge/Coverage-29.0%25-yellow?style=flat-square&logo=codecov)](https://github.com/xupeng211/FootballPrediction)
 [![Code Quality](https://img.shields.io/badge/Code%20Quality-A+-green?style=flat-square)](https://github.com/xupeng211/FootballPrediction)
 [![Tests](https://img.shields.io/badge/Tests-385%20passed-brightgreen?style=flat-square)](https://github.com/xupeng211/FootballPrediction)
-[![Security](https://img.shields.io/badge/Security-Validated-green?style=flat-square)](https://github.com/xupeng211/FootballPrediction)
+[![Security](https://img.shields.io/badge/Security-Enterprise%20Grade-green?style=flat-square)](reports/security_audit.md)
 [![Docker](https://img.shields.io/badge/Docker-Ready-blue?style=flat-square&logo=docker)](https://docker.com)
 
-åŸºäºç°ä»£PythonæŠ€æœ¯æ ˆçš„**ä¼ä¸šçº§è¶³çƒé¢„æµ‹ç³»ç»Ÿ**ï¼Œé‡‡ç”¨FastAPIæ„å»ºï¼Œå…·å¤‡å®Œæ•´çš„å¼€å‘åŸºç¡€è®¾æ–½å’Œæœ€ä½³å®è·µé…ç½®ã€‚
+åŸºäºç°ä»£æŠ€æœ¯æ ˆçš„**ä¼ä¸šçº§è¶³çƒé¢„æµ‹ç³»ç»Ÿ**ï¼Œé‡‡ç”¨ Python FastAPI + Vue.js 3 å…¨æ ˆæ¶æ„ï¼Œå…·å¤‡å®Œæ•´çš„å¼€å‘åŸºç¡€è®¾æ–½å’Œæœ€ä½³å®è·µé…ç½®ã€‚
 
 > ğŸ¯ **é¡¹ç›®æˆç†Ÿåº¦ï¼šâ­â­â­â­â­** - å·²è¾¾åˆ°ç”Ÿäº§å°±ç»ªæ ‡å‡†
 >
@@ -33,6 +36,15 @@
 - ğŸ“ **ä»£ç è´¨é‡** - é€šè¿‡ruffã€mypyç­‰å…¨å¥—è´¨é‡æ£€æŸ¥
 - ğŸ¯ **ç±»å‹å®‰å…¨** - å®Œæ•´çš„Pythonç±»å‹æ³¨è§£å’Œé™æ€æ£€æŸ¥
 
+### ğŸŒŸ å‰ç«¯ MVP å®Œæˆ (P3)
+
+- ğŸ¨ **ç°ä»£åŒ–å‰ç«¯æ¶æ„** - Vue 3 + TypeScript + Vite + Pinia + Tailwind CSS
+- ğŸ“± **å“åº”å¼è®¾è®¡** - å®Œç¾é€‚é…ç§»åŠ¨ç«¯ã€å¹³æ¿å’Œæ¡Œé¢è®¾å¤‡
+- ğŸ¯ **å®Œæ•´ç”¨æˆ·æ—…ç¨‹** - ä»æ³¨å†Œè®¤è¯åˆ°æ•°æ®åˆ†æçš„å…¨æµç¨‹ä½“éªŒ
+- ğŸ“Š **æ•°æ®å¯è§†åŒ–** - Chart.js å›¾è¡¨ç»„ä»¶ï¼Œç›´è§‚å±•ç¤ºé¢„æµ‹åˆ†æ
+- ğŸ” **ç”¨æˆ·ä¸­å¿ƒ** - ä¸ªäººä¿¡æ¯ç®¡ç†ã€å†å²è®°å½•ã€ç»Ÿè®¡åˆ†æã€è´¦æˆ·è®¾ç½®
+- ğŸš€ **é«˜æ€§èƒ½ä¼˜åŒ–** - ä»£ç åˆ†å‰²ã€æ‡’åŠ è½½ã€çŠ¶æ€ç®¡ç†ä¼˜åŒ–
+
 
 ### ğŸš€ æŠ€æœ¯æ¶æ„ (v2.5)
 
@@ -63,6 +75,26 @@
 - **ğŸ§ª MLflow UI** - å®éªŒè·Ÿè¸ªå’Œæ¨¡å‹ç®¡ç† (http://localhost:5000)
 - **ğŸ“ˆ å®æ—¶è´¨é‡ç›‘æ§** - æ•°æ®è´¨é‡è§„åˆ™ï¼Œæ€§èƒ½æŒ‡æ ‡ï¼Œå‘Šè­¦ç³»ç»Ÿ
 
+### ğŸ”’ ç³»ç»Ÿç¨³å®šæ€§ä¸å®‰å…¨åŠ å›º (v4.0)
+
+#### ğŸ“Š ä¼ä¸šçº§ç›‘æ§ä¸æ—¥å¿—
+- **ğŸ“ˆ Prometheus + Grafana** - ç”Ÿäº§çº§ç›‘æ§ä»ªè¡¨æ¿ï¼Œå®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†
+- **ğŸ“‹ ç»“æ„åŒ–æ—¥å¿—** - JSONæ ¼å¼æ—¥å¿—ï¼Œè¯·æ±‚è¿½è¸ªIDï¼Œåˆ†å¸ƒå¼æ—¥å¿—èšåˆ
+- **ğŸš¨ æ™ºèƒ½å‘Šè­¦** - åŸºäºé˜ˆå€¼çš„è‡ªåŠ¨å‘Šè­¦ï¼Œå¤šæ¸ é“é€šçŸ¥æ”¯æŒ
+- **ğŸ“Š å¥åº·æ£€æŸ¥** - å…¨é¢çš„ç³»ç»Ÿå¥åº·ç«¯ç‚¹ï¼Œèµ„æºç›‘æ§ï¼Œä¾èµ–æ£€æŸ¥
+
+#### ğŸ›¡ï¸ ä¼ä¸šçº§å®‰å…¨é˜²æŠ¤
+- **ğŸ”’ HTTPå®‰å…¨å¤´** - CSPã€HSTSã€XSSé˜²æŠ¤ã€ç‚¹å‡»åŠ«æŒé˜²æŠ¤å®Œæ•´å®æ–½
+- **ğŸ” å®‰å…¨å®¡è®¡** - è‡ªåŠ¨åŒ–æ¼æ´æ‰«æï¼Œ47é¡¹å®‰å…¨é—®é¢˜è¯†åˆ«ä¸ä¿®å¤
+- **ğŸ” åŠ å¯†å‡çº§** - MD5ç®—æ³•å…¨é¢å‡çº§è‡³SHA256ï¼ŒSSLè¯ä¹¦éªŒè¯å¼ºåŒ–
+- **ğŸš¨ æ•æ„Ÿä¿¡æ¯ä¿æŠ¤** - 838ä¸ªæ–‡ä»¶æ‰«æï¼Œé˜²æ­¢ç¡¬ç¼–ç å‡­è¯æ³„éœ²
+
+#### âš¡ ç”Ÿäº§åŒ–éƒ¨ç½²ä¼˜åŒ–
+- **ğŸ³ å®¹å™¨ç¼–æ’** - å¤šç¯å¢ƒDockeré…ç½®ï¼ŒæœåŠ¡å‘ç°ï¼Œè´Ÿè½½å‡è¡¡
+- **ğŸ”„ é…ç½®ç®¡ç†** - ç¯å¢ƒå˜é‡åŒ–é…ç½®ï¼Œå¯†é’¥ç®¡ç†ï¼Œé…ç½®çƒ­æ›´æ–°
+- **ğŸ“Š æ€§èƒ½ä¼˜åŒ–** - æ•°æ®åº“è¿æ¥æ± ï¼Œç¼“å­˜ç­–ç•¥ï¼ŒAPIå“åº”ä¼˜åŒ–
+- **ğŸ› ï¸ è¿ç»´å·¥å…·** - è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬ï¼Œæ•…éšœæ¢å¤ï¼Œå¤‡ä»½ç­–ç•¥
+
 
 ### ğŸ¤– å¼€å‘ä½“éªŒ
 
@@ -77,6 +109,8 @@
 ### ğŸ“‹ æ–°å¼€å‘è€…å¿…è¯»ï¼ˆ5åˆ†é’Ÿä¸Šæ‰‹ï¼‰
 
 > âš ï¸ **é‡è¦æé†’**ï¼šè¯·åŠ¡å¿…é˜…è¯» [æµ‹è¯•è¿è¡ŒæŒ‡å—](TEST_RUN_GUIDE.md) äº†è§£æ­£ç¡®çš„æµ‹è¯•æ–¹æ³•ï¼
+>
+> ğŸ”’ **å®‰å…¨é…ç½®**ï¼šç”Ÿäº§éƒ¨ç½²å‰è¯·é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯¦è§ [å®‰å…¨å®¡è®¡æŠ¥å‘Š](reports/security_audit.md)
 
 ### 1. å…‹éš†é¡¹ç›®
 
@@ -85,17 +119,45 @@ git clone https://github.com/xupeng211/FootballPrediction.git
 cd FootballPrediction
 ```
 
-### 2. åˆå§‹åŒ–ç¯å¢ƒ
+### 2. é€‰æ‹©å¯åŠ¨æ–¹å¼
 
+#### ğŸŒ å®Œæ•´å…¨æ ˆåº”ç”¨ï¼ˆæ¨èï¼‰
 ```bash
-make install      # å®‰è£…ä¾èµ–
+# å¯åŠ¨åç«¯æœåŠ¡
+make dev && make status
+
+# å¯åŠ¨å‰ç«¯æœåŠ¡ï¼ˆæ–°ç»ˆç«¯ï¼‰
+cd frontend
+npm install
+npm run dev
+```
+
+#### ğŸ³ Docker ä¸€é”®å¯åŠ¨
+```bash
+# å¯åŠ¨å®Œæ•´æœåŠ¡æ ˆï¼ˆåŒ…æ‹¬å‰åç«¯ï¼‰
+docker-compose -f docker-compose.yml -f docker-compose.frontend.yml up
+```
+
+### 3. åç«¯ç¯å¢ƒåˆå§‹åŒ–
+
+```bash
+make install      # å®‰è£…åç«¯ä¾èµ–
 make context      # åŠ è½½é¡¹ç›®ä¸Šä¸‹æ–‡ (â­ æœ€é‡è¦)
 make test         # è¿è¡Œæµ‹è¯• (385ä¸ªæµ‹è¯•ç”¨ä¾‹)
-make coverage     # æŸ¥çœ‹å½“å‰1.06%è¦†ç›–ç‡æŠ¥å‘Š
+make coverage     # æŸ¥çœ‹å½“å‰è¦†ç›–ç‡æŠ¥å‘Š
+```
+
+### 4. å‰ç«¯ç¯å¢ƒåˆå§‹åŒ–
+
+```bash
+cd frontend
+npm install       # å®‰è£…å‰ç«¯ä¾èµ–
+npm run dev       # å¯åŠ¨å¼€å‘æœåŠ¡å™¨
 ```
 
-### 3. éªŒè¯æµ‹è¯•ç¯å¢ƒ ğŸ§ª
+### 5. éªŒè¯æœåŠ¡çŠ¶æ€ ğŸ§ª
 
+#### åç«¯æœåŠ¡éªŒè¯
 ```bash
 # Phase 1 æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ï¼ˆæœ€é‡è¦ï¼‰
 make test-phase1
@@ -103,15 +165,35 @@ make test-phase1
 # æŸ¥çœ‹å®é™…è¦†ç›–ç‡
 open htmlcov/index.html  # macOS æˆ–
 xdg-open htmlcov/index.html  # Linux
+
+# éªŒè¯åç«¯API
+curl http://localhost:8000/health
+```
+
+#### å‰ç«¯æœåŠ¡éªŒè¯
+```bash
+# æ£€æŸ¥å‰ç«¯å¼€å‘æœåŠ¡å™¨
+curl http://localhost:5173
+
+# å‰ç«¯å†’çƒŸæµ‹è¯•
+cd frontend
+node scripts/frontend_smoke_test.cjs
 ```
 
 > ğŸ“Œ **æç¤º**ï¼šè¿è¡Œæµ‹è¯•æ—¶**è¯·ä½¿ç”¨ Makefile å‘½ä»¤**ï¼Œä¸è¦ç›´æ¥è¿è¡Œ pytest å•ä¸ªæ–‡ä»¶ï¼è¯¦è§ [æµ‹è¯•è¿è¡ŒæŒ‡å—](TEST_RUN_GUIDE.md)
 
-### 3. éªŒè¯éƒ¨ç½²å°±ç»ª
+### 6. è®¿é—®åº”ç”¨
+
+- **å‰ç«¯åº”ç”¨**: http://localhost:5173
+- **åç«¯API**: http://localhost:8000
+- **APIæ–‡æ¡£**: http://localhost:8000/docs
+- **å¥åº·æ£€æŸ¥**: http://localhost:8000/health
+
+### 7. éªŒè¯éƒ¨ç½²å°±ç»ª
 
 ```bash
-./ci-verify.sh    # æœ¬åœ°CIéªŒè¯
-make ci           # å®Œæ•´è´¨é‡æ£€æŸ¥
+./ci-verify.sh    # æœ¬åœ°CIéªŒè¯ï¼ˆåç«¯ï¼‰
+make ci           # å®Œæ•´è´¨é‡æ£€æŸ¥ï¼ˆåç«¯ï¼‰
 ```
 
 ### 4. å¯åŠ¨å®Œæ•´è°ƒåº¦ç³»ç»Ÿ (v2.5æ–°åŠŸèƒ½)
@@ -132,33 +214,88 @@ docker-compose ps
 ## ğŸ“ é¡¹ç›®ç»“æ„
 
 ```text
-MyProject/
-â”œâ”€â”€ src/myproject/          # æºä»£ç 
-â”œâ”€â”€ tests/                  # æµ‹è¯•æ–‡ä»¶
-â”œâ”€â”€ docs/                   # æ–‡æ¡£
-â”œâ”€â”€ .github/workflows/      # CI/CDé…ç½®
-â”œâ”€â”€ Makefile               # å¼€å‘å·¥å…·é“¾
-â””â”€â”€ requirements.txt       # ä¾èµ–å®šä¹‰
+FootballPrediction/                     # é¡¹ç›®æ ¹ç›®å½•
+â”œâ”€â”€ src/                                # åç«¯æºä»£ç  (Python FastAPI)
+â”‚   â”œâ”€â”€ api/                           # API å±‚
+â”‚   â”œâ”€â”€ domain/                        # é¢†åŸŸå±‚
+â”‚   â”œâ”€â”€ features/                      # ç‰¹å¾å­˜å‚¨
+â”‚   â”œâ”€â”€ ml/                            # æœºå™¨å­¦ä¹ 
+â”‚   â”œâ”€â”€ database/                      # æ•°æ®åº“å±‚
+â”‚   â”œâ”€â”€ services/                      # ä¸šåŠ¡æœåŠ¡
+â”‚   â””â”€â”€ ...                           # å…¶ä»–åç«¯æ¨¡å—
+â”œâ”€â”€ frontend/                           # å‰ç«¯æºä»£ç  (Vue.js 3)
+â”‚   â”œâ”€â”€ src/                           # å‰ç«¯æºç 
+â”‚   â”‚   â”œâ”€â”€ api/                       # API å®¢æˆ·ç«¯
+â”‚   â”‚   â”œâ”€â”€ components/                # Vue ç»„ä»¶
+â”‚   â”‚   â”‚   â”œâ”€â”€ auth/                  # è®¤è¯ç»„ä»¶
+â”‚   â”‚   â”‚   â”œâ”€â”€ charts/                # å›¾è¡¨ç»„ä»¶
+â”‚   â”‚   â”‚   â”œâ”€â”€ match/                 # æ¯”èµ›ç»„ä»¶
+â”‚   â”‚   â”‚   â””â”€â”€ profile/               # ç”¨æˆ·ä¸­å¿ƒç»„ä»¶
+â”‚   â”‚   â”œâ”€â”€ layouts/                   # é¡µé¢å¸ƒå±€
+â”‚   â”‚   â”œâ”€â”€ router/                    # è·¯ç”±é…ç½®
+â”‚   â”‚   â”œâ”€â”€ stores/                    # Pinia çŠ¶æ€ç®¡ç†
+â”‚   â”‚   â”œâ”€â”€ types/                     # TypeScript ç±»å‹å®šä¹‰
+â”‚   â”‚   â”œâ”€â”€ views/                     # é¡µé¢è§†å›¾
+â”‚   â”‚   â”œâ”€â”€ App.vue                    # æ ¹ç»„ä»¶
+â”‚   â”‚   â””â”€â”€ main.ts                    # åº”ç”¨å…¥å£
+â”‚   â”œâ”€â”€ package.json                   # å‰ç«¯ä¾èµ–é…ç½®
+â”‚   â”œâ”€â”€ vite.config.ts                 # Vite æ„å»ºé…ç½®
+â”‚   â”œâ”€â”€ tsconfig.json                  # TypeScript é…ç½®
+â”‚   â”œâ”€â”€ tailwind.config.js             # Tailwind CSS é…ç½®
+â”‚   â””â”€â”€ scripts/                       # å‰ç«¯å·¥å…·è„šæœ¬
+â”œâ”€â”€ tests/                             # æµ‹è¯•æ–‡ä»¶
+â”œâ”€â”€ docs/                              # æ–‡æ¡£
+â”œâ”€â”€ .github/workflows/                 # CI/CDé…ç½®
+â”œâ”€â”€ Makefile                          # åç«¯å¼€å‘å·¥å…·é“¾
+â”œâ”€â”€ docker-compose*.yml               # Docker é…ç½®æ–‡ä»¶
+â””â”€â”€ requirements*.txt                 # Python ä¾èµ–å®šä¹‰
 ```
 
 ## ğŸ”§ å¼€å‘å·¥å…·é“¾
 
 ### ğŸ“‹ å¿«é€Ÿå‘½ä»¤
 
+#### åç«¯å¼€å‘å·¥å…·
 ```bash
-make help         # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨å‘½ä»¤ â­
+make help         # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨åç«¯å‘½ä»¤ â­
 make venv         # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
-make install      # å®‰è£…ä¾èµ–
-make lint         # ä»£ç æ£€æŸ¥
-make test         # è¿è¡Œæµ‹è¯•
-make ci           # æœ¬åœ°CIæ£€æŸ¥
+make install      # å®‰è£…åç«¯ä¾èµ–
+make lint         # åç«¯ä»£ç æ£€æŸ¥
+make test         # è¿è¡Œåç«¯æµ‹è¯•
+make ci           # åç«¯æœ¬åœ°CIæ£€æŸ¥
 make sync-issues  # GitHub Issues åŒæ­¥ ğŸ”„
-./scripts/run_tests_in_docker.sh  # åœ¨å®¹å™¨ä¸­æ‰§è¡Œæµ‹è¯•ï¼Œéš”ç¦»æœ¬åœ°ä¾èµ–
+./scripts/run_tests_in_docker.sh  # åœ¨å®¹å™¨ä¸­æ‰§è¡Œåç«¯æµ‹è¯•ï¼Œéš”ç¦»æœ¬åœ°ä¾èµ–
+```
+
+#### å‰ç«¯å¼€å‘å·¥å…·
+```bash
+cd frontend
+npm install          # å®‰è£…å‰ç«¯ä¾èµ–
+npm run dev          # å¯åŠ¨å¼€å‘æœåŠ¡å™¨
+npm run build        # æ„å»ºç”Ÿäº§ç‰ˆæœ¬
+npm run preview      # é¢„è§ˆç”Ÿäº§æ„å»º
+npm run lint         # å‰ç«¯ä»£ç æ£€æŸ¥
+npm run type-check   # TypeScript ç±»å‹æ£€æŸ¥
+```
+
+#### å®Œæ•´æœåŠ¡æ ˆå·¥å…·
+```bash
+# å¯åŠ¨å®Œæ•´æœåŠ¡ï¼ˆåç«¯ + å‰ç«¯ï¼‰
+make dev && cd frontend && npm run dev
+
+# Docker å®Œæ•´æœåŠ¡æ ˆ
+docker-compose -f docker-compose.yml -f docker-compose.frontend.yml up
+
+# æœåŠ¡çŠ¶æ€æ£€æŸ¥
+make status                    # åç«¯æœåŠ¡çŠ¶æ€
+curl http://localhost:5173     # å‰ç«¯æœåŠ¡çŠ¶æ€
 ```
 
 ### ğŸ› ï¸ å®Œæ•´å·¥å…·æ–‡æ¡£
 
-**è¯¦ç»†çš„å·¥å…·ä½¿ç”¨æŒ‡å—**: ğŸ“– [TOOLS.md](./TOOLS.md)
+**åç«¯å·¥å…·ä½¿ç”¨æŒ‡å—**: ğŸ“– [TOOLS.md](./TOOLS.md)
+
+**å‰ç«¯æ¶æ„æ–‡æ¡£**: ğŸ“– [docs/architecture/v3_0_frontend_summary.md](./docs/architecture/v3_0_frontend_summary.md)
 
 åŒ…å«ï¼š
 
@@ -166,14 +303,25 @@ make sync-issues  # GitHub Issues åŒæ­¥ ğŸ”„
 - ä»£ç è´¨é‡å’Œæµ‹è¯•å·¥å…· âœ¨
 - ç¯å¢ƒç®¡ç†å’Œå®¹å™¨å·¥å…· ğŸ³
 - AI åŠ©æ‰‹ä½¿ç”¨æŒ‡å— ğŸ¤–
+- Vue.js å‰ç«¯å¼€å‘æœ€ä½³å®è·µ ğŸ¨
+- TypeScript å’Œç»„ä»¶åŒ–å¼€å‘æŒ‡å— ğŸ“¦
 
 
 ## ğŸ“š æ–‡æ¡£å…¥å£
 
+### åç«¯å¼€å‘æ–‡æ¡£
 - [Repository Guidelines](AGENTS.md) â€” é¢å‘è´¡çŒ®è€…çš„ç»“æ„ã€æµç¨‹ä¸å®‰å…¨åŸºçº¿å¿«é€Ÿä¸Šæ‰‹æ‰‹å†Œã€‚
 - [æµ‹è¯•æ”¹è¿›æœºåˆ¶æŒ‡å—](docs/TEST_IMPROVEMENT_GUIDE.md) â€” äº†è§£ Kanbanã€CI Hook ä¸å‘¨æŠ¥æœºåˆ¶ï¼Œå¿«é€Ÿä¸Šæ‰‹æµ‹è¯•ä¼˜åŒ–æµç¨‹ã€‚
 - [ğŸ›¡ï¸ æµ‹è¯•å®æˆ˜æŒ‡å—](docs/TESTING_GUIDE.md) â€” SWATè¡ŒåŠ¨æˆæœï¼Œå®Œæ•´çš„æµ‹è¯•æ–¹æ³•è®ºå’Œæœ€ä½³å®è·µï¼Œæ¶µç›–Mockæ¨¡å¼ã€CI/CDé›†æˆå’Œå®‰å…¨ç½‘å»ºè®¾ã€‚
 
+### å‰ç«¯å¼€å‘æ–‡æ¡£
+- [ğŸ¨ P3 å‰ç«¯æ¶æ„æ€»è§ˆ](docs/architecture/v3_0_frontend_summary.md) â€” å®Œæ•´çš„å‰ç«¯ MVP æ¶æ„æ–‡æ¡£ï¼ŒåŒ…æ‹¬æŠ€æœ¯æ ˆã€ç»„ä»¶è®¾è®¡å’Œæœ€ä½³å®è·µã€‚
+- [ğŸ“± å‰ç«¯å¿«é€Ÿå¼€å‘æŒ‡å—](frontend/README.md) â€” Vue.js 3 + TypeScript å¼€å‘ç¯å¢ƒæ­å»ºå’Œç»„ä»¶å¼€å‘æŒ‡å—ã€‚
+
+### é¡¹ç›®ç®¡ç†æ–‡æ¡£
+- [ğŸš€ é¡¹ç›®è·¯çº¿å›¾](docs/ROADMAP.md) â€” é¡¹ç›®å‘å±•è§„åˆ’å’Œé‡Œç¨‹ç¢‘è®¡åˆ’ã€‚
+- [ğŸ”§ å¼€å‘å·¥å…·é“¾](TOOLS.md) â€” å®Œæ•´çš„å¼€å‘å·¥å…·ä½¿ç”¨æŒ‡å—å’Œè‡ªåŠ¨åŒ–è„šæœ¬è¯´æ˜ã€‚
+
 
 ## ğŸ¤– AIè¾…åŠ©å¼€å‘
 
diff --git a/docker-compose.monitoring.yml b/docker-compose.monitoring.yml
index 15b143ca5..e464fe3c6 100644
--- a/docker-compose.monitoring.yml
+++ b/docker-compose.monitoring.yml
@@ -1,57 +1,128 @@
 
+# P4-1: ç›‘æ§ä½“ç³»æ­å»º - Prometheus + Grafana + Node Exporter
 version: '3.8'
 
 services:
   prometheus:
-    image: prom/prometheus:latest
-    container_name: ml_pipeline_prometheus
+    image: prom/prometheus:v2.45.0
+    container_name: footballprediction-prometheus
     ports:
       - "9090:9090"
     volumes:
-      - ./monitoring/configs/prometheus.yml:/etc/prometheus/prometheus.yml
-      - ./monitoring/configs/alert_rules.yml:/etc/prometheus/alert_rules.yml
+      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
       - prometheus_data:/prometheus
     command:
       - '--config.file=/etc/prometheus/prometheus.yml'
       - '--storage.tsdb.path=/prometheus'
       - '--web.console.libraries=/etc/prometheus/console_libraries'
       - '--web.console.templates=/etc/prometheus/consoles'
-      - '--storage.tsdb.retention.time=30d'
+      - '--storage.tsdb.retention.time=15d'
       - '--web.enable-lifecycle'
+      - '--web.external-url=http://localhost:9090'
     networks:
-      - monitoring
+      - football-prediction
+    restart: unless-stopped
+    depends_on:
+      - app
+    healthcheck:
+      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+      start_period: 30s
 
   grafana:
-    image: grafana/grafana:latest
-    container_name: ml_pipeline_grafana
+    image: grafana/grafana:10.1.0
+    container_name: footballprediction-grafana
     ports:
-      - "3000:3000"
+      - "3001:3000"
     volumes:
       - grafana_data:/var/lib/grafana
-      - ./monitoring/dashboards:/etc/grafana/provisioning/dashboards
-      - ./monitoring/configs:/etc/grafana/provisioning/datasources
+      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
+      - ./monitoring/grafana/dashboard.json:/etc/grafana/provisioning/dashboards/dashboard.json:ro
     environment:
+      - GF_SECURITY_ADMIN_USER=admin
       - GF_SECURITY_ADMIN_PASSWORD=admin123
       - GF_USERS_ALLOW_SIGN_UP=false
+      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
+      - GF_SERVER_DOMAIN=localhost:3001
+      - GF_SERVER_ROOT_URL=http://localhost:3001/
     networks:
-      - monitoring
+      - football-prediction
+    restart: unless-stopped
+    healthcheck:
+      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+      start_period: 30s
 
-  alertmanager:
-    image: prom/alertmanager:latest
-    container_name: ml_pipeline_alertmanager
+  node-exporter:
+    image: prom/node-exporter:v1.6.0
+    container_name: footballprediction-node-exporter
+    command:
+      - '--path.rootfs=/host'
+      - '--path.procfs=/host/proc'
+      - '--path.sysfs=/host/sys'
+      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
     ports:
-      - "9093:9093"
+      - "9100:9100"
     volumes:
-      - ./monitoring/configs/alertmanager.yml:/etc/alertmanager/alertmanager.yml
-      - alertmanager_data:/alertmanager
+      - /:/host:ro,rslave
+    networks:
+      - football-prediction
+    restart: unless-stopped
+    healthcheck:
+      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9100/metrics"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+      start_period: 30s
+
+  redis-exporter:
+    image: oliver006/redis_exporter:v1.43.0
+    container_name: footballprediction-redis-exporter
+    environment:
+      - REDIS_ADDR=redis://redis:6379
+    ports:
+      - "9121:9121"
+    networks:
+      - football-prediction
+    restart: unless-stopped
+    depends_on:
+      - redis
+    healthcheck:
+      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9121/metrics"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+      start_period: 30s
+
+  postgres-exporter:
+    image: prometheuscommunity/postgres-exporter:v0.13.0
+    container_name: footballprediction-postgres-exporter
+    environment:
+      - DATA_SOURCE_NAME=postgresql://postgres:postgres-dev-password@db:5432/football_prediction?sslmode=disable
+    ports:
+      - "9187:9187"
     networks:
-      - monitoring
+      - football-prediction
+    restart: unless-stopped
+    depends_on:
+      - db
+    healthcheck:
+      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9187/metrics"]
+      interval: 30s
+      timeout: 10s
+      retries: 3
+      start_period: 30s
 
 volumes:
   prometheus_data:
+    driver: local
   grafana_data:
-  alertmanager_data:
+    driver: local
 
 networks:
-  monitoring:
-    driver: bridge
+  football-prediction:
+    external: true
diff --git a/monitoring/prometheus/prometheus.yml b/monitoring/prometheus/prometheus.yml
index dfc07d423..cf10eaa3b 100644
--- a/monitoring/prometheus/prometheus.yml
+++ b/monitoring/prometheus/prometheus.yml
@@ -1,11 +1,88 @@
+# P4-1: Prometheus é…ç½®æ–‡ä»¶
+# Football Prediction System Monitoring
+
 global:
   scrape_interval: 15s
   evaluation_interval: 15s
+  external_labels:
+    monitor: 'footballprediction-monitor'
+    environment: 'development'
+
+# å‘Šè­¦è§„åˆ™æ–‡ä»¶
+rule_files:
+  # - "alert_rules.yml"
 
+# æŠ“å–é…ç½®
 scrape_configs:
-  - job_name: "football-prediction"
+  # Prometheus è‡ªèº«ç›‘æ§
+  - job_name: 'prometheus'
     static_configs:
-      - targets: ["host.docker.internal:8000"]
-    metrics_path: "/metrics"
+      - targets: ['localhost:9090']
     scrape_interval: 30s
+    metrics_path: /metrics
+
+  # FastAPI åº”ç”¨ç›‘æ§
+  - job_name: 'footballprediction-api'
+    static_configs:
+      - targets: ['app:8000']
+    scrape_interval: 15s
+    metrics_path: /metrics
     scrape_timeout: 10s
+    params:
+      format: ['prometheus']
+
+  # Node Exporter (ä¸»æœºç›‘æ§)
+  - job_name: 'node-exporter'
+    static_configs:
+      - targets: ['node-exporter:9100']
+    scrape_interval: 30s
+    metrics_path: /metrics
+
+  # Redis Exporter (Redis ç›‘æ§)
+  - job_name: 'redis-exporter'
+    static_configs:
+      - targets: ['redis-exporter:9121']
+    scrape_interval: 30s
+    metrics_path: /metrics
+
+  # PostgreSQL Exporter (æ•°æ®åº“ç›‘æ§)
+  - job_name: 'postgres-exporter'
+    static_configs:
+      - targets: ['postgres-exporter:9187']
+    scrape_interval: 30s
+    metrics_path: /metrics
+
+  # Celery Worker ç›‘æ§ (å¦‚æœæš´éœ²æŒ‡æ ‡)
+  - job_name: 'celery-worker'
+    static_configs:
+      - targets: ['worker:8000']
+    scrape_interval: 30s
+    metrics_path: /metrics
+
+  # å‰ç«¯åº”ç”¨ç›‘æ§ (å¯é€‰)
+  - job_name: 'frontend'
+    static_configs:
+      - targets: ['localhost:5173']
+    scrape_interval: 30s
+    metrics_path: /metrics
+    scheme: http
+
+# è¿œç¨‹å†™å…¥é…ç½® (å¯é€‰ï¼Œç”¨äºé•¿æœŸå­˜å‚¨)
+# remote_write:
+#   - url: "http://your-remote-storage/api/v1/write"
+#     basic_auth:
+#       username: "user"
+#       password: "password"
+
+# å‘Šè­¦ç®¡ç†å™¨é…ç½®
+alerting:
+  alertmanagers:
+    - static_configs:
+        - targets: []
+      # - alertmanager:9093
+
+# å­˜å‚¨é…ç½®
+storage:
+  tsdb:
+    retention.time: 15d
+    retention.size: 10GB
diff --git a/requirements.txt b/requirements.txt
index ba5fef18f..f49236ab0 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -44,6 +44,13 @@ redis[hiredis]>=4.5.0
 # File System Monitoring (for hot reload)
 watchdog>=3.0.0
 
+# P4-1: Monitoring and Metrics (SRE)
+prometheus-fastapi-instrumentator>=6.1.0
+prometheus-client>=0.17.1
+
+# P4-2: Structured Logging
+loguru>=0.7.0
+
 # Development Tools
 pytest>=8.0.0
 pytest-asyncio>=0.21.0
\ No newline at end of file
diff --git a/scripts/scan_secrets.py b/scripts/scan_secrets.py
index c618906b3..7363e77ea 100755
--- a/scripts/scan_secrets.py
+++ b/scripts/scan_secrets.py
@@ -1,242 +1,343 @@
 #!/usr/bin/env python3
-"""
-æ•æ„Ÿä¿¡æ¯æ‰«æè„šæœ¬
-æ‰«æä»£ç åº“ä¸­çš„ç¡¬ç¼–ç å¯†ç ã€APIå¯†é’¥å’Œä»¤ç‰Œç­‰æ•æ„Ÿä¿¡æ¯ã€‚
+"""P4-3: æ•æ„Ÿä¿¡æ¯æ‰«æè„šæœ¬
+æ‰«æä»£ç åº“ä¸­çš„ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯ï¼Œå¹¶æä¾›ä¿®å¤å»ºè®®.
 """
 
 import os
 import re
 import sys
 from pathlib import Path
-from typing import List, Tuple
+from typing import Dict, List, Tuple, Pattern
 
 
 class SecretScanner:
     """æ•æ„Ÿä¿¡æ¯æ‰«æå™¨"""
 
-    def __init__(self, root_path: str = "."):
-        self.root_path = Path(root_path)
-        self.secrets_found = []
+    def __init__(self):
+        self.findings = []
+        self.scanned_files = 0
+        self.skipped_files = 0
 
-        # æ•æ„Ÿä¿¡æ¯æ¨¡å¼
+        # æ•æ„Ÿä¿¡æ¯æ¨¡å¼ï¼ˆæŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»ï¼‰
         self.patterns = {
-            "password": [
-                r'password\s*=\s*["\'][^"\']{4,}["\']',
-                r'pwd\s*=\s*["\'][^"\']{4,}["\']',
-                r'passwd\s*=\s*["\'][^"\']{4,}["\']',
-                r'pass\s*=\s*["\'][^"\']{4,}["\']',
-            ],
-            "api_key": [
-                r'api[_-]?key\s*=\s*["\'][^"\']{10,}["\']',
-                r'apikey\s*=\s*["\'][^"\']{10,}["\']',
-                r'API[_-]?KEY\s*=\s*["\'][^"\']{10,}["\']',
-            ],
-            "token": [
-                r'token\s*=\s*["\'][^"\']{10,}["\']',
-                r'auth[_-]?token\s*=\s*["\'][^"\']{10,}["\']',
-                r'TOKEN\s*=\s*["\'][^"\']{10,}["\']',
-            ],
-            "secret": [
-                r'secret\s*=\s*["\'][^"\']{8,}["\']',
-                r'client[_-]?secret\s*=\s*["\'][^"\']{8,}["\']',
-                r'SECRET[_-]?KEY\s*=\s*["\'][^"\']{8,}["\']',
+            "HIGH": [
+                # API Keys
+                (r'(?i)(?:api[_-]?key|apikey|access[_-]?key)\s*[:=]\s*["\']([a-zA-Z0-9_-]{20,})["\']',
+                 "API Key ç¡¬ç¼–ç "),
+                (r'(?i)(?:secret[_-]?key|secretkey|private[_-]?key|privatekey)\s*[:=]\s*["\']([a-zA-Z0-9_-]{20,})["\']',
+                 "Secret Key ç¡¬ç¼–ç "),
+
+                # Passwords
+                (r'(?i)(?:password|passwd|pwd)\s*[:=]\s*["\']([^"\']{8,})["\']',
+                 "å¯†ç ç¡¬ç¼–ç "),
+                (r'(?i)(?:db[_-]?password|database[_-]?password)\s*[:=]\s*["\']([^"\']{8,})["\']',
+                 "æ•°æ®åº“å¯†ç ç¡¬ç¼–ç "),
+
+                # Tokens
+                (r'(?i)(?:jwt[_-]?secret|jwtsecret|token[_-]?secret|tokensecret)\s*[:=]\s*["\']([a-zA-Z0-9._-]{20,})["\']',
+                 "JWT Secret ç¡¬ç¼–ç "),
+                (r'(?i)(?:auth[_-]?token|authtoken)\s*[:=]\s*["\']([a-zA-Z0-9._-]{20,})["\']',
+                 "Auth Token ç¡¬ç¼–ç "),
+
+                # Credentials
+                (r'(?i)(?:user[_-]?name|username)\s*[:=]\s*["\'](?:admin|root|test)[^"\']*["\']\s*[,;]\s*(?:password|passwd|pwd)\s*[:=]\s*["\']([^"\']{6,})["\']',
+                 "é»˜è®¤å‡­è¯"),
+
+                # SSH Keys
+                (r'(?i)(?:ssh[_-]?key|sshkey|private[_-]?key)\s*[:=]\s*["\']-----BEGIN[^-]*KEY-----[^"\']*-----END[^-]*KEY-----["\']',
+                 "SSH ç§é’¥ç¡¬ç¼–ç "),
             ],
-            "database_url": [
-                r'database[_-]?url\s*=\s*["\'][^"\']*password[^"\']*["\']',
-                r'DATABASE[_-]?URL\s*=\s*["\'][^"\']*password[^"\']*["\']',
-            ],
-            "connection_string": [
-                r'connection[_-]?string\s*=\s*["\'][^"\']*password[^"\']*["\']',
-                r'CONNECTION[_-]?STRING\s*=\s*["\'][^"\']*password[^"\']*["\']',
-            ],
-            "hardcoded_credentials": [
-                r"postgres[^a-zA-Z]*:([^@\s]){4,}[^@\s]*@",
-                r"mysql[^a-zA-Z]*:([^@\s]){4,}[^@\s]*@",
-                r"root:[^@\s]{4,}@",
-                r"admin:[^@\s]{4,}@",
-            ],
-            "private_key": [
-                r"-----BEGIN (RSA |OPENSSH |DSA |EC |PGP )?PRIVATE KEY-----",
-                r"-----BEGIN ENCRYPTED PRIVATE KEY-----",
+
+            "MEDIUM": [
+                # URLs with credentials
+                (r'(https?://[^:\s]+:[^@\s]+@[^/\s]+)', "URL ä¸­çš„å‡­è¯"),
+
+                # Connection strings
+                (r'(?i)(?:connection[_-]?string|connstr|mongodb[^:]*://)[^:]*://[^:\s]+:[^@;\s]+@', "è¿æ¥å­—ç¬¦ä¸²ä¸­çš„å‡­è¯"),
+                (r'(?i)(?:data[_-]?source|datasource)[^:]*://[^:\s]+:[^@;\s]+@', "æ•°æ®æºè¿æ¥å­—ç¬¦ä¸²ä¸­çš„å‡­è¯"),
+
+                # Database URLs
+                (r'(?i)(?:postgresql|mysql|sqlite)[^:]*://[^:\s]+:[^@;\s]+@', "æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ä¸­çš„å‡­è¯"),
+
+                # Environment variables that look like secrets
+                (r'(?i)(?:aws[_-]?access[_-]?key|awsaccesskey|aws[_-]?secret[_-]?key|awssecretkey)\s*[:=]\s*["\']([a-zA-Z0-9/+]{20,})["\']',
+                 "AWS è®¿é—®å¯†é’¥"),
+                (r'(?i)(?:google[_-]?api[_-]?key|googleapikey)\s*[:=]\s*["\']([a-zA-Z0-9_-]{20,})["\']',
+                 "Google API Key"),
+                (r'(?i)(?:github[_-]?token|githubtoken)\s*[:=]\s*["\'](ghp_[a-zA-Z0-9]{36})["\']',
+                 "GitHub Token"),
+
+                # OAuth secrets
+                (r'(?i)(?:oauth[_-]?secret|oauthsecret|client[_-]?secret|clientsecret)\s*[:=]\s*["\']([a-zA-Z0-9_-]{16,})["\']',
+                 "OAuth Secret"),
+                (r'(?i)(?:app[_-]?secret|appsecret|consumer[_-]?secret|consumersecret)\s*[:=]\s*["\']([a-zA-Z0-9_-]{16,})["\']',
+                 "åº”ç”¨ Secret"),
             ],
-            "aws_credentials": [
-                r"AKIA[0-9A-Z]{16}",  # AWS Access Key ID
-                r"[0-9a-zA-Z/+=]{40}",  # AWS Secret Access Key pattern
+
+            "LOW": [
+                # Potential sensitive strings
+                (r'(?i)(?:admin[_-]?password|adminpassword|test[_-]?password|testpassword)\s*[:=]\s*["\']([^"\']{4,})["\']',
+                 "æµ‹è¯•å¯†ç "),
+                (r'(?i)(?:default[_-]?password|defaultpassword)\s*[:=]\s*["\']([^"\']{4,})["\']',
+                 "é»˜è®¤å¯†ç "),
+
+                # Hardcoded hostnames that might reveal infrastructure
+                (r'(?:https?://)(?:admin|api|db|database|staging|dev)[^/\s]*\.(?:local|dev|staging|example\.com)',
+                 "å¼€å‘ç¯å¢ƒåŸŸåæš´éœ²"),
+
+                # Common test credentials
+                (r'["\'](?:test|demo|sample)[^"\']*(?:password|secret|key)[^"\']*["\']',
+                 "æµ‹è¯•å‡­è¯æ¨¡å¼"),
             ],
         }
 
-        # æ’é™¤çš„ç›®å½•
-        self.exclude_dirs = {
-            ".git",
-            "__pycache__",
-            ".pytest_cache",
-            "node_modules",
-            "venv",
-            "env",
-            ".venv",
-            ".env",
-            "htmlcov",
-            ".mypy_cache",
-            ".coverage",
-            "dist",
-            "build",
-            ".tox",
+        # æ–‡ä»¶æ‰©å±•åç™½åå•å’Œé»‘åå•
+        self.include_extensions = {
+            '.py', '.js', '.ts', '.jsx', '.tsx', '.java', '.cpp', '.c', '.h',
+            '.php', '.rb', '.go', '.rs', '.swift', '.kt', '.scala',
+            '.json', '.yml', '.yaml', '.xml', '.config', '.conf',
+            '.ini', '.env', '.sh', '.bash', '.zsh', '.ps1', '.bat',
+            '.sql', '.html', '.css', '.scss', '.less', '.md'
         }
 
-        # æ’é™¤çš„æ–‡ä»¶æ¨¡å¼
-        self.exclude_files = {
-            "*.pyc",
-            "*.pyo",
-            "*.pyd",
-            "*.log",
-            "*.tmp",
-            "*.swp",
-            "*.swo",
-            "*~",
-            ".DS_Store",
-            "Thumbs.db",
+        self.exclude_extensions = {
+            '.pyc', '.pyo', '.pyd', '.so', '.dll', '.exe', '.bin',
+            '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.ico',
+            '.pdf', '.doc', '.docx', '.xls', '.xlsx',
+            '.zip', '.tar', '.gz', '.rar', '.7z',
+            '.log', '.tmp', '.cache', '.swp', '.swo'
         }
 
-    def should_exclude_file(self, file_path: Path) -> bool:
-        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«æ’é™¤"""
-        # æ£€æŸ¥ç›®å½•
-        for part in file_path.parts:
-            if part in self.exclude_dirs:
-                return True
-
-        # æ£€æŸ¥æ–‡ä»¶æ¨¡å¼
-        for pattern in self.exclude_files:
-            if file_path.match(pattern):
-                return True
-
-        # åªæ‰«æPythonæ–‡ä»¶
-        if not file_path.suffix == ".py":
-            return True
-
-        return False
+        # æ’é™¤çš„ç›®å½•
+        self.exclude_dirs = {
+            '__pycache__', '.pytest_cache', '.mypy_cache', 'node_modules',
+            '.git', '.svn', '.hg', 'venv', 'env', '.env',
+            'build', 'dist', 'target', 'bin', 'obj', 'out',
+            '.idea', '.vscode', '.eclipse',
+            'vendor', 'bower_components', '.bundle',
+            'logs', 'temp', 'tmp'
+        }
 
-    def scan_file(self, file_path: Path) -> list[tuple[str, int, str, str]]:
+    def scan_file(self, file_path: Path) -> List[Dict]:
         """æ‰«æå•ä¸ªæ–‡ä»¶"""
-        secrets = []
+        findings = []
 
         try:
-            with open(file_path, encoding="utf-8", errors="ignore") as f:
-                lines = f.readlines()
-
-            for line_num, line in enumerate(lines, 1):
-                line_content = line.strip()
-
-                # è·³è¿‡æ³¨é‡Šè¡Œ
-                if (
-                    line_content.startswith("#")
-                    or line_content.startswith('"""')
-                    or line_content.startswith("'''")
-                ):
+            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
+                content = f.read()
+
+                # æ£€æŸ¥æ¯ä¸€è¡Œ
+                for line_num, line in enumerate(content.split('\n'), 1):
+                    for severity, patterns in self.patterns.items():
+                        for pattern, description in patterns:
+                            if hasattr(pattern, 'finditer'):
+                                matches = list(pattern.finditer(line))
+                            else:
+                                # å¯¹äºå­—ç¬¦ä¸²ï¼Œä½¿ç”¨ re.finditer
+                                matches = [m for m in re.finditer(pattern, line)]
+
+                            for match in matches:
+                                findings.append({
+                                    'severity': severity,
+                                    'file': str(file_path),
+                                    'line': line_num,
+                                    'column': match.start() + 1,
+                                    'pattern': pattern if isinstance(pattern, str) else pattern.pattern,
+                                    'match': match.group(0),
+                                    'description': description,
+                                    'matched_text': match.groups()[0] if match.groups() else match.group(0),
+                                    'context': line.strip(),
+                                })
+
+        except (UnicodeDecodeError, PermissionError, IOError) as e:
+            # è®°å½•ä½†ä¸è¦ä¸­æ–­æ‰«æ
+            pass
+
+        return findings
+
+    def scan_directory(self, directory: Path, recursive: bool = True) -> None:
+        """æ‰«æç›®å½•"""
+        print(f"ğŸ” æ‰«æç›®å½•: {directory}")
+
+        for item in directory.iterdir():
+            if item.is_dir():
+                # æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤æ­¤ç›®å½•
+                if item.name in self.exclude_dirs:
                     continue
 
-                # è·³è¿‡æ˜æ˜¾çš„ç¤ºä¾‹ä»£ç 
-                if "example" in line_content.lower() or "dummy" in line_content.lower():
-                    continue
+                if recursive:
+                    self.scan_directory(item, recursive)
+
+            elif item.is_file():
+                # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
+                if self._should_include_file(item):
+                    self.scanned_files += 1
+                    findings = self.scan_file(item)
+                    self.findings.extend(findings)
+                else:
+                    self.skipped_files += 1
+
+    def _should_include_file(self, file_path: Path) -> bool:
+        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«æ­¤æ–‡ä»¶"""
+        ext = file_path.suffix.lower()
+
+        # æ£€æŸ¥æ’é™¤æ‰©å±•å
+        if ext in self.exclude_extensions:
+            return False
+
+        # æ£€æŸ¥åŒ…å«æ‰©å±•åï¼ˆå¦‚æœä¸¤è€…éƒ½ä¸ºç©ºï¼Œåˆ™åŒ…å«ï¼‰
+        if self.include_extensions and ext not in self.include_extensions:
+            return False
+
+        return True
+
+    def generate_report(self) -> Dict:
+        """ç”Ÿæˆæ‰«ææŠ¥å‘Š"""
+        high_count = len([f for f in self.findings if f['severity'] == 'HIGH'])
+        medium_count = len([f for f in self.findings if f['severity'] == 'MEDIUM'])
+        low_count = len([f for f in self.findings if f['severity'] == 'LOW'])
+
+        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
+        grouped_findings = {}
+        for severity in ['HIGH', 'MEDIUM', 'LOW']:
+            grouped_findings[severity] = [
+                f for f in self.findings if f['severity'] == severity
+            ]
+
+        # æŒ‰æ–‡ä»¶åˆ†ç»„
+        file_findings = {}
+        for finding in self.findings:
+            file_path = finding['file']
+            if file_path not in file_findings:
+                file_findings[file_path] = []
+            file_findings[file_path].append(finding)
+
+        return {
+            'summary': {
+                'total_findings': len(self.findings),
+                'high_severity': high_count,
+                'medium_severity': medium_count,
+                'low_severity': low_count,
+                'scanned_files': self.scanned_files,
+                'skipped_files': self.skipped_files,
+            },
+            'findings_by_severity': grouped_findings,
+            'findings_by_file': file_findings,
+            'all_findings': self.findings,
+        }
+
+    def print_report(self, report: Dict) -> None:
+        """æ‰“å°æ‰«ææŠ¥å‘Š"""
+        summary = report['summary']
+
+        print(f"\n{'='*60}")
+        print("ğŸ” æ•æ„Ÿä¿¡æ¯æ‰«ææŠ¥å‘Š")
+        print(f"{'='*60}")
+
+        print(f"\nğŸ“Š æ‰«æç»Ÿè®¡:")
+        print(f"  æ‰«ææ–‡ä»¶æ•°: {summary['scanned_files']}")
+        print(f"  è·³è¿‡æ–‡ä»¶æ•°: {summary['skipped_files']}")
+        print(f"  å‘ç°é—®é¢˜æ€»æ•°: {summary['total_findings']}")
+
+        print(f"\nğŸš¨ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
+        print(f"  ğŸ”´ é«˜å±: {summary['high_severity']}")
+        print(f"  ğŸŸ¡ ä¸­å±: {summary['medium_severity']}")
+        print(f"  ğŸŸ¢ ä½å±: {summary['low_severity']}")
+
+        # æ˜¾ç¤ºé«˜å±å’Œéƒ¨åˆ†ä¸­å±é—®é¢˜
+        if summary['high_severity'] > 0 or summary['medium_severity'] > 0:
+            print(f"\nâŒ å‘ç°çš„æ•æ„Ÿä¿¡æ¯:")
+
+            for severity in ['HIGH', 'MEDIUM']:
+                findings = report['findings_by_severity'].get(severity, [])
+                if findings:
+                    print(f"\n{severity} ä¸¥é‡ç¨‹åº¦ ({len(findings)} é¡¹):")
+
+                    # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤º
+                    file_groups = {}
+                    for finding in findings:
+                        file_path = finding['file']
+                        if file_path not in file_groups:
+                            file_groups[file_path] = []
+                        file_groups[file_path].append(finding)
+
+                    for file_path, file_findings in file_groups.items():
+                        print(f"\n  ğŸ“ {file_path}")
+                        for finding in file_findings[:3]:  # æ¯ä¸ªæ–‡ä»¶æœ€å¤šæ˜¾ç¤º3ä¸ªé—®é¢˜
+                            severity_emoji = {"HIGH": "ğŸ”´", "MEDIUM": "ğŸŸ¡", "LOW": "ğŸŸ¢"}[severity]
+                            print(f"    {severity_emoji} ç¬¬{finding['line']}è¡Œ: {finding['description']}")
+                            print(f"       {finding['context']}")
+                            print(f"       åŒ¹é…: {finding['matched_text']}")
+
+                        if len(file_findings) > 3:
+                            print(f"    ... è¿˜æœ‰ {len(file_findings) - 3} ä¸ªé—®é¢˜")
+
+        # ä¿®å¤å»ºè®®
+        if summary['total_findings'] > 0:
+            print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
+            print(f"  1. å°†æ•æ„Ÿä¿¡æ¯ç§»åŠ¨åˆ°ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶ä¸­")
+            print(f"  2. ä½¿ç”¨ os.getenv() æˆ–ç±»ä¼¼çš„å®‰å…¨é…ç½®ç®¡ç†æ–¹å¼")
+            print(f"  3. å¯¹äº API Keysï¼Œè€ƒè™‘ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡")
+            print(f"  4. å¯¹äºæ•°æ®åº“å¯†ç ï¼Œä½¿ç”¨è¿æ¥æ± æˆ–ç¯å¢ƒå˜é‡")
+            print(f"  5. ç¡®ä¿ .env æ–‡ä»¶è¢«æ·»åŠ åˆ° .gitignore")
+            print(f"  6. è€ƒè™‘ä½¿ç”¨ git-secrets é¢„æäº¤é’©å­é˜²æ­¢æ„å¤–æäº¤")
+
+    def write_report(self, report: Dict, output_path: str) -> None:
+        """å°†æŠ¥å‘Šå†™å…¥æ–‡ä»¶"""
+        import json
+
+        # å‡†å¤‡ JSON å‹å¥½çš„æŠ¥å‘Š
+        json_report = {
+            'scan_timestamp': str(__import__('datetime').datetime.now()),
+            'scanner_version': '1.0.0',
+            'report': report
+        }
 
-                for secret_type, patterns in self.patterns.items():
-                    for pattern in patterns:
-                        matches = re.finditer(pattern, line_content, re.IGNORECASE)
-                        for match in matches:
-                            secrets.append(
-                                (secret_type, line_num, line_content, match.group())
-                            )
-
-        except Exception as e:
-            print(f"è­¦å‘Šï¼šæ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")
-
-        return secrets
-
-    def scan_directory(self) -> None:
-        """æ‰«ææ•´ä¸ªç›®å½•"""
-        print("ğŸ” å¼€å§‹æ•æ„Ÿä¿¡æ¯æ‰«æ...")
-        print(f"ğŸ“ æ‰«æç›®å½•: {self.root_path.absolute()}")
-        print("=" * 60)
-
-        scanned_files = 0
-
-        for py_file in self.root_path.rglob("*.py"):
-            if not self.should_exclude_file(py_file):
-                scanned_files += 1
-                file_secrets = self.scan_file(py_file)
-
-                if file_secrets:
-                    self.secrets_found.extend(
-                        [(str(py_file), *secret) for secret in file_secrets]
-                    )
-
-        print(f"ğŸ“Š æ‰«æå®Œæˆï¼å…±æ£€æŸ¥ {scanned_files} ä¸ª Python æ–‡ä»¶")
-        print("=" * 60)
-
-    def report_results(self) -> bool:
-        """æŠ¥å‘Šæ‰«æç»“æœ"""
-        if not self.secrets_found:
-            print("âœ… æœªå‘ç°æ•æ„Ÿä¿¡æ¯æ³„éœ²")
-            print("ğŸ›¡ï¸ ä»£ç åº“å®‰å…¨æ€§æ£€æŸ¥é€šè¿‡")
-            return True
-
-        print(f"ğŸš¨ å‘ç° {len(self.secrets_found)} å¤„æ½œåœ¨æ•æ„Ÿä¿¡æ¯ï¼š")
-        print("=" * 80)
-
-        # æŒ‰æ–‡ä»¶åˆ†ç»„æ˜¾ç¤ºç»“æœ
-        file_secrets = {}
-        for file_path, secret_type, line_num, line_content, match in self.secrets_found:
-            if file_path not in file_secrets:
-                file_secrets[file_path] = []
-            file_secrets[file_path].append((secret_type, line_num, line_content, match))
-
-        for file_path, secrets in file_secrets.items():
-            print(f"\nğŸ“„ æ–‡ä»¶: {file_path}")
-            print("-" * 60)
-
-            for secret_type, line_num, line_content, match in secrets:
-                print(f"  ğŸ”´ ç±»å‹: {secret_type}")
-                print(f"  ğŸ“ è¡Œå·: {line_num}")
-                print(f"  ğŸ“ å†…å®¹: {line_content[:80]}...")
-                print(f"  ğŸ¯ åŒ¹é…: {match[:60]}...")
-                print()
-
-        print("=" * 80)
-        print("âš ï¸  è¯·ç«‹å³å¤„ç†ä»¥ä¸Šæ•æ„Ÿä¿¡æ¯ï¼")
-        print("ğŸ’¡ å»ºè®®ï¼š")
-        print("   1. ä½¿ç”¨ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯")
-        print("   2. ä½¿ç”¨ .env æ–‡ä»¶å¹¶ç¡®ä¿ .env åœ¨ .gitignore ä¸­")
-        print("   3. ä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼ˆå¦‚ AWS Secrets Managerï¼‰")
-
-        return False
+        with open(output_path, 'w', encoding='utf-8') as f:
+            json.dump(json_report, f, indent=2, ensure_ascii=False)
 
 
 def main():
     """ä¸»å‡½æ•°"""
-    if len(sys.argv) > 1:
-        root_path = sys.argv[1]
-    else:
-        root_path = "."
+    print("ğŸš€ P4-3 æ•æ„Ÿä¿¡æ¯æ‰«æå¼€å§‹\n")
+
+    # åˆå§‹åŒ–æ‰«æå™¨
+    scanner = SecretScanner()
+
+    # æ‰«ææºä»£ç ç›®å½•
+    src_dir = Path("src")
+    config_dir = Path("config")
+
+    if src_dir.exists():
+        scanner.scan_directory(src_dir, recursive=True)
 
-    scanner = SecretScanner(root_path)
+    if config_dir.exists():
+        scanner.scan_directory(config_dir, recursive=True)
 
-    try:
-        scanner.scan_directory()
-        is_safe = scanner.report_results()
+    # ç”ŸæˆæŠ¥å‘Š
+    report = scanner.generate_report()
 
-        if not is_safe:
-            print("\nâŒ å®‰å…¨æ£€æŸ¥å¤±è´¥ï¼")
-            sys.exit(1)
-        else:
-            print("\nâœ… å®‰å…¨æ£€æŸ¥é€šè¿‡ï¼")
-            sys.exit(0)
+    # æ‰“å°æŠ¥å‘Š
+    scanner.print_report(report)
 
-    except KeyboardInterrupt:
-        print("\nâ¹ï¸  æ‰«æè¢«ç”¨æˆ·ä¸­æ–­")
-        sys.exit(130)
-    except Exception as e:
-        print(f"\nğŸ’¥ æ‰«æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
-        sys.exit(1)
+    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
+    os.makedirs("reports", exist_ok=True)
+    output_path = "reports/secrets_scan.json"
+    scanner.write_report(report, output_path)
+
+    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_path}")
+
+    # è¿”å›é€‚å½“çš„é€€å‡ºç 
+    if report['summary']['high_severity'] > 0:
+        print(f"\nâŒ å‘ç°é«˜å±æ•æ„Ÿä¿¡æ¯ï¼Œè¯·ç«‹å³ä¿®å¤ï¼")
+        return 2
+    elif report['summary']['medium_severity'] > 0:
+        print(f"\nâš ï¸ å‘ç°ä¸­å±æ•æ„Ÿä¿¡æ¯ï¼Œå»ºè®®ä¿®å¤")
+        return 1
+    else:
+        print(f"\nâœ… æœªå‘ç°ä¸¥é‡æ•æ„Ÿä¿¡æ¯")
+        return 0
 
 
 if __name__ == "__main__":
-    main()
+    sys.exit(main())
diff --git a/src/api/prometheus_metrics.py b/src/api/prometheus_metrics.py
index c17738eef..bcd20d683 100644
--- a/src/api/prometheus_metrics.py
+++ b/src/api/prometheus_metrics.py
@@ -1,10 +1,11 @@
 """Prometheus Metrics API
-PrometheusæŒ‡æ ‡APIç«¯ç‚¹.
+P4-1: å¢å¼ºçš„PrometheusæŒ‡æ ‡APIç«¯ç‚¹.
 """
 
 import logging
+from typing import Optional
 
-from fastapi import APIRouter, Response
+from fastapi import APIRouter, Response, Query
 from prometheus_client import (
     CONTENT_TYPE_LATEST,
     Counter,
@@ -15,6 +16,22 @@ from prometheus_client import (
 
 logger = logging.getLogger(__name__)
 
+# P4-1: å°è¯•å¯¼å…¥å¢å¼ºçš„ç›‘æ§æ¨¡å—
+try:
+    from src.monitoring.prometheus_instrumentator import (
+        HTTP_REQUESTS_TOTAL,
+        HTTP_REQUEST_DURATION_SECONDS,
+        PREDICTION_REQUESTS_TOTAL,
+        PREDICTION_REQUEST_DURATION_SECONDS,
+        ACTIVE_CONNECTIONS,
+        generate_latest as enhanced_generate_latest,
+    )
+    ENHANCED_MONITORING_AVAILABLE = True
+    logger.info("âœ… å¢å¼ºç›‘æ§æ¨¡å—åŠ è½½æˆåŠŸ")
+except ImportError as e:
+    ENHANCED_MONITORING_AVAILABLE = False
+    logger.warning(f"âš ï¸ å¢å¼ºç›‘æ§æ¨¡å—åŠ è½½å¤±è´¥: {e}")
+
 # åˆ›å»ºPrometheusæŒ‡æ ‡
 router = APIRouter(prefix="/metrics", tags=["monitoring"])
 
@@ -54,18 +71,97 @@ system_memory_usage = Gauge(
 
 
 @router.get("/")
-async def prometheus_metrics():
-    """PrometheusæŒ‡æ ‡ç«¯ç‚¹."""
+async def prometheus_metrics(
+    format: Optional[str] = Query(None, description="è¾“å‡ºæ ¼å¼: prometheus æˆ– json"),
+):
+    """P4-1: å¢å¼ºçš„PrometheusæŒ‡æ ‡ç«¯ç‚¹."""
     try:
         # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
         await update_system_metrics()
 
-        # è¿”å›Prometheusæ ¼å¼çš„æŒ‡æ ‡
-        metrics_data = generate_latest()
-        return Response(content=metrics_data, media_type=CONTENT_TYPE_LATEST)
+        # æ ¹æ®æ ¼å¼å‚æ•°è¿”å›ä¸åŒæ ¼å¼çš„æ•°æ®
+        if format == "json":
+            return await get_metrics_json()
+        else:
+            # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç›‘æ§æ¨¡å—
+            if ENHANCED_MONITORING_AVAILABLE:
+                metrics_data = enhanced_generate_latest()
+            else:
+                metrics_data = generate_latest()
+
+            return Response(content=metrics_data, media_type=CONTENT_TYPE_LATEST)
+
     except Exception as e:
         logger.error(f"Error generating metrics: {e}")
-        return Response(content="# Error generating metrics", media_type="text/plain")
+        return Response(
+            content="# Error generating metrics\n" + str(e),
+            media_type="text/plain",
+            status_code=500
+        )
+
+
+@router.get("/json")
+async def prometheus_metrics_json():
+    """JSONæ ¼å¼çš„æŒ‡æ ‡ç«¯ç‚¹ï¼ˆå…¼å®¹æ€§æ¥å£ï¼‰."""
+    try:
+        await update_system_metrics()
+        return await get_metrics_json()
+    except Exception as e:
+        logger.error(f"Error generating JSON metrics: {e}")
+        return {"error": str(e), "status": "failed"}
+
+
+async def get_metrics_json():
+    """è·å–JSONæ ¼å¼çš„æŒ‡æ ‡æ•°æ®."""
+    try:
+        import psutil
+
+        # è·å–ç³»ç»ŸæŒ‡æ ‡
+        cpu_percent = psutil.cpu_percent(interval=1)
+        memory = psutil.virtual_memory()
+        disk = psutil.disk_usage('/')
+
+        metrics = {
+            "timestamp": logger.handlers[0].formatter.formatTime(
+                logger.makeRecord(
+                    "metrics", logging.INFO, "", (), "", ""
+                )
+            ) if logger.handlers else "",
+            "system": {
+                "cpu_usage_percent": cpu_percent,
+                "memory_usage_percent": memory.percent,
+                "memory_available_gb": memory.available / (1024**3),
+                "disk_usage_percent": (disk.used / disk.total) * 100,
+                "disk_free_gb": disk.free / (1024**3),
+            },
+            "application": {
+                "http_requests_total": HTTP_REQUESTS_TOTAL._value._sum._value if ENHANCED_MONITORING_AVAILABLE else 0,
+                "active_connections": {
+                    "database": ACTIVE_CONNECTIONS.labels(connection_type="database")._value._value if ENHANCED_MONITORING_AVAILABLE else 0,
+                    "redis": ACTIVE_CONNECTIONS.labels(connection_type="redis")._value._value if ENHANCED_MONITORING_AVAILABLE else 0,
+                },
+            },
+            "monitoring": {
+                "enhanced_monitoring": ENHANCED_MONITORING_AVAILABLE,
+                "prometheus_available": True,
+            }
+        }
+
+        return metrics
+
+    except Exception as e:
+        logger.error(f"Error collecting system metrics: {e}")
+        # è¿”å›åŸºæœ¬æŒ‡æ ‡
+        return {
+            "error": str(e),
+            "timestamp": "",
+            "system": {},
+            "application": {},
+            "monitoring": {
+                "enhanced_monitoring": ENHANCED_MONITORING_AVAILABLE,
+                "prometheus_available": False,
+            }
+        }
 
 
 async def update_system_metrics():
diff --git a/src/core/cache_main.py b/src/core/cache_main.py
index 2f79bf4eb..43171ba56 100644
--- a/src/core/cache_main.py
+++ b/src/core/cache_main.py
@@ -469,15 +469,15 @@ def cache_key_builder(
         if isinstance(arg, (str, int, float, bool)):
             parts.append(str(arg))
         else:
-            # å¯¹äºå¤æ‚å¯¹è±¡ï¼Œä½¿ç”¨å“ˆå¸Œ
-            parts.append(hashlib.md5(str(arg).encode()).hexdigest()[:8])
+            # å¯¹äºå¤æ‚å¯¹è±¡ï¼Œä½¿ç”¨å®‰å…¨å“ˆå¸Œ
+            parts.append(hashlib.sha256(str(arg).encode()).hexdigest()[:8])
 
     # æ·»åŠ å…³é”®å­—å‚æ•°
     for key, value in sorted(kwargs.items()):
         if isinstance(value, (str, int, float, bool)):
             parts.append(f"{key}:{value}")
         else:
-            parts.append(f"{key}:{hashlib.md5(str(value).encode()).hexdigest()[:8]}")
+            parts.append(f"{key}:{hashlib.sha256(str(value).encode()).hexdigest()[:8]}")
 
     return ":".join(parts)
 
diff --git a/src/evaluation/report_builder.py b/src/evaluation/report_builder.py
index 5362420ea..147b30ba9 100644
--- a/src/evaluation/report_builder.py
+++ b/src/evaluation/report_builder.py
@@ -65,8 +65,8 @@ class ReportBuilder:
         template_str = self._get_html_template()
         template = Template(template_str)
 
-        # åˆ›å»ºç®€å•çš„ç¯å¢ƒ
-        env = Environment()
+        # åˆ›å»ºå®‰å…¨çš„ç¯å¢ƒï¼ˆå¯ç”¨è‡ªåŠ¨è½¬ä¹‰ï¼‰
+        env = Environment(autoescape=True)
         env.globals['template'] = template
 
         return env
diff --git a/src/inference/loader.py b/src/inference/loader.py
index ab73bf3e4..9b0160071 100644
--- a/src/inference/loader.py
+++ b/src/inference/loader.py
@@ -70,11 +70,11 @@ class ModelMetadata:
 
     def _calculate_file_hash(self, file_path: str) -> str:
         """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
-        hash_md5 = hashlib.md5()
+        hash_sha256 = hashlib.sha256()
         with open(file_path, "rb") as f:
             for chunk in iter(lambda: f.read(4096), b""):
-                hash_md5.update(chunk)
-        return hash_md5.hexdigest()
+                hash_sha256.update(chunk)
+        return hash_sha256.hexdigest()
 
     def is_modified(self) -> bool:
         """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹"""
diff --git a/src/inference/predictor.py b/src/inference/predictor.py
index 15aeb6d82..78bff2f67 100644
--- a/src/inference/predictor.py
+++ b/src/inference/predictor.py
@@ -547,9 +547,9 @@ class Predictor:
         ]
 
         if request.features:
-            # å¯¹è‡ªå®šä¹‰ç‰¹å¾è¿›è¡Œå“ˆå¸Œ
+            # å¯¹è‡ªå®šä¹‰ç‰¹å¾è¿›è¡Œå®‰å…¨å“ˆå¸Œ
             import hashlib
-            features_hash = hashlib.md5(
+            features_hash = hashlib.sha256(
                 str(sorted(request.features.items())).encode()
             ).hexdigest()[:8]
             key_parts.append(features_hash)
diff --git a/src/jobs/run_season_backfill.py b/src/jobs/run_season_backfill.py
index d1e6de0ad..a4b8a948d 100644
--- a/src/jobs/run_season_backfill.py
+++ b/src/jobs/run_season_backfill.py
@@ -92,7 +92,7 @@ class SeasonBackfillJob:
                         headers=self.html_collector._get_current_headers(),
                         timeout=self.html_collector.timeout,
                         allow_redirects=True,
-                        verify=False,
+                        verify=True,  # å¯ç”¨ SSL è¯ä¹¦éªŒè¯
                     )
 
                     self.logger.info(
diff --git a/src/jobs/run_season_fixtures.py b/src/jobs/run_season_fixtures.py
index 39b7e26d9..9f7ec1930 100644
--- a/src/jobs/run_season_fixtures.py
+++ b/src/jobs/run_season_fixtures.py
@@ -361,7 +361,7 @@ async def main():
             headers=collector._get_current_headers(),
             timeout=collector.timeout,
             allow_redirects=True,
-            verify=False,
+            verify=True,  # å¯ç”¨ SSL è¯ä¹¦éªŒè¯
         )
 
         logger.info(
diff --git a/src/main.py b/src/main.py
index ccea82fdf..faf000644 100644
--- a/src/main.py
+++ b/src/main.py
@@ -10,6 +10,7 @@ from contextlib import asynccontextmanager
 import uvicorn
 from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
 from fastapi.middleware.cors import CORSMiddleware
+from fastapi.responses import Response
 
 # å¯é€‰çš„é€Ÿç‡é™åˆ¶åŠŸèƒ½
 try:
@@ -42,13 +43,28 @@ from src.observers import ObserverManager
 from src.performance.integration import setup_performance_monitoring
 from src.performance.middleware import PerformanceMonitoringMiddleware
 
-# é…ç½®æ—¥å¿—
+# P4-2: ç»“æ„åŒ–æ—¥å¿—é…ç½®
+import os
+from src.core.logging_config import setup_logging, get_logger
+from src.api.middleware.request_id import add_request_id_middleware
+
+# P4-3: å®‰å…¨å¤´ä¸­é—´ä»¶
+from src.api.middleware.security import add_security_middleware
+
+# è®¾ç½®ç»Ÿä¸€æ—¥å¿—ç³»ç»Ÿ
 warnings.filterwarnings("ignore", category=DeprecationWarning)
-logging.basicConfig(
-    level=logging.INFO,
-    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
-)
-logger = logging.getLogger(__name__)
+log_level = os.getenv("LOG_LEVEL", "INFO")
+setup_logging(service_name="football-prediction", log_level=log_level)
+logger = get_logger(__name__)
+
+# P4-1: Prometheus ç›‘æ§é›†æˆ
+try:
+    from src.monitoring.prometheus_instrumentator import create_instrumentator, generate_latest
+    PROMETHEUS_AVAILABLE = True
+    logger.info("âœ… Prometheus ç›‘æ§æ¨¡å—åŠ è½½æˆåŠŸ")
+except ImportError as e:
+    PROMETHEUS_AVAILABLE = False
+    logger.warning(f"âš ï¸ Prometheus ç›‘æ§æ¨¡å—åŠ è½½å¤±è´¥: {e}")
 
 
 async def check_and_trigger_initial_data_fill() -> None:
@@ -280,20 +296,58 @@ app = FastAPI(
     lifespan=lifespan,
 )
 
-# é…ç½®CORS
-app.add_middleware(
-    CORSMiddleware,
-    allow_origins=[
+# P4-1: é›†æˆ Prometheus ç›‘æ§
+if PROMETHEUS_AVAILABLE:
+    instrumentator = create_instrumentator()
+    instrumentator.instrument(app)
+    logger.info("âœ… Prometheus ç›‘æ§å·²é›†æˆåˆ° FastAPI åº”ç”¨")
+else:
+    instrumentator = None
+    logger.warning("âš ï¸ Prometheus ç›‘æ§æœªé›†æˆ")
+
+# P4-2: æ·»åŠ  Request ID ä¸­é—´ä»¶ï¼ˆå¿…é¡»åœ¨å…¶ä»–ä¸­é—´ä»¶ä¹‹å‰ï¼‰
+add_request_id_middleware(app)
+logger.info("âœ… Request ID ä¸­é—´ä»¶å·²é›†æˆåˆ° FastAPI åº”ç”¨")
+
+# P4-3: æ·»åŠ å®‰å…¨å¤´ä¸­é—´ä»¶
+add_security_middleware(app)
+logger.info("âœ… å®‰å…¨å¤´ä¸­é—´ä»¶å·²é›†æˆåˆ° FastAPI åº”ç”¨")
+
+# é…ç½®CORS - å®‰å…¨çš„è·¨åŸŸèµ„æºå…±äº«é…ç½®
+environment = os.getenv("ENV", "development").lower()
+
+if environment in ["production", "prod"]:
+    # ç”Ÿäº§ç¯å¢ƒï¼šåªå…è®¸æŒ‡å®šåŸŸå
+    allowed_origins = os.getenv(
+        "CORS_ORIGINS",
+        "https://yourdomain.com,https://www.yourdomain.com"
+    ).split(",")
+    allowed_origins = [origin.strip() for origin in allowed_origins if origin.strip()]
+else:
+    # å¼€å‘ç¯å¢ƒï¼šå…è®¸æœ¬åœ°å¼€å‘åŸŸå
+    allowed_origins = [
         "http://localhost:3000",  # Reactå‰ç«¯å¼€å‘æœåŠ¡å™¨
         "http://127.0.0.1:3000",
         "http://localhost:3001",  # Reactå‰ç«¯å¼€å‘æœåŠ¡å™¨ï¼ˆå¤‡ç”¨ç«¯å£ï¼‰
         "http://127.0.0.1:3001",
         "http://localhost:8000",  # æœ¬åœ°å¼€å‘
         "http://127.0.0.1:8000",
-    ],
+    ]
+
+app.add_middleware(
+    CORSMiddleware,
+    allow_origins=allowed_origins,
     allow_credentials=True,
     allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
-    allow_headers=["*"],
+    allow_headers=[
+        "accept",
+        "accept-language",
+        "content-language",
+        "content-type",
+        "authorization",
+        "x-request-id",
+        "x-client-version",
+    ],
 )
 
 # æ·»åŠ æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
@@ -318,7 +372,7 @@ app.include_router(predictions_router, prefix="/api/v1", tags=["é¢„æµ‹"])
 app.include_router(
     optimized_predictions_router, prefix="/api/v2/predictions", tags=["é¢„æµ‹"]
 )
-app.include_router(prometheus_router, prefix="/metrics", tags=["ç›‘æ§"])
+app.include_router(prometheus_router, tags=["ç›‘æ§"])
 
 # é…ç½®OpenAPI
 setup_openapi(app)
