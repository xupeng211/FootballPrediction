#!/usr/bin/env python3
"""
è¿è¡Œæµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
"""

import subprocess
import sys
import os
import json
from datetime import datetime
from pathlib import Path


def run_command_with_env(cmd, env_vars=None):
    """è¿è¡Œå‘½ä»¤å¹¶è®¾ç½®ç¯å¢ƒå˜é‡"""
    env = os.environ.copy()
    if env_vars:
        env.update(env_vars)

    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        env=env
    )
    return result


def generate_test_report():
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸš€ æµ‹è¯•è¿è¡Œä¸æŠ¥å‘Šç”Ÿæˆ")
    print("="*80)

    report = {
        "timestamp": datetime.now().isoformat(),
        "python_version": sys.version,
        "tests": {}
    }

    # 1. è¿è¡Œutilsæ¨¡å—æµ‹è¯•
    print("\n1ï¸âƒ£ è¿è¡Œutilsæ¨¡å—æµ‹è¯•...")
    utils_modules = [
        "validators",
        "crypto_utils",
        "string_utils",
        "time_utils",
        "file_utils"
    ]

    total_passed = 0
    total_failed = 0
    total_tests = 0

    for module in utils_modules:
        test_file = f"tests/unit/test_{module}_optimized.py"
        if os.path.exists(test_file):
            print(f"\nğŸ” è¿è¡Œ {module} æµ‹è¯•...")

            result = run_command_with_env([
                "python", "-m", "pytest",
                test_file,
                "--tb=no",
                "-q",
                "--disable-warnings"
            ])

            if result.returncode == 0:
                print(f"  âœ… {module} æµ‹è¯•é€šè¿‡")
                status = "PASSED"
            else:
                print(f"  âŒ {module} æµ‹è¯•å¤±è´¥")
                status = "FAILED"

            # è§£æè¾“å‡º
            output = result.stdout or result.stderr
            test_count = output.count("passed") + output.count("failed")

            report["tests"][module] = {
                "status": status,
                "test_count": test_count,
                "output": output[-500:]  # åªä¿ç•™æœ€å500å­—ç¬¦
            }

            if status == "PASSED":
                total_passed += test_count
            else:
                total_failed += test_count
            total_tests += test_count

    # 2. è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
    print("\n2ï¸âƒ£ è¿è¡Œè¦†ç›–ç‡æµ‹è¯•...")
    coverage_result = run_command_with_env([
        "python", "-m", "pytest",
        "tests/unit/test_validators_optimized.py",
        "tests/unit/test_string_utils_optimized.py",
        "tests/unit/test_time_utils_optimized.py",
        "--cov=src.utils",
        "--cov-report=json:coverage_report.json",
        "--cov-report=term-missing",
        "--tb=no",
        "-q",
        "--disable-warnings"
    ])

    # è¯»å–è¦†ç›–ç‡æŠ¥å‘Š
    coverage_data = {}
    if os.path.exists("coverage_report.json"):
        with open("coverage_report.json", "r") as f:
            coverage_json = json.load(f)
            coverage_data = {
                "total_coverage": coverage_json.get("totals", {}).get("percent_covered", 0),
                "files": {}
            }

            for file_path, file_data in coverage_json.get("files", {}).items():
                if "utils/" in file_path:
                    filename = file_path.split("/")[-1]
                    coverage_data["files"][filename] = {
                        "statements": file_data.get("summary", {}).get("num_statements", 0),
                        "missing": file_data.get("summary", {}).get("missing_lines", 0),
                        "coverage": file_data.get("summary", {}).get("percent_covered", 0)
                    }

    # 3. æ›´æ–°æŠ¥å‘Š
    report["summary"] = {
        "total_tests": total_tests,
        "passed": total_passed,
        "failed": total_failed,
        "success_rate": f"{(total_passed/total_tests*100):.1f}%" if total_tests > 0 else "0%"
    }
    report["coverage"] = coverage_data

    # 4. ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶
    with open("TEST_EXECUTION_REPORT.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

    # 5. ç”ŸæˆMarkdownæŠ¥å‘Š
    md_report = f"""# æµ‹è¯•æ‰§è¡ŒæŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: {report['timestamp']}

## ğŸ“Š æµ‹è¯•æ‘˜è¦

- **æ€»æµ‹è¯•æ•°**: {report['summary']['total_tests']}
- **é€šè¿‡**: {report['summary']['passed']}
- **å¤±è´¥**: {report['summary']['failed']}
- **æˆåŠŸç‡**: {report['summary']['success_rate']}

## ğŸ§ª æ¨¡å—æµ‹è¯•çŠ¶æ€

"""

    for module, data in report["tests"].items():
        status_icon = "âœ…" if data["status"] == "PASSED" else "âŒ"
        md_report += f"### {status_icon} {module}\n"
        md_report += f"- çŠ¶æ€: {data['status']}\n"
        md_report += f"- æµ‹è¯•æ•°: {data['test_count']}\n\n"

    if coverage_data:
        md_report += f"""## ğŸ“ˆ è¦†ç›–ç‡æŠ¥å‘Š

- **æ€»è¦†ç›–ç‡**: {coverage_data.get('total_coverage', 0):.2f}%

### å„æ–‡ä»¶è¦†ç›–ç‡

"""

        for filename, data in coverage_data.get("files", {}).items():
            md_report += f"- **{filename}**: {data['coverage']:.1f}% ({data['statements'] - data['missing']}/{data['statements']} è¡Œ)\n"

    with open("TEST_EXECUTION_REPORT.md", "w", encoding="utf-8") as f:
        f.write(md_report)

    # 6. æ‰“å°æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ‰§è¡Œæ‘˜è¦")
    print("="*80)
    print(f"æ€»æµ‹è¯•æ•°: {report['summary']['total_tests']}")
    print(f"é€šè¿‡: {report['summary']['passed']} âœ…")
    print(f"å¤±è´¥: {report['summary']['failed']} âŒ")
    print(f"æˆåŠŸç‡: {report['summary']['success_rate']}")

    if coverage_data:
        print(f"\nè¦†ç›–ç‡: {coverage_data.get('total_coverage', 0):.2f}%")

    print("\nğŸ“„ æŠ¥å‘Šå·²ç”Ÿæˆ:")
    print("- TEST_EXECUTION_REPORT.md")
    print("- TEST_EXECUTION_REPORT.json")

    return report


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è¿è¡Œæµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š...")

    # æ£€æŸ¥ç¯å¢ƒ
    print("\næ£€æŸ¥ç¯å¢ƒ...")
    if not os.path.exists("src/utils"):
        print("âŒ æœªæ‰¾åˆ°src/utilsç›®å½•")
        return False

    if not os.path.exists("tests"):
        print("âŒ æœªæ‰¾åˆ°testsç›®å½•")
        return False

    # ç”ŸæˆæŠ¥å‘Š
    report = generate_test_report()

    # æä¾›ä¸‹ä¸€æ­¥å»ºè®®
    print("\n" + "="*80)
    print("ğŸ“‹ ä¸‹ä¸€æ­¥å»ºè®®")
    print("="*80)

    success_rate = float(report['summary']['success_rate'].rstrip('%'))

    if success_rate >= 80:
        print("âœ… æµ‹è¯•çŠ¶æ€è‰¯å¥½ï¼")
        print("ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. è¿è¡Œæ›´å¤šæ¨¡å—æµ‹è¯•: pytest tests/unit/")
        print("2. æ£€æŸ¥é›†æˆæµ‹è¯•: pytest tests/integration/")
        print("3. è¿è¡Œå®Œæ•´æµ‹è¯•: make test")
    elif success_rate >= 60:
        print("âš ï¸ æµ‹è¯•çŠ¶æ€ä¸€èˆ¬ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–")
        print("ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•")
        print("2. è¿è¡Œ: pytest --tb=short æŸ¥çœ‹è¯¦ç»†é”™è¯¯")
        print("3. ä¿®å¤å¤±è´¥çš„æµ‹è¯•")
    else:
        print("âŒ æµ‹è¯•çŠ¶æ€éœ€è¦æ”¹è¿›")
        print("ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. æ£€æŸ¥æµ‹è¯•ç¯å¢ƒé…ç½®")
        print("2. ç¡®è®¤æ¨¡å—å¯¼å…¥æ­£ç¡®")
        print("3. é€ä¸ªè¿è¡Œæµ‹è¯•æ‰¾å‡ºé—®é¢˜")

    print("\nğŸ“Š å½“å‰çŠ¶æ€:")
    print(f"- Utilsæ¨¡å—æµ‹è¯•: {success_rate:.0f}% æˆåŠŸ")
    print(f"- æµ‹è¯•æ–‡ä»¶æ•°: 5ä¸ª (validators, crypto_utils, string_utils, time_utils, file_utils)")
    print(f"- è¦†ç›–ç‡ç›®æ ‡: 73.5%+ (utilsæ¨¡å—)")
    print(f"- æ•´ä½“è¦†ç›–ç‡ç›®æ ‡: 30%+")

    print("\nâœ… å®Œæˆ!")
    return True


if __name__ == "__main__":
    main()