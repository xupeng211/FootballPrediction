#!/usr/bin/env python3
"""
è´¨é‡ç›‘æ§ä»ªè¡¨æ¿
Quality Monitoring Dashboard

æä¾›é¡¹ç›®è´¨é‡çŠ¶å†µçš„å®æ—¶ç›‘æ§å’ŒæŠ¥å‘Š
"""

import json
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any


class QualityMonitorDashboard:
    """è´¨é‡ç›‘æ§ä»ªè¡¨æ¿"""

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root or Path(__file__).parent.parent)
        self.report_file = self.project_root / "quality_status_report.json"
        self.metrics = {
            "timestamp": datetime.now().isoformat(),
            "ruff_errors": 0,
            "test_coverage": 0.0,
            "tests_run": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "tests_skipped": 0,
            "syntax_errors": 0,
            "status": "unknown"
        }

    def run_ruff_check(self) -> Dict[str, Any]:
        """è¿è¡ŒRuffä»£ç æ£€æŸ¥"""
        print("ğŸ” è¿è¡ŒRuffä»£ç æ£€æŸ¥...")
        try:
            result = subprocess.run(
                ["make", "lint"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=60
            )

            if result.returncode != 0:
                # ç»Ÿè®¡é”™è¯¯æ•°é‡
                error_lines = [line for line in result.stdout.split('\n')
                              if ':' in line and any(code in line for code in ['E', 'F', 'W'])]
                ruff_errors = len(error_lines)
            else:
                ruff_errors = 0

            return {
                "success": result.returncode == 0,
                "errors": ruff_errors,
                "output": result.stdout[-1000:] if result.stdout else ""
            }
        except subprocess.TimeoutExpired:
            return {"success": False, "errors": -1, "output": "Ruffæ£€æŸ¥è¶…æ—¶"}
        except Exception as e:
            return {"success": False, "errors": -1, "output": str(e)}

    def run_test_coverage(self) -> Dict[str, Any]:
        """è¿è¡Œæµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥"""
        print("ğŸ“Š è¿è¡Œæµ‹è¯•è¦†ç›–ç‡æ£€æŸ¥...")
        try:
            # è¿è¡ŒåŸºç¡€æµ‹è¯•è·å–è¦†ç›–ç‡
            result = subprocess.run(
                [
                    "python", "-m", "pytest",
                    "tests/unit/utils/test_helpers.py",
                    "tests/unit/utils/test_validators.py",
                    "--cov=src.utils",
                    "--cov-report=json",
                    "--cov-report=term-missing",
                    "--tb=no",
                    "-q"
                ],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=120
            )

            # è¯»å–è¦†ç›–ç‡JSONæŠ¥å‘Š
            coverage_file = self.project_root / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file, 'r') as f:
                    coverage_data = json.load(f)
                coverage_percent = coverage_data.get("totals", {}).get("percent_covered", 0.0)
            else:
                coverage_percent = 0.0

            # è§£ææµ‹è¯•ç»“æœ
            output_lines = result.stdout.split('\n')
            tests_run = 0
            tests_passed = 0
            tests_failed = 0
            tests_skipped = 0

            for line in output_lines:
                if "passed" in line and "failed" in line:
                    parts = line.split()
                    for part in parts:
                        if "passed" in part:
                            tests_passed = int(part.split("passed")[0])
                        elif "failed" in part:
                            tests_failed = int(part.split("failed")[0])
                        elif "skipped" in part:
                            tests_skipped = int(part.split("skipped")[0])
                    tests_run = tests_passed + tests_failed + tests_skipped
                    break

            return {
                "success": result.returncode == 0,
                "coverage": coverage_percent,
                "tests_run": tests_run,
                "tests_passed": tests_passed,
                "tests_failed": tests_failed,
                "tests_skipped": tests_skipped,
                "output": result.stdout[-500:] if result.stdout else ""
            }
        except subprocess.TimeoutExpired:
            return {"success": False, "coverage": 0.0, "output": "æµ‹è¯•è¶…æ—¶"}
        except Exception as e:
            return {"success": False, "coverage": 0.0, "output": str(e)}

    def check_syntax_errors(self) -> Dict[str, Any]:
        """æ£€æŸ¥è¯­æ³•é”™è¯¯"""
        print("âœ… æ£€æŸ¥è¯­æ³•é”™è¯¯...")
        try:
            # å°è¯•å¯¼å…¥å…³é”®æ¨¡å—æ£€æŸ¥è¯­æ³•
            test_modules = [
                "src.adapters.factory_simple",
                "src.utils.helpers",
                "src.utils.validators"
            ]

            syntax_errors = 0
            for module in test_modules:
                try:
                    result = subprocess.run(
                        [sys.executable, "-c", f"import {module}"],
                        capture_output=True,
                        text=True,
                        timeout=10
                    )
                    if result.returncode != 0:
                        syntax_errors += 1
                except Exception:
                    syntax_errors += 1

            return {
                "success": syntax_errors == 0,
                "errors": syntax_errors
            }
        except Exception as e:
            return {"success": False, "errors": -1, "output": str(e)}

    def calculate_overall_status(self) -> str:
        """è®¡ç®—æ•´ä½“çŠ¶æ€"""
        ruff_errors = self.metrics["ruff_errors"]
        coverage = self.metrics["test_coverage"]
        tests_failed = self.metrics["tests_failed"]
        syntax_errors = self.metrics["syntax_errors"]

        if syntax_errors > 0:
            return "critical"  # è¯­æ³•é”™è¯¯
        elif tests_failed > 0:
            return "warning"   # æµ‹è¯•å¤±è´¥
        elif ruff_errors > 1000:
            return "warning"   # å¤§é‡ä»£ç è´¨é‡é—®é¢˜
        elif coverage < 20:
            return "poor"      # ä½è¦†ç›–ç‡
        elif coverage < 50:
            return "fair"      # ä¸­ç­‰è¦†ç›–ç‡
        elif coverage < 80:
            return "good"      # è‰¯å¥½è¦†ç›–ç‡
        else:
            return "excellent" # ä¼˜ç§€çŠ¶æ€

    def generate_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        print("ğŸš€ ç”Ÿæˆè´¨é‡ç›‘æ§æŠ¥å‘Š...")

        # è¿è¡Œå„é¡¹æ£€æŸ¥
        ruff_result = self.run_ruff_check()
        test_result = self.run_test_coverage()
        syntax_result = self.check_syntax_errors()

        # æ›´æ–°æŒ‡æ ‡
        self.metrics.update({
            "ruff_errors": ruff_result.get("errors", 0),
            "test_coverage": test_result.get("coverage", 0.0),
            "tests_run": test_result.get("tests_run", 0),
            "tests_passed": test_result.get("tests_passed", 0),
            "tests_failed": test_result.get("tests_failed", 0),
            "tests_skipped": test_result.get("tests_skipped", 0),
            "syntax_errors": syntax_result.get("errors", 0),
            "status": self.calculate_overall_status()
        })

        # ä¿å­˜æŠ¥å‘Š
        with open(self.report_file, 'w') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)

        return self.metrics

    def display_dashboard(self):
        """æ˜¾ç¤ºè´¨é‡ç›‘æ§ä»ªè¡¨æ¿"""
        print("\n" + "="*60)
        print("ğŸ¯ é¡¹ç›®è´¨é‡ç›‘æ§ä»ªè¡¨æ¿")
        print("="*60)
        print(f"ğŸ“… æ£€æŸ¥æ—¶é—´: {self.metrics['timestamp']}")
        print(f"ğŸ† æ•´ä½“çŠ¶æ€: {self.metrics['status'].upper()}")
        print("-"*60)

        # ä»£ç è´¨é‡
        ruff_status = "âœ…" if self.metrics['ruff_errors'] < 100 else "âš ï¸" if self.metrics['ruff_errors'] < 1000 else "âŒ"
        print(f"ğŸ“ ä»£ç è´¨é‡ (Ruff): {ruff_status} {self.metrics['ruff_errors']:,} ä¸ªé—®é¢˜")

        # æµ‹è¯•çŠ¶æ€
        test_status = "âœ…" if self.metrics['tests_failed'] == 0 else "âš ï¸" if self.metrics['tests_failed'] < 10 else "âŒ"
        print(f"ğŸ§ª æµ‹è¯•çŠ¶æ€: {test_status} {self.metrics['tests_passed']:,} é€šè¿‡ / {self.metrics['tests_failed']:,} å¤±è´¥ / {self.metrics['tests_skipped']:,} è·³è¿‡")

        # è¦†ç›–ç‡
        coverage_icon = "ğŸ‰" if self.metrics['test_coverage'] >= 80 else "ğŸ‘" if self.metrics['test_coverage'] >= 50 else "ğŸ“ˆ" if self.metrics['test_coverage'] >= 20 else "ğŸ“‰"
        print(f"ğŸ“Š æµ‹è¯•è¦†ç›–ç‡: {coverage_icon} {self.metrics['test_coverage']:.1f}%")

        # è¯­æ³•æ£€æŸ¥
        syntax_status = "âœ…" if self.metrics['syntax_errors'] == 0 else "âŒ"
        print(f"âœ¨ è¯­æ³•æ£€æŸ¥: {syntax_status} {self.metrics['syntax_errors']} ä¸ªé”™è¯¯")

        print("-"*60)

        # çŠ¶æ€è¯´æ˜
        if self.metrics['status'] == 'excellent':
            print("ğŸ‰ é¡¹ç›®è´¨é‡ä¼˜ç§€ï¼å¯ä»¥å®‰å¿ƒéƒ¨ç½²ã€‚")
        elif self.metrics['status'] == 'good':
            print("ğŸ‘ é¡¹ç›®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­æå‡è¦†ç›–ç‡ã€‚")
        elif self.metrics['status'] == 'fair':
            print("ğŸ“ˆ é¡¹ç›®è´¨é‡ä¸­ç­‰ï¼Œéœ€è¦å…³æ³¨ä»£ç è´¨é‡ã€‚")
        elif self.metrics['status'] == 'poor':
            print("ğŸ“‰ é¡¹ç›®è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨æµ‹è¯•è¦†ç›–ç‡ã€‚")
        elif self.metrics['status'] == 'warning':
            print("âš ï¸ é¡¹ç›®å­˜åœ¨è­¦å‘Šï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤æµ‹è¯•å¤±è´¥ã€‚")
        else:
            print("ğŸš¨ é¡¹ç›®å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ç«‹å³ä¿®å¤ï¼")

        print("="*60)

    def run_continuous_monitoring(self, interval: int = 300):
        """è¿è¡ŒæŒç»­ç›‘æ§"""
        print(f"ğŸ”„ å¯åŠ¨æŒç»­è´¨é‡ç›‘æ§ (é—´éš”: {interval}ç§’)")
        print("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§")

        try:
            while True:
                self.generate_report()
                self.display_dashboard()

                import time
                time.sleep(interval)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ è´¨é‡ç›‘æ§å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="é¡¹ç›®è´¨é‡ç›‘æ§ä»ªè¡¨æ¿")
    parser.add_argument("--continuous", "-c", action="store_true",
                       help="å¯åŠ¨æŒç»­ç›‘æ§")
    parser.add_argument("--interval", "-i", type=int, default=300,
                       help="ç›‘æ§é—´éš”ï¼ˆç§’ï¼Œé»˜è®¤300ç§’ï¼‰")
    parser.add_argument("--project-root", "-p", type=str,
                       help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")

    args = parser.parse_args()

    # åˆ›å»ºç›‘æ§ä»ªè¡¨æ¿
    dashboard = QualityMonitorDashboard(args.project_root)

    if args.continuous:
        dashboard.run_continuous_monitoring(args.interval)
    else:
        dashboard.generate_report()
        dashboard.display_dashboard()


if __name__ == "__main__":
    main()