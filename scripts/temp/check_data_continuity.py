#!/usr/bin/env python3
"""
æ•°æ®è¿ç»­æ€§æˆ˜ç•¥åˆ†æå·¥å…·
Data Strategy Expert: åˆ†ææ¯”èµ›æ•°æ®çš„æ—¶é—´è¿ç»­æ€§ï¼Œä¸ºEloè®¡ç®—æä¾›æ•°æ®è´¨é‡è¯„ä¼°
"""

import asyncio
import logging
from datetime import datetime, timedelta, date
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import pandas as pd
import numpy as np

# å°è¯•å¯¼å…¥å¯è§†åŒ–åº“ï¼Œå¦‚æœå¤±è´¥åˆ™è·³è¿‡
try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates

    VISUALIZATION_AVAILABLE = True
except ImportError:
    VISUALIZATION_AVAILABLE = False
    plt = None

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
from sqlalchemy import text
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DataContinuityAnalyzer:
    """æ•°æ®è¿ç»­æ€§åˆ†æå™¨ - ä¸“æ³¨äºæ—¶åºæ•°æ®è´¨é‡è¯„ä¼°"""

    def __init__(self):
        # ä»ç¯å¢ƒå˜é‡è·å–æ•°æ®åº“URL
        database_url = os.getenv(
            "DATABASE_URL",
            "postgresql://postgres:postgres-dev-password@localhost:5432/football_prediction",
        )
        self.engine = create_async_engine(
            database_url.replace("postgresql://", "postgresql+asyncpg://"), echo=False
        )
        self.AsyncSessionLocal = async_sessionmaker(
            self.engine, class_=AsyncSession, expire_on_commit=False
        )

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        await self.engine.dispose()

    async def get_daily_match_counts(self) -> pd.DataFrame:
        """è·å–æ¯æ—¥æ¯”èµ›æ•°é‡"""
        logger.info("ğŸ“… åˆ†ææ¯æ—¥æ¯”èµ›æ•°é‡åˆ†å¸ƒ...")

        async with self.AsyncSessionLocal() as session:
            query = text("""
                SELECT
                    DATE(match_date) as match_day,
                    COUNT(*) as match_count,
                    COUNT(DISTINCT league_id) as unique_leagues,
                    MIN(match_date) as earliest_time,
                    MAX(match_date) as latest_time
                FROM matches
                WHERE match_date IS NOT NULL
                GROUP BY DATE(match_date)
                ORDER BY match_day
            """)

            result = await session.execute(query)
            rows = result.fetchall()

            # è½¬æ¢ä¸ºDataFrame
            data = []
            for row in rows:
                data.append(
                    {
                        "match_day": row.match_day,
                        "match_count": row.match_count,
                        "unique_leagues": row.unique_leagues,
                        "earliest_time": row.earliest_time,
                        "latest_time": row.latest_time,
                    }
                )

            return pd.DataFrame(data)

    async def analyze_date_gaps(self, daily_counts: pd.DataFrame) -> dict[str, Any]:
        """åˆ†ææ—¶é—´é—´éš”å’Œç©ºç¼º"""
        logger.info("ğŸ•³ï¸ åˆ†ææ—¶é—´é—´éš”å’Œç©ºç¼º...")

        # ç”Ÿæˆå®Œæ•´çš„æ—¥æœŸèŒƒå›´
        start_date = daily_counts["match_day"].min()
        end_date = daily_counts["match_day"].max()
        full_date_range = pd.date_range(start=start_date, end=end_date, freq="D")

        # è¯†åˆ«ç¼ºå¤±çš„æ—¥æœŸ
        missing_dates = []
        for d in full_date_range:
            if d.date() not in daily_counts["match_day"].values:
                missing_dates.append(d.date())

        # åˆ†æè¿ç»­æ€§
        consecutive_groups = []
        if len(daily_counts) > 0:
            current_group = [daily_counts["match_day"].iloc[0]]

            for i in range(1, len(daily_counts)):
                prev_date = daily_counts["match_day"].iloc[i - 1]
                curr_date = daily_counts["match_day"].iloc[i]

                if (curr_date - prev_date).days == 1:
                    current_group.append(curr_date)
                else:
                    consecutive_groups.append(current_group)
                    current_group = [curr_date]

            consecutive_groups.append(current_group)

        # è®¡ç®—è¿ç»­æ€§ç»Ÿè®¡
        group_lengths = [len(group) for group in consecutive_groups]
        max_consecutive_days = max(group_lengths) if group_lengths else 0
        avg_consecutive_days = np.mean(group_lengths) if group_lengths else 0

        return {
            "total_days_span": (end_date - start_date).days + 1,
            "days_with_data": len(daily_counts),
            "missing_dates": missing_dates,
            "missing_count": len(missing_dates),
            "data_coverage_percentage": (len(daily_counts) / len(full_date_range))
            * 100,
            "max_consecutive_days": max_consecutive_days,
            "avg_consecutive_days": avg_consecutive_days,
            "consecutive_groups": len(consecutive_groups),
            "consecutive_group_lengths": group_lengths,
        }

    async def identify_sparse_dates(
        self, daily_counts: pd.DataFrame, threshold: int = 10
    ) -> dict[str, Any]:
        """è¯†åˆ«æ¯”èµ›ç¨€ç–æ—¥æœŸ"""
        logger.info(f"ğŸ” è¯†åˆ«æ¯”èµ›æ•°é‡ < {threshold} çš„ç¨€ç–æ—¥æœŸ...")

        # ç¨€ç–æ—¥æœŸ (< 10åœºæ¯”èµ›)
        sparse_dates = daily_counts[daily_counts["match_count"] < threshold].copy()
        sparse_dates = sparse_dates.sort_values("match_count", ascending=True)

        # æŒ‰ç¨€ç–ç¨‹åº¦åˆ†ç±»
        empty_dates = sparse_dates[sparse_dates["match_count"] == 0]
        low_activity_dates = sparse_dates[
            (sparse_dates["match_count"] >= 1) & (sparse_dates["match_count"] < 5)
        ]
        medium_sparse_dates = sparse_dates[
            (sparse_dates["match_count"] >= 5) & (sparse_dates["match_count"] < 10)
        ]

        return {
            "threshold": threshold,
            "total_sparse_dates": len(sparse_dates),
            "sparse_dates_detail": sparse_dates.to_dict("records"),
            "empty_dates": {
                "count": len(empty_dates),
                "dates": empty_dates["match_day"].tolist(),
            },
            "low_activity_dates": {
                "count": len(low_activity_dates),
                "dates": low_activity_dates["match_day"].tolist(),
                "avg_matches": low_activity_dates["match_count"].mean()
                if len(low_activity_dates) > 0
                else 0,
            },
            "medium_sparse_dates": {
                "count": len(medium_sparse_dates),
                "dates": medium_sparse_dates["match_day"].tolist(),
                "avg_matches": medium_sparse_dates["match_count"].mean()
                if len(medium_sparse_dates) > 0
                else 0,
            },
        }

    async def analyze_weekly_patterns(
        self, daily_counts: pd.DataFrame
    ) -> dict[str, Any]:
        """åˆ†æå‘¨åº¦æ¨¡å¼"""
        logger.info("ğŸ“Š åˆ†æå‘¨åº¦æ¯”èµ›æ¨¡å¼...")

        # æ·»åŠ æ˜ŸæœŸä¿¡æ¯
        daily_counts["weekday"] = pd.to_datetime(
            daily_counts["match_day"]
        ).dt.day_name()
        daily_counts["weekday_num"] = pd.to_datetime(
            daily_counts["match_day"]
        ).dt.dayofweek

        # è®¡ç®—æ¯å‘¨æ¨¡å¼
        weekly_stats = (
            daily_counts.groupby("weekday_num")
            .agg(
                {
                    "match_count": ["mean", "std", "min", "max", "count"],
                    "unique_leagues": "mean",
                }
            )
            .round(2)
        )

        # é‡å‘½ååˆ—
        weekly_stats.columns = [
            "avg_matches",
            "std_matches",
            "min_matches",
            "max_matches",
            "total_days",
            "avg_leagues",
        ]
        weekday_names = [
            "Monday",
            "Tuesday",
            "Wednesday",
            "Thursday",
            "Friday",
            "Saturday",
            "Sunday",
        ]
        weekly_stats.index = [weekday_names[i] for i in weekly_stats.index]

        return {
            "weekly_stats": weekly_stats.to_dict(),
            "busiest_day": weekly_stats["avg_matches"].idxmax(),
            "quietest_day": weekly_stats["avg_matches"].idxmin(),
            "weekend_vs_weekday": {
                "weekend_avg": weekly_stats.loc[
                    ["Saturday", "Sunday"], "avg_matches"
                ].mean(),
                "weekday_avg": weekly_stats.loc[
                    ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday"],
                    "avg_matches",
                ].mean(),
            },
        }

    def create_density_calendar(
        self,
        daily_counts: pd.DataFrame,
        save_path: str = "/app/data_density_calendar.png",
    ):
        """åˆ›å»ºæ•°æ®å¯†åº¦æ—¥å†å›¾"""
        if not VISUALIZATION_AVAILABLE:
            logger.warning("ğŸ¨ å¯è§†åŒ–åº“ä¸å¯ç”¨ï¼Œè·³è¿‡æ—¥å†ç”Ÿæˆ")
            return None

        logger.info("ğŸ¨ ç”Ÿæˆæ•°æ®å¯†åº¦æ—¥å†å›¾...")

        try:
            # å‡†å¤‡æ•°æ®
            daily_counts["match_day_dt"] = pd.to_datetime(daily_counts["match_day"])
            daily_counts["year"] = daily_counts["match_day_dt"].dt.year
            daily_counts["month"] = daily_counts["match_day_dt"].dt.month
            daily_counts["day"] = daily_counts["match_day_dt"].dt.day

            # æŒ‰å¹´æœˆåˆ†ç»„åˆ›å»ºæ—¥å†
            years = sorted(daily_counts["year"].unique())

            fig, axes = plt.subplots(len(years), 12, figsize=(20, 3 * len(years)))
            if len(years) == 1:
                axes = axes.reshape(1, -1)

            for i, year in enumerate(years):
                year_data = daily_counts[daily_counts["year"] == year]

                for month in range(1, 13):
                    ax = axes[i, month - 1] if len(years) > 1 else axes[month - 1]

                    # è·å–è¯¥æœˆæ•°æ®
                    month_data = year_data[year_data["month"] == month]

                    # åˆ›å»ºæ—¥å†çŸ©é˜µ
                    calendar_matrix = np.zeros((6, 7))  # 6 weeks x 7 days

                    for _, row in month_data.iterrows():
                        day = row["day"]
                        weekday = pd.to_datetime(
                            f"{year}-{month:02d}-{day:02d}"
                        ).dayofweek
                        week = day // 7

                        # å¯¹æ•°å€¼è¿›è¡Œå¯¹æ•°å˜æ¢ä»¥ä¾¿å¯è§†åŒ–
                        match_count = row["match_count"]
                        if match_count > 0:
                            calendar_matrix[week, weekday] = np.log1p(
                                match_count
                            )  # log(1 + x)

                    # ç»˜åˆ¶æ—¥å†çƒ­å›¾
                    im = ax.imshow(
                        calendar_matrix,
                        cmap="YlOrRd",
                        aspect="auto",
                        vmin=0,
                        vmax=np.log1p(50),
                    )

                    # è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
                    month_names = [
                        "Jan",
                        "Feb",
                        "Mar",
                        "Apr",
                        "May",
                        "Jun",
                        "Jul",
                        "Aug",
                        "Sep",
                        "Oct",
                        "Nov",
                        "Dec",
                    ]
                    ax.set_title(f"{month_names[month - 1]} {year}", fontsize=8)
                    ax.set_xticks(range(7))
                    ax.set_xticklabels(["M", "T", "W", "T", "F", "S", "S"], fontsize=6)
                    ax.set_yticks([])

                    # éšè—ç©ºæœˆä»½
                    if len(month_data) == 0:
                        ax.axis("off")

            # æ·»åŠ é¢œè‰²æ¡
            cbar = fig.colorbar(
                axes[0, 0].images[0],
                ax=axes,
                orientation="vertical",
                fraction=0.02,
                pad=0.04,
            )
            cbar.set_label("log(Matches + 1)", fontsize=10)

            plt.tight_layout()

            # ä¿å­˜å›¾ç‰‡
            plt.savefig(save_path, dpi=150, bbox_inches="tight")
            logger.info(f"ğŸ“Š æ•°æ®å¯†åº¦æ—¥å†å·²ä¿å­˜è‡³: {save_path}")

            return save_path

        except Exception as e:
            logger.error(f"ğŸ¨ æ—¥å†ç”Ÿæˆå¤±è´¥: {e}")
            return None

    async def generate_patch_plan(
        self, gap_analysis: dict, sparse_analysis: dict
    ) -> dict[str, Any]:
        """ç”Ÿæˆæ•°æ®è¡¥æ¼è®¡åˆ’"""
        logger.info("ğŸ”§ åˆ¶å®šæ•°æ®è¡¥æ¼æˆ˜ç•¥è®¡åˆ’...")

        # è®¡ç®—è¡¥æ¼ä¼˜å…ˆçº§
        total_missing_days = len(gap_analysis["missing_dates"])
        total_sparse_days = sparse_analysis["total_sparse_dates"]

        # è¡¥æ¼ç­–ç•¥
        patch_strategies = []

        # 1. é«˜ä¼˜å…ˆçº§ï¼šå®Œå…¨ç©ºç™½çš„æ—¥æœŸ
        if gap_analysis["missing_dates"]:
            patch_strategies.append(
                {
                    "priority": "HIGH",
                    "category": "Complete Data Gaps",
                    "affected_days": len(gap_analysis["missing_dates"]),
                    "description": "å®Œå…¨ç©ºç™½çš„æ¯”èµ›æ—¥æœŸï¼Œå½±å“Eloè®¡ç®—è¿ç»­æ€§",
                    "recommended_action": "ä¼˜å…ˆä»å†å²æ•°æ®æºè¡¥å…¨æˆ–æ ‡è®°ä¸ºç‰¹æ®Šæ—¥æœŸ",
                    "estimated_effort": "High - éœ€è¦å¤–éƒ¨æ•°æ®æº",
                    "impact_on_elo": "Critical - ç ´åæ—¶é—´åºåˆ—è¿ç»­æ€§",
                }
            )

        # 2. ä¸­ä¼˜å…ˆçº§ï¼šä½æ´»è·ƒåº¦æ—¥æœŸ (1-4åœºæ¯”èµ›)
        if sparse_analysis["low_activity_dates"]["count"] > 0:
            patch_strategies.append(
                {
                    "priority": "MEDIUM",
                    "category": "Low Activity Dates",
                    "affected_days": sparse_analysis["low_activity_dates"]["count"],
                    "description": f"å¹³å‡{sparse_analysis['low_activity_dates']['avg_matches']:.1f}åœºæ¯”èµ›/å¤©",
                    "recommended_action": "æ£€æŸ¥æ•°æ®é‡‡é›†å®Œæ•´æ€§ï¼Œè¡¥å……æ¬¡è¦è”èµ›æ•°æ®",
                    "estimated_effort": "Medium - å†…éƒ¨æ•°æ®æºä¼˜å…ˆ",
                    "impact_on_elo": "Moderate - å½±å“Eloç¨³å®šæ€§",
                }
            )

        # 3. ä½ä¼˜å…ˆçº§ï¼šä¸­ç­‰ç¨€ç–æ—¥æœŸ (5-9åœºæ¯”èµ›)
        if sparse_analysis["medium_sparse_dates"]["count"] > 0:
            patch_strategies.append(
                {
                    "priority": "LOW",
                    "category": "Medium Sparse Dates",
                    "affected_days": sparse_analysis["medium_sparse_dates"]["count"],
                    "description": "æ¯”èµ›æ•°é‡åå°‘ï¼Œä½†åŸºæœ¬å¯ç”¨",
                    "recommended_action": "ç›‘æ§æ•°æ®è´¨é‡ï¼Œå¯é€‰è¡¥å……",
                    "estimated_effort": "Low - å¯é€‰æ‹©æ€§æ‰§è¡Œ",
                    "impact_on_elo": "Minor - è½»å¾®å½±å“ç²¾ç¡®åº¦",
                }
            )

        # æ•°æ®è´¨é‡è¯„çº§
        coverage_percentage = gap_analysis["data_coverage_percentage"]
        if coverage_percentage >= 90:
            quality_grade = "A"
            quality_description = "ä¼˜ç§€ - æ•°æ®è¿ç»­æ€§å¾ˆå¥½ï¼ŒEloè®¡ç®—é«˜åº¦å¯é "
        elif coverage_percentage >= 75:
            quality_grade = "B"
            quality_description = "è‰¯å¥½ - æ•°æ®åŸºæœ¬è¿ç»­ï¼ŒEloè®¡ç®—è¾ƒå¯é "
        elif coverage_percentage >= 60:
            quality_grade = "C"
            quality_description = "ä¸€èˆ¬ - éƒ¨åˆ†ç¼ºå¤±ï¼ŒEloè®¡ç®—éœ€è°¨æ…"
        else:
            quality_grade = "D"
            quality_description = "è¾ƒå·® - æ•°æ®ç¼ºå¤±ä¸¥é‡ï¼Œä¸å»ºè®®ç›´æ¥è®¡ç®—Elo"

        return {
            "overall_quality_grade": quality_grade,
            "quality_description": quality_description,
            "data_coverage_percentage": coverage_percentage,
            "patch_strategies": patch_strategies,
            "implementation_timeline": {
                "phase_1": "1-2å‘¨ - ä¿®å¤å®Œå…¨ç©ºç™½æ—¥æœŸ",
                "phase_2": "2-3å‘¨ - è¡¥å……ä½æ´»è·ƒåº¦æ—¥æœŸ",
                "phase_3": "3-4å‘¨ - ä¼˜åŒ–ä¸­ç­‰ç¨€ç–æ—¥æœŸ",
            },
            "success_metrics": {
                "target_coverage": "â‰¥85% æ•°æ®è¦†ç›–",
                "target_consecutive_days": "â‰¥30å¤©è¿ç»­æ•°æ®",
                "elo_reliability_threshold": "â‰¥80% è¿ç»­æ€§",
            },
        }

    async def generate_comprehensive_report(self) -> dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæ•°æ®è¿ç»­æ€§æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆç»¼åˆæ•°æ®è¿ç»­æ€§æˆ˜ç•¥æŠ¥å‘Š...")

        # 1. è·å–åŸºç¡€æ•°æ®
        daily_counts = await self.get_daily_match_counts()

        # 2. åˆ†ææ—¶é—´é—´éš”
        gap_analysis = await self.analyze_date_gaps(daily_counts)

        # 3. è¯†åˆ«ç¨€ç–æ—¥æœŸ
        sparse_analysis = await self.identify_sparse_dates(daily_counts, threshold=10)

        # 4. åˆ†æå‘¨åº¦æ¨¡å¼
        weekly_patterns = await self.analyze_weekly_patterns(daily_counts)

        # 5. ç”Ÿæˆè¡¥æ¼è®¡åˆ’
        patch_plan = await self.generate_patch_plan(gap_analysis, sparse_analysis)

        # 6. ç”Ÿæˆå¯è§†åŒ–
        try:
            calendar_path = self.create_density_calendar(daily_counts)
        except Exception as e:
            logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            calendar_path = None

        # æ•´åˆæŠ¥å‘Š
        comprehensive_report = {
            "analysis_timestamp": datetime.now().isoformat(),
            "data_overview": {
                "total_days_analyzed": len(daily_counts),
                "date_range": {
                    "start": daily_counts["match_day"].min().isoformat()
                    if len(daily_counts) > 0
                    else None,
                    "end": daily_counts["match_day"].max().isoformat()
                    if len(daily_counts) > 0
                    else None,
                },
                "total_matches": daily_counts["match_count"].sum(),
                "avg_matches_per_day": daily_counts["match_count"].mean(),
                "max_matches_single_day": daily_counts["match_count"].max(),
                "min_matches_single_day": daily_counts["match_count"].min(),
            },
            "continuity_analysis": gap_analysis,
            "sparsity_analysis": sparse_analysis,
            "weekly_patterns": weekly_patterns,
            "patch_plan": patch_plan,
            "visualizations": {"density_calendar_path": calendar_path},
            "elo_feasibility_assessment": {
                "recommended": gap_analysis["data_coverage_percentage"] >= 75,
                "confidence_level": "High"
                if gap_analysis["data_coverage_percentage"] >= 90
                else "Medium"
                if gap_analysis["data_coverage_percentage"] >= 75
                else "Low",
                "key_considerations": [
                    f"æ•°æ®è¦†ç›–ç‡: {gap_analysis['data_coverage_percentage']:.1f}%",
                    f"æœ€é•¿è¿ç»­å¤©æ•°: {gap_analysis['max_consecutive_days']}å¤©",
                    f"ç¨€ç–æ—¥æœŸæ•°é‡: {sparse_analysis['total_sparse_dates']}å¤©",
                ],
            },
        }

        return comprehensive_report

    def print_strategic_summary(self, report: dict[str, Any]):
        """æ‰“å°æˆ˜ç•¥åˆ†ææ‘˜è¦"""
        print("\n" + "=" * 90)
        print("ğŸ•’ æ•°æ®è¿ç»­æ€§æˆ˜ç•¥åˆ†ææŠ¥å‘Š")
        print("=" * 90)

        # æ•°æ®æ¦‚è§ˆ
        overview = report["data_overview"]
        print("\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        print(f"   åˆ†æå¤©æ•°: {overview['total_days_analyzed']:,} å¤©")
        print(
            f"   æ—¶é—´è·¨åº¦: {overview['date_range']['start']} è‡³ {overview['date_range']['end']}"
        )
        print(f"   æ€»æ¯”èµ›æ•°: {overview['total_matches']:,} åœº")
        print(f"   æ—¥å‡æ¯”èµ›: {overview['avg_matches_per_day']:.1f} åœº/å¤©")
        print(f"   å•æ—¥æœ€å¤š: {overview['max_matches_single_day']:,} åœº")
        print(f"   å•æ—¥æœ€å°‘: {overview['min_matches_single_day']:,} åœº")

        # è¿ç»­æ€§åˆ†æ
        continuity = report["continuity_analysis"]
        print("\nğŸ•³ï¸ æ•°æ®è¿ç»­æ€§åˆ†æ:")
        print(f"   æ€»æ—¶é—´è·¨åº¦: {continuity['total_days_span']} å¤©")
        print(f"   æœ‰æ•°æ®å¤©æ•°: {continuity['days_with_data']} å¤©")
        print(f"   æ•°æ®è¦†ç›–ç‡: {continuity['data_coverage_percentage']:.1f}%")
        print(f"   ç¼ºå¤±å¤©æ•°: {continuity['missing_count']} å¤©")
        print(f"   æœ€é•¿è¿ç»­: {continuity['max_consecutive_days']} å¤©")
        print(f"   å¹³å‡è¿ç»­: {continuity['avg_consecutive_days']:.1f} å¤©")

        # ç¨€ç–æ€§åˆ†æ
        sparsity = report["sparsity_analysis"]
        print("\nğŸ” ç¨€ç–æ—¥æœŸåˆ†æ (<10åœºæ¯”èµ›):")
        print(f"   ç¨€ç–æ—¥æœŸæ€»æ•°: {sparsity['total_sparse_dates']} å¤©")
        print(f"   å®Œå…¨ç©ºç™½: {sparsity['empty_dates']['count']} å¤©")
        print(f"   ä½æ´»è·ƒåº¦(1-4åœº): {sparsity['low_activity_dates']['count']} å¤©")
        print(f"   ä¸­ç­‰ç¨€ç–(5-9åœº): {sparsity['medium_sparse_dates']['count']} å¤©")

        # å‘¨åº¦æ¨¡å¼
        weekly = report["weekly_patterns"]
        print("\nğŸ“… å‘¨åº¦æ¯”èµ›æ¨¡å¼:")
        print(f"   æœ€ç¹å¿™: {weekly['busiest_day']}")
        print(f"   æœ€å®‰é™: {weekly['quietest_day']}")
        print(
            f"   å‘¨æœ«å¹³å‡: {weekly['weekend_vs_weekday']['weekend_avg']:.1f}åœº vs å·¥ä½œæ—¥: {weekly['weekend_vs_weekday']['weekday_avg']:.1f}åœº"
        )

        # è´¨é‡è¯„çº§
        patch_plan = report["patch_plan"]
        print("\nğŸ¯ æ•°æ®è´¨é‡è¯„çº§:")
        print(f"   ç»¼åˆè¯„çº§: {patch_plan['overall_quality_grade']} çº§")
        print(f"   è¯„çº§æè¿°: {patch_plan['quality_description']}")

        # Eloå¯è¡Œæ€§
        elo = report["elo_feasibility_assessment"]
        print("\nğŸ¤– Eloè®¡ç®—å¯è¡Œæ€§:")
        print(f"   æ˜¯å¦æ¨è: {'âœ… æ˜¯' if elo['recommended'] else 'âŒ å¦'}")
        print(f"   å¯ä¿¡åº¦: {elo['confidence_level']}")
        print("   å…³é”®è€ƒè™‘å› ç´ :")
        for consideration in elo["key_considerations"]:
            print(f"     â€¢ {consideration}")

        # è¡¥æ¼ç­–ç•¥
        print("\nğŸ”§ æ•°æ®è¡¥æ¼ç­–ç•¥:")
        for i, strategy in enumerate(patch_plan["patch_strategies"], 1):
            print(f"   {i}. {strategy['priority']}ä¼˜å…ˆçº§ - {strategy['category']}")
            print(f"      å½±å“å¤©æ•°: {strategy['affected_days']} å¤©")
            print(f"      Eloå½±å“: {strategy['impact_on_elo']}")
            print(f"      å»ºè®®è¡ŒåŠ¨: {strategy['recommended_action']}")

        # å®æ–½æ—¶é—´çº¿
        timeline = patch_plan["implementation_timeline"]
        print("\nğŸ“… å®æ–½æ—¶é—´çº¿:")
        print(f"   Phase 1: {timeline['phase_1']}")
        print(f"   Phase 2: {timeline['phase_2']}")
        print(f"   Phase 3: {timeline['phase_3']}")

        # æˆåŠŸæŒ‡æ ‡
        metrics = patch_plan["success_metrics"]
        print("\nğŸ¯ æˆåŠŸæŒ‡æ ‡:")
        print(f"   ç›®æ ‡è¦†ç›–ç‡: {metrics['target_coverage']}")
        print(f"   ç›®æ ‡è¿ç»­å¤©æ•°: {metrics['target_consecutive_days']}")
        print(f"   Eloå¯é æ€§é˜ˆå€¼: {metrics['elo_reliability_threshold']}")

        print("\n" + "=" * 90)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æ•°æ®è¿ç»­æ€§æˆ˜ç•¥åˆ†æ...")

    analyzer = DataContinuityAnalyzer()

    try:
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = await analyzer.generate_comprehensive_report()

        # æ‰“å°æˆ˜ç•¥æ‘˜è¦
        analyzer.print_strategic_summary(report)

        logger.info("âœ… æ•°æ®è¿ç»­æ€§åˆ†æå®Œæˆï¼")

        # è¾“å‡ºå…³é”®ç»“è®º
        coverage_pct = report["continuity_analysis"]["data_coverage_percentage"]
        elo_recommended = report["elo_feasibility_assessment"]["recommended"]
        print("\nğŸ” å…³é”®ç»“è®º:")
        print(f"   æ•°æ®è¦†ç›–ç‡: {coverage_pct:.1f}%")
        print(f"   Eloè®¡ç®—æ¨è: {'âœ…' if elo_recommended else 'âŒ'}")

    except Exception as e:
        logger.error(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        raise
    finally:
        await analyzer.close()


if __name__ == "__main__":
    asyncio.run(main())
