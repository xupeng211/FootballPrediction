#!/usr/bin/env python3
"""
æ™ºèƒ½æ•°æ®è¡¥æ¼ç³»ç»Ÿ - Smart Gap Filler
Data Operations Engineer: ä¿®å¤966å¤©æ•°æ®ç©ºç¼ºï¼Œæ”¯æŒæœªæ¥Eloè®¡ç®—

æ ¸å¿ƒç­–ç•¥ï¼šå®‰å…¨æ…¢é€Ÿé‡‡é›†ï¼Œæ™ºèƒ½é”™è¯¯å¤„ç†ï¼Œè¿›åº¦å®æ—¶ç›‘æ§
"""

import asyncio
import logging
import random
import time
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional
import traceback
import sys
import signal

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine, async_sessionmaker
from sqlalchemy import text
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("/app/gap_fill.log"),
        logging.StreamHandler(sys.stdout),
    ],
)
logger = logging.getLogger(__name__)


class SmartGapFiller:
    """æ™ºèƒ½æ•°æ®è¡¥æ¼ç³»ç»Ÿ - ä¸“æ³¨äºå®‰å…¨é«˜æ•ˆçš„æ•°æ®ç©ºç¼ºä¿®å¤"""

    def __init__(self):
        # æ•°æ®åº“è¿æ¥é…ç½®
        database_url = os.getenv(
            "DATABASE_URL",
            "postgresql://postgres:postgres-dev-password@localhost:5432/football_prediction",
        )
        self.engine = create_async_engine(
            database_url.replace("postgresql://", "postgresql+asyncpg://"),
            echo=False,
            pool_size=5,
            max_overflow=10,
        )
        self.AsyncSessionLocal = async_sessionmaker(
            self.engine, class_=AsyncSession, expire_on_commit=False
        )

        # è¡¥æ¼é…ç½®
        self.start_date = date(2022, 1, 1)  # ä»2022å¹´å¼€å§‹è¡¥æ¼
        self.end_date = datetime.now().date()  # åˆ°ä»Šå¤©
        self.min_sleep = 8  # æœ€å°ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        self.max_sleep = 15  # æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        self.max_retries = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.batch_size = 10  # æ¯æ¬¡å¤„ç†çš„å¤©æ•°

        # è¿è¡ŒçŠ¶æ€
        self.is_running = True
        self.processed_dates = 0
        self.successful_fills = 0
        self.failed_attempts = 0

        # æ³¨å†Œä¿¡å·å¤„ç†å™¨
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)

    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨ - ä¼˜é›…å…³é—­"""
        logger.info(f"ğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡ä¼˜é›…å…³é—­...")
        self.is_running = False

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        await self.engine.dispose()

    async def get_async_session(self) -> AsyncSession:
        """è·å–æ•°æ®åº“ä¼šè¯"""
        async with self.AsyncSessionLocal() as session:
            yield session

    async def get_existing_match_dates(self) -> list[date]:
        """è·å–å·²æœ‰æ¯”èµ›æ•°æ®çš„æ—¥æœŸ"""
        logger.info("ğŸ“… æŸ¥è¯¢ç°æœ‰æ¯”èµ›æ•°æ®æ—¥æœŸ...")

        async with self.AsyncSessionLocal() as session:
            query = text("""
                SELECT DISTINCT DATE(match_date) as match_day
                FROM matches
                WHERE match_date IS NOT NULL
                ORDER BY match_day
            """)

            result = await session.execute(query)
            dates = [row.match_day for row in result.fetchall()]

            logger.info(f"âœ… å‘ç° {len(dates)} å¤©æœ‰æ¯”èµ›æ•°æ®")
            return dates

    async def get_gap_dates(self, existing_dates: list[date]) -> list[date]:
        """è¯†åˆ«æ•°æ®ç©ºç¼ºçš„æ—¥æœŸ"""
        logger.info("ğŸ” è¯†åˆ«æ•°æ®ç©ºç¼ºæ—¥æœŸ...")

        # ç”Ÿæˆå®Œæ•´æ—¥æœŸèŒƒå›´
        full_date_range = []
        current_date = self.start_date
        while current_date <= self.end_date:
            full_date_range.append(current_date)
            current_date += timedelta(days=1)

        # æ‰¾å‡ºç©ºç¼ºæ—¥æœŸ
        existing_set = set(existing_dates)
        gap_dates = [d for d in full_date_range if d not in existing_set]

        logger.info(f"ğŸ¯ å‘ç° {len(gap_dates)} ä¸ªæ•°æ®ç©ºç¼ºæ—¥æœŸ")
        return gap_dates

    async def check_date_data_quality(self, target_date: date) -> dict[str, Any]:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•°æ®è´¨é‡"""
        logger.debug(f"ğŸ” æ£€æŸ¥ {target_date} çš„æ•°æ®è´¨é‡...")

        async with self.AsyncSessionLocal() as session:
            query = text("""
                SELECT
                    COUNT(*) as match_count,
                    COUNT(DISTINCT league_id) as unique_leagues,
                    MIN(match_date) as earliest_time,
                    MAX(match_date) as latest_time
                FROM matches
                WHERE DATE(match_date) = :target_date
            """)

            result = await session.execute(query, {"target_date": target_date})
            row = result.fetchone()

            return {
                "match_count": row.match_count or 0,
                "unique_leagues": row.unique_leagues or 0,
                "has_data": row.match_count > 0,
                "earliest_time": row.earliest_time,
                "latest_time": row.latest_time,
            }

    async def trigger_fotmob_collection_for_date(self, target_date: date) -> bool:
        """ä¸ºæŒ‡å®šæ—¥æœŸè§¦å‘FotMobæ•°æ®é‡‡é›†"""
        logger.info(f"ğŸš€ ä¸º {target_date} è§¦å‘FotMobæ•°æ®é‡‡é›†...")

        try:
            # å¯¼å…¥æ•°æ®é‡‡é›†ä»»åŠ¡
            from src.tasks.data_collection_tasks import collect_fotmob_data

            # å‡†å¤‡æ—¥æœŸå‚æ•° (YYYYMMDDæ ¼å¼)
            date_str = target_date.strftime("%Y%m%d")

            # è°ƒç”¨Celeryä»»åŠ¡
            task = collect_fotmob_data.delay(date=date_str)

            logger.info(f"ğŸ“¤ å·²æäº¤ {target_date} çš„æ•°æ®é‡‡é›†ä»»åŠ¡: {task.id}")

            # ç­‰å¾…ä»»åŠ¡å®Œæˆï¼ˆæœ€å¤šç­‰å¾…10åˆ†é’Ÿï¼‰
            try:
                result = task.get(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                logger.info(f"âœ… {target_date} æ•°æ®é‡‡é›†å®Œæˆ: {result}")
                return True
            except Exception:
                logger.error(f"âŒ {target_date} æ•°æ®é‡‡é›†è¶…æ—¶æˆ–å¤±è´¥: {e}")
                return False

        except ImportError as e:
            logger.error(f"âŒ æ— æ³•å¯¼å…¥æ•°æ®é‡‡é›†æ¨¡å—: {e}")
            return False
        except Exception:
            logger.error(f"âŒ {target_date} é‡‡é›†è§¦å‘å¤±è´¥: {e}")
            return False

    async def verify_data_filling(self, target_date: date) -> bool:
        """éªŒè¯æ•°æ®å¡«è¡¥æ˜¯å¦æˆåŠŸ"""
        logger.debug(f"ğŸ” éªŒè¯ {target_date} æ•°æ®å¡«è¡¥ç»“æœ...")

        quality_info = await self.check_date_data_quality(target_date)

        if quality_info["has_data"]:
            logger.info(
                f"âœ… {target_date} æ•°æ®å¡«è¡¥æˆåŠŸ: {quality_info['match_count']}åœºæ¯”èµ›"
            )
            return True
        else:
            logger.warning(f"âš ï¸ {target_date} æ•°æ®å¡«è¡¥åä»æ— æ•°æ®")
            return False

    async def safe_fill_single_date(
        self, target_date: date, retry_count: int = 0
    ) -> bool:
        """å®‰å…¨åœ°å¡«è¡¥å•ä¸ªæ—¥æœŸçš„æ•°æ®"""
        logger.info(
            f"ğŸ”§ å¼€å§‹å¡«è¡¥ {target_date} æ•°æ® (å°è¯• {retry_count + 1}/{self.max_retries})"
        )

        try:
            # 1. æ£€æŸ¥å½“å‰æ•°æ®çŠ¶æ€
            current_quality = await self.check_date_data_quality(target_date)
            if current_quality["has_data"]:
                logger.info(
                    f"â„¹ï¸ {target_date} å·²æœ‰æ•°æ® ({current_quality['match_count']}åœº)ï¼Œè·³è¿‡"
                )
                return True

            # 2. è§¦å‘æ•°æ®é‡‡é›†
            collection_success = await self.trigger_fotmob_collection_for_date(
                target_date
            )

            if collection_success:
                # 3. ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ•°æ®å†™å…¥æ•°æ®åº“
                await asyncio.sleep(random.uniform(3, 6))

                # 4. éªŒè¯æ•°æ®å¡«è¡¥ç»“æœ
                fill_success = await self.verify_data_filling(target_date)

                if fill_success:
                    self.successful_fills += 1
                    logger.info(f"ğŸ‰ {target_date} æ•°æ®å¡«è¡¥æˆåŠŸ!")
                    return True
                else:
                    logger.warning(f"âš ï¸ {target_date} æ•°æ®é‡‡é›†æˆåŠŸä½†éªŒè¯å¤±è´¥")
            else:
                logger.warning(f"âš ï¸ {target_date} æ•°æ®é‡‡é›†å¤±è´¥")

            # 5. é‡è¯•é€»è¾‘
            if retry_count < self.max_retries - 1:
                wait_time = random.uniform(30, 60) * (retry_count + 1)  # é€’å¢ç­‰å¾…æ—¶é—´
                logger.info(f"ğŸ”„ {retry_count + 1}ç§’åé‡è¯• {target_date}...")
                await asyncio.sleep(wait_time)
                return await self.safe_fill_single_date(target_date, retry_count + 1)
            else:
                logger.error(f"ğŸ’€ {target_date} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ")
                self.failed_attempts += 1
                return False

        except Exception:
            logger.error(f"ğŸ’¥ {target_date} å¡«è¡¥è¿‡ç¨‹å¼‚å¸¸: {traceback.format_exc()}")

            if retry_count < self.max_retries - 1:
                wait_time = random.uniform(60, 120) * (retry_count + 1)
                logger.info(f"ğŸ”„ å¼‚å¸¸é‡è¯• {retry_count + 1}ç§’åé‡è¯• {target_date}...")
                await asyncio.sleep(wait_time)
                return await self.safe_fill_single_date(target_date, retry_count + 1)
            else:
                logger.error(f"ğŸ’€ {target_date} å¼‚å¸¸è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒ")
                self.failed_attempts += 1
                return False

    async def process_batch_dates(self, date_batch: list[date]) -> dict[str, int]:
        """å¤„ç†ä¸€æ‰¹æ—¥æœŸçš„æ•°æ®å¡«è¡¥"""
        logger.info(
            f"ğŸ“¦ å¤„ç†æ—¥æœŸæ‰¹æ¬¡: {date_batch[0]} è‡³ {date_batch[-1]} ({len(date_batch)}å¤©)"
        )

        batch_results = {"success": 0, "failed": 0, "skipped": 0}

        for i, target_date in enumerate(date_batch):
            if not self.is_running:
                logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»“æŸæ‰¹æ¬¡å¤„ç†")
                break

            logger.info(f"ğŸ¯ å¤„ç†è¿›åº¦: {i + 1}/{len(date_batch)} - {target_date}")

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰æ•°æ®
            current_quality = await self.check_date_data_quality(target_date)
            if current_quality["has_data"]:
                logger.info(f"â­ï¸ {target_date} å·²æœ‰æ•°æ®ï¼Œè·³è¿‡")
                batch_results["skipped"] += 1
                continue

            # æ‰§è¡Œæ•°æ®å¡«è¡¥
            success = await self.safe_fill_single_date(target_date)

            if success:
                batch_results["success"] += 1
            else:
                batch_results["failed"] += 1

            self.processed_dates += 1

            # å®‰å…¨ç­‰å¾… - é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(date_batch) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                sleep_time = random.uniform(self.min_sleep, self.max_sleep)
                logger.info(f"ğŸ˜´ ç­‰å¾… {sleep_time:.1f}ç§’åå¤„ç†ä¸‹ä¸€å¤©...")
                await asyncio.sleep(sleep_time)

        return batch_results

    async def execute_gap_filling(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¡«è¡¥æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨æ™ºèƒ½æ•°æ®è¡¥æ¼ç³»ç»Ÿ...")
        logger.info(f"ğŸ“… ç›®æ ‡æ—¥æœŸèŒƒå›´: {self.start_date} è‡³ {self.end_date}")
        logger.info(
            f"âš™ï¸ å®‰å…¨é…ç½®: ç­‰å¾…æ—¶é—´ {self.min_sleep}-{self.max_sleep}ç§’, æœ€å¤§é‡è¯• {self.max_retries}æ¬¡"
        )

        try:
            # 1. è·å–ç°æœ‰æ•°æ®æ—¥æœŸ
            existing_dates = await self.get_existing_match_dates()

            # 2. è¯†åˆ«ç©ºç¼ºæ—¥æœŸ
            gap_dates = await self.get_gap_dates(existing_dates)

            if not gap_dates:
                logger.info("ğŸ‰ æœªå‘ç°æ•°æ®ç©ºç¼ºï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸!")
                return

            logger.info(f"ğŸ¯ éœ€è¦å¡«è¡¥ {len(gap_dates)} ä¸ªç©ºç¼ºæ—¥æœŸ")

            # 3. æŒ‰æ‰¹æ¬¡å¤„ç†
            total_batches = (len(gap_dates) + self.batch_size - 1) // self.batch_size

            for batch_num in range(total_batches):
                if not self.is_running:
                    logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç»“æŸæ•°æ®å¡«è¡¥")
                    break

                start_idx = batch_num * self.batch_size
                end_idx = min((batch_num + 1) * self.batch_size, len(gap_dates))
                batch_dates = gap_dates[start_idx:end_idx]

                logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_num + 1}/{total_batches}")

                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_results = await self.process_batch_dates(batch_dates)

                # æ‰¹æ¬¡é—´ä¼‘æ¯æ—¶é—´
                if batch_num < total_batches - 1 and self.is_running:
                    batch_sleep = random.uniform(self.min_sleep * 2, self.max_sleep * 2)
                    logger.info(f"ğŸ›Œ æ‰¹æ¬¡é—´ä¼‘æ¯ {batch_sleep:.1f}ç§’...")
                    await asyncio.sleep(batch_sleep)

            # 4. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            await self.generate_final_report()

        except Exception:
            logger.error(f"ğŸ’¥ æ•°æ®å¡«è¡¥ç³»ç»Ÿå¼‚å¸¸: {traceback.format_exc()}")
            raise
        finally:
            await self.close()

    async def generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        logger.info("ğŸ“Š ç”Ÿæˆæ•°æ®å¡«è¡¥æœ€ç»ˆæŠ¥å‘Š...")

        # é‡æ–°æ£€æŸ¥æ•°æ®è´¨é‡
        current_dates = await self.get_existing_match_dates()
        gap_dates = await self.get_gap_dates(current_dates)

        report = {
            "timestamp": datetime.now().isoformat(),
            "processed_dates": self.processed_dates,
            "successful_fills": self.successful_fills,
            "failed_attempts": self.failed_attempts,
            "remaining_gaps": len(gap_dates),
            "total_dates_with_data": len(current_dates),
            "data_coverage_percentage": (
                len(current_dates) / ((self.end_date - self.start_date).days + 1)
            )
            * 100,
            "success_rate": (self.successful_fills / max(self.processed_dates, 1))
            * 100,
        }

        logger.info("=" * 80)
        logger.info("ğŸ“Š æ•°æ®å¡«è¡¥ç³»ç»Ÿæœ€ç»ˆæŠ¥å‘Š")
        logger.info("=" * 80)
        logger.info(f"ğŸ“… å¤„ç†æ—¥æœŸæ€»æ•°: {report['processed_dates']}")
        logger.info(f"âœ… æˆåŠŸå¡«è¡¥: {report['successful_fills']}")
        logger.info(f"âŒ å¤±è´¥æ¬¡æ•°: {report['failed_attempts']}")
        logger.info(f"ğŸ•³ï¸ å‰©ä½™ç©ºç¼º: {report['remaining_gaps']}")
        logger.info(f"ğŸ“ˆ æ•°æ®è¦†ç›–ç‡: {report['data_coverage_percentage']:.1f}%")
        logger.info(f"ğŸ¯ æˆåŠŸç‡: {report['success_rate']:.1f}%")
        logger.info("=" * 80)

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open("/app/gap_fill_report.json", "w", encoding="utf-8") as f:
            import json

            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info("ğŸ“‹ æŠ¥å‘Šå·²ä¿å­˜è‡³: /app/gap_fill_report.json")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ æ™ºèƒ½æ•°æ®è¡¥æ¼ç³»ç»Ÿå¯åŠ¨")
    logger.info("ğŸ¯ ç›®æ ‡: ä¿®å¤966å¤©æ•°æ®ç©ºç¼ºï¼Œæ”¯æŒEloè®¡ç®—")
    logger.info("ğŸ›¡ï¸ å®‰å…¨ç¬¬ä¸€: æ…¢é€Ÿé‡‡é›†ï¼Œæ™ºèƒ½é‡è¯•ï¼Œä¼˜é›…å…³é—­")

    filler = SmartGapFiller()

    try:
        await filler.execute_gap_filling()
        logger.info("ğŸ‰ æ•°æ®è¡¥æ¼ç³»ç»Ÿå®Œæˆ!")
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œç³»ç»Ÿä¼˜é›…å…³é—­")
    except Exception:
        logger.error(f"ğŸ’¥ ç³»ç»Ÿå¼‚å¸¸é€€å‡º: {e}")
        raise
    finally:
        logger.info("ğŸ‘‹ æ™ºèƒ½æ•°æ®è¡¥æ¼ç³»ç»Ÿé€€å‡º")


if __name__ == "__main__":
    asyncio.run(main())
