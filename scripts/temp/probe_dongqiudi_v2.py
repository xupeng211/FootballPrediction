#!/usr/bin/env python3
"""
ğŸ•µï¸â€â™‚ï¸ æ‡‚çƒå¸æ·±åº¦æ¢æµ‹è„šæœ¬ v2.0

ä¸“æ³¨æ¢æµ‹æ‡‚çƒå¸çš„å®é™…æ•°æ®ç»“æ„ï¼Œç‰¹åˆ«å¯»æ‰¾ xG å’Œé˜µå®¹æ•°æ®
ä½¿ç”¨çœŸå®çš„æ¯”èµ› ID è¿›è¡Œè¯¦æƒ…æ¥å£æ¢æµ‹
"""

import asyncio
import json
import logging
import re
from typing import Dict, List, Optional, Any

import httpx
from curl_cffi import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DongqiudiDeepProbe:
    """æ‡‚çƒå¸æ·±åº¦æ¢æµ‹å™¨"""

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Mobile/15E148 Safari/604.1",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "zh-CN,zh-Hans;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Referer": "https://m.dongqiudi.com/",
                "X-Requested-With": "XMLHttpRequest",
            }
        )

    async def extract_real_match_data(self):
        """ä»æ‡‚çƒå¸ç½‘é¡µæå–çœŸå®çš„æ¯”èµ›æ•°æ®"""
        logger.info("ğŸŒ ä»æ‡‚çƒå¸ä¸»é¡µæå–çœŸå®æ¯”èµ›æ•°æ®...")

        urls = ["https://m.dongqiudi.com/", "https://dongqiudi.com/"]

        for url in urls:
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    html = response.text

                    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„ JSON æ•°æ®
                    json_patterns = [
                        r"window\.__INITIAL_STATE__\s*=\s*({.*?});",
                        r"window\.__NUXT__\s*=\s*({.*?});",
                        r"window\.g_config\s*=\s*({.*?});",
                        r"var\s+matchData\s*=\s*({.*?});",
                        r"const\s+matchList\s*=\s*({.*?});",
                    ]

                    for pattern in json_patterns:
                        matches = re.findall(pattern, html, re.DOTALL)
                        for match_json in matches:
                            try:
                                data = json.loads(match_json)
                                matches_info = self._extract_matches_from_json(data)
                                if matches_info:
                                    logger.info(
                                        f"âœ… ä» {url} æå–åˆ° {len(matches_info)} åœºæ¯”èµ›"
                                    )
                                    return matches_info
                            except json.JSONDecodeError:
                                continue

                    # ç›´æ¥åœ¨ HTML ä¸­æŸ¥æ‰¾æ¯”èµ›ç›¸å…³çš„é“¾æ¥å’ŒID
                    match_id_pattern = r"/data/(\d+)\.html"
                    match_ids = re.findall(match_id_pattern, html)
                    if match_ids:
                        logger.info(f"ğŸ¯ åœ¨é¡µé¢ä¸­æ‰¾åˆ° {len(set(match_ids))} ä¸ªæ¯”èµ›ID")
                        return [
                            {"id": mid, "source": "html_link"}
                            for mid in set(match_ids[:10])
                        ]

            except Exception:
                logger.error(f"è®¿é—® {url} å¤±è´¥: {e}")
                continue

        return None

    def _extract_matches_from_json(self, data: Any) -> list[dict]:
        """ä»JSONæ•°æ®ä¸­æå–æ¯”èµ›ä¿¡æ¯"""
        matches = []

        def find_objects_with_key(obj, target_keys):
            """é€’å½’æŸ¥æ‰¾åŒ…å«ç›®æ ‡é”®çš„å¯¹è±¡"""
            found = []

            if isinstance(obj, dict):
                for key, value in obj.items():
                    if any(
                        target_key in str(key).lower() for target_key in target_keys
                    ):
                        found.append(obj)
                    elif isinstance(value, (dict, list)):
                        found.extend(find_objects_with_key(value, target_keys))

            elif isinstance(obj, list):
                for item in obj:
                    found.extend(find_objects_with_key(item, target_keys))

            return found

        # å¯»æ‰¾åŒ…å«æ¯”èµ›ç›¸å…³é”®çš„å¯¹è±¡
        match_keywords = ["match", "game", "æ¯”èµ›", "èµ›ç¨‹", "fixture"]
        match_objects = find_objects_with_key(data, match_keywords)

        for obj in match_objects:
            if isinstance(obj, dict):
                # æå–æ¯”èµ›ID
                match_id = None
                for id_key in ["id", "match_id", "matchId", "game_id", "gameId"]:
                    if id_key in obj:
                        match_id = str(obj[id_key])
                        break

                if match_id:
                    match_info = {"id": match_id, "raw_data": obj}
                    matches.append(match_info)

        return matches

    async def probe_match_detail_endpoints(self, match_id: str) -> list[dict]:
        """é’ˆå¯¹ç‰¹å®šæ¯”èµ›IDæ¢æµ‹æ‰€æœ‰å¯èƒ½çš„è¯¦æƒ…ç«¯ç‚¹"""
        logger.info(f"ğŸ”¬ æ¢æµ‹æ¯”èµ› {match_id} çš„è¯¦æƒ…æ¥å£...")

        # æ‡‚çƒå¸å¯èƒ½ä½¿ç”¨çš„è¯¦æƒ…ç«¯ç‚¹æ¨¡å¼
        endpoint_patterns = [
            # åŸºç¡€APIç«¯ç‚¹
            f"https://m.dongqiudi.com/api/match/detail?id={match_id}",
            f"https://m.dongqiudi.com/api/match/{match_id}",
            f"https://m.dongqiudi.com/api/v1/match/detail?id={match_id}",
            f"https://m.dongqiudi.com/api/v1/match/{match_id}",
            f"https://m.dongqiudi.com/data/{match_id}.json",
            # å®Œæ•´åŸŸå
            f"https://dongqiudi.com/api/match/detail?id={match_id}",
            f"https://dongqiudi.com/api/match/{match_id}",
            f"https://dongqiudi.com/api/v1/match/detail?id={match_id}",
            f"https://dongqiudi.com/api/v1/match/{match_id}",
            f"https://dongqiudi.com/data/{match_id}.json",
            # APIå­åŸŸå
            f"https://api.dongqiudi.com/match/detail?id={match_id}",
            f"https://api.dongqiudi.com/match/{match_id}",
            f"https://api.dongqiudi.com/v1/match/detail?id={match_id}",
            f"https://api.dongqiudi.com/v1/match/{match_id}",
            # ç§»åŠ¨ç«¯ä¸“ç”¨
            f"https://m.dongqiudi.com/mobile/match/{match_id}",
            f"https://m.dongqiudi.com/h5/match/{match_id}",
            f"https://m.dongqiudi.com/app/match/{match_id}",
            # ç»Ÿè®¡æ•°æ®ç«¯ç‚¹
            f"https://m.dongqiudi.com/api/match/statistics?id={match_id}",
            f"https://m.dongqiudi.com/api/match/stats?id={match_id}",
            f"https://m.dongqiudi.com/api/match/lineup?id={match_id}",
            f"https://m.dongqiudi.com/api/v1/match/statistics?id={match_id}",
            f"https://m.dongqiudi.com/api/v1/match/lineup?id={match_id}",
        ]

        successful_endpoints = []

        for url in endpoint_patterns:
            try:
                logger.info(f"å°è¯•: {url}")
                response = self.session.get(url, timeout=10)

                if response.status_code == 200:
                    try:
                        data = response.json()
                        validation = self._analyze_match_data(data, match_id)

                        if validation["is_match_data"]:
                            successful_endpoints.append(
                                {"url": url, "data": data, "validation": validation}
                            )
                            logger.info(f"âœ… æˆåŠŸ: {url}")
                            logger.info(
                                f"   xGæ•°æ®: {'æœ‰' if validation['has_xg'] else 'æ— '}"
                            )
                            logger.info(
                                f"   é˜µå®¹æ•°æ®: {'æœ‰' if validation['has_lineup'] else 'æ— '}"
                            )
                            logger.info(
                                f"   æŠ€æœ¯ç»Ÿè®¡: {'æœ‰' if validation['has_stats'] else 'æ— '}"
                            )

                    except json.JSONDecodeError:
                        logger.debug(f"éJSONå“åº”: {url}")
                        continue

                elif response.status_code == 403:
                    logger.debug(f"è®¿é—®è¢«æ‹’ç»: {url}")
                elif response.status_code == 404:
                    logger.debug(f"ä¸å­˜åœ¨: {url}")

            except Exception:
                logger.debug(f"è¯·æ±‚å¤±è´¥ {url}: {e}")
                continue

        return successful_endpoints

    def _analyze_match_data(self, data: Any, expected_id: str) -> dict:
        """åˆ†ææ¯”èµ›æ•°æ®çš„è´¨é‡å’Œå†…å®¹"""
        analysis = {
            "is_match_data": False,
            "has_xg": False,
            "has_lineup": False,
            "has_stats": False,
            "match_id_found": False,
            "data_structure": {},
            "key_findings": [],
        }

        if not isinstance(data, (dict, list)) or not data:
            return analysis

        # åŸºæœ¬æ•°æ®ç»“æ„åˆ†æ
        if isinstance(data, dict):
            analysis["data_structure"] = {
                k: type(v).__name__ for k, v in data.items()[:10]
            }

        # éªŒè¯æ˜¯å¦åŒ…å«é¢„æœŸçš„æ¯”èµ›ID
        if self._search_for_id(data, expected_id):
            analysis["match_id_found"] = True
            analysis["is_match_data"] = True

        # æœç´¢xGç›¸å…³æ•°æ®
        xg_patterns = [
            r"xg",
            r"expected_goal",
            r"æœŸæœ›è¿›çƒ",
            r"xG",
            r"expected_goals",
            r"xg_total",
            r"xg_home",
            r"xg_away",
            r"xg[\'\"]?\s*:\s*[\d.]+",
        ]

        if self._search_patterns_in_data(data, xg_patterns):
            analysis["has_xg"] = True
            analysis["key_findings"].append("å‘ç°xGç›¸å…³æ•°æ®")

        # æœç´¢é˜µå®¹ç›¸å…³æ•°æ®
        lineup_patterns = [
            r"lineup",
            r"formation",
            r"é¦–å‘",
            r"starting",
            r"players",
            r"squad",
            r"é˜µå®¹",
            r"lineups",
            r"team_lineup",
        ]

        if self._search_patterns_in_data(data, lineup_patterns):
            analysis["has_lineup"] = True
            analysis["key_findings"].append("å‘ç°é˜µå®¹ç›¸å…³æ•°æ®")

        # æœç´¢æŠ€æœ¯ç»Ÿè®¡æ•°æ®
        stats_patterns = [
            r"statistic",
            r"technical",
            r"possession",
            r"å°„æ­£",
            r"ä¼ çƒ",
            r"æ§çƒç‡",
            r"å°„é—¨",
            r"passes",
            r"shots",
            r"corners",
        ]

        if self._search_patterns_in_data(data, stats_patterns):
            analysis["has_stats"] = True
            analysis["key_findings"].append("å‘ç°æŠ€æœ¯ç»Ÿè®¡æ•°æ®")

        return analysis

    def _search_for_id(self, data: Any, target_id: str) -> bool:
        """åœ¨æ•°æ®ä¸­æœç´¢ç›®æ ‡ID"""
        data_str = json.dumps(data, ensure_ascii=False).lower()
        return target_id.lower() in data_str

    def _search_patterns_in_data(self, data: Any, patterns: list[str]) -> bool:
        """åœ¨æ•°æ®ä¸­æœç´¢æ¨¡å¼"""
        data_str = json.dumps(data, ensure_ascii=False)

        for pattern in patterns:
            if re.search(pattern, data_str, re.IGNORECASE):
                return True

        return False

    async def probe_web_page_match_data(self, match_id: str):
        """æ¢æµ‹ç½‘é¡µç«¯æ¯”èµ›æ•°æ®"""
        logger.info(f"ğŸŒ æ¢æµ‹æ¯”èµ› {match_id} çš„ç½‘é¡µæ•°æ®...")

        web_urls = [
            f"https://m.dongqiudi.com/data/{match_id}.html",
            f"https://dongqiudi.com/data/{match_id}.html",
            f"https://www.dongqiudi.com/data/{match_id}.html",
        ]

        for url in web_urls:
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code == 200:
                    html = response.text

                    # ä»ç½‘é¡µä¸­æå–JSONæ•°æ®
                    json_patterns = [
                        r"window\.__INITIAL_STATE__\s*=\s*({.*?});",
                        r"window\.__NUXT__\s*=\s*({.*?});",
                        r"window\.matchData\s*=\s*({.*?});",
                    ]

                    for pattern in json_patterns:
                        matches = re.findall(pattern, html, re.DOTALL)
                        for match_json in matches:
                            try:
                                data = json.loads(match_json)
                                validation = self._analyze_match_data(data, match_id)

                                if validation["is_match_data"]:
                                    logger.info(f"âœ… ä»ç½‘é¡µæå–åˆ°æ¯”èµ›æ•°æ®: {url}")
                                    return {
                                        "url": url,
                                        "source": "web_page",
                                        "data": data,
                                        "validation": validation,
                                    }
                            except json.JSONDecodeError:
                                continue

            except Exception:
                logger.debug(f"è®¿é—®ç½‘é¡µå¤±è´¥ {url}: {e}")
                continue

        return None

    def print_detailed_analysis(self, results: list[dict]):
        """æ‰“å°è¯¦ç»†çš„åˆ†æç»“æœ"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¯ æ‡‚çƒå¸APIæ¢æµ‹ç»“æœåˆ†æ")
        logger.info("=" * 80)

        for i, result in enumerate(results, 1):
            logger.info(f"\nğŸ“Š ç»“æœ {i}: {result['url']}")
            validation = result["validation"]

            logger.info(f"  âœ… æ¯”èµ›IDåŒ¹é…: {validation['match_id_found']}")
            logger.info(f"  ğŸ“ˆ xGæ•°æ®: {'âœ… æœ‰' if validation['has_xg'] else 'âŒ æ— '}")
            logger.info(
                f"  ğŸ‘¥ é˜µå®¹æ•°æ®: {'âœ… æœ‰' if validation['has_lineup'] else 'âŒ æ— '}"
            )
            logger.info(
                f"  ğŸ“Š æŠ€æœ¯ç»Ÿè®¡: {'âœ… æœ‰' if validation['has_stats'] else 'âŒ æ— '}"
            )

            if validation["key_findings"]:
                logger.info(f"  ğŸ¯ å…³é”®å‘ç°: {', '.join(validation['key_findings'])}")

            if validation["data_structure"]:
                logger.info("  ğŸ“‹ æ•°æ®ç»“æ„:")
                for key, type_name in validation["data_structure"].items():
                    logger.info(f"     {key}: {type_name}")

    async def run_comprehensive_probe(self):
        """è¿è¡Œå…¨é¢çš„æ¢æµ‹æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ‡‚çƒå¸æ·±åº¦æ¢æµ‹...")

        # æ­¥éª¤1: æå–çœŸå®æ¯”èµ›æ•°æ®
        matches = await self.extract_real_match_data()

        if not matches:
            logger.error("âŒ æ— æ³•æå–åˆ°æ¯”èµ›æ•°æ®")
            return

        # é€‰æ‹©å‰å‡ ä¸ªæ¯”èµ›IDè¿›è¡Œæ¢æµ‹
        target_matches = matches[:3]
        logger.info(f"ğŸ“‹ é€‰æ‹© {len(target_matches)} åœºæ¯”èµ›è¿›è¡Œè¯¦ç»†æ¢æµ‹")

        all_results = []

        for match in target_matches:
            match_id = match["id"]
            logger.info(f"\n{'=' * 60}")
            logger.info(f"ğŸ” æ¢æµ‹æ¯”èµ› ID: {match_id}")
            logger.info(f"{'=' * 60}")

            # æ­¥éª¤2: æ¢æµ‹APIç«¯ç‚¹
            api_results = await self.probe_match_detail_endpoints(match_id)

            # æ­¥éª¤3: æ¢æµ‹ç½‘é¡µæ•°æ®
            web_result = await self.probe_web_page_match_data(match_id)

            # æ”¶é›†ç»“æœ
            match_results = api_results
            if web_result:
                match_results.append(web_result)

            all_results.extend(match_results)

        # æ‰“å°æ€»ç»“
        self.print_summary(all_results)

        return all_results

    def print_summary(self, all_results: list[dict]):
        """æ‰“å°æ¢æµ‹æ€»ç»“"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¯ æ‡‚çƒå¸APIæ¢æµ‹æ€»ç»“")
        logger.info("=" * 80)

        total_endpoints = len(all_results)
        endpoints_with_xg = sum(1 for r in all_results if r["validation"]["has_xg"])
        endpoints_with_lineup = sum(
            1 for r in all_results if r["validation"]["has_lineup"]
        )
        endpoints_with_stats = sum(
            1 for r in all_results if r["validation"]["has_stats"]
        )

        logger.info("ğŸ“Š æ¢æµ‹ç»Ÿè®¡:")
        logger.info(f"  â€¢ æˆåŠŸç«¯ç‚¹: {total_endpoints}")
        logger.info(f"  â€¢ åŒ…å«xGæ•°æ®: {endpoints_with_xg}")
        logger.info(f"  â€¢ åŒ…å«é˜µå®¹æ•°æ®: {endpoints_with_lineup}")
        logger.info(f"  â€¢ åŒ…å«æŠ€æœ¯ç»Ÿè®¡: {endpoints_with_stats}")

        if endpoints_with_xg > 0:
            logger.info("\nâœ… xGæ•°æ®å¯ç”¨æ€§: æ‡‚çƒå¸åŒ…å«æœŸæœ›è¿›çƒæ•°æ®!")
        else:
            logger.info("\nâŒ xGæ•°æ®å¯ç”¨æ€§: æœªå‘ç°æœŸæœ›è¿›çƒæ•°æ®")

        if endpoints_with_lineup > 0:
            logger.info("âœ… é˜µå®¹æ•°æ®å¯ç”¨æ€§: æ‡‚çƒå¸åŒ…å«è¯¦ç»†é˜µå®¹ä¿¡æ¯!")
        else:
            logger.info("âŒ é˜µå®¹æ•°æ®å¯ç”¨æ€§: æœªå‘ç°é˜µå®¹æ•°æ®")

        if endpoints_with_stats > 0:
            logger.info("âœ… æŠ€æœ¯ç»Ÿè®¡å¯ç”¨æ€§: æ‡‚çƒå¸åŒ…å«å®Œæ•´æŠ€æœ¯ç»Ÿè®¡!")
        else:
            logger.info("âŒ æŠ€æœ¯ç»Ÿè®¡å¯ç”¨æ€§: æœªå‘ç°æŠ€æœ¯ç»Ÿè®¡æ•°æ®")


async def main():
    """ä¸»å‡½æ•°"""
    probe = DongqiudiDeepProbe()
    results = await probe.run_comprehensive_probe()

    if results:
        probe.print_detailed_analysis(results)
    else:
        logger.error("âŒ æ¢æµ‹å¤±è´¥ï¼Œæœªè·å–åˆ°ä»»ä½•æ•°æ®")


if __name__ == "__main__":
    asyncio.run(main())
