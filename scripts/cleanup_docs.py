#!/usr/bin/env python3
"""
æ–‡æ¡£æ¸…ç†è„šæœ¬
è‡ªåŠ¨æ¸…ç†è¿‡æœŸçš„æŠ¥å‘Šã€é‡å¤çš„æ–‡æ¡£å’Œæ— ç”¨çš„ä»»åŠ¡æ–‡ä»¶
"""

import os
import shutil
from pathlib import Path
from datetime import datetime, timedelta

class DocumentCleaner:
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.docs_dir = self.project_root / "docs"
        self.archive_dir = self.docs_dir / "_archive"
        self.deleted_files = []
        self.archived_files = []
        self.errors = []

    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] {level}: {message}")

    def analyze_docs(self):
        """åˆ†ææ–‡æ¡£ç›®å½•ç»“æ„"""
        self.log("å¼€å§‹åˆ†ææ–‡æ¡£ç›®å½•...")

        # ç»Ÿè®¡å„ç±»æ–‡ä»¶
        stats = {}
        for root, dirs, files in os.walk(self.docs_dir):
            for file in files:
                if file.endswith('.md'):
                    path = Path(root) / file
                    rel_path = path.relative_to(self.docs_dir)
                    category = str(rel_path).split('/')[0]
                    stats[category] = stats.get(category, 0) + 1

        self.log("\nğŸ“Š æ–‡æ¡£ç»Ÿè®¡:")
        for category, count in sorted(stats.items()):
            self.log(f"  {category}: {count} ä¸ªæ–‡ä»¶")

        return stats

    def is_temporary_report(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯ä¸´æ—¶æŠ¥å‘Š"""
        name_patterns = [
            "COVERAGE_BASELINE_*",
            "CLEANUP_UNIT_*",
            "BATCH*",
            "BUGFIX_TODO",
            "CI_*",
            "TEMP_*",
            "CLAUDE_*",
            "coverage_*.json",
            "ruff_*.json",
            "bandit*.json",
            "*_REPORT.json",
        ]

        file_name = file_path.name
        for pattern in name_patterns:
            if pattern.endswith('*'):
                if file_name.startswith(pattern[:-1]):
                    return True
            elif pattern.startswith('*'):
                if file_name.endswith(pattern[1:]):
                    return True
            elif file_name == pattern:
                return True

        return False

    def is_duplicate_coverage_report(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯é‡å¤çš„è¦†ç›–ç‡æŠ¥å‘Š"""
        name = file_path.name
        # åŸºçº¿æŠ¥å‘Šæœ‰å¾ˆå¤šé‡å¤çš„
        if "COVERAGE_BASELINE_P1_" in name:
            return True
        if name in ["COVERAGE_PROGRESS_NEW.md", "COVERAGE_PROGRESS.md"]:
            return True
        return False

    def is_old_report(self, file_path: Path, days: int = 30) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯æ—§æŠ¥å‘Š"""
        if not file_path.exists():
            return False

        mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
        cutoff_date = datetime.now() - timedelta(days=days)

        # ç‰¹åˆ«æ£€æŸ¥æŸäº›ç±»å‹çš„æŠ¥å‘Š
        if file_path.parent.name == "_reports":
            # ä¸´æ—¶æŠ¥å‘Š7å¤©åå°±ç®—æ—§
            if self.is_temporary_report(file_path):
                return mtime < (datetime.now() - timedelta(days=7))
            # å…¶ä»–æŠ¥å‘Š30å¤©åç®—æ—§
            return mtime < cutoff_date

        return False

    def clean_reports(self):
        """æ¸…ç† _reports ç›®å½•"""
        self.log("\nğŸ—‚ï¸ æ¸…ç† _reports ç›®å½•...")

        reports_dir = self.docs_dir / "_reports"
        if not reports_dir.exists():
            return

        # ä¿ç•™çš„é‡è¦æ–‡ä»¶
        keep_files = {
            "PRODUCTION_READINESS_REPORT.md",
            "MLFLOW_SECURITY_REPORT.md",
            "DEPENDENCY_VULNERABILITY_REPORT.md",
            "CONTINUOUS_IMPROVEMENT_REPORT.md",
        }

        for file_path in reports_dir.rglob("*"):
            if not file_path.is_file():
                continue

            rel_path = file_path.relative_to(self.docs_dir)

            # è·³è¿‡é‡è¦æ–‡ä»¶
            if file_path.name in keep_files:
                continue

            # æ¸…ç†ä¸´æ—¶æŠ¥å‘Š
            if self.is_temporary_report(file_path):
                self.archive_file(file_path, "ä¸´æ—¶æŠ¥å‘Š")
            # æ¸…ç†æ—§æŠ¥å‘Š
            elif self.is_old_report(file_path, days=30):
                self.archive_file(file_path, "è¿‡æœŸæŠ¥å‘Š")

    def clean_tasks(self):
        """æ¸…ç† _tasks ç›®å½•"""
        self.log("\nğŸ“‹ æ¸…ç† _tasks ç›®å½•...")

        tasks_dir = self.docs_dir / "_tasks"
        if not tasks_dir.exists():
            return

        # æ£€æŸ¥ä»»åŠ¡æ–‡ä»¶æ˜¯å¦å·²å®Œæˆ
        for file_path in tasks_dir.glob("*.md"):
            # ä¿ç•™é‡è¦çš„ä»»åŠ¡æ–‡ä»¶
            if file_path.name in ["PRODUCTION_READINESS_BOARD.md"]:
                continue

            # å…¶ä»–ä»»åŠ¡æ–‡ä»¶å½’æ¡£
            self.archive_file(file_path, "ä»»åŠ¡æ–‡ä»¶")

    def clean_duplicate_docs(self):
        """æ¸…ç†é‡å¤çš„æ–‡æ¡£"""
        self.log("\nğŸ”„ æ¸…ç†é‡å¤æ–‡æ¡£...")

        # å¤„ç†é‡å¤çš„è¦†ç›–ç‡è¿›åº¦æŠ¥å‘Š
        coverage_reports = list(self.docs_dir.rglob("COVERAGE_PROGRESS*.md"))
        if len(coverage_reports) > 1:
            # ä¿ç•™æœ€æ–°çš„ä¸€ä¸ª
            coverage_reports.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            for file_path in coverage_reports[1:]:
                self.archive_file(file_path, "é‡å¤æ–‡æ¡£")

        # å¤„ç†é‡å¤çš„æµ‹è¯•ç­–ç•¥æ–‡æ¡£
        test_strategies = list(self.docs_dir.rglob("*TEST_STRATEGY*.md"))
        if len(test_strategies) > 1:
            test_strategies.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            for file_path in test_strategies[1:]:
                self.archive_file(file_path, "é‡å¤æ–‡æ¡£")

    def archive_file(self, file_path: Path, reason: str):
        """å½’æ¡£æ–‡ä»¶"""
        try:
            # åˆ›å»ºå½’æ¡£ç›®å½•
            archive_path = self.archive_dir / reason.replace(" ", "_")
            archive_path.mkdir(parents=True, exist_ok=True)

            # è®¡ç®—ç›¸å¯¹è·¯å¾„
            rel_path = file_path.relative_to(self.docs_dir)
            target_path = archive_path / rel_path.name

            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œæ·»åŠ æ—¶é—´æˆ³
            if target_path.exists():
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                stem = target_path.stem
                suffix = target_path.suffix
                target_path = archive_path / f"{stem}_{timestamp}{suffix}"

            shutil.move(str(file_path), str(target_path))
            self.archived_files.append((str(rel_path), reason))
            self.log(f"  å½’æ¡£: {rel_path} ({reason})")

        except Exception as e:
            self.errors.append(f"å½’æ¡£å¤±è´¥ {file_path}: {e}")

    def organize_docs(self):
        """æ•´ç†æ–‡æ¡£ç»“æ„"""
        self.log("\nğŸ“š æ•´ç†æ–‡æ¡£ç»“æ„...")

        # åˆ›å»ºæ ‡å‡†çš„æ–‡æ¡£ç›®å½•ç»“æ„
        standard_dirs = {
            "api": self.docs_dir / "api",
            "architecture": self.docs_dir / "architecture",
            "ops": self.docs_dir / "ops",
            "reference": self.docs_dir / "reference",
            "guides": self.docs_dir / "guides",
        }

        for dir_name, dir_path in standard_dirs.items():
            if not dir_path.exists():
                dir_path.mkdir(exist_ok=True)
                self.log(f"  åˆ›å»ºç›®å½•: {dir_name}/")

        # ç§»åŠ¨æ•£è½çš„æ–‡æ¡£
        for file_path in self.docs_dir.glob("*.md"):
            if file_path.name in ["README.md", "INDEX.md", "CONTRIBUTING.md"]:
                continue

            # æ ¹æ®æ–‡ä»¶ååˆ¤æ–­åº”è¯¥æ”¾åœ¨å“ªé‡Œ
            name = file_path.name.lower()
            if "api" in name:
                target_dir = standard_dirs["api"]
            elif "architecture" in name or "arch" in name:
                target_dir = standard_dirs["architecture"]
            elif "deploy" in name or "ops" in name or "production" in name:
                target_dir = standard_dirs["ops"]
            elif "guide" in name or "tutorial" in name:
                target_dir = standard_dirs["guides"]
            else:
                continue

            if target_dir != file_path.parent:
                target_path = target_dir / file_path.name
                if not target_path.exists():
                    shutil.move(str(file_path), str(target_path))
                    self.log(f"  ç§»åŠ¨: {file_path.name} -> {dir_name}/")

    def create_docs_index(self):
        """åˆ›å»ºæ–‡æ¡£ç´¢å¼•"""
        self.log("\nğŸ“ åˆ›å»ºæ–‡æ¡£ç´¢å¼•...")

        index_path = self.docs_dir / "INDEX.md"
        if index_path.exists():
            return

        content = """# æ–‡æ¡£ç´¢å¼•

## ğŸ“š æ ¸å¿ƒæ–‡æ¡£

### å¿«é€Ÿå¼€å§‹
- [CLAUDE.md](../CLAUDE.md) - AIå¼€å‘æŒ‡å—
- [å¿«é€Ÿå‚è€ƒ](../CLAUDE_QUICK_REFERENCE.md)
- [æ•…éšœæ’é™¤](../CLAUDE_TROUBLESHOOTING.md)

### APIæ–‡æ¡£
- [APIå‚è€ƒ](reference/API_REFERENCE.md)
- [æ•°æ®åº“æ¶æ„](reference/DATABASE_SCHEMA.md)
- [å¼€å‘æŒ‡å—](reference/DEVELOPMENT_GUIDE.md)

### æ¶æ„è®¾è®¡
- [ç³»ç»Ÿæ¶æ„](architecture/)
- [ç”Ÿäº§ç¯å¢ƒæ¶æ„](architecture/PRODUCTION_ARCHITECTURE.md)

### è¿ç»´æ‰‹å†Œ
- [éƒ¨ç½²æŒ‡å—](ops/)
- [ç›‘æ§æŒ‡å—](ops/monitoring/)
- [å®‰å…¨é…ç½®](ops/security/)

### æµ‹è¯•æ–‡æ¡£
- [æµ‹è¯•ç­–ç•¥](testing/)
- [è¦†ç›–ç‡æŠ¥å‘Š](testing/COVERAGE_PROGRESS.md)

### é¡¹ç›®ç®¡ç†
- [å·¥ä½œæµæŒ‡å—](ai/CLAUDE_WORKFLOW_GUIDE.md)
- [AIå¼€å‘è§„åˆ™](AI_DEVELOPMENT_DOCUMENTATION_RULES.md)

## ğŸ“Š æŠ¥å‘Šå½’æ¡£

è¿‡æœŸæŠ¥å‘Šå·²å½’æ¡£åˆ° [_archive/](./_archive/) ç›®å½•ã€‚

## ğŸ”— ç›¸å…³é“¾æ¥

- GitHubä»“åº“
- CI/CDæµæ°´çº¿
- ç›‘æ§ä»ªè¡¨æ¿
"""

        with open(index_path, "w", encoding="utf-8") as f:
            f.write(content)

        self.log(f"  åˆ›å»ºæ–‡æ¡£ç´¢å¼•: INDEX.md")

    def run_cleanup(self):
        """æ‰§è¡Œå®Œæ•´çš„æ¸…ç†æµç¨‹"""
        self.log("=" * 60)
        self.log("å¼€å§‹æ–‡æ¡£æ¸…ç†...")
        self.log(f"é¡¹ç›®æ ¹ç›®å½•: {self.project_root}")
        self.log("=" * 60)

        # åˆ†æå½“å‰çŠ¶æ€
        self.analyze_docs()

        # æ‰§è¡Œæ¸…ç†
        self.clean_reports()
        self.clean_tasks()
        self.clean_duplicate_docs()
        self.organize_docs()
        self.create_docs_index()

        # ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
        self.log("\n" + "=" * 60)
        self.log("æ–‡æ¡£æ¸…ç†å®Œæˆ!")
        self.log(f"å½’æ¡£æ–‡ä»¶æ•°: {len(self.archived_files)}")
        self.log(f"åˆ é™¤æ–‡ä»¶æ•°: {len(self.deleted_files)}")
        if self.errors:
            self.log(f"é”™è¯¯æ•°: {len(self.errors)}")

        # ä¿å­˜æ¸…ç†æŠ¥å‘Š
        self.save_cleanup_report()

        self.log("=" * 60)

    def save_cleanup_report(self):
        """ä¿å­˜æ¸…ç†æŠ¥å‘Š"""
        report_path = self.docs_dir / "_reports"
        report_path.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_path / f"docs_cleanup_report_{timestamp}.md"

        with open(report_file, "w", encoding="utf-8") as f:
            f.write("# æ–‡æ¡£æ¸…ç†æŠ¥å‘Š\n\n")
            f.write(f"**æ¸…ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            f.write("## æ¸…ç†ç»Ÿè®¡\n\n")
            f.write(f"- å½’æ¡£æ–‡ä»¶æ•°: {len(self.archived_files)}\n")
            f.write(f"- åˆ é™¤æ–‡ä»¶æ•°: {len(self.deleted_files)}\n")
            f.write(f"- é”™è¯¯æ•°: {len(self.errors)}\n\n")

            if self.archived_files:
                f.write("## å½’æ¡£çš„æ–‡ä»¶\n\n")
                for file_path, reason in self.archived_files:
                    f.write(f"- `{file_path}` ({reason})\n")
                f.write("\n")

            if self.errors:
                f.write("## é”™è¯¯åˆ—è¡¨\n\n")
                for error in self.errors:
                    f.write(f"- {error}\n")
                f.write("\n")

        self.log(f"\nğŸ“„ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜: {report_file.relative_to(self.project_root)}")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description = os.getenv("CLEANUP_DOCS_DESCRIPTION_358"))
    parser.add_argument("--project-root", help = os.getenv("CLEANUP_DOCS_HELP_359"), default=None)
    parser.add_argument("--dry-run", action = os.getenv("CLEANUP_DOCS_ACTION_359"), help = os.getenv("CLEANUP_DOCS_HELP_359"))

    args = parser.parse_args()

    cleaner = DocumentCleaner(args.project_root)
    cleaner.dry_run = args.dry_run

    if args.dry_run:
        cleaner.log("âš ï¸ è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶")

    cleaner.run_cleanup()