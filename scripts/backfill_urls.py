#!/usr/bin/env python3
"""
URL è¡¥å…¨è„šæœ¬ - The Link Fixer
é¦–å¸­çˆ¬è™«å·¥ç¨‹å¸ˆ: ä¸“é—¨ä¸ºå·²å…¥åº“çš„26,000æ¡è®°å½•è¡¥å…¨match_report_url

Purpose:
1. é‡æ–°è®¿é—®FBrefèµ›ç¨‹é¡µé¢ï¼Œæå–match_report_url
2. æ ¹æ®dateã€home_teamã€away_teamåŒ¹é…ç°æœ‰è®°å½•
3. æ‰¹é‡æ›´æ–°æ•°æ®åº“ï¼Œä¸ºL2é‡‡é›†å™¨æä¾›å·¥ä½œURL
"""

import asyncio
import json
import logging
import re
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import psycopg2
from bs4 import BeautifulSoup
import pandas as pd
from curl_cffi import requests
import time
import random

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)8s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class URLBackfiller:
    """URLè¡¥å…¨å™¨ - ä¸“é—¨ä¸ºå·²å…¥åº“æ•°æ®è¡¥å…¨match_report_url"""

    def __init__(self):
        # ä½¿ç”¨æ›´è½»é‡çš„HTTPå®¢æˆ·ç«¯
        self.session = requests.Session(
            impersonate="chrome",
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
            }
        )

        # æ•°æ®åº“è¿æ¥é…ç½®
        self.db_config = {
            'host': 'db',
            'port': 5432,
            'user': 'postgres',
            'password': 'postgres-dev-password',
            'database': 'football_prediction'
        }

    def get_league_seasons_to_process(self) -> List[Tuple[str, str, str]]:
        """
        è·å–éœ€è¦å¤„ç†çš„è”èµ›-èµ›å­£ç»„åˆ

        Returns:
            List of (league_name, fbref_url, season) tuples
        """
        conn = psycopg2.connect(**self.db_config)

        try:
            with conn.cursor() as cur:
                # æŸ¥æ‰¾éœ€è¦è¡¥å…¨URLçš„è”èµ›-èµ›å­£ç»„åˆ (ä¼˜å…ˆå¤„ç†å†å²æ•°æ®)
                cur.execute("""
                    SELECT DISTINCT
                        l.name as league_name,
                        l.fbref_url,
                        SUBSTRING(m.match_date::text, 1, 4) as season_year
                    FROM matches m
                    JOIN leagues l ON m.league_id = l.id
                    WHERE m.data_completeness = 'partial'
                      AND m.match_metadata->>'match_report_url' IS NULL
                      AND l.fbref_url IS NOT NULL
                      AND m.match_date >= '2019-01-01'
                    ORDER BY l.name, season_year
                """)

                results = cur.fetchall()
                logger.info(f"ğŸ“Š æ‰¾åˆ° {len(results)} ä¸ªè”èµ›-èµ›å­£éœ€è¦å¤„ç†")

                # è½¬æ¢season_yearä¸ºseasonæ ¼å¼
                processed_results = []
                for league_name, fbref_url, season_year in results:
                    if season_year:
                        # æ„å»ºèµ›å­£æ ¼å¼ (å¦‚: 2023 -> 2023-2024)
                        season_start = int(season_year)
                        season_end = season_start + 1
                        season = f"{season_start}-{season_end}"
                        processed_results.append((league_name, fbref_url, season))

                return processed_results

        finally:
            conn.close()

    async def fetch_schedule_with_urls(self, league_name: str, fbref_url: str, season: str) -> pd.DataFrame:
        """
        è·å–åŒ…å«URLçš„èµ›ç¨‹æ•°æ®

        Args:
            league_name: è”èµ›åç§°
            fbref_url: FBrefè”èµ›URL
            season: èµ›å­£ (å¦‚: 2023-2024)

        Returns:
            åŒ…å«match_report_urlçš„DataFrame
        """
        try:
            # æ„å»ºèµ›ç¨‹é¡µé¢URL
            if '/history/' in fbref_url:
                # è½¬æ¢å†å²URLä¸ºèµ›ç¨‹URL
                import re
                match = re.search(r'/comps/(\d+)/history/([^/]+)', fbref_url)
                if match:
                    comp_id = match.group(1)
                    comp_name = match.group(2)
                    schedule_url = f"https://fbref.com/en/comps/{comp_id}/schedule/{comp_name}-Scores-and-Fixtures"
                else:
                    logger.error(f"æ— æ³•è§£æFBref URL: {fbref_url}")
                    return pd.DataFrame()
            else:
                # ä½¿ç”¨ç°æœ‰URLï¼Œæ·»åŠ èµ›å­£å‚æ•°
                if '?' in fbref_url:
                    schedule_url = f"{fbref_url}&season={season.replace('-', '')}"
                else:
                    schedule_url = f"{fbref_url}?season={season.replace('-', '')}"

            logger.info(f"ğŸ”— è·å–èµ›ç¨‹: {league_name} {season}")
            logger.info(f"ğŸ“¡ URL: {schedule_url}")

            # æ·»åŠ éšæœºå»¶è¿Ÿé¿å…è¢«å°
            delay = random.uniform(2.0, 5.0)
            await asyncio.sleep(delay)

            # è·å–HTMLå†…å®¹
            response = self.session.get(schedule_url, timeout=30)

            if response.status_code != 200:
                logger.error(f"âŒ HTTP {response.status_code}: {schedule_url}")
                return pd.DataFrame()

            html_content = response.text
            logger.info(f"âœ… è·å–HTMLæˆåŠŸï¼Œå¤§å°: {len(html_content):,} å­—èŠ‚")

            # è§£æè¡¨æ ¼å’ŒURL
            from io import StringIO
            tables = pd.read_html(StringIO(html_content))
            if not tables:
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è¡¨æ ¼")
                return pd.DataFrame()

            # è·å–ç¬¬ä¸€ä¸ªè¡¨æ ¼ï¼ˆé€šå¸¸æ˜¯èµ›ç¨‹è¡¨ï¼‰
            schedule_df = tables[0]

            # æå–match_report_url
            match_report_urls = self._extract_match_report_urls(html_content)

            if match_report_urls:
                # ç¡®ä¿URLæ•°é‡ä¸è¡¨æ ¼è¡Œæ•°åŒ¹é…
                url_count = min(len(match_report_urls), len(schedule_df))
                schedule_df = schedule_df.copy()
                schedule_df['match_report_url'] = None
                schedule_df.iloc[:url_count, schedule_df.columns.get_loc('match_report_url')] = match_report_urls[:url_count]

                logger.info(f"âœ… æˆåŠŸæå–åˆ° {url_count} ä¸ªURL")
                return schedule_df
            else:
                logger.warning("âš ï¸ æœªæå–åˆ°ä»»ä½•match_report_url")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"âŒ è·å–èµ›ç¨‹å¤±è´¥ {league_name} {season}: {e}")
            return pd.DataFrame()

    def _extract_match_report_urls(self, html_content: str) -> List[str]:
        """
        ä»HTMLä¸­æå–match_report_url

        Args:
            html_content: HTMLå†…å®¹

        Returns:
            URLåˆ—è¡¨
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            urls = []

            # æ–¹æ³•1: æŸ¥æ‰¾data-stat="match_report"çš„é“¾æ¥
            for td in soup.find_all('td', {'data-stat': 'match_report'}):
                link = td.find('a', href=True)
                if link and '/matches/' in link['href']:
                    full_url = f"https://fbref.com{link['href']}"
                    urls.append(full_url)

            # æ–¹æ³•2: å¦‚æœæ–¹æ³•1å¤±è´¥ï¼ŒæŸ¥æ‰¾æ‰€æœ‰åŒ…å«/matches/çš„é“¾æ¥
            if not urls:
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if '/matches/' in href and 'fbref.com' not in href:
                        full_url = f"https://fbref.com{href}"
                        urls.append(full_url)

            # æ–¹æ³•3: æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
            if not urls:
                pattern = r'href="(/en/matches/[^"]+)"'
                matches = re.findall(pattern, html_content)
                for match in matches:
                    full_url = f"https://fbref.com{match}"
                    urls.append(full_url)

            logger.info(f"ğŸ”— æå–åˆ° {len(urls)} ä¸ªmatch_report_url")
            return urls

        except Exception as e:
            logger.error(f"âŒ URLæå–å¤±è´¥: {e}")
            return []

    def normalize_team_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–é˜Ÿå"""
        if pd.isna(name) or not name:
            return ""
        return str(name).strip().lower().replace(" ", "").replace("-", "").replace(".", "")

    def find_matching_records(self, conn, league_name: str, season: str,
                            schedule_df: pd.DataFrame) -> List[Tuple[int, str]]:
        """
        æ ¹æ®dateå’Œé˜ŸååŒ¹é…æ•°æ®åº“è®°å½•

        Args:
            conn: æ•°æ®åº“è¿æ¥
            league_name: è”èµ›åç§°
            season: èµ›å­£
            schedule_df: èµ›ç¨‹DataFrame

        Returns:
            List of (match_id, match_report_url) tuples
        """
        if schedule_df.empty:
            return []

        matches_to_update = []

        # è·å–league_id
        with conn.cursor() as cur:
            cur.execute("SELECT id FROM leagues WHERE name = %s", (league_name,))
            league_result = cur.fetchone()
            if not league_result:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°è”èµ›: {league_name}")
                return []
            league_id = league_result[0]

        # éå†èµ›ç¨‹è¡¨ä¸­çš„æ¯åœºæ¯”èµ›
        for _, row in schedule_df.iterrows():
            try:
                # æå–æ¯”èµ›ä¿¡æ¯
                raw_date = str(row.get('Date', '')).strip()
                home_team = str(row.get('Home', '')).strip()
                away_team = str(row.get('Away', '')).strip()
                score = str(row.get('Score', '')).strip()
                match_report_url = str(row.get('match_report_url', '')).strip()

                # å¤„ç†æ—¥æœŸæ ¼å¼ - FBrefå¯èƒ½ä½¿ç”¨ä¸åŒæ ¼å¼
                if raw_date:
                    # å°è¯•è§£æå„ç§æ—¥æœŸæ ¼å¼
                    try:
                        from datetime import datetime
                        # å°è¯•å¸¸è§æ ¼å¼
                        for fmt in ['%b %d, %Y', '%Y-%m-%d', '%d/%m/%Y', '%m/%d/%Y']:
                            try:
                                date_obj = datetime.strptime(raw_date, fmt)
                                date_str = date_obj.strftime('%Y-%m-%d')
                                break
                            except ValueError:
                                continue
                        else:
                            # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²
                            date_str = raw_date
                    except:
                        date_str = raw_date
                else:
                    date_str = ''

                # è·³è¿‡æ— æ•ˆè®°å½•
                if not date_str or not home_team or not away_team or not match_report_url:
                    continue

                # éªŒè¯æ—¥æœŸæ ¼å¼ - å¿…é¡»æ˜¯æœ‰æ•ˆçš„æ—¥æœŸå­—ç¬¦ä¸²
                if date_str in ['nan', 'Date', '', 'None']:
                    continue

                # éªŒè¯URLæ ¼å¼ - å¿…é¡»æ˜¯æœ‰æ•ˆçš„FBref URL
                if not match_report_url.startswith('https://fbref.com'):
                    continue

                # è·³è¿‡æœªå®Œæˆçš„æ¯”èµ›
                if not score or score in ['', '-']:
                    continue

                # æ ‡å‡†åŒ–é˜Ÿå
                home_team_norm = self.normalize_team_name(home_team)
                away_team_norm = self.normalize_team_name(away_team)

                # æŸ¥æ‰¾æ•°æ®åº“ä¸­åŒ¹é…çš„è®°å½•ï¼ˆä¿®å¤SQL JOINé”™è¯¯ï¼‰
                with conn.cursor() as cur:
                    # ä½¿ç”¨ç²¾ç¡®åŒ¹é…çš„SQLæŸ¥è¯¢ï¼ŒåŒ…å«teamsè¡¨JOIN
                    cur.execute("""
                        SELECT m.id, m.home_team_id, m.away_team_id,
                               ht.name as home_team_name, at.name as away_team_name
                        FROM matches m
                        JOIN teams ht ON m.home_team_id = ht.id
                        JOIN teams at ON m.away_team_id = at.id
                        WHERE m.league_id = %s
                          AND m.data_completeness = 'partial'
                          AND m.match_metadata->>'match_report_url' IS NULL
                          AND DATE(m.match_date) = %s
                    """, (league_id, date_str))

                    db_matches = cur.fetchall()

                    # æ™ºèƒ½åŒ¹é…ï¼šæ£€æŸ¥é˜Ÿåç›¸ä¼¼åº¦
                    for db_match_id, db_home_team_id, db_away_team_id, db_home_name, db_away_name in db_matches:
                        # æ ‡å‡†åŒ–æ•°æ®åº“ä¸­çš„é˜Ÿå
                        db_home_name_norm = self.normalize_team_name(db_home_name)
                        db_away_name_norm = self.normalize_team_name(db_away_name)

                        # æ£€æŸ¥é˜ŸååŒ¹é…ï¼ˆå…è®¸ä¸€å®šå·®å¼‚ï¼‰
                        home_match = (home_team_norm in db_home_name_norm or db_home_name_norm in home_team_norm)
                        away_match = (away_team_norm in db_away_name_norm or db_away_name_norm in away_team_norm)

                        # é¢å¤–æ£€æŸ¥ï¼šå®Œå…¨åŒ¹é…
                        exact_home_match = home_team.lower() == db_home_name.lower()
                        exact_away_match = away_team.lower() == db_away_name.lower()

                        if (home_match and away_match) or (exact_home_match and exact_away_match):
                            matches_to_update.append((db_match_id, match_report_url))
                            logger.info(f"âœ… åŒ¹é…æˆåŠŸ: {home_team} vs {away_team} -> MatchID: {db_match_id} (DB: {db_home_name} vs {db_away_name})")
                            break

            except Exception as e:
                logger.error(f"âŒ å¤„ç†èµ›ç¨‹è®°å½•å¤±è´¥: {e}")
                continue

        return matches_to_update

    def update_database_urls(self, matches_to_update: List[Tuple[int, str]]) -> int:
        """
        æ‰¹é‡æ›´æ–°æ•°æ®åº“ä¸­çš„match_report_url

        Args:
            matches_to_update: List of (match_id, match_report_url) tuples

        Returns:
            æˆåŠŸæ›´æ–°çš„è®°å½•æ•°
        """
        if not matches_to_update:
            return 0

        conn = psycopg2.connect(**self.db_config)
        updated_count = 0

        try:
            with conn.cursor() as cur:
                for match_id, match_report_url in matches_to_update:
                    try:
                        # æ›´æ–°match_metadata (ä¿®å¤JSONBç±»å‹è½¬æ¢)
                        cur.execute("""
                            UPDATE matches
                            SET match_metadata = jsonb_set(
                                jsonb_set(
                                    COALESCE(match_metadata, '{}'),
                                    '{match_report_url}',
                                    %s::jsonb
                                ),
                                '{match_report_url_source}',
                                '"fbref_backfill_v1"'::jsonb
                            ),
                            updated_at = CURRENT_TIMESTAMP
                            WHERE id = %s
                        """, (json.dumps(match_report_url), match_id))

                        updated_count += 1

                        if updated_count % 10 == 0:
                            logger.info(f"ğŸ“Š å·²æ›´æ–° {updated_count} æ¡è®°å½•...")

                    except Exception as e:
                        logger.error(f"âŒ æ›´æ–°è®°å½• {match_id} å¤±è´¥: {e}")
                        continue

                conn.commit()
                logger.info(f"âœ… æˆåŠŸæ›´æ–° {updated_count} æ¡è®°å½•çš„match_report_url")

        except Exception as e:
            conn.rollback()
            logger.error(f"âŒ æ‰¹é‡æ›´æ–°å¤±è´¥: {e}")
        finally:
            conn.close()

        return updated_count

    async def run_backfill(self):
        """è¿è¡ŒURLè¡¥å…¨ä¸»æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨URLè¡¥å…¨ç¨‹åº")
        logger.info("ğŸ¯ ç›®æ ‡: ä¸º26,000æ¡è®°å½•è¡¥å…¨match_report_url")

        # è·å–éœ€è¦å¤„ç†çš„è”èµ›-èµ›å­£ç»„åˆ
        league_seasons = self.get_league_seasons_to_process()

        if not league_seasons:
            logger.info("ğŸ“‹ æ²¡æœ‰éœ€è¦å¤„ç†çš„è”èµ›-èµ›å­£ç»„åˆ")
            return

        total_updated = 0
        total_processed = 0

        for i, (league_name, fbref_url, season) in enumerate(league_seasons, 1):
            logger.info(f"\nğŸ”„ å¤„ç†ç¬¬ {i}/{len(league_seasons)} ä¸ªè”èµ›: {league_name} {season}")

            try:
                # 1. è·å–åŒ…å«URLçš„èµ›ç¨‹æ•°æ®
                schedule_df = await self.fetch_schedule_with_urls(league_name, fbref_url, season)

                if schedule_df.empty:
                    logger.warning(f"âš ï¸ {league_name} {season}: æœªè·å–åˆ°èµ›ç¨‹æ•°æ®")
                    continue

                # 2. åŒ¹é…æ•°æ®åº“è®°å½•
                conn = psycopg2.connect(**self.db_config)
                try:
                    matches_to_update = self.find_matching_records(conn, league_name, season, schedule_df)

                    if matches_to_update:
                        logger.info(f"ğŸ¯ {league_name} {season}: æ‰¾åˆ° {len(matches_to_update)} æ¡è®°å½•éœ€è¦æ›´æ–°")

                        # 3. æ›´æ–°æ•°æ®åº“
                        updated_count = self.update_database_urls(matches_to_update)
                        total_updated += updated_count

                    else:
                        logger.info(f"ğŸ“‹ {league_name} {season}: æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ›´æ–°çš„è®°å½•")

                finally:
                    conn.close()

                total_processed += 1

                # è¯·æ±‚é—´å»¶è¿Ÿ
                if i < len(league_seasons):
                    delay = random.uniform(5.0, 10.0)
                    logger.info(f"â³ å»¶è¿Ÿ {delay:.1f} ç§’...")
                    await asyncio.sleep(delay)

            except Exception as e:
                logger.error(f"âŒ å¤„ç† {league_name} {season} å¤±è´¥: {e}")
                continue

        logger.info(f"\nğŸ‰ URLè¡¥å…¨å®Œæˆ!")
        logger.info(f"ğŸ“Š å¤„ç†è”èµ›-èµ›å­£: {total_processed}/{len(league_seasons)}")
        logger.info(f"ğŸ”— æ›´æ–°è®°å½•æ€»æ•°: {total_updated}")

        # éªŒè¯ç»“æœ
        self.verify_results()

    def verify_results(self):
        """éªŒè¯è¡¥å…¨ç»“æœ"""
        logger.info("\nğŸ” éªŒè¯è¡¥å…¨ç»“æœ...")

        conn = psycopg2.connect(**self.db_config)

        try:
            with conn.cursor() as cur:
                # æ£€æŸ¥è¿˜æœ‰å¤šå°‘è®°å½•ç¼ºå°‘URL
                cur.execute("""
                    SELECT COUNT(*)
                    FROM matches
                    WHERE data_completeness = 'partial'
                      AND match_metadata->>'match_report_url' IS NULL
                """)
                missing_urls = cur.fetchone()[0]

                # æ£€æŸ¥æ€»å…±å¤šå°‘è®°å½•æœ‰URL
                cur.execute("""
                    SELECT COUNT(*)
                    FROM matches
                    WHERE match_metadata->>'match_report_url' IS NOT NULL
                """)
                has_urls = cur.fetchone()[0]

                logger.info(f"ğŸ“Š éªŒè¯ç»“æœ:")
                logger.info(f"  âŒ ä»ç¼ºå°‘URL: {missing_urls:,} æ¡è®°å½•")
                logger.info(f"  âœ… å·²æœ‰URL: {has_urls:,} æ¡è®°å½•")

                if missing_urls == 0:
                    logger.info("ğŸ‰ æ‰€æœ‰partialè®°å½•éƒ½æœ‰URLäº†!")
                else:
                    completion_rate = (has_urls / (has_urls + missing_urls)) * 100
                    logger.info(f"ğŸ“ˆ å®Œæˆç‡: {completion_rate:.1f}%")

                # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹URL
                cur.execute("""
                    SELECT id, match_metadata->>'match_report_url'
                    FROM matches
                    WHERE match_metadata->>'match_report_url' IS NOT NULL
                    LIMIT 5
                """)
                examples = cur.fetchall()

                if examples:
                    logger.info("ğŸ“‹ URLç¤ºä¾‹:")
                    for match_id, url in examples:
                        logger.info(f"  Match {match_id}: {url}")

        finally:
            conn.close()


async def main():
    """ä¸»å‡½æ•°"""
    backfiller = URLBackfiller()

    try:
        await backfiller.run_backfill()
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())