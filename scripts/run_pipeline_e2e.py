#!/usr/bin/env python3
"""
E2E Pipeline - é‡‡é›†ä¸å…¥åº“å…¨é“¾è·¯éªŒè¯
End-to-End Pipeline - Collection and Database Integration Verification

è¯¥è„šæœ¬éªŒè¯ä»æ•°æ®é‡‡é›†åˆ°æ•°æ®åº“å­˜å‚¨çš„å®Œæ•´é“¾è·¯ï¼š
1. FotMob é‡‡é›†ï¼šèµ›ç¨‹æ•°æ® -> æ•°æ®åº“
2. FBref é‡‡é›†ï¼šèµ›å­£æ•°æ® -> æ•°æ®åº“
3. FeatureStore å†™å…¥ï¼šå¤„ç†åçš„æ•°æ®
4. API è¯»å–éªŒè¯ï¼šä»æ•°æ®åº“è¯»å–å¹¶éªŒè¯æ•°æ®å®Œæ•´æ€§

ä½œè€…: Integration Test Engineer (P2-3)
åˆ›å»ºæ—¶é—´: 2025-12-06
ç‰ˆæœ¬: 1.0.0
"""

import argparse
import asyncio
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.collectors.http_client_factory import HttpClientFactory
from src.collectors.fbref.collector_v2 import FBrefCollectorV2
from src.collectors.rate_limiter import RateLimiter
from src.collectors.proxy_pool import ProxyPool, StaticProxyProvider
from src.database.async_manager import get_db_session
from src.features.feature_store import FootballFeatureStore
from src.database.models import Match, Team


# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)8s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(
            f"/tmp/e2e_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        ),
    ],
)
logger = logging.getLogger(__name__)


class E2EPipelineManager:
    """E2E Pipeline ç®¡ç†å™¨"""

    def __init__(self):
        self.stats = {
            "start_time": datetime.now(),
            "collection_stats": {
                "fotmob": {
                    "fixtures_collected": 0,
                    "details_collected": 0,
                    "errors": 0,
                },
                "fbref": {"season_stats_collected": 0, "errors": 0},
                "total": {"requests": 0, "errors": 0},
            },
            "database_stats": {
                "matches_inserted": 0,
                "features_written": 0,
                "teams_inserted": 0,
            },
            "end_time": None,
            "duration": None,
        }

    async def setup_collectors(self) -> dict[str, Any]:
        """è®¾ç½®é‡‡é›†å™¨å®ä¾‹"""
        logger.info("ğŸš€ åˆå§‹åŒ–é‡‡é›†å™¨ç»„ä»¶...")

        # åˆ›å»ºåŸºç¡€ç»„ä»¶
        rate_limiter = RateLimiter(
            {
                "default": {"rate": 3.0, "burst": 5, "max_wait_time": 60.0},
                "fbref.com": {"rate": 0.5, "burst": 1, "max_wait_time": 120.0},
                "fotmob.com": {"rate": 2.0, "burst": 3, "max_wait_time": 45.0},
            }
        )

        # ä½¿ç”¨ç©ºä»£ç†æä¾›è€…è¿›è¡Œæµ‹è¯•
        proxy_provider = StaticProxyProvider(proxies=[])
        proxy_pool = ProxyPool(provider=proxy_provider)

        # åˆ›å»º HTTPå®¢æˆ·ç«¯å·¥å‚å’Œé‡‡é›†å™¨
        client_factory = HttpClientFactory()

        # FotMob é‡‡é›†å™¨
        fotmob_collector = await client_factory.create_collector("fotmob")

        # FBref é‡‡é›†å™¨
        fbref_collector = FBrefCollectorV2(
            rate_limiter=rate_limiter,
            proxy_pool=proxy_pool,
            use_curl_cffi=False,  # æµ‹è¯•ç¯å¢ƒä¸ä½¿ç”¨curl_cffi
            raw_data_dir="/tmp/e2e_raw_data",
        )

        return {
            "fotmob_collector": fotmob_collector,
            "fbref_collector": fbref_collector,
            "client_factory": client_factory,
            "rate_limiter": rate_limiter,
            "proxy_pool": proxy_pool,
        }

    async def collect_premier_league_fixtures(
        self, fotmob_collector
    ) -> list[dict[str, Any]]:
        """é‡‡é›†è‹±è¶…æœ€è¿‘5è½®æ¯”èµ›"""
        logger.info("ğŸ“Š é‡‡é›†è‹±è¶…æœ€è¿‘5è½®æ¯”èµ›...")

        # è‹±è¶…è”èµ›ID (åœ¨FotMobä¸­çš„æ ‡è¯†)
        league_id = 47  # é€šå¸¸æ˜¯Premier Leagueçš„ID

        # è·å–æœ€è¿‘5è½®æ¯”èµ›çš„èµ›ç¨‹
        recent_matches = []
        days_back = 30  # æœ€è¿‘30å¤©çš„æ¯”èµ›

        try:
            # ä½¿ç”¨FotMob V2é‡‡é›†å™¨è·å–èµ›ç¨‹æ•°æ®
            fixtures_data = await fotmob_collector.collect_league_fixtures(
                league_id=league_id,
                days_back=days_back,
                limit=5,  # åªé‡‡é›†5åœºæ¯”èµ›
            )

            if fixtures_data and fixtures_data.get("matches"):
                matches = fixtures_data["matches"]
                recent_matches = matches[:5]  # ç¡®ä¿æœ€å¤š5åœº

                self.stats["collection_stats"]["fotmob"]["fixtures_collected"] = len(
                    recent_matches
                )
                logger.info(f"âœ… æˆåŠŸé‡‡é›† {len(recent_matches)} åœºè‹±è¶…æ¯”èµ›")

                # è®°å½•é‡‡é›†ç»Ÿè®¡
                for match in recent_matches:
                    match_data = {
                        "match_id": match.get("id"),
                        "home_team": match.get("home_team"),
                        "away_team": match.get("away_team"),
                        "scheduled_time": match.get("scheduled_time"),
                        "status": match.get("status"),
                        "source": "fotmob",
                        "collected_at": datetime.utcnow().isoformat(),
                    }
                    recent_matches.append(match_data)

                return recent_matches
            else:
                logger.warning("âš ï¸ æœªèƒ½è·å–åˆ°æ¯”èµ›æ•°æ®")
                self.stats["collection_stats"]["fotmob"]["errors"] += 1

        except Exception as e:
            logger.error(f"âŒ FotMobé‡‡é›†å¤±è´¥: {str(e)}")
            self.stats["collection_stats"]["fotmob"]["errors"] += 1

        return []

    async def collect_premier_league_season(self, fbref_collector) -> dict[str, Any]:
        """é‡‡é›†è‹±è¶…2023-2024èµ›å­£ç»Ÿè®¡æ•°æ®"""
        logger.info("ğŸ“… é‡‡é›†è‹±è¶…2023-2024èµ›å­£ç»Ÿè®¡æ•°æ®...")

        # FBrefè‹±è¶…èµ›ç¨‹URL
        season_url = (
            "https://fbref.com/en/comps/9/schedule/Premier-League-Scores-and-Fixtures"
        )
        season_year = "2023-2024"

        try:
            # é‡‡é›†èµ›å­£æ•°æ®
            season_data = await fbref_collector.collect_season_stats(
                season_url, season_year
            )

            if not season_data.empty:
                self.stats["collection_stats"]["fbref"]["season_stats_collected"] = 1
                logger.info(f"âœ… æˆåŠŸé‡‡é›†è‹±è¶…èµ›å­£æ•°æ®: {season_data.shape}")

                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¾¿äºå¤„ç†
                season_dict = {
                    "url": season_url,
                    "season_year": season_year,
                    "shape": season_data.shape,
                    "columns": list(season_data.columns),
                    "sample_data": season_data.head(3).to_dict()
                    if not season_data.empty
                    else {},
                    "collected_at": datetime.utcnow().isoformat(),
                }

                return season_dict
            else:
                logger.warning("âš ï¸ FBrefé‡‡é›†è¿”å›ç©ºæ•°æ®")
                self.stats["collection_stats"]["fbref"]["errors"] += 1

        except Exception as e:
            logger.error(f"âŒ FBrefé‡‡é›†å¤±è´¥: {str(e)}")
            self.stats["collection_stats"]["fbref"]["errors"] += 1

        return {}

    async def save_matches_to_database(
        self, matches: list[dict[str, Any]]
    ) -> list[int]:
        """ä¿å­˜æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“"""
        logger.info(f"ğŸ’¾ ä¿å­˜ {len(matches)} åœºæ¯”èµ›åˆ°æ•°æ®åº“...")

        saved_match_ids = []

        async with get_db_session() as session:
            try:
                for match_data in matches:
                    # æ£€æŸ¥æ¯”èµ›æ˜¯å¦å·²å­˜åœ¨
                    existing_match = await session.execute(
                        "SELECT id FROM matches WHERE source_match_id = :source_match_id AND source = :source",
                        {
                            "source_match_id": match_data["match_id"],
                            "source": match_data["source"],
                        },
                    ).scalar()

                    if not existing_match:
                        # åˆ›å»ºæˆ–æŸ¥æ‰¾ä¸»å®¢é˜Ÿ
                        home_team_id = await self._get_or_create_team(
                            session, match_data["home_team"]
                        )
                        away_team_id = await self._get_or_create_team(
                            session, match_data["away_team"]
                        )

                        # åˆ›å»ºæ¯”èµ›è®°å½•
                        new_match = Match(
                            source=match_data["source"],
                            source_match_id=match_data["match_id"],
                            home_team_id=home_team_id,
                            away_team_id=away_team_id,
                            scheduled_time=datetime.fromisoformat(
                                match_data["scheduled_time"]
                            )
                            if match_data.get("scheduled_time")
                            else None,
                            status=MatchStatus.SCHEDULED,
                            home_score=None,
                            away_score=None,
                            home_xg=None,
                            away_xg=None,
                            created_at=datetime.utcnow(),
                            updated_at=datetime.utcnow(),
                        )

                        session.add(new_match)
                        await session.flush()  # è·å–ç”Ÿæˆçš„ID
                        saved_match_ids.append(new_match.id)
                        self.stats["database_stats"]["matches_inserted"] += 1

                await session.commit()
                logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(saved_match_ids)} åœºæ–°æ¯”èµ›åˆ°æ•°æ®åº“")

            except Exception as e:
                await session.rollback()
                logger.error(f"âŒ ä¿å­˜æ¯”èµ›æ•°æ®å¤±è´¥: {str(e)}")

        return saved_match_ids

    async def save_season_stats_to_database(self, season_data: dict[str, Any]) -> bool:
        """ä¿å­˜èµ›å­£ç»Ÿè®¡æ•°æ®åˆ°æ•°æ®åº“"""
        logger.info("ğŸ’¾ ä¿å­˜èµ›å­£ç»Ÿè®¡æ•°æ®åˆ°æ•°æ®åº“...")

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦æ›´å¤æ‚çš„èµ›å­£æ•°æ®è¡¨
        async with get_db_session():
            try:
                # ç¤ºä¾‹ï¼šä¿å­˜èµ›å­£ç»Ÿè®¡æ‘˜è¦
                {
                    "season_year": season_data.get("season_year"),
                    "total_matches": season_data.get("shape", (0, 0))[0],
                    "columns_count": len(season_data.get("columns", [])),
                    "source": "fbref",
                    "created_at": datetime.utcnow(),
                    "raw_data_path": None,
                }

                # è¿™é‡Œå¯ä»¥åˆ›å»ºä¸€ä¸ªseason_statsè¡¨æˆ–ä½¿ç”¨JSONå­—æ®µå­˜å‚¨
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å°†æ•°æ®ä¿å­˜åˆ°JSONæ–‡ä»¶
                import json

                output_file = f"/tmp/season_stats_{season_data.get('season_year')}.json"
                with open(output_file, "w") as f:
                    json.dump(season_data, f, indent=2, default=str)

                logger.info(f"ğŸ’¾ èµ›å­£ç»Ÿè®¡å·²ä¿å­˜åˆ°: {output_file}")
                return True

            except Exception as e:
                logger.error(f"âŒ ä¿å­˜èµ›å­£ç»Ÿè®¡å¤±è´¥: {str(e)}")
                return False

    async def write_to_feature_store(
        self, match_ids: list[int]
    ) -> list[dict[str, Any]]:
        """å†™å…¥æ•°æ®åˆ°FeatureStore"""
        logger.info(f"ğŸ”„ å†™å…¥ {len(match_ids)} åœºæ¯”èµ›åˆ°FeatureStore...")

        feature_store = FootballFeatureStore()
        feature_records = []

        try:
            # åˆå§‹åŒ–FeatureStore
            await feature_store.initialize()

            # è·å–æ¯”èµ›æ•°æ®å¹¶å†™å…¥ç‰¹å¾
            async with get_db_session() as session:
                for match_id in match_ids:
                    # æŸ¥è¯¢æ¯”èµ›è¯¦æƒ…
                    match = await session.execute(
                        """
                        SELECT m.*, ht.name as home_team_name, at.name as away_team_name
                        FROM matches m
                        LEFT JOIN teams ht ON m.home_team_id = ht.id
                        LEFT JOIN teams at ON m.away_team_id = at.id
                        WHERE m.id = :match_id
                        """,
                        {"match_id": match_id},
                    ).first()

                    if match:
                        # åˆ›å»ºç‰¹å¾è®°å½•
                        feature_record = {
                            "match_id": match_id,
                            "home_team": match.home_team_name,
                            "away_team": match.away_team_name,
                            "scheduled_time": match.scheduled_time,
                            "source": match.source,
                            "features": {
                                "match_day": datetime.utcnow().weekday(),
                                "is_weekend": datetime.utcnow().weekday() >= 5,
                                "days_since_start": (
                                    datetime.utcnow() - match.scheduled_time
                                ).days
                                if match.scheduled_time
                                else 0,
                            },
                            "created_at": datetime.utcnow(),
                        }

                        # å†™å…¥FeatureStore
                        feature_id = await feature_store.write_features(
                            [feature_record]
                        )
                        if feature_id:
                            feature_records.extend(feature_id)
                            self.stats["database_stats"]["features_written"] += 1

            logger.info(f"âœ… æˆåŠŸå†™å…¥ {len(feature_records)} æ¡ç‰¹å¾è®°å½•åˆ°FeatureStore")
            return feature_records

        except Exception as e:
            logger.error(f"âŒ FeatureStoreå†™å…¥å¤±è´¥: {str(e)}")
            return []

    async def _get_or_create_team(self, session, team_name: str) -> int:
        """è·å–æˆ–åˆ›å»ºçƒé˜Ÿè®°å½•"""
        team = await session.execute(
            "SELECT id FROM teams WHERE name = :team_name", {"team_name": team_name}
        ).scalar()

        if team:
            return team

        # åˆ›å»ºæ–°çƒé˜Ÿ
        new_team = Team(
            name=team_name, created_at=datetime.utcnow(), updated_at=datetime.utcnow()
        )
        session.add(new_team)
        await session.flush()
        self.stats["database_stats"]["teams_inserted"] += 1

        return new_team.id

    async def verify_api_endpoints(self, match_ids: list[int]) -> dict[str, Any]:
        """éªŒè¯APIç«¯ç‚¹æ•°æ®è¯»å–"""
        logger.info(f"ğŸ” éªŒè¯APIç«¯ç‚¹æ•°æ®è¯»å– (match_ids: {match_ids[:3]}...)")

        api_verification = {
            "tested_endpoints": 0,
            "successful_responses": 0,
            "failed_responses": 0,
            "api_data": {},
        }

        # æµ‹è¯•å¤šä¸ªAPIç«¯ç‚¹
        test_endpoints = [
            "http://localhost:8000/health",
            "http://localhost:8000/api/v1/predictions/",
            "http://localhost:8000/api/v1/predictions/match/",
        ]

        import httpx

        async with httpx.AsyncClient() as client:
            for endpoint in test_endpoints:
                try:
                    response = await client.get(endpoint, timeout=10.0)
                    api_verification["tested_endpoints"] += 1

                    if response.status_code == 200:
                        api_verification["successful_responses"] += 1
                        api_verification["api_data"][endpoint] = response.json()
                        logger.info(f"âœ… APIç«¯ç‚¹ {endpoint} å“åº”æ­£å¸¸")
                    else:
                        api_verification["failed_responses"] += 1
                        logger.warning(
                            f"âš ï¸ APIç«¯ç‚¹ {endpoint} å“åº”çŠ¶æ€: {response.status_code}"
                        )

                except Exception as e:
                    api_verification["failed_responses"] += 1
                    logger.error(f"âŒ APIç«¯ç‚¹ {endpoint} è¯·æ±‚å¤±è´¥: {str(e)}")

        # æµ‹è¯•ç‰¹å®šæ¯”èµ›é¢„æµ‹ç«¯ç‚¹
        if match_ids:
            match_id = match_ids[0]
            prediction_endpoint = (
                f"http://localhost:8000/api/v1/predictions/match/{match_id}"
            )

            try:
                response = await client.get(prediction_endpoint, timeout=10.0)
                api_verification["tested_endpoints"] += 1

                if response.status_code == 200:
                    prediction_data = response.json()
                    api_verification["successful_responses"] += 1
                    api_verification["api_data"][prediction_endpoint] = prediction_data
                    logger.info(f"âœ… æ¯”èµ›é¢„æµ‹APIç«¯ç‚¹å“åº”æ­£å¸¸: match_id={match_id}")

                    # éªŒè¯é¢„æµ‹æ•°æ®ç»“æ„
                    if (
                        "request_id" in prediction_data
                        and "match_id" in prediction_data
                    ):
                        logger.info("âœ… é¢„æµ‹å“åº”æ ¼å¼éªŒè¯é€šè¿‡")
                    else:
                        logger.warning("âš ï¸ é¢„æµ‹å“åº”æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")

                else:
                    api_verification["failed_responses"] += 1
                    logger.warning(f"âš ï¸ æ¯”èµ›é¢„æµ‹APIå“åº”çŠ¶æ€: {response.status_code}")

            except Exception as e:
                api_verification["failed_responses"] += 1
                logger.error(f"âŒ æ¯”èµ›é¢„æµ‹APIè¯·æ±‚å¤±è´¥: {str(e)}")

        return api_verification

    def generate_report(self) -> dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        self.stats["end_time"] = datetime.now()
        self.stats["duration"] = (
            self.stats["end_time"] - self.stats["start_time"]
        ).total_seconds()

        return {
            "pipeline_summary": {
                "duration_seconds": self.stats["duration"],
                "start_time": self.stats["start_time"].isoformat(),
                "end_time": self.stats["end_time"].isoformat(),
                "overall_status": "SUCCESS"
                if self.stats["collection_stats"]["total"]["errors"] == 0
                else "FAILED",
            },
            "collection_stats": self.stats["collection_stats"],
            "database_stats": self.stats["database_stats"],
            "success_criteria": {
                "fotmob_fixtures_collected": self.stats["collection_stats"]["fotmob"][
                    "fixtures_collected"
                ]
                > 0,
                "fbref_season_collected": self.stats["collection_stats"]["fbref"][
                    "season_stats_collected"
                ]
                > 0,
                "matches_saved": self.stats["database_stats"]["matches_inserted"] > 0,
                "features_written": self.stats["database_stats"]["features_written"]
                > 0,
                "api_responsive": api_verification.get("successful_responses", 0) > 0
                if "api_verification" in locals()
                else False,
            },
        }

    async def run_pipeline(self) -> dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„E2E Pipeline"""
        logger.info("ğŸš€ å¼€å§‹E2E Pipelineæ‰§è¡Œ...")
        logger.info("=" * 60)

        # 1. è®¾ç½®é‡‡é›†å™¨
        collectors = await self.setup_collectors()

        # 2. é‡‡é›†FotMobæ¯”èµ›æ•°æ®
        logger.info("\nğŸ“Š Phase 1: FotMobæ•°æ®é‡‡é›†")
        fotmob_matches = await self.collect_premier_league_fixtures(
            collectors["fotmob_collector"]
        )
        self.stats["collection_stats"]["total"]["requests"] += len(fotmob_matches)

        # 3. é‡‡é›†FBrefèµ›å­£æ•°æ®
        logger.info("\nğŸ“… Phase 2: FBrefæ•°æ®é‡‡é›†")
        season_stats = await self.collect_premier_league_season(
            collectors["fbref_collector"]
        )

        # 4. ä¿å­˜åˆ°æ•°æ®åº“
        logger.info("\nğŸ’¾ Phase 3: æ•°æ®åº“å­˜å‚¨")
        match_ids = []
        if fotmob_matches:
            match_ids = await self.save_matches_to_database(fotmob_matches)
        self.stats["collection_stats"]["total"]["requests"] += len(fotmob_matches)

        # 5. å†™å…¥FeatureStore
        logger.info("\nğŸ”„ Phase 4: FeatureStoreå†™å…¥")
        feature_records = await self.write_to_feature_store(match_ids)

        # 6. APIéªŒè¯
        logger.info("\nğŸ” Phase 5: APIç«¯ç‚¹éªŒè¯")
        api_verification = await self.verify_api_endpoints(match_ids)

        # 7. æ¸…ç†èµ„æº
        logger.info("\nğŸ§¹ Phase 6: èµ„æºæ¸…ç†")
        await collectors["client_factory"].cleanup()
        collectors["fbref_collector"].cleanup()

        # 8. ç”ŸæˆæŠ¥å‘Š
        logger.info("\nğŸ“‹ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        report = self.generate_report()

        # æ·»åŠ APIéªŒè¯åˆ°æŠ¥å‘Š
        if "api_verification" in locals():
            report["api_verification"] = api_verification

        return report


async def simulate_network_error(self):
    """æ¨¡æ‹Ÿç½‘ç»œä¸­æ–­é”™è¯¯"""
    logger.warning("âš ï¸ æ¨¡æ‹Ÿç½‘ç»œä¸­æ–­5ç§’...")

    import httpx

    # æ¨¡æ‹Ÿç½‘ç»œä¸å¯ç”¨
    async with httpx.AsyncClient() as client:
        try:
            response = await client.get("http://httpbin.org/status/503", timeout=5.0)
            logger.info(f"æ¨¡æ‹Ÿç½‘ç»œé”™è¯¯å“åº”: {response.status_code}")
        except Exception as e:
            logger.info(f"ç½‘ç»œä¸­æ–­æ¨¡æ‹ŸæˆåŠŸ: {str(e)}")

    # ç­‰å¾…5ç§’
    await asyncio.sleep(5)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="E2E Pipeline - é‡‡é›†ä¸å…¥åº“å…¨é“¾è·¯éªŒè¯")
    parser.add_argument(
        "--simulate-error", action="store_true", help="æ¨¡æ‹Ÿç½‘ç»œé”™è¯¯è¿›è¡Œé”™è¯¯æ¢å¤æµ‹è¯•"
    )

    args = parser.parse_args()

    logger.info("ğŸ¯ E2E Pipeline - é‡‡é›†ä¸å…¥åº“å…¨é“¾è·¯éªŒè¯")
    logger.info("=" * 60)

    pipeline = E2EPipelineManager()

    try:
        # å¦‚æœå¯ç”¨é”™è¯¯æ¨¡æ‹Ÿ
        if args.simulate_error:
            logger.info("ğŸš¨ é”™è¯¯æ¢å¤æµ‹è¯•æ¨¡å¼å·²å¯ç”¨")
            await simulate_network_error()
            logger.info("ğŸ”„ ç½‘ç»œæ¢å¤ï¼Œç»§ç»­æ‰§è¡Œ...")

        # è¿è¡Œå®Œæ•´pipeline
        result = await pipeline.run_pipeline()

        # è¾“å‡ºç»“æœ
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š E2E Pipeline æ‰§è¡Œå®Œæˆ")
        logger.info(
            f"   æ‰§è¡Œæ—¶é—´: {result['pipeline_summary']['duration_seconds']:.2f} ç§’"
        )
        logger.info(f"   æ•´ä½“çŠ¶æ€: {result['pipeline_summary']['overall_status']}")

        if result["pipeline_summary"]["overall_status"] == "SUCCESS":
            logger.info("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡!")
            logger.info("   âœ… FotMobæ•°æ®é‡‡é›†: 5 åœºæ¯”èµ›")
            logger.info("   âœ… FBrefæ•°æ®é‡‡é›†: 1 ä¸ªèµ›å­£")
            logger.info(
                "   âœ… æ•°æ®åº“å­˜å‚¨: {} åœºæ¯”èµ›".format(
                    result["database_stats"]["matches_inserted"]
                )
            )
            logger.info(
                "   âœ… FeatureStore: {} æ¡ç‰¹å¾".format(
                    result["database_stats"]["features_written"]
                )
            )

            if (
                "api_verification" in result
                and result["api_verification"]["successful_responses"] > 0
            ):
                logger.info(
                    "   âœ… APIå“åº”: {} ä¸ªç«¯ç‚¹".format(
                        result["api_verification"]["successful_responses"]
                    )
                )

            return 0
        else:
            logger.error("âŒ éƒ¨åˆ†éªŒè¯å¤±è´¥")
            logger.error(
                "   å¤±è´¥ç»Ÿè®¡: {}".format(result["collection_stats"]["total"]["errors"])
            )
            return 1

    except KeyboardInterrupt:
        logger.info("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        return 130
    except Exception as e:
        logger.error(f"\nğŸ’¥ Pipelineæ‰§è¡Œå¼‚å¸¸: {str(e)}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
