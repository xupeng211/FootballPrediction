#!/usr/bin/env python3
"""
é¦–å¸­æ•°æ®åº“æ¶æ„å¸ˆä¸“ç”¨æ™ºèƒ½COPYå¯¼å…¥å™¨
è‡ªåŠ¨æ£€æµ‹CSVåˆ—ç»“æ„ï¼Œé€‚é…æœ‰æ— xGæ•°æ®çš„æ–‡ä»¶
"""

import subprocess
import logging
import csv
import time
from pathlib import Path
from datetime import datetime

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class IntelligentCopyImporter:
    """é¦–å¸­æ•°æ®åº“æ¶æ„å¸ˆä¸“ç”¨æ™ºèƒ½COPYå¯¼å…¥å™¨"""

    def __init__(self):
        self.csv_dir = Path("data/fbref")
        self.stats = {
            "total_files": 0,
            "success_files": 0,
            "failed_files": 0,
            "total_rows": 0,
            "xg_files": 0,
            "no_xg_files": 0,
            "start_time": datetime.now(),
        }

    def detect_csv_structure(self, csv_file: Path) -> dict:
        """æ£€æµ‹CSVæ–‡ä»¶ç»“æ„"""
        with open(csv_file, encoding="utf-8") as f:
            reader = csv.reader(f)
            headers = next(reader)

            has_xg = "xG" in headers
            has_xg_away = "xG.1" in headers

            return {
                "headers": headers,
                "has_xg": has_xg,
                "has_xg_away": has_xg_away,
                "col_count": len(headers),
            }

    def build_copy_sql(self, structure: dict, filename: str) -> str:
        """æ ¹æ®CSVç»“æ„æ„å»ºCOPY SQL"""
        headers = structure["headers"]

        # åŸºç¡€åˆ—ï¼ˆæ‰€æœ‰æ–‡ä»¶éƒ½æœ‰çš„åˆ—ï¼‰
        base_columns = [
            "wk",
            "Day",
            "Date",
            "Time",
            "Home",
            "Score",
            "Away",
            "Attendance",
            "Venue",
            "Referee",
            "Match Report",
            "Notes",
        ]

        # æ ¹æ®ç»“æ„è°ƒæ•´åˆ—æ˜ å°„
        if structure["has_xg"]:
            # æœ‰xGæ•°æ®çš„æ–‡ä»¶ï¼Œéœ€è¦å¤„ç†xGå’ŒxG.1åˆ—
            columns = [
                "wk",
                "Day",
                "Date",
                "Time",
                "Home",
                "xG",
                "Score",
                "xG.1",
                "Away",
                "Attendance",
                "Venue",
                "Referee",
                "Match Report",
                "Notes",
            ]
        else:
            # æ— xGæ•°æ®çš„æ–‡ä»¶ï¼Œè·³è¿‡xGåˆ—ï¼Œä½¿ç”¨DEFAULT NULL
            columns = [
                "wk",
                "Day",
                "Date",
                "Time",
                "Home",
                "Score",
                "Away",
                "Attendance",
                "Venue",
                "Referee",
                "Match Report",
                "Notes",
            ]

        # æ·»åŠ source_fileåˆ—
        columns.append("source_file")

        # æ„å»ºCOPY SQL
        columns_str = ", ".join(
            [
                (
                    f'"{col}"'
                    if col
                    in [
                        "Day",
                        "Date",
                        "Time",
                        "Home",
                        "Score",
                        "Away",
                        "Match Report",
                        "Notes",
                    ]
                    else col
                )
                for col in columns
            ]
        )

        copy_sql = f"""
        -- æ¸…ç†ä¹‹å‰çš„ç›¸åŒæ–‡ä»¶æ•°æ®
        DELETE FROM stg_fbref_matches WHERE source_file = '{filename}';

        -- æ‰§è¡ŒCOPYå¯¼å…¥
        COPY stg_fbref_matches ({columns_str})
        FROM STDIN WITH CSV HEADER;
        """

        return copy_sql

    def transform_csv_for_copy(self, csv_file: Path, structure: dict) -> str:
        """è½¬æ¢CSVæ•°æ®ä»¥åŒ¹é…è¡¨ç»“æ„"""
        import io

        output = io.StringIO()

        with open(csv_file, encoding="utf-8") as f:
            reader = csv.DictReader(f)

            # å†™å…¥è¡¨å¤´
            if structure["has_xg"]:
                fieldnames = [
                    "wk",
                    "Day",
                    "Date",
                    "Time",
                    "Home",
                    "xG",
                    "Score",
                    "xG.1",
                    "Away",
                    "Attendance",
                    "Venue",
                    "Referee",
                    "Match Report",
                    "Notes",
                    "source_file",
                ]
            else:
                fieldnames = [
                    "wk",
                    "Day",
                    "Date",
                    "Time",
                    "Home",
                    "Score",
                    "Away",
                    "Attendance",
                    "Venue",
                    "Referee",
                    "Match Report",
                    "Notes",
                    "source_file",
                ]

            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()

            # è½¬æ¢æ•°æ®è¡Œ
            for row in reader:
                if structure["has_xg"]:
                    # æœ‰xGæ•°æ®çš„æ–‡ä»¶ï¼Œç›´æ¥æ˜ å°„
                    new_row = {
                        "wk": row.get("Wk", ""),
                        "Day": row.get("Day", ""),
                        "Date": row.get("Date", ""),
                        "Time": row.get("Time", ""),
                        "Home": row.get("Home", ""),
                        "xG": row.get("xG", ""),
                        "Score": row.get("Score", ""),
                        "xG.1": row.get("xG.1", ""),
                        "Away": row.get("Away", ""),
                        "Attendance": row.get("Attendance", ""),
                        "Venue": row.get("Venue", ""),
                        "Referee": row.get("Referee", ""),
                        "Match Report": row.get("Match Report", ""),
                        "Notes": row.get("Notes", ""),
                        "source_file": csv_file.name,
                    }
                else:
                    # æ— xGæ•°æ®çš„æ–‡ä»¶ï¼Œæ·»åŠ ç©ºxGåˆ—
                    new_row = {
                        "wk": row.get("Wk", ""),
                        "Day": row.get("Day", ""),
                        "Date": row.get("Date", ""),
                        "Time": row.get("Time", ""),
                        "Home": row.get("Home", ""),
                        "Score": row.get("Score", ""),
                        "Away": row.get("Away", ""),
                        "Attendance": row.get("Attendance", ""),
                        "Venue": row.get("Venue", ""),
                        "Referee": row.get("Referee", ""),
                        "Match Report": row.get("Match Report", ""),
                        "Notes": row.get("Notes", ""),
                        "source_file": csv_file.name,
                    }

                writer.writerow(new_row)

        return output.getvalue()

    def execute_copy_with_transformation(self, csv_file: Path) -> bool:
        """æ‰§è¡Œå¸¦æ•°æ®è½¬æ¢çš„COPY"""
        try:
            filename = csv_file.name
            logger.info(f"ğŸ“„ å¼€å§‹å¯¼å…¥: {filename}")

            # æ£€æµ‹CSVç»“æ„
            structure = self.detect_csv_structure(csv_file)

            if structure["has_xg"]:
                self.stats["xg_files"] += 1
                logger.info(f"ğŸ“Š æ£€æµ‹åˆ°xGæ•°æ® ({structure['col_count']}åˆ—)")
            else:
                self.stats["no_xg_files"] += 1
                logger.info(f"ğŸ“Š æ— xGæ•°æ® ({structure['col_count']}åˆ—)")

            # è½¬æ¢CSVæ•°æ®
            transformed_csv = self.transform_csv_for_copy(csv_file, structure)

            # æ„å»ºCOPY SQL
            copy_sql = self.build_copy_sql(structure, filename)

            # ä½¿ç”¨Dockeræ‰§è¡ŒCOPY
            cmd = [
                "docker-compose",
                "exec",
                "-T",
                "db",
                "psql",
                "-U",
                "postgres",
                "-d",
                "football_prediction",
                "-c",
                copy_sql,
            ]

            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )
            stdout, stderr = process.communicate(input=transformed_csv)

            if process.returncode == 0:
                # è·å–å¯¼å…¥è¡Œæ•°
                count_cmd = [
                    "docker-compose",
                    "exec",
                    "db",
                    "psql",
                    "-U",
                    "postgres",
                    "-d",
                    "football_prediction",
                    "-tAc",
                    f"SELECT COUNT(*) FROM stg_fbref_matches WHERE source_file = '{filename}';",
                ]

                count_result = subprocess.run(count_cmd, capture_output=True, text=True)
                if count_result.returncode == 0:
                    rows = int(count_result.stdout.strip())
                    self.stats["total_rows"] += rows
                    logger.info(f"âœ… å¯¼å…¥æˆåŠŸ: {rows:,} è¡Œ")
                    return True
                else:
                    logger.error(f"âŒ è·å–è¡Œæ•°å¤±è´¥: {count_result.stderr}")
                    return False
            else:
                logger.error(f"âŒ COPYå¤±è´¥: {stderr}")
                return False

        except Exception as e:
            logger.error(f"âŒ å¯¼å…¥å¼‚å¸¸ {filename}: {e}")
            return False

    def run(self):
        """æ‰§è¡Œæ™ºèƒ½COPYå¯¼å…¥"""
        logger.info("ğŸš€ å¯åŠ¨é¦–å¸­æ•°æ®åº“æ¶æ„å¸ˆä¸“ç”¨æ™ºèƒ½COPYå¯¼å…¥å™¨")

        # æ‰«æCSVæ–‡ä»¶
        csv_files = list(self.csv_dir.glob("*.csv"))
        self.stats["total_files"] = len(csv_files)

        logger.info(f"ğŸ“ å‘ç° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")

        if not csv_files:
            logger.error("âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶!")
            return self.stats

        # é¦–å…ˆç¡®ä¿è¡¨å­˜åœ¨å¹¶æ¸…ç©º
        init_cmd = [
            "docker-compose",
            "exec",
            "db",
            "psql",
            "-U",
            "postgres",
            "-d",
            "football_prediction",
            "-c",
            "TRUNCATE TABLE stg_fbref_matches;",
        ]

        subprocess.run(init_cmd, capture_output=True)
        logger.info("ğŸ§¹ ä¸´æ—¶è¡¨å·²æ¸…ç©º")

        # é€ä¸ªå¯¼å…¥æ–‡ä»¶
        for i, csv_file in enumerate(csv_files, 1):
            logger.info(f"ğŸ”„ è¿›åº¦: {i}/{len(csv_files)} ({i/len(csv_files)*100:.1f}%)")

            success = self.execute_copy_with_transformation(csv_file)

            if success:
                self.stats["success_files"] += 1
            else:
                self.stats["failed_files"] += 1

        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_cmd = [
            "docker-compose",
            "exec",
            "db",
            "psql",
            "-U",
            "postgres",
            "-d",
            "football_prediction",
            "-c",
            """
            SELECT
                COUNT(*) as total_rows,
                COUNT(DISTINCT source_file) as unique_files,
                COUNT(DISTINCT "Home") as unique_teams,
                COUNT(CASE WHEN "xG" IS NOT NULL AND "xG" != '' THEN 1 END) as xg_matches
            FROM stg_fbref_matches;
            """,
        ]

        result = subprocess.run(final_cmd, capture_output=True, text=True)
        if result.returncode == 0:
            logger.info("ğŸ“Š æœ€ç»ˆæ•°æ®åº“ç»Ÿè®¡:")
            logger.info(result.stdout)

        # è¾“å‡ºæœ€ç»ˆæŠ¥å‘Š
        end_time = datetime.now()
        duration = (end_time - self.stats["start_time"]).total_seconds()

        logger.info("=" * 60)
        logger.info("ğŸ‰ æ™ºèƒ½COPYå¯¼å…¥å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration:.1f}ç§’")
        logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {self.stats['total_files']}")
        logger.info(f"âœ… æˆåŠŸæ–‡ä»¶: {self.stats['success_files']}")
        logger.info(f"âŒ å¤±è´¥æ–‡ä»¶: {self.stats['failed_files']}")
        logger.info(f"ğŸ“ˆ å«xGæ–‡ä»¶: {self.stats['xg_files']}")
        logger.info(f"ğŸ“‰ æ— xGæ–‡ä»¶: {self.stats['no_xg_files']}")
        logger.info(f"âš½ æ€»æ•°æ®è¡Œ: {self.stats['total_rows']:,}")
        if duration > 0:
            logger.info(f"ğŸš€ å¹³å‡é€Ÿåº¦: {self.stats['total_rows']/duration:.0f} è¡Œ/ç§’")

        return self.stats


def main():
    """ä¸»å‡½æ•°"""
    try:
        importer = IntelligentCopyImporter()
        stats = importer.run()

        if stats["success_files"] > 0:
            logger.info(
                f"âœ… æ™ºèƒ½å¯¼å…¥æˆåŠŸ! {stats['success_files']} ä¸ªæ–‡ä»¶, {stats['total_rows']:,} è¡Œæ•°æ®"
            )
            return 0
        else:
            logger.error("âŒ å¯¼å…¥å¤±è´¥!")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
