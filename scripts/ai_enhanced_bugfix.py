#!/usr/bin/env python3
"""
AI å¢å¼ºçš„ Bug ä¿®å¤ä¸»è„šæœ¬

é›†æˆåˆ°ç°æœ‰çš„è‡ªåŠ¨åŒ– bug ä¿®å¤é—­ç¯ä¸­ï¼Œæä¾›æ™ºèƒ½åˆ†æå’Œä¿®å¤å»ºè®®ã€‚
"""

import sys
import subprocess
import json
import logging
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# æ·»åŠ  src åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.ai.code_analyzer import CodeAnalyzer, AnalysisResult
from src.ai.fix_applier import FixApplier
from src.ai.claude_cli_wrapper import TestFailure
from src.ai.mutation_tester import SelectiveMutationTester
from src.ai.flaky_test_detector import SmartFlakyTestDetector
from src.ai.performance_benchmark import RelativePerformanceBenchmark
from src.ai.test_quality_aggregator import TestQualityAggregator
from src.ai.test_generator import AITestGenerator
from src.ai.fix_validator import FixValidator
from src.ai.feedback_db import FeedbackDatabase

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AIEnhancedBugfix:
    """AI å¢å¼ºçš„ Bug ä¿®å¤ç³»ç»Ÿ"""

    def __init__(self):
        self.analyzer = CodeAnalyzer()
        self.applier = FixApplier()

        # Phase 2: Test Effectiveness Enhancement components
        self.mutation_tester = SelectiveMutationTester()
        self.flaky_detector = SmartFlakyTestDetector()
        self.performance_benchmark = RelativePerformanceBenchmark()
        self.quality_aggregator = TestQualityAggregator()

        # Phase 3: Intelligent Enhancement components
        self.test_generator = AITestGenerator()
        self.fix_validator = FixValidator()
        self.feedback_db = FeedbackDatabase()

        self.reports_dir = Path("docs/_reports")
        self.reports_dir.mkdir(parents=True, exist_ok=True)
        logger.info("AI Enhanced Bugfix system initialized with Phase 3 components")

    def run_analysis_and_generate_fixes(self, interactive: bool = True) -> bool:
        """
        è¿è¡Œå®Œæ•´çš„åˆ†æå’Œä¿®å¤ç”Ÿæˆæµç¨‹

        Args:
            interactive: æ˜¯å¦äº¤äº’å¼æ¨¡å¼

        Returns:
            æµç¨‹æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            print("ğŸš€ Starting AI-enhanced bugfix analysis...")

            # 1. è¿è¡Œæµ‹è¯•è·å–å¤±è´¥ä¿¡æ¯
            print("ğŸ“Š Running tests to collect failures...")
            test_output = self._run_tests_collect_failures()

            # 2. è·å–è¦†ç›–ç‡æ•°æ®
            print("ğŸ“ˆ Collecting coverage data...")
            coverage_data = self._collect_coverage_data()

            # 3. AI åˆ†ææµ‹è¯•å¤±è´¥
            print("ğŸ¤– AI analyzing test failures...")
            analysis_result = self.analyzer.analyze_test_results(test_output, coverage_data)

            # 4. ç”Ÿæˆ BUGFIX_TODO
            print("ğŸ“ Generating BUGFIX_TODO...")
            todo_path = self.reports_dir / "BUGFIX_TODO.md"
            self.analyzer.generate_bugfix_todo(analysis_result, todo_path)

            # 5. æ˜¾ç¤ºåˆ†æç»“æœ
            self._display_analysis_summary(analysis_result)

            # 6. å¦‚æœæœ‰ä¿®å¤å»ºè®®ï¼Œè¯¢é—®æ˜¯å¦åº”ç”¨
            if analysis_result.fix_suggestions and interactive:
                return self._handle_fix_application(analysis_result)

            print("âœ… Analysis completed successfully!")
            return True

        except Exception as e:
            logger.error(f"Error in bugfix analysis: {e}")
            print(f"âŒ Error: {e}")
            return False

    def apply_recommended_fixes(self, todo_path: Optional[Path] = None) -> bool:
        """
        åº”ç”¨æ¨èçš„ä¿®å¤ï¼ˆä»ç°æœ‰ TODO æˆ–æ–°çš„åˆ†æï¼‰

        Args:
            todo_path: TODO æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸º None åˆ™é‡æ–°åˆ†æ

        Returns:
            æ˜¯å¦æˆåŠŸåº”ç”¨ä¿®å¤
        """
        try:
            # å¦‚æœæ²¡æœ‰æä¾› TODO è·¯å¾„ï¼Œé‡æ–°åˆ†æ
            if todo_path is None:
                print("ğŸ”„ Running fresh analysis...")
                if not self.run_analysis_and_generate_fixes(interactive=False):
                    return False

                todo_path = self.reports_dir / "BUGFIX_TODO.md"

            # è§£æ TODO æ–‡ä»¶è·å–ä¿®å¤å»ºè®®
            print("ğŸ“‹ Parsing TODO file for fix suggestions...")
            fix_suggestions = self._parse_todo_for_fixes(todo_path)

            if not fix_suggestions:
                print("â„¹ï¸  No fix suggestions found in TODO file")
                return True

            print(f"ğŸ¯ Found {len(fix_suggestions)} fix suggestions")

            # æ‰¹é‡åº”ç”¨ä¿®å¤
            results = self.applier.batch_apply_fixes(fix_suggestions, interactive=True)

            # æ˜¾ç¤ºåº”ç”¨ç»“æœ
            self._display_application_results(results)

            # æ›´æ–° TODO æ–‡ä»¶
            self._update_todo_after_fixes(todo_path, results)

            return True

        except Exception as e:
            logger.error(f"Error applying fixes: {e}")
            print(f"âŒ Error: {e}")
            return False

    def validate_fixes(self) -> bool:
        """
        éªŒè¯å·²åº”ç”¨çš„ä¿®å¤

        Returns:
            éªŒè¯æ˜¯å¦æˆåŠŸ
        """
        try:
            print("ğŸ” Validating applied fixes...")

            # è¿è¡Œæµ‹è¯•æ£€æŸ¥æ˜¯å¦ä¿®å¤æˆåŠŸ
            print("ğŸ“Š Running tests to validate fixes...")
            test_result = self._run_tests_collect_failures()

            # åˆ†ææµ‹è¯•ç»“æœ
            if "FAIL:" in test_result:
                print("âš ï¸  Some tests are still failing after fixes")
                # å¯ä»¥åœ¨è¿™é‡Œè¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æ
                return False
            else:
                print("âœ… All tests are passing! Fixes are successful.")
                return True

        except Exception as e:
            logger.error(f"Error validating fixes: {e}")
            return False

    def run_phase2_quality_analysis(self, incremental: bool = True) -> bool:
        """
        è¿è¡Œ Phase 2 ç»¼åˆè´¨é‡åˆ†æ

        Args:
            incremental: æ˜¯å¦ä½¿ç”¨å¢é‡æ¨¡å¼

        Returns:
            åˆ†ææ˜¯å¦æˆåŠŸå®Œæˆ
        """
        try:
            print("ğŸš€ Starting Phase 2 comprehensive quality analysis...")

            # 1. çªå˜æµ‹è¯•
            print("ğŸ”¬ Running mutation testing...")
            mutation_results = self.mutation_tester.run_mutation_tests(incremental=incremental)

            # 2. Flakyæµ‹è¯•æ£€æµ‹
            print("ğŸ”„ Detecting flaky tests...")
            flaky_results = self.flaky_detector.detect_flaky_tests(incremental=incremental)

            # 3. æ€§èƒ½åŸºå‡†æµ‹è¯•
            print("âš¡ Running performance benchmarks...")
            performance_results = self.performance_benchmark.run_performance_benchmarks(update_baselines=False)

            # 4. ç»¼åˆè´¨é‡è¯„ä¼°
            print("ğŸ“Š Aggregating quality metrics...")
            quality_results = self.quality_aggregator.run_comprehensive_quality_check(incremental=incremental)

            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            print("ğŸ“ Generating comprehensive report...")
            report_path = self._generate_phase2_report(mutation_results, flaky_results, performance_results, quality_results)

            print(f"âœ… Phase 2 analysis completed. Report: {report_path}")
            return True

        except Exception as e:
            logger.error(f"Error in Phase 2 quality analysis: {e}")
            print(f"âŒ Error: {e}")
            return False

    def run_phase3_intelligent_enhancement(self, target: Optional[str] = None) -> bool:
        """
        è¿è¡Œ Phase 3 æ™ºèƒ½åŒ–å®Œå–„

        Args:
            target: ç›®æ ‡æ–‡ä»¶æˆ–ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ†ææ‰€æœ‰å¤±è´¥æµ‹è¯•

        Returns:
            æ‰§è¡Œæ˜¯å¦æˆåŠŸ
        """
        try:
            print("ğŸ¤– Starting Phase 3 intelligent enhancement...")

            # 1. è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±æµ‹è¯•
            print("ğŸ§ª Auto-generating tests for uncovered files...")
            test_generation_results = self._auto_generate_tests(target)

            # 2. éªŒè¯ç°æœ‰ä¿®å¤
            print("ğŸ” Validating existing fixes...")
            validation_results = self._validate_existing_fixes()

            # 3. åˆ†æå†å²æ•°æ®è·å–æ”¹è¿›å»ºè®®
            print("ğŸ“š Analyzing historical data for improvement patterns...")
            learning_patterns = self._analyze_learning_patterns()

            # 4. æ›´æ–° TODO æ–‡ä»¶
            print("ğŸ“‹ Updating TODO file with Phase 3 results...")
            phase3_summary = {
                'tests_generated': len(test_generation_results.get('generated_files', [])),
                'test_cases_added': sum(len(result.get('validation', {}).get('test_count', 0)) for result in test_generation_results.get('results', [])),
                'coverage_improvement': 'Estimated 5-15%',
                'fixes_validated': validation_results.get('validated_count', 0),
                'fixes_failed': validation_results.get('failed_count', 0),
                'average_validation_score': validation_results.get('average_score', '0%'),
                'feedback_records_added': learning_patterns.get('new_feedback_records', 0),
                'patterns_learned': learning_patterns.get('patterns_discovered', 0),
                'suggestions_improved': learning_patterns.get('improvements_suggested', 0)
            }

            todo_path = Path("docs/_reports/BUGFIX_TODO.md")
            self.update_todo_with_phase3_results(todo_path, phase3_summary)

            # 5. ç”Ÿæˆæ™ºèƒ½æŠ¥å‘Š
            print("ğŸ“ Generating intelligent enhancement report...")
            report_path = self._generate_phase3_report(test_generation_results, validation_results, learning_patterns)

            print(f"âœ… Phase 3 enhancement completed. Report: {report_path}")
            return True

        except Exception as e:
            logger.error(f"Error in Phase 3 intelligent enhancement: {e}")
            print(f"âŒ Error: {e}")
            return False

    def auto_generate_tests(self, target: str) -> bool:
        """
        è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•

        Args:
            target: ç›®æ ‡æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ§ª Auto-generating tests for: {target}")

            target_path = Path(target)
            if target_path.is_file():
                result = self.test_generator.generate_tests_for_file(str(target_path))
            elif target_path.is_dir():
                result = self.test_generator.generate_tests_for_directory(str(target_path))
            else:
                print(f"âŒ Target not found: {target}")
                return False

            # è®°å½•åˆ°åé¦ˆæ•°æ®åº“
            self.feedback_db.add_test_generation(str(target_path), result)

            # æ˜¾ç¤ºç»“æœ
            if "error" in result:
                print(f"âŒ Test generation failed: {result['error']}")
                return False
            else:
                print(f"âœ… Tests generated: {result['test_file']}")
                validation = result.get("validation", {})
                if validation.get("syntax_valid"):
                    print(f"âœ… Generated {validation.get('test_count', 0)} tests with {validation.get('assertion_count', 0)} assertions")
                else:
                    print("âš ï¸ Generated tests have syntax issues")
                return True

        except Exception as e:
            logger.error(f"Error auto-generating tests: {e}")
            print(f"âŒ Error: {e}")
            return False

    def validate_fix(self, file_path: str) -> bool:
        """
        éªŒè¯ä¿®å¤æ•ˆæœ

        Args:
            file_path: è¦éªŒè¯çš„æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ” Validating fix for: {file_path}")

            # æ„å»ºä¿®å¤ä¿¡æ¯
            fix_info = {
                "file_path": file_path,
                "description": "Manual fix validation",
                "timestamp": datetime.now().isoformat()
            }

            # è¿è¡ŒéªŒè¯
            validation_result = self.fix_validator.validate_fix(fix_info)

            # è®°å½•åˆ°åé¦ˆæ•°æ®åº“
            fix_id = self.feedback_db.add_fix_attempt(fix_info)
            if fix_id:
                self.feedback_db.add_validation_result(fix_id, validation_result)

            # æ˜¾ç¤ºç»“æœ
            overall_result = validation_result.get("overall_result", "unknown")
            quality_score = validation_result.get("quality_score", 0)

            print(f"ğŸ“Š Validation result: {overall_result}")
            print(f"ğŸ“ˆ Quality score: {quality_score:.2%}")

            if validation_result.get("recommendations"):
                print("ğŸ’¡ Recommendations:")
                for rec in validation_result["recommendations"][:3]:
                    print(f"   - {rec}")

            return overall_result in ["excellent", "good", "acceptable"]

        except Exception as e:
            logger.error(f"Error validating fix: {e}")
            print(f"âŒ Error: {e}")
            return False

    def show_feedback_history(self, days: int = 30) -> bool:
        """
        æ˜¾ç¤ºå†å²ä¿®å¤ç»éªŒ

        Args:
            days: æŸ¥è¯¢å¤©æ•°

        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            print(f"ğŸ“š Analyzing feedback history for last {days} days...")

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.feedback_db.query_fix_statistics(days)

            if not stats:
                print("â„¹ï¸ No feedback data available")
                return True

            print(f"\nğŸ“Š Fix Statistics (Last {days} days):")
            print(f"   Total fix attempts: {stats.get('total_fix_attempts', 0)}")
            print(f"   Successful fixes: {stats.get('successful_fixes', 0)}")
            print(f"   Failed fixes: {stats.get('failed_fixes', 0)}")
            print(f"   Average quality score: {stats.get('average_quality_score', 0):.2%}")

            # æ˜¾ç¤ºéªŒè¯ç»“æœåˆ†å¸ƒ
            validation_results = stats.get('validation_results', {})
            if validation_results:
                print(f"\nğŸ“ˆ Validation Results Distribution:")
                for result_type, count in validation_results.items():
                    print(f"   {result_type}: {count}")

            # æ˜¾ç¤ºå¸¸è§é”™è¯¯ç±»å‹
            top_errors = stats.get('top_error_types', {})
            if top_errors:
                print(f"\nğŸ” Top Error Types:")
                for error_type, count in list(top_errors.items())[:5]:
                    print(f"   {error_type}: {count}")

            # æ˜¾ç¤ºå¸¸è§æ–‡ä»¶è·¯å¾„
            top_files = stats.get('top_file_paths', {})
            if top_files:
                print(f"\nğŸ“ Most Fixed Files:")
                for file_path, count in list(top_files.items())[:5]:
                    print(f"   {file_path}: {count}")

            # è·å–å­¦ä¹ æ¨¡å¼
            patterns = self.feedback_db.get_learning_patterns()
            if patterns.get("effective_strategies"):
                print(f"\nâœ¨ Effective Strategies:")
                for strategy in list(patterns["effective_strategies"].keys())[:3]:
                    print(f"   - {strategy}")

            return True

        except Exception as e:
            logger.error(f"Error showing feedback history: {e}")
            print(f"âŒ Error: {e}")
            return False

    def _auto_generate_tests(self, target: Optional[str] = None) -> Dict:
        """è‡ªåŠ¨ç”Ÿæˆæµ‹è¯•çš„å†…éƒ¨å®ç°"""
        if target:
            target_path = Path(target)
            if target_path.is_file():
                return self.test_generator.generate_tests_for_file(str(target_path))
            elif target_path.is_dir():
                return self.test_generator.generate_tests_for_directory(str(target_path))

        # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ï¼Œåˆ†æä½è¦†ç›–ç‡æ–‡ä»¶
        print("ğŸ” Analyzing low coverage files...")
        # è¿™é‡Œå¯ä»¥æ·»åŠ è¦†ç›–ç‡åˆ†æé€»è¾‘
        return {"message": "No specific target provided, use --auto-generate-tests with specific target"}

    def _validate_existing_fixes(self) -> Dict:
        """éªŒè¯ç°æœ‰ä¿®å¤çš„å†…éƒ¨å®ç°"""
        # ä»TODOæ–‡ä»¶æˆ–å†å²æ•°æ®ä¸­è·å–ä¿®å¤ä¿¡æ¯
        todo_path = self.reports_dir / "BUGFIX_TODO.md"
        if todo_path.exists():
            # è§£æTODOæ–‡ä»¶è·å–ä¿®å¤ä¿¡æ¯
            # è¿™é‡Œå¯ä»¥æ‰©å±•TODOè§£æé€»è¾‘
            return {"message": "TODO parsing not fully implemented yet"}

        return {"message": "No existing fixes to validate"}

    def _analyze_learning_patterns(self) -> Dict:
        """åˆ†æå­¦ä¹ æ¨¡å¼çš„å†…éƒ¨å®ç°"""
        return self.feedback_db.get_learning_patterns()

    def _generate_phase3_report(self, test_results: Dict, validation_results: Dict, learning_patterns: Dict) -> Path:
        """ç”ŸæˆPhase 3æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        report_path = self.reports_dir / f"PHASE3_INTELLIGENT_REPORT_{timestamp}.md"

        # è·å–åé¦ˆç»Ÿè®¡
        stats = self.feedback_db.query_fix_statistics(30)

        report_content = f"""# ğŸ¤– Phase 3: æ™ºèƒ½åŒ–å®Œå–„ - ç»¼åˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´:** {datetime.now().isoformat()}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### æµ‹è¯•ç”Ÿæˆç»“æœ
- ç›®æ ‡: {test_results.get('target', 'Auto-detected')}
- çŠ¶æ€: {'âœ… æˆåŠŸ' if 'error' not in test_results else 'âŒ å¤±è´¥'}
- ç”Ÿæˆæ–‡ä»¶: {test_results.get('test_file', 'N/A')}

### ä¿®å¤éªŒè¯ç»“æœ
- éªŒè¯çŠ¶æ€: {validation_results.get('message', 'N/A')}

### å­¦ä¹ æ¨¡å¼åˆ†æ
- æœ‰æ•ˆç­–ç•¥æ•°é‡: {len(learning_patterns.get('effective_strategies', {}))}
- é—®é¢˜æ¨¡å¼æ•°é‡: {len(learning_patterns.get('problematic_approaches', {}))}

## ğŸ“ˆ å†å²ç»Ÿè®¡ (æœ€è¿‘30å¤©)

- æ€»ä¿®å¤å°è¯•: {stats.get('total_fix_attempts', 0)}
- æˆåŠŸä¿®å¤: {stats.get('successful_fixes', 0)}
- å¤±è´¥ä¿®å¤: {stats.get('failed_fixes', 0)}
- å¹³å‡è´¨é‡åˆ†æ•°: {stats.get('average_quality_score', 0):.2%}

## ğŸ¯ å…³é”®å‘ç°

### é«˜æ•ˆä¿®å¤ç­–ç•¥
{self._format_patterns_for_report(learning_patterns.get('effective_strategies', {}))}

### éœ€è¦é¿å…çš„æ¨¡å¼
{self._format_patterns_for_report(learning_patterns.get('problematic_approaches', {}))}

## ğŸš€ æ”¹è¿›å»ºè®®

### çŸ­æœŸä¼˜åŒ–
1. é‡ç‚¹å…³æ³¨é«˜é¢‘é”™è¯¯æ–‡ä»¶
2. åº”ç”¨æˆåŠŸçš„ä¿®å¤æ¨¡å¼
3. é¿å…å·²çŸ¥çš„é—®é¢˜ä¿®å¤æ–¹å¼

### é•¿æœŸç­–ç•¥
1. å»ºç«‹æ›´å®Œå–„çš„æµ‹è¯•è‡ªåŠ¨åŒ–æµç¨‹
2. æŒç»­å­¦ä¹ å’Œä¼˜åŒ–ä¿®å¤ç­–ç•¥
3. æ‰©å¤§åé¦ˆæ•°æ®åº“çš„è¦†ç›–èŒƒå›´

## âš ï¸ é£é™©æç¤º

- è‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•éœ€è¦äººå·¥å®¡æ ¸
- ä¿®å¤éªŒè¯ç»“æœä¾èµ–äºæµ‹è¯•è¦†ç›–ç‡
- å­¦ä¹ æ¨¡å¼éœ€è¦è¶³å¤Ÿçš„å†å²æ•°æ®æ”¯æŒ

---
*æ­¤æŠ¥å‘Šç”± AI æ™ºèƒ½åŒ–å®Œå–„ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        report_path.write_text(report_content, encoding='utf-8')
        return report_path

    def _format_patterns_for_report(self, patterns: Dict) -> str:
        """æ ¼å¼åŒ–æ¨¡å¼ä¸ºæŠ¥å‘Šæ–‡æœ¬"""
        if not patterns:
            return "æš‚æ— æ•°æ®"

        formatted_lines = []
        for pattern, count in list(patterns.items())[:5]:
            formatted_lines.append(f"- {pattern}: {count} æ¬¡")

        return "\n".join(formatted_lines) if formatted_lines else "æš‚æ— æ•°æ®"

    def generate_report(self) -> bool:
        """
        ç”Ÿæˆ AI ä¿®å¤æ´»åŠ¨æŠ¥å‘Š

        Returns:
            æ˜¯å¦æˆåŠŸç”ŸæˆæŠ¥å‘Š
        """
        try:
            print("ğŸ“Š Generating AI bugfix activity report...")

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            analyzer_stats = {
                "timestamp": datetime.now().isoformat(),
                "analysis_history": "Not implemented",  # å¯ä»¥æ‰©å±•è®°å½•åˆ†æå†å²
            }

            applier_stats = self.applier.get_fix_statistics()

            # ç”ŸæˆæŠ¥å‘Š
            report_path = self.reports_dir / f"AI_BUGFIX_REPORT_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.md"
            report_content = self._generate_report_content(analyzer_stats, applier_stats)

            report_path.write_text(report_content, encoding='utf-8')
            print(f"âœ… Report generated: {report_path}")

            return True

        except Exception as e:
            logger.error(f"Error generating report: {e}")
            return False

    def _run_tests_collect_failures(self) -> str:
        """è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†å¤±è´¥ä¿¡æ¯"""
        # ä½¿ç”¨ç°æœ‰çš„æµ‹è¯•è„šæœ¬é€»è¾‘
        cmd = [
            "pytest",
            "--maxfail=10",
            "--tb=short",
            "--disable-warnings",
            "tests/test_ai_demo_failure.py"  # æ˜ç¡®æŒ‡å®šæµ‹è¯•æ–‡ä»¶
        ]

        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

        # ä¿å­˜å¤±è´¥æ—¥å¿—åˆ°æ–‡ä»¶
        log_file = Path("pytest_failures.log")
        log_file.write_text(result.stdout + "\n" + result.stderr, encoding='utf-8')

        return result.stdout + "\n" + result.stderr

    def _collect_coverage_data(self) -> Optional[Dict]:
        """æ”¶é›†è¦†ç›–ç‡æ•°æ®"""
        try:
            # è¿è¡Œå¸¦è¦†ç›–ç‡çš„æµ‹è¯•
            cmd = [
                "pytest",
                "--cov=src",
                "--cov-report=json:coverage.json",
                "--maxfail=10",
                "--disable-warnings",
                "tests/test_ai_demo_failure.py"  # æ˜ç¡®æŒ‡å®šæµ‹è¯•æ–‡ä»¶
            ]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

            if Path("coverage.json").exists():
                return json.load(open("coverage.json"))
            else:
                logger.warning("Coverage data not available")
                return None

        except Exception as e:
            logger.error(f"Error collecting coverage data: {e}")
            return None

    def _display_analysis_summary(self, analysis_result: AnalysisResult):
        """æ˜¾ç¤ºåˆ†ææ‘˜è¦"""
        print("\n" + "="*50)
        print("ğŸ“Š AI åˆ†æç»“æœæ‘˜è¦")
        print("="*50)
        print(f"ğŸ” å‘ç°æµ‹è¯•å¤±è´¥: {analysis_result.analysis_summary['total_failures']} ä¸ª")
        print(f"ğŸ› ï¸  ç”Ÿæˆä¿®å¤å»ºè®®: {analysis_result.analysis_summary['generated_fixes']} ä¸ª")
        print(f"ğŸ“ˆ åˆ†ææˆåŠŸç‡: {analysis_result.analysis_summary['success_rate']:.1%}")
        print(f"ğŸ¯ æ•´ä½“ç½®ä¿¡åº¦: {analysis_result.confidence_score:.1%}")

        # æ˜¾ç¤ºé”™è¯¯ç±»å‹åˆ†å¸ƒ
        if analysis_result.analysis_summary["error_types"]:
            print("\nğŸ“‹ é”™è¯¯ç±»å‹åˆ†å¸ƒ:")
            for error_type, count in analysis_result.analysis_summary["error_types"].items():
                print(f"   - {error_type}: {count} æ¬¡")

        # æ˜¾ç¤ºç½®ä¿¡åº¦åˆ†å¸ƒ
        conf_dist = analysis_result.analysis_summary["confidence_distribution"]
        print(f"\nğŸ¯ ä¿®å¤å»ºè®®ç½®ä¿¡åº¦:")
        print(f"   - é«˜ç½®ä¿¡åº¦ (â‰¥80%): {conf_dist['high']} ä¸ª")
        print(f"   - ä¸­ç­‰ç½®ä¿¡åº¦ (50-80%): {conf_dist['medium']} ä¸ª")
        print(f"   - ä½ç½®ä¿¡åº¦ (<50%): {conf_dist['low']} ä¸ª")

        print("="*50)

    def _handle_fix_application(self, analysis_result: AnalysisResult) -> bool:
        """å¤„ç†ä¿®å¤åº”ç”¨"""
        if not analysis_result.fix_suggestions:
            print("â„¹ï¸  No fix suggestions available")
            return True

        response = input("\nğŸ¤– AI has generated fix suggestions. Apply them? [Y/n]: ").strip().lower()
        if response == 'n':
            print("â­ï¸  Skipping fix application")
            return True

        # è·å–ä¼˜å…ˆçº§æ’åºçš„ä¿®å¤å»ºè®®
        prioritized_fixes = self.analyzer.prioritize_fixes(analysis_result)

        # åº”ç”¨ä¿®å¤
        results = self.applier.batch_apply_fixes(prioritized_fixes, interactive=True)

        # æ˜¾ç¤ºåº”ç”¨ç»“æœ
        self._display_application_results(results)

        return True

    def _parse_todo_for_fixes(self, todo_path: Path) -> List:
        """ä» TODO æ–‡ä»¶è§£æä¿®å¤å»ºè®®"""
        # è¿™é‡Œå¯ä»¥æ‰©å±•è§£æ TODO æ–‡ä»¶çš„é€»è¾‘
        # ç›®å‰è¿”å›ç©ºåˆ—è¡¨ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ ¹æ® TODO å†…å®¹ç”Ÿæˆä¿®å¤å»ºè®®
        print("â„¹ï¸  TODO parsing not fully implemented yet")
        return []

    def _display_application_results(self, results: Dict[str, any]):
        """æ˜¾ç¤ºä¿®å¤åº”ç”¨ç»“æœ"""
        print("\n" + "="*50)
        print("ğŸ”§ ä¿®å¤åº”ç”¨ç»“æœ")
        print("="*50)

        successful = sum(1 for r in results.values() if r.success)
        failed = len(results) - successful

        print(f"âœ… æˆåŠŸåº”ç”¨: {successful} ä¸ª")
        print(f"âŒ åº”ç”¨å¤±è´¥: {failed} ä¸ª")

        if failed > 0:
            print("\nå¤±è´¥çš„ä¿®å¤:")
            for file_path, result in results.items():
                if not result.success:
                    print(f"   - {file_path}: {result.message}")

        print("="*50)

    def _update_todo_after_fixes(self, todo_path: Path, results: Dict[str, any]):
        """ä¿®å¤åº”ç”¨åæ›´æ–° TODO æ–‡ä»¶"""
        try:
            if todo_path.exists():
                content = todo_path.read_text(encoding='utf-8')

                # æ›´æ–°å®Œæˆç»Ÿè®¡
                successful = sum(1 for result in results.values() if result.success)
                failed = len(results) - successful

                # æ›´æ–°å®Œæˆçš„éƒ¨åˆ†
                completed_section = "## âœ… å·²å®Œæˆçš„ä¿®å¤\n"
                completed_content = []

                for file_path, result in results.items():
                    if result.success:
                        completed_content.append(f"- **{file_path}**: âœ… ä¿®å¤æˆåŠŸ")

                        # æ·»åŠ éªŒè¯ç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(result, 'validation_score'):
                            completed_content.append(f"  - éªŒè¯å¾—åˆ†: {result.validation_score:.2%}")

                        # æ·»åŠ æµ‹è¯•ç”Ÿæˆç»“æœï¼ˆå¦‚æœæœ‰ï¼‰
                        if hasattr(result, 'tests_generated'):
                            completed_content.append(f"  - ç”Ÿæˆæµ‹è¯•: {result.tests_generated} ä¸ª")
                    else:
                        completed_content.append(f"- **{file_path}**: âŒ ä¿®å¤å¤±è´¥ - {result.message}")

                if completed_content:
                    completed_section += "\n".join(completed_content)

                    # æ›¿æ¢åŸæœ‰çš„å®Œæˆéƒ¨åˆ†
                    if "## âœ… å·²å®Œæˆçš„ä¿®å¤" in content:
                        content = re.sub(
                            r"## âœ… å·²å®Œæˆçš„ä¿®å¤.*?(?=\n##|\n---|$)",
                            completed_section,
                            content,
                            flags=re.DOTALL
                        )
                    else:
                        # æ’å…¥æ–°çš„å®Œæˆéƒ¨åˆ†
                        insert_pos = content.find("## ğŸ”§ AI å»ºè®®çš„è¡ŒåŠ¨")
                        if insert_pos != -1:
                            content = content[:insert_pos] + completed_section + "\n\n" + content[insert_pos:]

                # æ›´æ–°æ—¶é—´æˆ³
                content = re.sub(
                    r"è‡ªåŠ¨æ›´æ–°äº:.*",
                    f"è‡ªåŠ¨æ›´æ–°äº: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                    content
                )

                todo_path.write_text(content, encoding='utf-8')
                print(f"âœ… TODO æ–‡ä»¶å·²æ›´æ–°: {todo_path}")

        except Exception as e:
            print(f"âš ï¸  æ›´æ–° TODO æ–‡ä»¶å¤±è´¥: {e}")

    def update_todo_with_phase3_results(self, todo_path: Path, phase3_results: Dict[str, any]):
        """ä½¿ç”¨ Phase 3 ç»“æœæ›´æ–° TODO æ–‡ä»¶"""
        try:
            if todo_path.exists():
                content = todo_path.read_text(encoding='utf-8')

                # æ·»åŠ  Phase 3 ç»“æœéƒ¨åˆ†
                phase3_section = f"""## ğŸ¤– Phase 3 æ™ºèƒ½åŒ–å¢å¼ºç»“æœ

### ğŸ“Š è‡ªåŠ¨æµ‹è¯•ç”Ÿæˆ
- ç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶: {phase3_results.get('tests_generated', 0)} ä¸ª
- æ–°å¢çš„æµ‹è¯•ç”¨ä¾‹: {phase3_results.get('test_cases_added', 0)} ä¸ª
- æµ‹è¯•è¦†ç›–ç‡æå‡: {phase3_results.get('coverage_improvement', '0%')}

### ğŸ” ä¿®å¤éªŒè¯ç»“æœ
- éªŒè¯é€šè¿‡çš„ä¿®å¤: {phase3_results.get('fixes_validated', 0)} ä¸ª
- éªŒè¯å¤±è´¥çš„ä¿®å¤: {phase3_results.get('fixes_failed', 0)} ä¸ª
- å¹³å‡éªŒè¯å¾—åˆ†: {phase3_results.get('average_validation_score', '0%')}

### ğŸ“š æŒç»­å­¦ä¹ ç»Ÿè®¡
- åé¦ˆè®°å½•æ–°å¢: {phase3_results.get('feedback_records_added', 0)} æ¡
- å­¦ä¹ åˆ°çš„æ¨¡å¼: {phase3_results.get('patterns_learned', 0)} ä¸ª
- ä¿®å¤å»ºè®®æ”¹è¿›: {phase3_results.get('suggestions_improved', 0)} ä¸ª

### ğŸ¯ AI å»ºè®®çš„åç»­è¡ŒåŠ¨
1. é‡ç‚¹å…³æ³¨éªŒè¯å¤±è´¥çš„ä¿®å¤ï¼Œé‡æ–°åˆ†æé—®é¢˜åŸå› 
2. åŸºäºç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–ä»£ç è´¨é‡
3. åˆ©ç”¨æŒç»­å­¦ä¹ æœºåˆ¶ï¼Œæé«˜æœªæ¥ä¿®å¤çš„å‡†ç¡®æ€§
4. å®šæœŸè¿è¡Œ Phase 3 åˆ†æä»¥ä¿æŒæ™ºèƒ½åŒ–çš„æŒç»­æå‡

"""

                # ç§»é™¤æ—§çš„ Phase 3 éƒ¨åˆ†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                content = re.sub(
                    r"## ğŸ¤– Phase 3 æ™ºèƒ½åŒ–å¢å¼ºç»“æœ.*?(?=\n##|\n---|$)",
                    "",
                    content,
                    flags=re.DOTALL
                )

                # æ’å…¥æ–°çš„ Phase 3 éƒ¨åˆ†
                insert_pos = content.find("---")
                if insert_pos != -1:
                    content = content[:insert_pos] + phase3_section + "\n---" + content[insert_pos:]

                # æ›´æ–°æ—¶é—´æˆ³
                content = re.sub(
                    r"è‡ªåŠ¨æ›´æ–°äº:.*",
                    f"è‡ªåŠ¨æ›´æ–°äº: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
                    content
                )

                todo_path.write_text(content, encoding='utf-8')
                print(f"âœ… TODO æ–‡ä»¶å·²æ›´æ–° Phase 3 ç»“æœ: {todo_path}")

        except Exception as e:
            print(f"âš ï¸  æ›´æ–° TODO æ–‡ä»¶ Phase 3 ç»“æœå¤±è´¥: {e}")

    def _generate_phase2_report(self, mutation_results: Dict, flaky_results: Dict, performance_results: Dict, quality_results: Dict) -> Path:
        """ç”Ÿæˆ Phase 2 ç»¼åˆæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
        report_path = self.reports_dir / f"PHASE2_QUALITY_REPORT_{timestamp}.md"

        # ç”Ÿæˆå„ç»„ä»¶çš„æŠ¥å‘Š
        mutation_report = self.mutation_tester.generate_mutation_report()
        flaky_report = self.flaky_detector.generate_flaky_report()
        performance_report = self.performance_benchmark.generate_performance_report()
        quality_report = self.quality_aggregator.generate_quality_report()

        report_content = f"""# ğŸš€ Phase 2: æµ‹è¯•æœ‰æ•ˆæ€§æå‡ - ç»¼åˆè´¨é‡æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´:** {datetime.now().isoformat()}
**æ‰§è¡Œæ¨¡å¼:** {"å¢é‡" if quality_results.get("execution_mode") == "incremental" else "å®Œæ•´"}

## ğŸ“Š è´¨é‡æ¦‚è§ˆ

- **æ•´ä½“è´¨é‡ç­‰çº§:** {quality_results.get('quality_grade', 'unknown').upper()}
- **ç»¼åˆå¾—åˆ†:** {quality_results.get('overall_score', 0):.2%}
- **æ€»æ‰§è¡Œæ—¶é—´:** {quality_results.get('execution_time', 0):.1f}ç§’

## ğŸ”¬ çªå˜æµ‹è¯•ç»“æœ

{mutation_report}

## ğŸ”„ Flakyæµ‹è¯•æ£€æµ‹ç»“æœ

{flaky_report}

## âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ

{performance_report}

## ğŸ“ˆ ç»¼åˆè´¨é‡è¯„ä¼°

{quality_report}

## ğŸ¯ æ”¹è¿›å»ºè®®

### çŸ­æœŸè¡ŒåŠ¨é¡¹
1. ä¼˜å…ˆå¤„ç†ä¸¥é‡è­¦æŠ¥å’Œé«˜é£é™©é—®é¢˜
2. ä¿®å¤å‘ç°çš„Flakyæµ‹è¯•ä»¥æé«˜æµ‹è¯•ç¨³å®šæ€§
3. è§£å†³æ€§èƒ½é€€åŒ–é—®é¢˜
4. æé«˜ä½è¦†ç›–ç‡æ¨¡å—çš„æµ‹è¯•è¦†ç›–ç‡

### é•¿æœŸç­–ç•¥
1. å»ºç«‹æŒç»­çš„æ€§èƒ½ç›‘æ§æœºåˆ¶
2. å®šæœŸè¿è¡Œçªå˜æµ‹è¯•ç¡®ä¿æµ‹è¯•è´¨é‡
3. å®æ–½ä»£ç å®¡æŸ¥æµç¨‹é¢„é˜²è´¨é‡é—®é¢˜
4. ä¼˜åŒ–CI/CDæµæ°´çº¿é›†æˆè´¨é‡æ£€æŸ¥

## âš ï¸ é£é™©æç¤º

- **éé˜»å¡æ¨¡å¼:** å½“å‰é…ç½®ä¸ºéé˜»å¡æ¨¡å¼ï¼Œä¸¥é‡è´¨é‡é—®é¢˜ä¸ä¼šé˜»æ–­CIæµç¨‹
- **å¢é‡æµ‹è¯•:** ä½¿ç”¨å¢é‡æ¨¡å¼æé«˜æµ‹è¯•æ•ˆç‡ï¼Œä½†å¯èƒ½é—æ¼æŸäº›é—®é¢˜
- **ç¯å¢ƒä¾èµ–:** æŸäº›æµ‹è¯•å¯èƒ½ä¾èµ–ç‰¹å®šç¯å¢ƒé…ç½®

---
*æ­¤æŠ¥å‘Šç”± AI å¢å¼ºçš„æµ‹è¯•æœ‰æ•ˆæ€§æå‡ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        report_path.write_text(report_content, encoding='utf-8')
        return report_path

    def _generate_report_content(self, analyzer_stats: Dict, applier_stats: Dict) -> str:
        """ç”ŸæˆæŠ¥å‘Šå†…å®¹"""
        lines = [
            "# ğŸ¤– AI Bugfix Activity Report",
            "",
            f"**Generated:** {analyzer_stats['timestamp']}",
            "",
            "## ğŸ“Š ä¿®å¤ç»Ÿè®¡",
            f"- æ€»ä¿®å¤å°è¯•: {applier_stats['total']}",
            f"- æˆåŠŸä¿®å¤: {applier_stats['successful']}",
            f"- å¤±è´¥ä¿®å¤: {applier_stats['failed']}",
            f"- æˆåŠŸç‡: {applier_stats['success_rate']:.1%}",
            "",
            "## ğŸ¯ å»ºè®®",
            "- å®šæœŸè¿è¡Œ AI åˆ†æä»¥ä¿æŒä»£ç è´¨é‡",
            "- ä¼˜å…ˆå¤„ç†é«˜ç½®ä¿¡åº¦çš„ä¿®å¤å»ºè®®",
            "- åº”ç”¨ä¿®å¤åå§‹ç»ˆè¿è¡Œæµ‹è¯•éªŒè¯",
            "- ä¿æŒå¤‡ä»½ä»¥ä¾¿å¿…è¦æ—¶å›æ»š",
            "",
            "---",
            "*æ­¤æŠ¥å‘Šç”± AI å¢å¼ºçš„ Bug ä¿®å¤ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*"
        ]

        return "\n".join(lines)

    def run_continuous_loop(self, max_iterations: int = None, non_interactive: bool = True) -> bool:
        """
        è¿è¡ŒæŒç»­ bug ä¿®å¤å¾ªç¯

        Args:
            max_iterations: æœ€å¤§å¾ªç¯æ¬¡æ•°ï¼ŒNone è¡¨ç¤ºæ— é™å¾ªç¯
            non_interactive: æ˜¯å¦éäº¤äº’å¼æ¨¡å¼

        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆæ‰€æœ‰å¾ªç¯
        """
        iteration = 0
        continuous_report_path = Path("docs/_reports") / f"CONTINUOUS_FIX_REPORT_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.md"

        # åˆ›å»ºæŒç»­ä¿®å¤æŠ¥å‘Š
        report_content = f"""# ğŸ”„ AI å¢å¼ºçš„æŒç»­ Bug ä¿®å¤å¾ªç¯æŠ¥å‘Š

**å¼€å§‹æ—¶é—´:** {datetime.now().isoformat()}
**æœ€å¤§å¾ªç¯æ¬¡æ•°:** {'æ— é™' if max_iterations is None else max_iterations}
**æ¨¡å¼:** {'éäº¤äº’å¼' if non_interactive else 'äº¤äº’å¼'}

## ğŸ“Š å¾ªç¯æ‰§è¡Œè®°å½•

"""

    try:
            while max_iterations is None or iteration < max_iterations:
            iteration += 1
            print(f"\n{'='*60}")
            print(f"ğŸ”„ å¼€å§‹ç¬¬ {iteration} è½®æŒç»­ Bug ä¿®å¤å¾ªç¯")
            print(f"{'='*60}")

            loop_start_time = datetime.now()
            loop_results = {
                'iteration': iteration,
                'start_time': loop_start_time.isoformat(),
                'phase1_success': False,
                'phase2_success': False,
                'phase3_success': False,
                'fixes_applied': 0,
                'tests_generated': 0,
                'errors': []
            }

            # Phase 1: æµ‹è¯•å¤±è´¥æ£€æµ‹ + Claude Code åˆ†æ
            try:
                print("\nğŸš€ Phase 1: æµ‹è¯•å¤±è´¥æ£€æµ‹ + AI åˆ†æ...")
                loop_results['phase1_success'] = self.run_analysis_and_generate_fixes(interactive=False)
                if loop_results['phase1_success']:
                    print("âœ… Phase 1 å®Œæˆ")
                else:
                    print("âš ï¸ Phase 1 éƒ¨åˆ†å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
            except Exception as e:
                error_msg = f"Phase 1 æ‰§è¡Œå¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                loop_results['errors'].append(error_msg)

            # Phase 2: æµ‹è¯•æœ‰æ•ˆæ€§åˆ†æ
            try:
                print("\nğŸ”¬ Phase 2: æµ‹è¯•æœ‰æ•ˆæ€§åˆ†æ...")
                loop_results['phase2_success'] = self.run_phase2_quality_analysis(incremental=True)
                if loop_results['phase2_success']:
                    print("âœ… Phase 2 å®Œæˆ")
                else:
                    print("âš ï¸ Phase 2 éƒ¨åˆ†å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
            except Exception as e:
                error_msg = f"Phase 2 æ‰§è¡Œå¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                loop_results['errors'].append(error_msg)

            # Phase 3: AI æ™ºèƒ½åŒ–åŠŸèƒ½
            try:
                print("\nğŸ¤– Phase 3: AI æ™ºèƒ½åŒ–å¢å¼º...")
                loop_results['phase3_success'] = self.run_phase3_intelligent_enhancement()
                if loop_results['phase3_success']:
                    print("âœ… Phase 3 å®Œæˆ")
                else:
                    print("âš ï¸ Phase 3 éƒ¨åˆ†å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œ")
            except Exception as e:
                error_msg = f"Phase 3 æ‰§è¡Œå¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                loop_results['errors'].append(error_msg)

            # è®°å½•å¾ªç¯ç»“æœ
            loop_end_time = datetime.now()
            loop_duration = (loop_end_time - loop_start_time).total_seconds()
            loop_results['end_time'] = loop_end_time.isoformat()
            loop_results['duration'] = f"{loop_duration:.1f}ç§’"

            # æ›´æ–°æŒç»­æŠ¥å‘Š
            report_content += f"""### å¾ªç¯ {iteration} - {loop_start_time.strftime('%H:%M:%S')}
- **æ‰§è¡Œæ—¶é—´:** {loop_results['duration']}
- **Phase 1:** {'âœ… æˆåŠŸ' if loop_results['phase1_success'] else 'âŒ å¤±è´¥'}
- **Phase 2:** {'âœ… æˆåŠŸ' if loop_results['phase2_success'] else 'âŒ å¤±è´¥'}
- **Phase 3:** {'âœ… æˆåŠŸ' if loop_results['phase3_success'] else 'âŒ å¤±è´¥'}
- **é”™è¯¯æ•°é‡:** {len(loop_results['errors'])}

"""

            if loop_results['errors']:
                report_content += "**é”™è¯¯è¯¦æƒ…:**\n"
                for error in loop_results['errors'][:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                    report_content += f"- {error}\n"
                report_content += "\n"

            # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
            continuous_report_path.write_text(report_content + "\n\n*æŠ¥å‘ŠæŒç»­æ›´æ–°ä¸­...*", encoding='utf-8')

            print(f"\nğŸ“Š ç¬¬ {iteration} è½®å¾ªç¯å®Œæˆ")
            print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {loop_results['duration']}")
            print(f"ğŸ“‹ æŠ¥å‘Šå·²æ›´æ–°: {continuous_report_path}")

            # å¦‚æœæ‰€æœ‰é˜¶æ®µéƒ½å¤±è´¥ï¼Œç­‰å¾…æ›´é•¿æ—¶é—´
            all_phases_failed = not any([loop_results['phase1_success'], loop_results['phase2_success'], loop_results['phase3_success']])
            if all_phases_failed:
                print("âš ï¸ æ‰€æœ‰é˜¶æ®µéƒ½å¤±è´¥ï¼Œç­‰å¾…60ç§’åé‡è¯•...")
                import time
                time.sleep(60)
            else:
                print("â³ ç­‰å¾…30ç§’åå¼€å§‹ä¸‹ä¸€è½®å¾ªç¯...")
                import time
                time.sleep(30)

        # å¾ªç¯ç»“æŸï¼Œå®ŒæˆæŠ¥å‘Š
        final_report = report_content + f"""
## ğŸ“Š å¾ªç¯æ‰§è¡Œæ€»ç»“

- **æ€»å¾ªç¯æ¬¡æ•°:** {iteration}
- **å®Œæˆæ—¶é—´:** {datetime.now().isoformat()}
- **æ€»ä½“çŠ¶æ€:** å®Œæˆ

## ğŸ“ˆ å„é˜¶æ®µæˆåŠŸç‡

- **Phase 1 (æµ‹è¯•åˆ†æ):** {'N/A' if iteration == 0 else f"{sum(1 for i in range(1, iteration+1) if getattr(self, f'_loop_{i}_phase1_success', False)) / iteration * 100:.1f}%"}
- **Phase 2 (è´¨é‡åˆ†æ):** {'N/A' if iteration == 0 else f"{sum(1 for i in range(1, iteration+1) if getattr(self, f'_loop_{i}_phase2_success', False)) / iteration * 100:.1f}%"}
- **Phase 3 (æ™ºèƒ½å¢å¼º):** {'N/A' if iteration == 0 else f"{sum(1 for i in range(1, iteration+1) if getattr(self, f'_loop_{i}_phase3_success', False)) / iteration * 100:.1f}%"}

## ğŸ¯ å»ºè®®çš„åç»­è¡ŒåŠ¨

1. **å®šæœŸè¿è¡ŒæŒç»­å¾ªç¯**ä»¥ä¿æŒä»£ç è´¨é‡
2. **ç›‘æ§ Phase æˆåŠŸç‡**ï¼Œå¦‚æœæœ‰é˜¶æ®µæŒç»­å¤±è´¥éœ€è¦è°ƒæŸ¥
3. **æ£€æŸ¥ç”Ÿæˆçš„æŠ¥å‘Š**äº†è§£ä¿®å¤è¿›å±•
4. **ç»“åˆäººå·¥å®¡æŸ¥**ç¡®ä¿ä¿®å¤è´¨é‡

---
*æ­¤æŠ¥å‘Šç”± AI å¢å¼ºçš„æŒç»­ Bug ä¿®å¤ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
"""

        continuous_report_path.write_text(final_report, encoding='utf-8')
        print(f"\nğŸ‰ æŒç»­ Bug ä¿®å¤å¾ªç¯å®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Š: {continuous_report_path}")

        return True

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  æŒç»­å¾ªç¯è¢«ç”¨æˆ·ä¸­æ–­ (ç¬¬ {iteration} è½®)")
        # ä¿å­˜ä¸­æ–­æ—¶çš„æŠ¥å‘Š
        interrupt_report = report_content + f"""
## â¹ï¸ å¾ªç¯ä¸­æ–­

- **ä¸­æ–­æ—¶é—´:** {datetime.now().isoformat()}
- **å®Œæˆå¾ªç¯æ•°:** {iteration - 1}
- **ä¸­æ–­åŸå› :** ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­

---
*æŠ¥å‘Šåœ¨å¾ªç¯ä¸­æ–­æ—¶è‡ªåŠ¨ä¿å­˜*
"""
        continuous_report_path.write_text(interrupt_report, encoding='utf-8')
        return True

    except Exception as e:
        print(f"\nâŒ æŒç»­å¾ªç¯å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        logger.error(f"Error in continuous loop: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="AI Enhanced Bugfix System")
    parser.add_argument("--mode", choices=["analyze", "fix", "validate", "report", "phase2", "phase3", "continuous"],
                       default="analyze", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--max-iterations", type=int, help="æŒç»­æ¨¡å¼çš„æœ€å¤§å¾ªç¯æ¬¡æ•°")
    parser.add_argument("--todo", type=str, help="TODO æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--non-interactive", action="store_true", help="éäº¤äº’å¼æ¨¡å¼")
    parser.add_argument("--full-analysis", action="store_true", help="è¿è¡Œå®Œæ•´åˆ†æï¼ˆéå¢é‡ï¼‰")

    # Phase 3 é€‰é¡¹
    parser.add_argument("--auto-generate-tests", type=str, help="è‡ªåŠ¨ç”Ÿæˆç¼ºå¤±æµ‹è¯•ï¼ˆæŒ‡å®šæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•ï¼‰")
    parser.add_argument("--validate-fix", type=str, help="éªŒè¯ä¿®å¤æ•ˆæœï¼ˆæŒ‡å®šä¿®å¤åçš„æ–‡ä»¶è·¯å¾„ï¼‰")
    parser.add_argument("--feedback", action="store_true", help="è¾“å‡ºå†å²ä¿®å¤ç»éªŒ")

    args = parser.parse_args()

    # åˆ›å»º AI å¢å¼ºä¿®å¤ç³»ç»Ÿ
    bugfix_system = AIEnhancedBugfix()

    # å¤„ç† Phase 3 é€‰é¡¹ï¼ˆä¼˜å…ˆå¤„ç†ï¼‰
    if args.auto_generate_tests:
        success = bugfix_system.auto_generate_tests(args.auto_generate_tests)
    elif args.validate_fix:
        success = bugfix_system.validate_fix(args.validate_fix)
    elif args.feedback:
        success = bugfix_system.show_feedback_history()
    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    elif args.mode == "analyze":
        success = bugfix_system.run_analysis_and_generate_fixes(
            interactive=not args.non_interactive
        )
    elif args.mode == "fix":
        todo_path = Path(args.todo) if args.todo else None
        success = bugfix_system.apply_recommended_fixes(todo_path)
    elif args.mode == "validate":
        success = bugfix_system.validate_fixes()
    elif args.mode == "report":
        success = bugfix_system.generate_report()
    elif args.mode == "phase2":
        success = bugfix_system.run_phase2_quality_analysis(
            incremental=not args.full_analysis
        )
    elif args.mode == "phase3":
        success = bugfix_system.run_phase3_intelligent_enhancement()
    elif args.mode == "continuous":
        success = bugfix_system.run_continuous_loop(
            max_iterations=args.max_iterations,
            non_interactive=args.non_interactive or True  # é»˜è®¤éäº¤äº’å¼
        )
    else:
        print(f"Unknown mode: {args.mode}")
        success = False

    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()