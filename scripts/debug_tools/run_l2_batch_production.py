#!/usr/bin/env python3
"""
L2æ‰¹å¤„ç†ä½œä¸š - ç”Ÿäº§ç‰ˆæœ¬
L2 Batch Job - Production Version
å®‰å…¨ã€å¯æ§çš„æ‰¹é‡æ•°æ®é‡‡é›†
"""

import asyncio
import logging
import sys
import os
import json
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.jobs.run_l2_details import FotMobL2DetailsJob
from src.database.async_manager import get_db_session
from sqlalchemy import text

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/l2_batch_production.log"),
        logging.StreamHandler(sys.stdout),
    ],
)

logger = logging.getLogger(__name__)

class ProductionL2BatchJob:
    """ç”Ÿäº§çº§L2æ‰¹å¤„ç†ä½œä¸š"""

    def __init__(self):
        self.logger = logger
        self.start_time = datetime.now()
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": self.start_time.isoformat(),
        }

    async def get_batch_stats(self) -> Dict[str, Any]:
        """è·å–å½“å‰æ‰¹æ¬¡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            async with get_db_session() as session:
                # æŸ¥è¯¢æ•´ä½“ç»Ÿè®¡
                total_query = text("""
                    SELECT
                        COUNT(*) as total_matches,
                        COUNT(CASE WHEN data_completeness = 'partial' THEN 1 END) as pending_matches,
                        COUNT(CASE WHEN data_completeness = 'complete' THEN 1 END) as completed_matches,
                        COUNT(CASE WHEN stats_json IS NOT NULL THEN 1 END) as with_stats,
                        COUNT(CASE WHEN lineups_json IS NOT NULL THEN 1 END) as with_lineups,
                        COUNT(CASE WHEN environment_json IS NOT NULL THEN 1 END) as with_environment
                    FROM matches
                    WHERE fotmob_id IS NOT NULL
                """)

                result = await session.execute(total_query)
                row = result.fetchone()

                if row:
                    return {
                        "total_matches": row.total_matches,
                        "pending_matches": row.pending_matches,
                        "completed_matches": row.completed_matches,
                        "matches_with_stats": row.with_stats,
                        "matches_with_lineups": row.with_lineups,
                        "matches_with_environment": row.with_environment,
                        "completion_rate": round((row.completed_matches / row.total_matches) * 100, 2) if row.total_matches > 0 else 0
                    }

        except Exception as e:
            self.logger.error(f"âŒ è·å–æ‰¹æ¬¡ç»Ÿè®¡å¤±è´¥: {e}")

        return {}

    async def estimate_completion_time(self, remaining_matches: int) -> str:
        """ä¼°ç®—å®Œæˆæ—¶é—´"""
        if self.stats["total_processed"] == 0:
            return "è®¡ç®—ä¸­..."

        elapsed = (datetime.now() - self.start_time).total_seconds()
        avg_time_per_match = elapsed / self.stats["total_processed"]

        estimated_seconds = remaining_matches * avg_time_per_match
        estimated_completion = datetime.now() + timedelta(seconds=estimated_seconds)

        return estimated_completion.strftime("%Y-%m-%d %H:%M:%S")

    async def run_production_job(self, max_matches: int = None):
        """è¿è¡Œç”Ÿäº§çº§æ‰¹å¤„ç†ä½œä¸š"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨L2æ‰¹å¤„ç†ä½œä¸š - ç”Ÿäº§ç‰ˆæœ¬")
            self.logger.info(f"ğŸ“… å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")

            # è·å–åˆå§‹ç»Ÿè®¡
            initial_stats = await self.get_batch_stats()
            self.logger.info("ğŸ“Š åˆå§‹æ•°æ®åº“çŠ¶æ€:")
            for key, value in initial_stats.items():
                self.logger.info(f"   {key}: {value}")

            # åˆå§‹åŒ–L2é‡‡é›†å™¨ä½œä¸š
            l2_job = FotMobL2DetailsJob()

            # è®¾ç½®åˆç†çš„é™åˆ¶ (åŸºäºé…ç½®)
            limit = max_matches or 1000  # é»˜è®¤æ¯æ‰¹æ¬¡1000åœº
            self.logger.info(f"ğŸ¯ æœ¬æ‰¹æ¬¡å¤„ç†ä¸Šé™: {limit} åœºæ¯”èµ›")

            # æ‰§è¡Œæ‰¹å¤„ç†
            await l2_job.run_job(limit=limit)

            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = await self.get_batch_stats()
            self.logger.info("ğŸ‰ æ‰¹å¤„ç†ä½œä¸šå®Œæˆ!")
            self.logger.info("ğŸ“Š æœ€ç»ˆæ•°æ®åº“çŠ¶æ€:")
            for key, value in final_stats.items():
                self.logger.info(f"   {key}: {value}")

            # è®¡ç®—æ‰§è¡Œæ—¶é—´
            end_time = datetime.now()
            duration = end_time - self.start_time
            self.logger.info(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {duration}")

        except Exception as e:
            self.logger.error(f"âŒ æ‰¹å¤„ç†ä½œä¸šå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

    async def run_with_monitoring(self, max_matches: int = None, monitoring_interval: int = 60):
        """å¸¦ç›‘æ§çš„æ‰¹å¤„ç†è¿è¡Œ"""
        self.logger.info("ğŸ“¡ å¯åŠ¨ç›‘æ§æ¨¡å¼...")

        # å¯åŠ¨ç›‘æ§ä»»åŠ¡
        monitor_task = asyncio.create_task(
            self.monitoring_loop(monitoring_interval)
        )

        try:
            # è¿è¡Œä¸»è¦ä½œä¸š
            await self.run_production_job(max_matches)
        finally:
            # åœæ­¢ç›‘æ§
            monitor_task.cancel()
            try:
                await monitor_task
            except asyncio.CancelledError:
                pass

    async def monitoring_loop(self, interval: int):
        """ç›‘æ§å¾ªç¯"""
        while True:
            try:
                await asyncio.sleep(interval)

                stats = await self.get_batch_stats()

                self.logger.info("ğŸ“Š [ç›‘æ§] å®æ—¶è¿›åº¦:")
                self.logger.info(f"   æ€»æ¯”èµ›: {stats.get('total_matches', 0)}")
                self.logger.info(f"   å¾…å¤„ç†: {stats.get('pending_matches', 0)}")
                self.logger.info(f"   å·²å®Œæˆ: {stats.get('completed_matches', 0)}")
                self.logger.info(f"   å®Œæˆç‡: {stats.get('completion_rate', 0)}%")

                if stats.get('pending_matches', 0) > 0:
                    eta = await self.estimate_completion_time(stats['pending_matches'])
                    self.logger.info(f"   é¢„è®¡å®Œæˆ: {eta}")

            except asyncio.CancelledError:
                break
            except Exception as e:
                self.logger.error(f"âŒ ç›‘æ§é”™è¯¯: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="L2æ‰¹å¤„ç†ä½œä¸š - ç”Ÿäº§ç‰ˆæœ¬")
    parser.add_argument("--max-matches", type=int, default=1000, help="æœ€å¤§å¤„ç†æ¯”èµ›æ•° (é»˜è®¤: 1000)")
    parser.add_argument("--monitor", action="store_true", help="å¯ç”¨å®æ—¶ç›‘æ§")
    parser.add_argument("--monitor-interval", type=int, default=60, help="ç›‘æ§é—´éš” (ç§’, é»˜è®¤: 60)")

    args = parser.parse_args()

    # åˆ›å»ºæ‰¹å¤„ç†ä½œä¸šå®ä¾‹
    job = ProductionL2BatchJob()

    if args.monitor:
        await job.run_with_monitoring(
            max_matches=args.max_matches,
            monitoring_interval=args.monitor_interval
        )
    else:
        await job.run_production_job(max_matches=args.max_matches)

if __name__ == "__main__":
    asyncio.run(main())