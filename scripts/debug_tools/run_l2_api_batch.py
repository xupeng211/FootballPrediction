#!/usr/bin/env python3
"""
L2 APIæ‰¹å¤„ç†ä½œä¸š - ä¿®å¤ç‰ˆæœ¬
L2 API Batch Job - Fixed Version
ä½¿ç”¨FotMob APIé‡‡é›†å™¨è¿›è¡Œæ‰¹é‡æ•°æ®é‡‡é›†
"""

import asyncio
import logging
import sys
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

from src.collectors.fotmob_api_collector import FotMobAPICollector, MatchDetailData
from src.database.async_manager import get_db_session, initialize_database
from sqlalchemy import text

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/l2_api_batch.log"),
        logging.StreamHandler(sys.stdout),
    ],
)

logger = logging.getLogger(__name__)

class L2APIBatchJob:
    """L2 APIæ‰¹å¤„ç†ä½œä¸š"""

    def __init__(self):
        self.logger = logger
        self.start_time = datetime.now()
        self.collector = None
        self.stats = {
            "total_processed": 0,
            "successful": 0,
            "failed": 0,
            "skipped": 0,
            "start_time": self.start_time.isoformat(),
        }

    async def initialize(self):
        """åˆå§‹åŒ–é‡‡é›†å™¨"""
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            await initialize_database()
            self.logger.info("âœ… æ•°æ®åº“è¿æ¥åˆå§‹åŒ–å®Œæˆ")

            # åˆ›å»ºAPIé‡‡é›†å™¨ (ä½¿ç”¨ä¿å®ˆè®¾ç½®)
            self.collector = FotMobAPICollector(
                max_concurrent=6,          # ä¿å®ˆå¹¶å‘æ•°
                timeout=45,                # è¶…æ—¶æ—¶é—´
                max_retries=3,             # æœ€å¤§é‡è¯•
                base_delay=2.0,            # åŸºç¡€å»¶è¿Ÿ
                enable_proxy=False,        # ç¦ç”¨ä»£ç†
                enable_jitter=True,        # å¯ç”¨éšæœºæŠ–åŠ¨
            )

            await self.collector.initialize()
            self.logger.info("âœ… FotMob APIé‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            self.logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def get_pending_matches(self, limit: int = 1000) -> list[str]:
        """è·å–å¾…å¤„ç†çš„æ¯”èµ›IDåˆ—è¡¨"""
        try:
            async with get_db_session() as session:
                query = text("""
                    SELECT fotmob_id
                    FROM matches
                    WHERE fotmob_id IS NOT NULL
                      AND data_completeness = 'partial'
                    ORDER BY match_date DESC NULLS LAST
                    LIMIT :limit
                """)

                result = await session.execute(query, {"limit": limit})
                matches = [row[0] for row in result.fetchall()]

                self.logger.info(f"ğŸ“Š æ‰¾åˆ° {len(matches)} åœºå¾…å¤„ç†çš„æ¯”èµ›")
                return matches

        except Exception as e:
            self.logger.error(f"âŒ è·å–å¾…å¤„ç†æ¯”èµ›å¤±è´¥: {e}")
            return []

    async def save_match_data(self, match_data: MatchDetailData) -> bool:
        """ä¿å­˜æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            async with get_db_session() as session:
                update_query = text("""
                    UPDATE matches SET
                        home_score = :home_score,
                        away_score = :away_score,
                        status = :status,
                        match_time = :match_time,
                        venue = :venue,
                        referee = :referee,
                        home_xg = :home_xg,
                        away_xg = :away_xg,
                        stats_json = :stats_json,
                        lineups_json = :lineups_json,
                        odds_snapshot_json = :odds_snapshot_json,
                        match_info = :match_info,
                        environment_json = :environment_json,
                        data_completeness = 'complete',
                        updated_at = :updated_at
                    WHERE fotmob_id = :fotmob_id
                """)

                await session.execute(update_query, {
                    "fotmob_id": match_data.fotmob_id,
                    "home_score": match_data.home_score,
                    "away_score": match_data.away_score,
                    "status": match_data.status,
                    "match_time": match_data.match_time,
                    "venue": match_data.venue,
                    "referee": match_data.referee,
                    "home_xg": match_data.xg_home,
                    "away_xg": match_data.xg_away,
                    "stats_json": match_data.stats_json,
                    "lineups_json": match_data.lineups_json,
                    "odds_snapshot_json": match_data.odds_snapshot_json,
                    "match_info": match_data.match_info,
                    "environment_json": match_data.environment_json,
                    "updated_at": datetime.now(),
                })

                await session.commit()
                return True

        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜æ¯”èµ›æ•°æ®å¤±è´¥ {match_data.fotmob_id}: {e}")
            return False

    async def process_match(self, fotmob_id: str) -> bool:
        """å¤„ç†å•åœºæ¯”èµ›"""
        try:
            # æ£€æŸ¥æ˜¯å¦å·²ç»å®Œæˆ
            async with get_db_session() as session:
                check_query = text("""
                    SELECT data_completeness FROM matches
                    WHERE fotmob_id = :fotmob_id
                """)
                result = await session.execute(check_query, {"fotmob_id": fotmob_id})
                row = result.fetchone()

                if row and row[0] == 'complete':
                    self.logger.info(f"â­ï¸ è·³è¿‡å·²å®Œæˆæ¯”èµ›: {fotmob_id}")
                    self.stats["skipped"] += 1
                    return True

            # é‡‡é›†æ•°æ®
            match_data = await self.collector.collect_match_details(fotmob_id)

            if match_data:
                # ä¿å­˜æ•°æ®
                success = await self.save_match_data(match_data)
                if success:
                    self.logger.info(f"âœ… æˆåŠŸå¤„ç†: {fotmob_id}")
                    self.stats["successful"] += 1
                    return True
                else:
                    self.logger.error(f"âŒ ä¿å­˜å¤±è´¥: {fotmob_id}")
                    self.stats["failed"] += 1
                    return False
            else:
                self.logger.warning(f"âš ï¸ æ•°æ®é‡‡é›†å¤±è´¥: {fotmob_id}")
                self.stats["failed"] += 1
                return False

        except Exception as e:
            self.logger.error(f"âŒ å¤„ç†æ¯”èµ›å¼‚å¸¸ {fotmob_id}: {e}")
            self.stats["failed"] += 1
            return False

    async def run_batch(self, limit: int = 1000):
        """è¿è¡Œæ‰¹å¤„ç†ä½œä¸š"""
        try:
            self.logger.info("ğŸš€ å¯åŠ¨L2 APIæ‰¹å¤„ç†ä½œä¸š")
            self.logger.info(f"ğŸ“… å¼€å§‹æ—¶é—´: {self.start_time.strftime('%Y-%m-%d %H:%M:%S')}")

            # åˆå§‹åŒ–
            await self.initialize()

            # è·å–å¾…å¤„ç†æ¯”èµ›
            pending_matches = await self.get_pending_matches(limit)

            if not pending_matches:
                self.logger.info("â„¹ï¸ æ²¡æœ‰å¾…å¤„ç†çš„æ¯”èµ›")
                return

            self.logger.info(f"ğŸ¯ å¼€å§‹å¤„ç† {len(pending_matches)} åœºæ¯”èµ›")

            # æ‰¹é‡å¤„ç†
            for i, fotmob_id in enumerate(pending_matches, 1):
                self.stats["total_processed"] += 1

                # æ˜¾ç¤ºè¿›åº¦
                if i % 10 == 0 or i == len(pending_matches):
                    progress = (i / len(pending_matches)) * 100
                    self.logger.info(f"ğŸ“Š è¿›åº¦: {i}/{len(pending_matches)} ({progress:.1f}%)")

                # å¤„ç†æ¯”èµ›
                await self.process_match(fotmob_id)

                # æ™ºèƒ½å»¶è¿Ÿ
                if i < len(pending_matches):
                    delay = 1.5 + (i % 3) * 0.5  # 1.5-3ç§’éšæœºå»¶è¿Ÿ
                    await asyncio.sleep(delay)

            # æœ€ç»ˆç»Ÿè®¡
            await self.print_final_stats()

        except Exception as e:
            self.logger.error(f"âŒ æ‰¹å¤„ç†ä½œä¸šå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise

        finally:
            if self.collector:
                await self.collector.close()

    async def print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡"""
        end_time = datetime.now()
        duration = end_time - self.start_time

        self.logger.info("ğŸ‰ æ‰¹å¤„ç†ä½œä¸šå®Œæˆ!")
        self.logger.info("="*50)
        self.logger.info(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {duration}")
        self.logger.info(f"ğŸ“Š æ€»å¤„ç†: {self.stats['total_processed']} åœº")
        self.logger.info(f"âœ… æˆåŠŸ: {self.stats['successful']} åœº")
        self.logger.info(f"âŒ å¤±è´¥: {self.stats['failed']} åœº")
        self.logger.info(f"â­ï¸ è·³è¿‡: {self.stats['skipped']} åœº")

        if self.stats['total_processed'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_processed']) * 100
            self.logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")

        # æ˜¾ç¤ºé‡‡é›†å™¨ç»Ÿè®¡
        if self.collector:
            collector_stats = self.collector.get_stats()
            self.logger.info(f"ğŸŒ APIè¯·æ±‚: {collector_stats.get('requests_made', 0)}")
            self.logger.info(f"ğŸ“¦ æ•°æ®å¤§å°: {collector_stats.get('total_data_size', 0)} å­—èŠ‚")

async def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="L2 APIæ‰¹å¤„ç†ä½œä¸š")
    parser.add_argument("--max-matches", type=int, default=1000, help="æœ€å¤§å¤„ç†æ¯”èµ›æ•° (é»˜è®¤: 1000)")

    args = parser.parse_args()

    # åˆ›å»ºå¹¶è¿è¡Œä½œä¸š
    job = L2APIBatchJob()
    await job.run_batch(limit=args.max_matches)

if __name__ == "__main__":
    asyncio.run(main())
