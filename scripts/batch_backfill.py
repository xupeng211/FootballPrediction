#!/usr/bin/env python3
"""
FotMobå†å²æ•°æ®æ‰¹é‡å›å¡«è„šæœ¬ - V2å…¨æ ˆæ¶æ„å¸ˆå‡çº§
è‡ªåŠ¨åŒ–å›å¡«è¿‡å»2å¹´ï¼ˆ2023-2024ï¼‰çš„FotMobæ·±åº¦æ¯”èµ›æ•°æ®
æ”¯æŒé˜µå®¹ã€ç»Ÿè®¡ã€äº‹ä»¶ç­‰AIè®­ç»ƒæ‰€éœ€çš„é«˜ä»·å€¼æ•°æ®
"""

import argparse
import asyncio
import json
import os
import random
import signal
import subprocess
import sys
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# ğŸš€ åˆ‡æ¢åˆ°V2é‡‡é›†å™¨å†…æ ¸
from src.data.collectors.fotmob_browser_v2 import (
    FotmobBrowserScraperV2,
    FotmobMatchDataV2,
)
from dataclasses import dataclass, asdict


class FotMobBackfill:
    """FotMobæ‰¹é‡å›å¡«å¤„ç†å™¨"""

    def __init__(self, max_concurrent: int = 2):
        self.start_date = None
        self.end_date = None
        self.output_dir = Path("data/fotmob/historical")
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # V2é…ç½®å‚æ•°
        self.max_concurrent = max_concurrent

        # å›å¡«çŠ¶æ€æ–‡ä»¶
        self.status_file = self.output_dir / "backfill_status.json"
        self.log_file = self.output_dir / "backfill.log"

        # ç»Ÿè®¡ä¿¡æ¯ - V2å‡çº§
        self.stats = {
            "start_time": None,
            "end_time": None,
            "total_days": 0,
            "processed_days": [],
            "successful_days": [],
            "failed_days": [],
            "total_matches": 0,
            "total_api_calls": 0,
            # V2æ·±åº¦æ•°æ®ç»Ÿè®¡
            "deep_matches": 0,
            "lineups_found": 0,
            "stats_found": 0,
            "complete_matches": 0,
            "zombie_cleanups": 0,
            "errors": [],
        }

    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] [{level}] {message}"

        print(log_entry)

        # åŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(log_entry + "\n")
        except Exception as e:
            print(f"âš ï¸ æ—¥å¿—å†™å…¥å¤±è´¥: {e}")

    def save_status(self):
        """ä¿å­˜å›å¡«çŠ¶æ€"""
        try:
            self.stats["end_time"] = datetime.now().isoformat()
            with open(self.status_file, "w", encoding="utf-8") as f:
                json.dump(self.stats, f, indent=2, ensure_ascii=False)
            self.log(f"ğŸ’¾ çŠ¶æ€å·²ä¿å­˜åˆ°: {self.status_file}")
        except Exception as e:
            self.log(f"âš ï¸ çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")

    def load_status(self):
        """åŠ è½½å›å¡«çŠ¶æ€"""
        try:
            if self.status_file.exists():
                with open(self.status_file, encoding="utf-8") as f:
                    saved_stats = json.load(f)

                    # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿›è¡Œçš„ä¸­æ–­ä»»åŠ¡
                    if saved_stats.get("processed_days") and not saved_stats.get(
                        "end_time"
                    ):
                        # æ¢å¤ä¸­æ–­çš„ä»»åŠ¡
                        self.stats = saved_stats
                        self.log("ğŸ”„ æ¢å¤ä¸­æ–­çš„å›å¡«ä»»åŠ¡")
                        return True

            self.log("â„¹ï¸ æœªæ‰¾åˆ°ä¸­æ–­ä»»åŠ¡ï¼Œå¼€å§‹æ–°çš„å›å¡«")
            return False
        except Exception as e:
            self.log(f"âš ï¸ çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
            return False

    def cleanup_zombie_processes(self) -> int:
        """ğŸ§¹ æ¸…ç†åƒµå°¸è¿›ç¨‹ - å…¨æ ˆæ¶æ„å¸ˆæ·»åŠ """
        cleaned_count = 0

        try:
            # æŸ¥æ‰¾Chrome/Playwrightç›¸å…³è¿›ç¨‹
            zombie_commands = [
                "chrome",
                "chromium",
                "chromium-browse",
                "playwright",
                "headless_shell",
            ]

            for cmd in zombie_commands:
                try:
                    # ä½¿ç”¨pgrepæŸ¥æ‰¾è¿›ç¨‹
                    result = subprocess.run(
                        ["pgrep", "-f", cmd], capture_output=True, text=True, timeout=10
                    )

                    if result.returncode == 0 and result.stdout.strip():
                        pids = result.stdout.strip().split("\n")
                        for pid in pids:
                            try:
                                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦çœŸçš„å­˜åœ¨
                                os.kill(int(pid), 0)  # ä¿¡å·0æ£€æŸ¥è¿›ç¨‹å­˜åœ¨æ€§
                                # ä¼˜é›…ç»ˆæ­¢è¿›ç¨‹
                                os.kill(int(pid), signal.SIGTERM)
                                cleaned_count += 1
                                self.log(f"ğŸ§¹ æ¸…ç†åƒµå°¸è¿›ç¨‹: {cmd} (PID: {pid})")
                            except (ProcessLookupError, PermissionError, OSError):
                                # è¿›ç¨‹å·²ç»ä¸å­˜åœ¨æˆ–æ— æƒé™ï¼Œè·³è¿‡
                                continue

                except (subprocess.TimeoutExpired, subprocess.CalledProcessError):
                    continue

            # ç­‰å¾…è¿›ç¨‹ä¼˜é›…é€€å‡º
            time.sleep(2)

            # å¼ºåˆ¶æ¸…ç†ä»åœ¨è¿è¡Œçš„è¿›ç¨‹
            for cmd in zombie_commands:
                try:
                    result = subprocess.run(
                        ["pkill", "-9", "-f", cmd],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                except:
                    pass

            if cleaned_count > 0:
                self.stats["zombie_cleanups"] += cleaned_count
                self.log(f"ğŸ§¹ åƒµå°¸è¿›ç¨‹æ¸…ç†å®Œæˆ: {cleaned_count} ä¸ªè¿›ç¨‹")

        except Exception as e:
            self.log(f"âš ï¸ åƒµå°¸è¿›ç¨‹æ¸…ç†å¤±è´¥: {e}")

        return cleaned_count

    def generate_date_range(self, start_date: str, end_date: str) -> list[str]:
        """ç”Ÿæˆå€’åºæ—¥æœŸèŒƒå›´åˆ—è¡¨ï¼ˆä»end_dateåˆ°start_dateï¼‰"""
        try:
            start_dt = datetime.strptime(start_date, "%Y%m%d")
            end_dt = datetime.strptime(end_date, "%Y%m%d")

            if start_dt > end_dt:
                raise ValueError("å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ")

            # ğŸ¯ å€’åºé‡‡é›†ï¼šä»æœ€è¿‘æ—¥æœŸå¼€å§‹
            current_dt = end_dt
            date_list = []

            while current_dt >= start_dt:
                date_list.append(current_dt.strftime("%Y%m%d"))
                current_dt -= timedelta(days=1)  # å€’åºé€’å‡

            return date_list

        except ValueError as e:
            self.log(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {e}")
            return []

    async def process_single_date(self, date_str: str) -> bool:
        """å¤„ç†å•ä¸ªæ—¥æœŸçš„æ•°æ®é‡‡é›† - V2æ·±åº¦é‡‡é›†"""
        self.log(f"ğŸ“… å€’åºå¤„ç†æ—¥æœŸ: {date_str} (V2æ·±åº¦é‡‡é›†æ¨¡å¼)")

        retry_count = 0
        max_retries = 3

        while retry_count < max_retries:
            try:
                # ğŸ§¹ æ¸…ç†åƒµå°¸è¿›ç¨‹ï¼Œé˜²æ­¢å†…å­˜æ³„æ¼
                self.cleanup_zombie_processes()

                # ğŸš€ ä½¿ç”¨V2æ·±åº¦é‡‡é›†å™¨
                async with FotmobBrowserScraperV2(
                    max_concurrent_details=self.max_concurrent
                ) as scraper:
                    match_data_list = await scraper.scrape_matches_deep(date_str)

                    if match_data_list:
                        # V2æ·±åº¦æ•°æ®ç»Ÿè®¡
                        self.stats["deep_matches"] += len(match_data_list)
                        self.stats["lineups_found"] += sum(
                            1 for m in match_data_list if m.lineups
                        )
                        self.stats["stats_found"] += sum(
                            1 for m in match_data_list if m.stats
                        )
                        self.stats["complete_matches"] += sum(
                            1
                            for m in match_data_list
                            if m.data_completeness == "complete"
                        )

                        # ä¿å­˜V2æ·±åº¦æ•°æ®åˆ°æ–‡ä»¶
                        output_file = (
                            self.output_dir / f"fotmob_matches_deep_{date_str}.json"
                        )

                        export_data = {
                            "collection_info": {
                                "collector_version": "v2_deep",
                                "collection_time": datetime.now().isoformat(),
                                "target_date": date_str,
                                "total_matches": len(match_data_list),
                                "api_calls": len(scraper.captured_data),
                            },
                            "collection_stats": scraper.stats,
                            "data_quality": {
                                "lineups_rate": self.stats["lineups_found"]
                                / max(len(match_data_list), 1),
                                "stats_rate": self.stats["stats_found"]
                                / max(len(match_data_list), 1),
                                "complete_rate": self.stats["complete_matches"]
                                / max(len(match_data_list), 1),
                            },
                            "matches": [asdict(match) for match in match_data_list],
                        }

                        with open(output_file, "w", encoding="utf-8") as f:
                            json.dump(export_data, f, indent=2, ensure_ascii=False)

                        self.stats["total_matches"] += len(match_data_list)
                        self.stats["total_api_calls"] += len(scraper.captured_data)

                        # V2è¯¦ç»†æ—¥å¿—
                        lineups_count = sum(1 for m in match_data_list if m.lineups)
                        stats_count = sum(1 for m in match_data_list if m.stats)
                        complete_count = sum(
                            1
                            for m in match_data_list
                            if m.data_completeness == "complete"
                        )

                        self.log("  âœ… V2æ·±åº¦é‡‡é›†æˆåŠŸ:")
                        self.log(f"    ğŸ“Š æ€»æ¯”èµ›: {len(match_data_list)} åœº")
                        self.log(
                            f"    ğŸ“‹ é˜µå®¹æ•°æ®: {lineups_count} åœº ({lineups_count/len(match_data_list)*100:.1f}%)"
                        )
                        self.log(
                            f"    ğŸ“ˆ ç»Ÿè®¡æ•°æ®: {stats_count} åœº ({stats_count/len(match_data_list)*100:.1f}%)"
                        )
                        self.log(
                            f"    ğŸ† å®Œæ•´æ•°æ®: {complete_count} åœº ({complete_count/len(match_data_list)*100:.1f}%)"
                        )
                        self.log(f"    ğŸ“¡ APIè°ƒç”¨: {len(scraper.captured_data)} æ¬¡")

                        return True
                    else:
                        self.log("  âš ï¸ è­¦å‘Š: V2æ·±åº¦é‡‡é›†æœªè·å–åˆ°ä»»ä½•æ¯”èµ›æ•°æ®")
                        return False

            except Exception as e:
                retry_count += 1
                error_msg = f"ç¬¬ {retry_count} æ¬¡å°è¯•å¤±è´¥: {str(e)}"
                self.log(f"  âŒ {error_msg}")

                if retry_count < max_retries:
                    wait_time = random.uniform(30, 60)  # å¤±è´¥åç­‰å¾…æ›´é•¿æ—¶é—´
                    self.log(f"  â³ ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                    await asyncio.sleep(wait_time)
                else:
                    self.log("  ğŸ’¥ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ­¤æ—¥æœŸ")
                    return False

    async def run_backfill(
        self, start_date: str, end_date: str, dry_run: bool = False
    ) -> dict:
        """è¿è¡Œæ‰¹é‡å›å¡«"""
        self.log("ğŸš€ å¯åŠ¨FotMobå†å²æ•°æ®æ‰¹é‡å›å¡«")
        self.log("ğŸ¯ é‡‡é›†ç­–ç•¥: å€’åºå›å¡« - ä»æœ€è¿‘æ—¥æœŸå¼€å§‹ï¼Œä¼˜å…ˆè·å–é«˜ä»·å€¼æ•°æ®")
        self.log(f"ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} â† {end_date} (ä»å³å¾€å·¦é‡‡é›†)")

        if dry_run:
            self.log("ğŸ” è¿è¡Œæ¨¡å¼: ä»…é¢„è§ˆï¼Œä¸å®é™…é‡‡é›†")

        # åˆå§‹åŒ–ç»Ÿè®¡
        self.stats["start_time"] = datetime.now().isoformat()
        self.stats["total_days"] = 0

        # ç”Ÿæˆæ—¥æœŸåˆ—è¡¨
        date_list = self.generate_date_range(start_date, end_date)
        if not date_list:
            self.log("âŒ æ—¥æœŸèŒƒå›´æ— æ•ˆ")
            return {"status": "failed", "error": "Invalid date range"}

        self.stats["total_days"] = len(date_list)
        self.log(f"ğŸ“Š æ€»è®¡éœ€è¦å¤„ç†: {len(date_list)} å¤©")

        if dry_run:
            self.log("ğŸ” é¢„è§ˆæ¨¡å¼ï¼Œå°†è¦å¤„ç†çš„æ—¥æœŸ:")
            for i, date in enumerate(date_list[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
                self.log(f"  {i+1:2d}. {date}")
            if len(date_list) > 10:
                self.log(f"  ... è¿˜æœ‰ {len(date_list) - 10} å¤©")
            return {"status": "preview", "total_days": len(date_list)}

        # å¤„ç†æ¯ä¸€å¤©
        for i, date_str in enumerate(date_list, 1):
            self.log(f"\n{'='*80}")
            self.log(
                f"ğŸ“Š è¿›åº¦: {i}/{len(date_list)} ({i/len(date_list)*100:.1f}%) - å¤„ç†æ—¥æœŸ: {date_str}"
            )
            self.log(f"{'='*80}")

            # æ£€æŸ¥æ˜¯å¦å·²ç»å¤„ç†è¿‡
            if date_str in self.stats["processed_days"]:
                self.log(f"â­ï¸ è·³è¿‡å·²å¤„ç†çš„æ—¥æœŸ: {date_str}")
                continue

            # å¤„ç†å•ä¸ªæ—¥æœŸ
            success = await self.process_single_date(date_str)

            # è®°å½•çŠ¶æ€
            self.stats["processed_days"].append(date_str)
            if success:
                self.stats["successful_days"].append(date_str)
            else:
                self.stats["failed_days"].append(date_str)
                error_msg = f"æ—¥æœŸ {date_str} é‡‡é›†å¤±è´¥"
                self.stats["errors"].append(error_msg)

            # ä¿å­˜çŠ¶æ€
            self.save_status()

            # æ™ºèƒ½ä¼‘çœ  - å…³é”®ç‰¹æ€§
            if i < len(date_list):  # ä¸æ˜¯æœ€åä¸€å¤©
                sleep_time = random.randint(10, 30)  # 10-30ç§’éšæœºä¼‘çœ 
                self.log(f"â³ æ™ºèƒ½ä¼‘çœ : {sleep_time} ç§’ (ä¿æŠ¤IP)")
                await asyncio.sleep(sleep_time)

        # å®Œæˆç»Ÿè®¡
        self.log(f"\n{'='*80}")
        self.log("ğŸ“Š æ‰¹é‡å›å¡«å®Œæˆç»Ÿè®¡:")
        processed_count = len(self.stats["processed_days"])
        self.log(f"  ğŸ“… å¤„ç†å¤©æ•°: {processed_count} å¤©")
        self.log(f"  âœ… æˆåŠŸå¤©æ•°: {len(self.stats['successful_days'])} å¤©")
        self.log(f"  âŒ å¤±è´¥å¤©æ•°: {len(self.stats['failed_days'])} å¤©")
        self.log(f"  âš½ æ€»æ¯”èµ›æ•°: {self.stats['total_matches']} åœº")
        self.log(f"  ğŸ“¡ æ€»APIè°ƒç”¨: {self.stats['total_api_calls']} æ¬¡")

        if self.stats["errors"]:
            self.log(f"  âŒ é”™è¯¯æ•°é‡: {len(self.stats['errors'])} ä¸ª")
            for i, error in enumerate(self.stats["errors"][:5]):  # æ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                self.log(f"    {i+1}. {error}")
            if len(self.stats["errors"]) > 5:
                self.log(f"    ... è¿˜æœ‰ {len(self.stats['errors']) - 5} ä¸ªé”™è¯¯")

        self.log(f"{'='*80}")

        return {
            "status": "completed",
            "total_days": len(date_list),
            "successful_days": len(self.stats["successful_days"]),
            "failed_days": len(self.stats["failed_days"]),
            "total_matches": self.stats["total_matches"],
            "total_api_calls": self.stats["total_api_calls"],
            "errors_count": len(self.stats["errors"]),
            "success_rate": (
                len(self.stats["successful_days"]) / len(date_list) if date_list else 0
            ),
        }


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="FotMobå†å²æ•°æ®æ‰¹é‡å›å¡«è„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ğŸ¯ å€’åºå›å¡«23/24èµ›å­£æ•°æ®ï¼ˆæ¨èï¼‰
  python batch_backfill.py --start 20230801 --end 20241130

  # å€’åºå›å¡«2024å¹´å…¨å¹´æ•°æ®
  python batch_backfill.py --start 20240101 --end 20241230

  # ä»æ˜¨å¤©å¼€å§‹å›å¡«åˆ°2023å¹´8æœˆ1æ—¥
  python batch_backfill.py --start 20230801 --end yesterday

  # é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…é‡‡é›†
  python batch_backfill.py --start 20230801 --end yesterday --dry-run

  # ç»§ç»­ä¸­æ–­çš„ä»»åŠ¡
  python batch_backfill.py --resume

  # æŒ‡å®šè¾“å‡ºç›®å½•
  python batch_backfill.py --start 20230801 --end yesterday --output-dir /custom/path
        """,
    )

    # æ—¥æœŸå‚æ•°
    parser.add_argument(
        "--start", type=str, required=True, help="å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼: YYYYMMDD"
    )

    parser.add_argument(
        "--end",
        type=str,
        help='ç»“æŸæ—¥æœŸï¼Œæ ¼å¼: YYYYMMDDï¼Œæˆ–ä½¿ç”¨ "yesterday"ï¼ˆé»˜è®¤ä¸ºæ˜¨å¤©ï¼‰',
    )

    parser.add_argument(
        "--dry-run", action="store_true", help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…é‡‡é›†æ•°æ®"
    )

    parser.add_argument("--resume", action="store_true", help="ç»§ç»­ä¹‹å‰ä¸­æ–­çš„ä»»åŠ¡")

    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/fotmob/historical",
        help="è¾“å‡ºç›®å½• (é»˜è®¤: data/fotmob/historical)",
    )

    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†æ—¥å¿—è¾“å‡º")

    # ğŸš€ V2ä¸“ç”¨å‚æ•° - å…¨æ ˆæ¶æ„å¸ˆæ·»åŠ 
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=2,
        help="V2é‡‡é›†å™¨å¹¶å‘æ•° (é»˜è®¤: 2ï¼Œæ¨èèŒƒå›´: 1-3)",
    )

    parser.add_argument(
        "--collector",
        type=str,
        default="v2",
        choices=["v1", "v2"],
        help="é‡‡é›†å™¨ç‰ˆæœ¬ (é»˜è®¤: v2)",
    )

    args = parser.parse_args()

    # åˆ›å»ºå›å¡«å¤„ç†å™¨ - V2é…ç½®
    backfill = FotMobBackfill(max_concurrent=args.max_concurrent)
    backfill.output_dir = Path(args.output_dir)

    # V2ç‰ˆæœ¬ç¡®è®¤
    if args.collector == "v2":
        backfill.log("ğŸš€ ä½¿ç”¨V2æ·±åº¦é‡‡é›†å™¨ (é˜µå®¹+ç»Ÿè®¡+äº‹ä»¶)")
    else:
        backfill.log("âš ï¸ ä½¿ç”¨V1åŸºç¡€é‡‡é›†å™¨ (ä»…åŸºç¡€æ•°æ®)")

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if not args.verbose:
        # ç®€åŒ–æ—¥å¿—è¾“å‡º
        def simple_log(msg, level="INFO"):
            if "ERROR" in level or "å¤±è´¥" in msg or "âŒ" in msg:
                print(msg)

        backfill.log = simple_log

    try:
        # å¤„ç†é»˜è®¤ç»“æŸæ—¥æœŸ
        if not args.end:
            end_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
            print(f"ğŸ“… æœªæŒ‡å®šç»“æŸæ—¥æœŸï¼Œé»˜è®¤ä½¿ç”¨æ˜¨å¤©: {end_date}")
        elif args.end.lower() == "yesterday":
            end_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")
        else:
            end_date = args.end

        if args.resume:
            # ç»§ç»­ä¸­æ–­çš„ä»»åŠ¡
            backfill.load_status()
            if backfill.stats.get("processed_days"):
                start_date = backfill.stats["processed_days"][
                    -1
                ]  # ä»æœ€åå¤„ç†çš„æ—¥æœŸç»§ç»­
                backfill.log(f"ğŸ”„ ä»ä¸­æ–­ç‚¹ç»§ç»­: {start_date} åˆ° {end_date}")
            else:
                backfill.log("â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°ä¸­æ–­ä»»åŠ¡ï¼Œæ‰§è¡Œå®Œæ•´å›å¡«")
                start_date = args.start
        else:
            start_date = args.start

        # è¿è¡Œå›å¡«
        result = await backfill.run_backfill(start_date, end_date, args.dry_run)

        # è¾“å‡ºç»“æœ
        if args.dry_run:
            print("\nğŸ” é¢„è§ˆæ¨¡å¼å®Œæˆ")
            print(f"ğŸ“… è®¡åˆ’å¤„ç†: {result.get('total_days', 0)} å¤©")
        else:
            print("\nğŸ‰ æ‰¹é‡å›å¡«å®Œæˆ!")
            print("ğŸ“Š ç»“æœç»Ÿè®¡:")
            print(f"  ğŸ“… æ€»å¤©æ•°: {result.get('total_days', 0)}")
            print(f"  âœ… æˆåŠŸ: {result.get('successful_days', 0)} å¤©")
            print(f"  âŒ å¤±è´¥: {result.get('failed_days', 0)} å¤©")
            print(f"  âš½ æ¯”èµ›: {result.get('total_matches', 0)} åœº")
            print(f"  ğŸ“¡ APIè°ƒç”¨: {result.get('total_api_calls', 0)} æ¬¡")
            print(f"  ğŸ“ˆ æˆåŠŸç‡: {result.get('success_rate', 0):.2%}")

            if result.get("success_rate", 0) > 0.9:
                print("ğŸŠ ä¼˜ç§€! æˆåŠŸç‡è¶…è¿‡90%")
            elif result.get("success_rate", 0) > 0.7:
                print("ğŸ‘ è‰¯å¥½! æˆåŠŸç‡è¶…è¿‡70%")
            else:
                print("âš ï¸  æ³¨æ„: æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥")

    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­äº†å›å¡«è¿‡ç¨‹")
        sys.exit(130)

    except Exception as e:
        print(f"\nâŒ å›å¡«è¿‡ç¨‹å¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    print("ğŸ”§ FotMobå†å²æ•°æ®æ‰¹é‡å›å¡«è„šæœ¬")
    print("ğŸ¯ è‡ªåŠ¨åŒ–å›å¡«è¿‡å»2å¹´çš„çœŸå®æ¯”èµ›æ•°æ®")
    print("âš¡ æ™ºèƒ½ä¼‘çœ ä¿æŠ¤IPï¼Œå®Œæ•´é”™è¯¯å¤„ç†")
    print()

    asyncio.run(main())
