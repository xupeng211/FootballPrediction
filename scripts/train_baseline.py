#!/usr/bin/env python3
"""
åŸºçº¿æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨æ»šåŠ¨ç‰¹å¾æ•°æ®é›†è®­ç»ƒXGBooståˆ†ç±»å™¨ä½œä¸ºæ€§èƒ½åŸºå‡†

æ ¸å¿ƒåŠŸèƒ½:
1. æ—¶åºæ•°æ®åˆ†å‰² (80%è®­ç»ƒ, 20%æµ‹è¯•)
2. ç‰¹å¾é€‰æ‹©å’Œé¢„å¤„ç†
3. XGBooståˆ†ç±»å™¨è®­ç»ƒ
4. æ€§èƒ½è¯„ä¼°å’Œç‰¹å¾é‡è¦æ€§åˆ†æ

ä½œè€…: Algorithm Engineer
åˆ›å»ºæ—¶é—´: 2025-12-10
ç‰ˆæœ¬: 1.0.0 - Baseline Model
"""

import pandas as pd
import numpy as np
from datetime import datetime
import logging
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, log_loss, classification_report
from xgboost import XGBClassifier
# import matplotlib.pyplot as plt  # å¯é€‰: ç”¨äºå¯è§†åŒ–

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class BaselineTrainer:
    """åŸºçº¿æ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self):
        self.model = None
        self.label_encoder = LabelEncoder()
        self.feature_names = None

    def load_and_prepare_data(self, file_path: str) -> pd.DataFrame:
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        logger.info(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {file_path}")
        df = pd.read_csv(file_path)

        # ä½¿ç”¨ç°æœ‰çš„resultåˆ— (H=Home, A=Away, D=Draw)
        if 'result' in df.columns:
            result_mapping = {'H': 'Home', 'A': 'Away', 'D': 'Draw'}
            df['true_result'] = df['result'].map(result_mapping)
        else:
            raise ValueError("âŒ æ•°æ®é›†ä¸­ç¼ºå°‘resultåˆ—")
        logger.info(f"ğŸ“ˆ çœŸå®æ¯”èµ›ç»“æœåˆ†å¸ƒ:")
        logger.info(f"   {df['true_result'].value_counts().to_dict()}")

        # åˆ›å»ºmatch_dateåˆ—ç”¨äºæ—¶åºæ’åº
        df['match_date'] = pd.to_datetime(
            df['year'].astype(str) + '-' +
            df['month'].astype(str).str.zfill(2) + '-' +
            '01'  # ä½¿ç”¨æ¯æœˆ1å·ä½œä¸ºä¼°ç®—æ—¥æœŸ
        )

        # æŒ‰æ—¥æœŸæ’åº (é‡è¦: æ—¶åºåˆ†å‰²)
        df = df.sort_values('match_date').reset_index(drop=True)

        logger.info(f"   æ•°æ®å½¢çŠ¶: {df.shape}")
        logger.info(f"   æ—¥æœŸèŒƒå›´: {df['match_date'].min()} åˆ° {df['match_date'].max()}")

        return df

    def select_features(self, df: pd.DataFrame) -> tuple:
        """é€‰æ‹©ç‰¹å¾å’Œç›®æ ‡å˜é‡"""
        logger.info("ğŸ¯ ç‰¹å¾é€‰æ‹©å’Œé¢„å¤„ç†")

        # å®šä¹‰éœ€è¦æ’é™¤çš„åˆ— (æ•°æ®æ³„éœ²æˆ–éç‰¹å¾)
        exclude_columns = [
            'id', 'match_date', 'home_team_name', 'away_team_name',
            'home_score', 'away_score', 'match_result', 'result_numeric',
            'home_xg', 'away_xg', 'xg_difference', 'total_xg',
            'has_xg_data', 'has_events', 'has_shotmap', 'has_lineups',
            'home_lineup_count', 'away_lineup_count', 'total_lineup_players', 'has_odds',
            'h2h_home_advantage', 'h2v_home_xg_boost'
        ]

        # é€‰æ‹©æ—¶åºå®‰å…¨çš„æ»šåŠ¨ç‰¹å¾ (æ— æ•°æ®æ³„éœ²)
        rolling_features = [col for col in df.columns if 'last_' in col]

        # é€‰æ‹©åŸºç¡€çš„èµ›å‰ä¸Šä¸‹æ–‡ç‰¹å¾
        context_features = [
            'year', 'month', 'day_of_week', 'is_weekend',
            # å¦‚æœæœ‰èµ”ç‡æ•°æ®ï¼Œå¯ä»¥æ·»åŠ 
            # 'home_win_odds', 'draw_odds', 'away_win_odds'
        ]

        context_features = [col for col in context_features if col in df.columns]

        # åˆå¹¶ç‰¹å¾åˆ— (åªåŒ…å«å®‰å…¨çš„èµ›å‰ç‰¹å¾)
        feature_columns = rolling_features + context_features

        logger.info(f"   âœ… å®‰å…¨ç‰¹å¾: {len(feature_columns)}")
        logger.info(f"   ğŸ“‹ æ€»ç‰¹å¾æ•°: {len(feature_columns)}")

        # æå–ç‰¹å¾å’Œç›®æ ‡
        X = df[feature_columns].copy()
        y = df['true_result'].copy()

        # å¤„ç†ç¼ºå¤±å€¼
        X = X.fillna(0)  # ç”¨0å¡«å……ç¼ºå¤±çš„æ»šåŠ¨ç‰¹å¾

        logger.info(f"   ğŸ“Š ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
        logger.info(f"   ğŸ¯ ç›®æ ‡å˜é‡å½¢çŠ¶: {y.shape}")

        return X, y, feature_columns

    def split_data_chronological(self, X: pd.DataFrame, y: pd.Series,
                                test_size: float = 0.2) -> tuple:
        """æ—¶åºæ•°æ®åˆ†å‰²"""
        logger.info("ğŸ“… æ‰§è¡Œæ—¶åºæ•°æ®åˆ†å‰²")

        # è®¡ç®—åˆ†å‰²ç‚¹ (å‰80%è®­ç»ƒ, å20%æµ‹è¯•)
        split_idx = int(len(X) * (1 - test_size))

        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]

        logger.info(f"   ğŸ“Š è®­ç»ƒé›†: {X_train.shape[0]} åœºæ¯”èµ›")
        logger.info(f"   ğŸ§ª æµ‹è¯•é›†: {X_test.shape[0]} åœºæ¯”èµ›")
        logger.info(f"   ğŸ“… è®­ç»ƒæ—¥æœŸ: {X_train.index.min()} åˆ° {X_train.index.max()}")
        logger.info(f"   ğŸ“… æµ‹è¯•æ—¥æœŸ: {X_test.index.min()} åˆ° {X_test.index.max()}")

        # ç¼–ç ç›®æ ‡å˜é‡
        y_train_encoded = self.label_encoder.fit_transform(y_train)
        y_test_encoded = self.label_encoder.transform(y_test)

        class_encoding = dict(zip(self.label_encoder.classes_,
                                 self.label_encoder.transform(self.label_encoder.classes_)))
        logger.info(f"   ğŸ·ï¸ ç±»åˆ«ç¼–ç : {class_encoding}")

        return X_train, X_test, y_train_encoded, y_test_encoded, y_train, y_test

    def train_xgboost(self, X_train: pd.DataFrame, y_train: np.ndarray) -> XGBClassifier:
        """è®­ç»ƒXGBoostæ¨¡å‹"""
        logger.info("ğŸš€ è®­ç»ƒXGBooståˆ†ç±»å™¨")

        # é…ç½®XGBoostå‚æ•° (å¯¹äº3åˆ†ç±»é—®é¢˜)
        params = {
            'n_estimators': 100,
            'max_depth': 6,
            'learning_rate': 0.1,
            'random_state': 42,
            'eval_metric': 'mlogloss',
            'objective': 'multi:softprob',
            'num_class': 3  # æ˜ç¡®æŒ‡å®š3åˆ†ç±»
        }

        self.model = XGBClassifier(**params)
        self.model.fit(X_train, y_train)

        self.feature_names = X_train.columns.tolist()

        logger.info(f"   âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")
        logger.info(f"   ğŸŒ³ æ ‘çš„æ•°é‡: {self.model.n_estimators}")
        logger.info(f"   ğŸ“Š ç‰¹å¾æ•°é‡: {len(self.feature_names)}")

        return self.model

    def evaluate_model(self, X_test: pd.DataFrame, y_test_encoded: np.ndarray,
                      y_test_original: pd.Series) -> dict:
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        logger.info("ğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°")

        # é¢„æµ‹
        y_pred = self.model.predict(X_test)
        y_pred_proba = self.model.predict_proba(X_test)

        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(y_test_encoded, y_pred)
        logloss = log_loss(y_test_encoded, y_pred_proba)

        # è§£ç é¢„æµ‹ç»“æœç”¨äºåˆ†ç±»æŠ¥å‘Š
        y_pred_original = self.label_encoder.inverse_transform(y_pred)

        logger.info(f"   ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")
        logger.info(f"   ğŸ“‰ æµ‹è¯•é›†Log Loss: {logloss:.4f}")

        # æ˜¾ç¤ºåˆ†ç±»æŠ¥å‘Š
        logger.info(f"   ğŸ“‹ åˆ†ç±»æŠ¥å‘Š:")
        report = classification_report(y_test_original, y_pred_original,
                                     output_dict=True, zero_division=0)

        for class_name in self.label_encoder.classes_:
            if class_name in report:
                precision = report[class_name]['precision']
                recall = report[class_name]['recall']
                f1 = report[class_name]['f1-score']
                logger.info(f"      {class_name:6}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")

        return {
            'accuracy': accuracy,
            'log_loss': logloss,
            'classification_report': report
        }

    def plot_feature_importance(self, top_n: int = 10):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾è¡¨"""
        if self.model is None or self.feature_names is None:
            logger.warning("âš ï¸ æ¨¡å‹æœªè®­ç»ƒï¼Œæ— æ³•æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§")
            return

        logger.info(f"ğŸ“ˆ ç‰¹å¾é‡è¦æ€§åˆ†æ (Top {top_n})")

        # è·å–ç‰¹å¾é‡è¦æ€§
        importance = self.model.feature_importances_
        feature_importance = list(zip(self.feature_names, importance))
        feature_importance.sort(key=lambda x: x[1], reverse=True)

        # æ˜¾ç¤ºTop Nç‰¹å¾
        logger.info(f"   ğŸ† Top {top_n} é‡è¦ç‰¹å¾:")
        for i, (feature, importance_score) in enumerate(feature_importance[:top_n], 1):
            logger.info(f"   {i:2d}. {feature:35}: {importance_score:.4f}")

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        importance_df = pd.DataFrame(feature_importance,
                                   columns=['feature', 'importance'])
        importance_df.to_csv('feature_importance.csv', index=False)
        logger.info(f"   ğŸ’¾ ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜è‡³: feature_importance.csv")

    def train_baseline_model(self, data_path: str) -> dict:
        """å®Œæ•´çš„åŸºçº¿æ¨¡å‹è®­ç»ƒæµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹åŸºçº¿æ¨¡å‹è®­ç»ƒæµç¨‹")

        # 1. åŠ è½½æ•°æ®
        df = self.load_and_prepare_data(data_path)

        # 2. ç‰¹å¾é€‰æ‹©
        X, y, feature_columns = self.select_features(df)

        # 3. æ—¶åºåˆ†å‰²
        X_train, X_test, y_train_enc, y_test_enc, y_train_orig, y_test_orig = \
            self.split_data_chronological(X, y)

        # 4. è®­ç»ƒæ¨¡å‹
        self.train_xgboost(X_train, y_train_enc)

        # 5. è¯„ä¼°æ¨¡å‹
        results = self.evaluate_model(X_test, y_test_enc, y_test_orig)

        # 6. ç‰¹å¾é‡è¦æ€§
        self.plot_feature_importance()

        logger.info("ğŸ‰ åŸºçº¿æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return results


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–åŸºçº¿è®­ç»ƒå™¨
    trainer = BaselineTrainer()

    try:
        # è®­ç»ƒè¯šå®åŸºçº¿æ¨¡å‹ (æ— æ•°æ®æ³„éœ²)
        results = trainer.train_baseline_model(
            data_path="data/processed/features_v2_rolling.csv"
        )

        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "="*80)
        print("ğŸ† BASELINE MODEL PERFORMANCE SUMMARY")
        print("="*80)

        print(f"\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"   ğŸ¯ å‡†ç¡®ç‡ (Accuracy): {results['accuracy']:.4f}")
        print(f"   ğŸ“‰ å¯¹æ•°æŸå¤± (Log Loss): {results['log_loss']:.4f}")

        print(f"\nâœ… åŸºçº¿æ¨¡å‹å·²å»ºç«‹ï¼Œå¯ä½œä¸ºåç»­æ¨¡å‹ä¼˜åŒ–çš„åŸºå‡†")
        print("="*80)

    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()