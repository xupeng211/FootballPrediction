#!/usr/bin/env python3
"""
æ·±åº¦è¦†ç›–ç‡åˆ†æå·¥å…·
å‡†ç¡®è¯„ä¼°å½“å‰é¡¹ç›®çš„æµ‹è¯•è¦†ç›–ç‡çŠ¶å†µ
"""

import subprocess
import json
import os
import ast
from pathlib import Path
from typing import Dict, List, Tuple
from datetime import datetime


class CoverageAnalyzer:
    def __init__(self):
        self.project_root = Path.cwd()
        self.source_dir = self.project_root / "src"
        self.test_dir = self.project_root / "tests"
        self.analysis_results = {}

    def analyze_python_files(self) -> Dict[str, any]:
        """åˆ†æPythonæºä»£ç æ–‡ä»¶"""
        python_files = list(self.source_dir.rglob("*.py"))

        total_files = len(python_files)
        total_lines = 0
        analyzable_lines = 0
        import_lines = 0
        class_lines = 0
        function_lines = 0

        file_details = []

        for file_path in python_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # ç®€å•çš„ä»£ç è¡Œç»Ÿè®¡
                lines = content.split("\n")
                file_line_count = len(lines)
                non_empty_lines = len([line for line in lines if line.strip()])

                # ä½¿ç”¨ASTåˆ†æä»£ç ç»“æ„
                try:
                    tree = ast.parse(content)

                    imports = 0
                    classes = 0
                    functions = 0

                    for node in ast.walk(tree):
                        if isinstance(node, (ast.Import, ast.ImportFrom)):
                            imports += 1
                        elif isinstance(node, ast.ClassDef):
                            classes += 1
                        elif isinstance(node, ast.FunctionDef):
                            functions += 1

                    file_details.append(
                        {
                            "path": str(file_path.relative_to(self.project_root)),
                            "total_lines": file_line_count,
                            "non_empty_lines": non_empty_lines,
                            "imports": imports,
                            "classes": classes,
                            "functions": functions,
                        }
                    )

                    total_lines += file_line_count
                    analyzable_lines += non_empty_lines
                    import_lines += imports * 2  # ä¼°ç®—importè¯­å¥è¡Œæ•°
                    class_lines += classes * 5  # ä¼°ç®—classå®šä¹‰è¡Œæ•°
                    function_lines += functions * 3  # ä¼°ç®—functionå®šä¹‰è¡Œæ•°

                except SyntaxError:
                    # æ–‡ä»¶æœ‰è¯­æ³•é”™è¯¯ï¼Œä½†ä»è®¡å…¥æ€»è¡Œæ•°
                    total_lines += file_line_count
                    file_details.append(
                        {
                            "path": str(file_path.relative_to(self.project_root)),
                            "total_lines": file_line_count,
                            "non_empty_lines": non_empty_lines,
                            "syntax_error": True,
                        }
                    )

            except Exception as e:
                print(f"åˆ†ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

        return {
            "total_files": total_files,
            "total_lines": total_lines,
            "analyzable_lines": analyzable_lines,
            "import_lines": import_lines,
            "class_lines": class_lines,
            "function_lines": function_lines,
            "file_details": file_details,
        }

    def analyze_test_files(self) -> Dict[str, any]:
        """åˆ†ææµ‹è¯•æ–‡ä»¶"""
        test_files = list(self.test_dir.rglob("test_*.py"))

        total_test_files = len(test_files)
        executable_tests = 0
        total_test_lines = 0
        actual_test_lines = 0

        test_file_details = []

        for file_path in test_files:
            try:
                with open(file_path, "r", encoding="utf-8") as f:
                    content = f.read()

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åŒ…å«å®è´¨æ€§æµ‹è¯•
                lines = content.split("\n")
                total_file_lines = len(lines)
                total_test_lines += total_file_lines

                # ç®€å•çš„å®è´¨æ€§æµ‹è¯•æ£€æµ‹
                has_assert = "assert" in content
                has_test_func = "def test_" in content
                has_pytest_mark = "@pytest.mark" in content
                has_fixture = "@pytest.fixture" in content

                # ä¼°ç®—å®è´¨æ€§æµ‹è¯•è¡Œæ•°
                actual_lines = len(
                    [
                        line
                        for line in lines
                        if line.strip()
                        and (
                            line.strip().startswith("def test_")
                            or "assert" in line
                            or "expect" in line.lower()
                        )
                    ]
                )

                actual_test_lines += actual_lines

                if has_assert or has_test_func:
                    executable_tests += 1

                test_file_details.append(
                    {
                        "path": str(file_path.relative_to(self.project_root)),
                        "total_lines": total_file_lines,
                        "actual_test_lines": actual_lines,
                        "has_assert": has_assert,
                        "has_test_func": has_test_func,
                        "has_pytest_mark": has_pytest_mark,
                        "has_fixture": has_fixture,
                    }
                )

            except Exception as e:
                print(f"åˆ†ææµ‹è¯•æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

        return {
            "total_test_files": total_test_files,
            "executable_tests": executable_tests,
            "total_test_lines": total_test_lines,
            "actual_test_lines": actual_test_lines,
            "test_file_details": test_file_details,
        }

    def estimate_coverage(self, source_analysis: Dict, test_analysis: Dict) -> Dict[str, any]:
        """ä¼°ç®—æµ‹è¯•è¦†ç›–ç‡"""

        # æ–¹æ³•1: åŸºäºæµ‹è¯•æ–‡ä»¶æ•°é‡
        coverage_by_files = (
            test_analysis["executable_tests"] / max(1, source_analysis["total_files"])
        ) * 100

        # æ–¹æ³•2: åŸºäºä»£ç è¡Œæ•°çš„ç†è®ºä¼°ç®—
        # å‡è®¾æ¯ä¸ªå¯æ‰§è¡Œæµ‹è¯•æ–‡ä»¶å¹³å‡æµ‹è¯•50è¡Œä»£ç 
        estimated_covered_lines = test_analysis["executable_tests"] * 50
        coverage_by_lines = (
            estimated_covered_lines / max(1, source_analysis["analyzable_lines"])
        ) * 100

        # æ–¹æ³•3: åŸºäºå®é™…æµ‹è¯•è¡Œæ•°
        coverage_by_test_lines = (
            test_analysis["actual_test_lines"] / max(1, source_analysis["analyzable_lines"])
        ) * 100

        # æ–¹æ³•4: åŸºäºæ ¸å¿ƒç»„ä»¶è¦†ç›–ä¼°ç®—
        # ç»Ÿè®¡æ ¸å¿ƒæ¨¡å—æ˜¯å¦æœ‰å¯¹åº”çš„æµ‹è¯•
        core_modules = ["config", "di", "exceptions", "database", "api", "services", "utils"]
        covered_modules = 0

        for module in core_modules:
            module_pattern = f"test_*{module}*"
            if any(
                module_pattern in test_file["path"]
                for test_file in test_analysis["test_file_details"]
            ):
                covered_modules += 1

        coverage_by_modules = (covered_modules / len(core_modules)) * 100

        # ç»¼åˆä¼°ç®— (åŠ æƒå¹³å‡)
        weights = {"files": 0.2, "lines": 0.3, "test_lines": 0.3, "modules": 0.2}

        estimated_coverage = (
            coverage_by_files * weights["files"]
            + coverage_by_lines * weights["lines"]
            + coverage_by_test_lines * weights["test_lines"]
            + coverage_by_modules * weights["modules"]
        )

        return {
            "estimated_coverage": round(estimated_coverage, 2),
            "coverage_by_files": round(coverage_by_files, 2),
            "coverage_by_lines": round(coverage_by_lines, 2),
            "coverage_by_test_lines": round(coverage_by_test_lines, 2),
            "coverage_by_modules": round(coverage_by_modules, 2),
            "method_weights": weights,
        }

    def run_actual_coverage_test(self) -> Dict[str, any]:
        """å°è¯•è¿è¡Œå®é™…çš„è¦†ç›–ç‡æµ‹è¯•"""
        try:
            # å°è¯•è¿è¡ŒåŸºç¡€çš„è¦†ç›–ç‡æµ‹è¯•
            result = subprocess.run(
                [
                    "python3",
                    "-m",
                    "pytest",
                    "--cov=src",
                    "--cov-report=json",
                    "--cov-report=term-missing",
                    "--tb=short",
                    "-q",
                ],
                capture_output=True,
                text=True,
                timeout=180,
                cwd=self.project_root,
            )

            if result.returncode == 0:
                # å°è¯•è¯»å–coverage.jsonæ–‡ä»¶
                coverage_file = self.project_root / "coverage.json"
                if coverage_file.exists():
                    with open(coverage_file, "r") as f:
                        coverage_data = json.load(f)

                    return {
                        "success": True,
                        "actual_coverage": coverage_data["totals"]["percent_covered"],
                        "lines_covered": coverage_data["totals"]["covered_lines"],
                        "lines_total": coverage_data["totals"]["num_statements"],
                        "files": coverage_data["files"],
                    }

            return {"success": False, "error": result.stderr, "returncode": result.returncode}

        except Exception as e:
            return {"success": False, "error": str(e)}

    def generate_comprehensive_report(self) -> Dict[str, any]:
        """ç”Ÿæˆç»¼åˆè¦†ç›–ç‡æŠ¥å‘Š"""

        print("ğŸ” å¼€å§‹æ·±åº¦è¦†ç›–ç‡åˆ†æ...")

        # 1. åˆ†ææºä»£ç 
        print("ğŸ“Š åˆ†ææºä»£ç æ–‡ä»¶...")
        source_analysis = self.analyze_python_files()

        # 2. åˆ†ææµ‹è¯•æ–‡ä»¶
        print("ğŸ§ª åˆ†ææµ‹è¯•æ–‡ä»¶...")
        test_analysis = self.analyze_test_files()

        # 3. ä¼°ç®—è¦†ç›–ç‡
        print("ğŸ“ˆ ä¼°ç®—æµ‹è¯•è¦†ç›–ç‡...")
        coverage_estimation = self.estimate_coverage(source_analysis, test_analysis)

        # 4. å°è¯•å®é™…æµ‹è¯•
        print("ğŸ¯ å°è¯•è¿è¡Œå®é™…è¦†ç›–ç‡æµ‹è¯•...")
        actual_coverage = self.run_actual_coverage_test()

        # 5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        report = {
            "analysis_time": datetime.now().isoformat(),
            "analyzer_version": "1.0.0",
            "project_root": str(self.project_root),
            "source_analysis": source_analysis,
            "test_analysis": test_analysis,
            "coverage_estimation": coverage_estimation,
            "actual_coverage": actual_coverage,
            "final_assessment": self.generate_final_assessment(
                source_analysis, test_analysis, coverage_estimation, actual_coverage
            ),
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = (
            self.project_root
            / f'deep_coverage_analysis_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        )
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ æ·±åº¦è¦†ç›–ç‡åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report

    def generate_final_assessment(
        self,
        source_analysis: Dict,
        test_analysis: Dict,
        coverage_estimation: Dict,
        actual_coverage: Dict,
    ) -> Dict[str, any]:
        """ç”Ÿæˆæœ€ç»ˆè¯„ä¼°"""

        if actual_coverage["success"]:
            final_coverage = actual_coverage["actual_coverage"]
            method = "actual_measurement"
            reliability = "high"
        else:
            final_coverage = coverage_estimation["estimated_coverage"]
            method = "estimation"
            reliability = "medium"

        # è¦†ç›–ç‡ç­‰çº§è¯„ä¼°
        if final_coverage >= 80:
            grade = "A+"
            assessment = "Excellent"
            recommendation = "Coverage meets enterprise standards"
        elif final_coverage >= 60:
            grade = "A"
            assessment = "Good"
            recommendation = "Coverage is good, consider adding more tests"
        elif final_coverage >= 40:
            grade = "B"
            assessment = "Fair"
            recommendation = "Coverage needs improvement"
        elif final_coverage >= 20:
            grade = "C"
            assessment = "Poor"
            recommendation = "Coverage is insufficient, major improvements needed"
        else:
            grade = "D"
            assessment = "Very Poor"
            recommendation = "Coverage is critically low, immediate action required"

        return {
            "final_coverage": round(final_coverage, 2),
            "measurement_method": method,
            "reliability": reliability,
            "grade": grade,
            "assessment": assessment,
            "recommendation": recommendation,
            "source_files": source_analysis["total_files"],
            "test_files": test_analysis["total_test_files"],
            "executable_tests": test_analysis["executable_tests"],
            "test_executable_rate": round(
                (test_analysis["executable_tests"] / max(1, test_analysis["total_test_files"]))
                * 100,
                2,
            ),
        }


def main():
    """ä¸»å‡½æ•°"""
    analyzer = CoverageAnalyzer()

    try:
        report = analyzer.generate_comprehensive_report()

        # æ‰“å°å…³é”®ç»“æœ
        print("\n" + "=" * 60)
        print("ğŸ“Š æ·±åº¦è¦†ç›–ç‡åˆ†æç»“æœ")
        print("=" * 60)

        final_assessment = report["final_assessment"]

        print(f"ğŸ¯ æœ€ç»ˆè¦†ç›–ç‡: {final_assessment['final_coverage']}%")
        print(f"ğŸ“ˆ æµ‹é‡æ–¹æ³•: {final_assessment['measurement_method']}")
        print(f"ğŸ† è¯„çº§: {final_assessment['grade']} ({final_assessment['assessment']})")
        print(f"ğŸ“ å»ºè®®: {final_assessment['recommendation']}")
        print(f"ğŸ“ æºä»£ç æ–‡ä»¶: {final_assessment['source_files']}")
        print(f"ğŸ§ª æµ‹è¯•æ–‡ä»¶: {final_assessment['test_files']}")
        print(f"âœ… å¯æ‰§è¡Œæµ‹è¯•: {final_assessment['executable_tests']}")
        print(f"ğŸ“Š æµ‹è¯•å¯æ‰§è¡Œç‡: {final_assessment['test_executable_rate']}%")

        print("\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")
        if final_assessment["final_coverage"] >= 80:
            print("âœ… 80%è¦†ç›–ç‡ç›®æ ‡: å·²è¾¾æˆ")
        elif final_assessment["final_coverage"] >= 50:
            print("âš ï¸ 80%è¦†ç›–ç‡ç›®æ ‡: éƒ¨åˆ†è¾¾æˆ")
        else:
            print("âŒ 80%è¦†ç›–ç‡ç›®æ ‡: æœªè¾¾æˆ")

        print("=" * 60)

        return report

    except Exception as e:
        print(f"âŒ æ·±åº¦è¦†ç›–ç‡åˆ†æå¤±è´¥: {e}")
        return None


if __name__ == "__main__":
    main()
