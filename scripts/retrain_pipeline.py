#!/usr/bin/env python3
"""
è‡ªåŠ¨é‡è®­ç»ƒç®¡é“

åŠŸèƒ½ï¼š
1. ç›‘æ§æ¨¡å‹å‡†ç¡®ç‡ï¼Œå½“ä½äºé˜ˆå€¼æ—¶è§¦å‘é‡è®­ç»ƒ
2. è‡ªåŠ¨è®­ç»ƒæ–°æ¨¡å‹å¹¶æ³¨å†Œåˆ°MLflow
3. ç”Ÿæˆæ–°æ—§æ¨¡å‹å¯¹æ¯”è¯„ä¼°æŒ‡æ ‡
4. æ”¯æŒæ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’Œå›æ»š
5. æä¾›äººå·¥å®¡æ ¸æµç¨‹
"""

import asyncio
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

import click
import mlflow.sklearn
from sqlalchemy import and_, func, select
from sqlalchemy.ext.asyncio import AsyncSession

import mlflow
from mlflow import MlflowClient

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.core.logging import get_logger  # noqa: E402
from src.database.connection import get_async_session  # noqa: E402
from src.database.models.predictions import Predictions  # noqa: E402

logger = get_logger(__name__)

# MLflowé…ç½®
MLFLOW_TRACKING_URI = "http://localhost:5002"
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)


class AutoRetrainPipeline:
    """è‡ªåŠ¨é‡è®­ç»ƒç®¡é“"""

    def __init__(
        self,
        accuracy_threshold: float = 0.45,
        min_predictions_required: int = 50,
        evaluation_window_days: int = 30,
    ):
        """
        åˆå§‹åŒ–è‡ªåŠ¨é‡è®­ç»ƒç®¡é“

        Args:
            accuracy_threshold: å‡†ç¡®ç‡é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§¦å‘é‡è®­ç»ƒ
            min_predictions_required: æœ€å°‘é¢„æµ‹æ•°é‡è¦æ±‚
            evaluation_window_days: è¯„ä¼°çª—å£å¤©æ•°
        """
        self.session: Optional[AsyncSession] = None
        self.mlflow_client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)
        self.accuracy_threshold = accuracy_threshold
        self.min_predictions_required = min_predictions_required
        self.evaluation_window_days = evaluation_window_days

        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("models/retrain_reports")
        self.output_dir.mkdir(parents=True, exist_ok=True)

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = get_async_session()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    async def evaluate_model_performance(
        self, model_name: str, model_version: str = None
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            model_name: æ¨¡å‹åç§°
            model_version: æ¨¡å‹ç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰

        Returns:
            Dict[str, Any]: æ€§èƒ½è¯„ä¼°ç»“æœ
        """
        if not self.session:
            raise RuntimeError("Session not initialized")

        cutoff_date = datetime.utcnow() - timedelta(days=self.evaluation_window_days)

        # æ„å»ºæŸ¥è¯¢æ¡ä»¶
        query_conditions = [
            Predictions.model_name == model_name,
            Predictions.verified_at >= cutoff_date,
            Predictions.verified_at.isnot(None),
        ]

        if model_version:
            query_conditions.append(Predictions.model_version == model_version)

        # æŸ¥è¯¢æ€§èƒ½æ•°æ®
        stmt = select(
            func.count().label("total_predictions"),
            func.sum(func.case((Predictions.is_correct.is_(True), 1), else_=0)).label(
                "correct_predictions"
            ),
            func.avg(Predictions.confidence_score).label("avg_confidence"),
            func.min(Predictions.verified_at).label("earliest_prediction"),
            func.max(Predictions.verified_at).label("latest_prediction"),
        ).where(and_(*query_conditions))

        result = await self.session.execute(stmt)
        stats = result.first()

        if not stats or stats.total_predictions == 0:
            return {
                "model_name": model_name,
                "model_version": model_version,
                "evaluation_period": self.evaluation_window_days,
                "total_predictions": 0,
                "accuracy": 0.0,
                "needs_retrain": False,
                "reason": "No predictions found",
            }

        accuracy = stats.correct_predictions / stats.total_predictions
        needs_retrain = (
            stats.total_predictions >= self.min_predictions_required
            and accuracy < self.accuracy_threshold
        )

        # è·å–æœ€è¿‘è¶‹åŠ¿
        recent_performance = await self._get_recent_performance_trend(
            model_name, model_version, days=7
        )

        evaluation = {
            "model_name": model_name,
            "model_version": model_version,
            "evaluation_period": self.evaluation_window_days,
            "total_predictions": stats.total_predictions,
            "correct_predictions": stats.correct_predictions,
            "accuracy": round(accuracy, 4),
            "avg_confidence": (
                float(stats.avg_confidence) if stats.avg_confidence else None
            ),
            "earliest_prediction": stats.earliest_prediction,
            "latest_prediction": stats.latest_prediction,
            "needs_retrain": needs_retrain,
            "threshold": self.accuracy_threshold,
            "recent_trend": recent_performance,
            "evaluated_at": datetime.utcnow(),
        }

        if needs_retrain:
            evaluation[
                "reason"
            ] = f"Accuracy {accuracy:.2%} below threshold {self.accuracy_threshold:.2%}"
        elif stats.total_predictions < self.min_predictions_required:
            evaluation[
                "reason"
            ] = f"Insufficient predictions ({stats.total_predictions} < {self.min_predictions_required})"
        else:
            evaluation["reason"] = "Performance satisfactory"

        logger.info(f"æ¨¡å‹ {model_name} è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡ {accuracy:.2%}, éœ€è¦é‡è®­ç»ƒ: {needs_retrain}")
        return evaluation

    async def _get_recent_performance_trend(
        self, model_name: str, model_version: str = None, days: int = 7
    ) -> Dict[str, Any]:
        """
        è·å–æœ€è¿‘çš„æ€§èƒ½è¶‹åŠ¿

        Args:
            model_name: æ¨¡å‹åç§°
            model_version: æ¨¡å‹ç‰ˆæœ¬
            days: åˆ†æå¤©æ•°

        Returns:
            Dict[str, Any]: è¶‹åŠ¿åˆ†æç»“æœ
        """
        if not self.session:
            raise RuntimeError("Session not initialized")

        cutoff_date = datetime.utcnow() - timedelta(days=days)

        query_conditions = [
            Predictions.model_name == model_name,
            Predictions.verified_at >= cutoff_date,
            Predictions.verified_at.isnot(None),
        ]

        if model_version:
            query_conditions.append(Predictions.model_version == model_version)

        # æŒ‰å¤©ç»Ÿè®¡å‡†ç¡®ç‡
        stmt = (
            select(
                func.date(Predictions.verified_at).label("prediction_date"),
                func.count().label("daily_predictions"),
                func.sum(
                    func.case((Predictions.is_correct.is_(True), 1), else_=0)
                ).label("daily_correct"),
            )
            .where(and_(*query_conditions))
            .group_by(func.date(Predictions.verified_at))
            .order_by(func.date(Predictions.verified_at))
        )

        result = await self.session.execute(stmt)
        daily_stats = result.all()

        if not daily_stats:
            return {"trend": "no_data", "daily_accuracies": []}

        daily_accuracies = [
            {
                "date": str(stat.prediction_date),
                "predictions": stat.daily_predictions,
                "accuracy": (
                    stat.daily_correct / stat.daily_predictions
                    if stat.daily_predictions > 0
                    else 0
                ),
            }
            for stat in daily_stats
        ]

        # åˆ†æè¶‹åŠ¿
        if len(daily_accuracies) >= 2:
            recent_acc = sum(d["accuracy"] for d in daily_accuracies[-3:]) / min(
                3, len(daily_accuracies)
            )
            early_acc = sum(d["accuracy"] for d in daily_accuracies[:3]) / min(
                3, len(daily_accuracies)
            )

            if recent_acc > early_acc * 1.05:
                trend = "improving"
            elif recent_acc < early_acc * 0.95:
                trend = "declining"
            else:
                trend = "stable"
        else:
            trend = "insufficient_data"

        return {
            "trend": trend,
            "daily_accuracies": daily_accuracies,
            "analysis_days": days,
        }

    async def get_models_for_evaluation(self) -> List[Dict[str, str]]:
        """
        è·å–éœ€è¦è¯„ä¼°çš„æ¨¡å‹åˆ—è¡¨

        Returns:
            List[Dict[str, str]]: æ¨¡å‹åˆ—è¡¨
        """
        try:
            # ä»MLflowè·å–æ³¨å†Œçš„æ¨¡å‹
            registered_models = self.mlflow_client.search_registered_models()

            models_to_evaluate = []

            for model in registered_models:
                model_name = model.name

                # è·å–ç”Ÿäº§ç¯å¢ƒç‰ˆæœ¬
                try:
                    production_versions = self.mlflow_client.get_latest_versions(
                        name=model_name, stages=["Production"]
                    )

                    if production_versions:
                        for version in production_versions:
                            models_to_evaluate.append(
                                {
                                    "model_name": model_name,
                                    "model_version": version.version,
                                    "stage": "Production",
                                }
                            )
                    else:
                        # å¦‚æœæ²¡æœ‰ç”Ÿäº§ç‰ˆæœ¬ï¼Œæ£€æŸ¥Stagingç‰ˆæœ¬
                        staging_versions = self.mlflow_client.get_latest_versions(
                            name=model_name, stages=["Staging"]
                        )

                        for version in staging_versions:
                            models_to_evaluate.append(
                                {
                                    "model_name": model_name,
                                    "model_version": version.version,
                                    "stage": "Staging",
                                }
                            )

                except Exception as e:
                    logger.error(f"è·å–æ¨¡å‹ {model_name} ç‰ˆæœ¬å¤±è´¥: {e}")
                    continue

            logger.info(f"æ‰¾åˆ°{len(models_to_evaluate)}ä¸ªæ¨¡å‹éœ€è¦è¯„ä¼°")
            return models_to_evaluate

        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def trigger_model_retraining(
        self, model_name: str, current_version: str, performance_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        è§¦å‘æ¨¡å‹é‡è®­ç»ƒ

        Args:
            model_name: æ¨¡å‹åç§°
            current_version: å½“å‰ç‰ˆæœ¬
            performance_data: æ€§èƒ½æ•°æ®

        Returns:
            Dict[str, Any]: é‡è®­ç»ƒç»“æœ
        """
        logger.info(f"å¼€å§‹é‡è®­ç»ƒæ¨¡å‹: {model_name} (å½“å‰ç‰ˆæœ¬: {current_version})")

        try:
            # åˆ›å»ºæ–°çš„å®éªŒè¿è¡Œ
            experiment_name = f"{model_name}_retrain"

            # ç¡®ä¿å®éªŒå­˜åœ¨
            try:
                experiment = mlflow.get_experiment_by_name(experiment_name)
                if experiment is None:
                    experiment_id = mlflow.create_experiment(experiment_name)
                else:
                    experiment_id = experiment.experiment_id
            except Exception:
                experiment_id = mlflow.create_experiment(experiment_name)

            with mlflow.start_run(experiment_id=experiment_id) as run:
                # è®°å½•é‡è®­ç»ƒè§¦å‘ä¿¡æ¯
                mlflow.log_params(
                    {
                        "trigger_reason": "automatic_retrain",
                        "previous_version": current_version,
                        "previous_accuracy": performance_data["accuracy"],
                        "threshold": self.accuracy_threshold,
                        "retrain_date": datetime.utcnow().isoformat(),
                    }
                )

                # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨¡å‹è®­ç»ƒé€»è¾‘
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„è®­ç»ƒè¿‡ç¨‹
                training_result = await self._execute_model_training(
                    model_name, performance_data
                )

                # è®°å½•è®­ç»ƒç»“æœ
                mlflow.log_metrics(training_result["metrics"])

                # è®°å½•æ¨¡å‹
                if "model" in training_result:
                    mlflow.sklearn.log_model(
                        training_result["model"],
                        "model",
                        registered_model_name=model_name,
                    )

                run_id = run.info.run_id

            # å°†æ–°æ¨¡å‹ç‰ˆæœ¬è®¾ç½®ä¸ºStaging
            latest_version = self._get_latest_model_version(model_name)
            if latest_version:
                self.mlflow_client.transition_model_version_stage(
                    name=model_name, version=latest_version, stage="Staging"
                )

                logger.info(f"æ–°æ¨¡å‹ç‰ˆæœ¬ {latest_version} å·²è®¾ç½®ä¸º Staging é˜¶æ®µ")

            return {
                "success": True,
                "model_name": model_name,
                "new_version": latest_version,
                "run_id": run_id,
                "training_metrics": training_result["metrics"],
                "retrained_at": datetime.utcnow(),
            }

        except Exception as e:
            logger.error(f"æ¨¡å‹é‡è®­ç»ƒå¤±è´¥: {e}")
            return {
                "success": False,
                "model_name": model_name,
                "error": str(e),
                "retrained_at": datetime.utcnow(),
            }

    async def _execute_model_training(
        self, model_name: str, performance_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ¨¡å‹è®­ç»ƒï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰

        Args:
            model_name: æ¨¡å‹åç§°
            performance_data: æ€§èƒ½æ•°æ®

        Returns:
            Dict[str, Any]: è®­ç»ƒç»“æœ
        """
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„æ¨¡å‹è®­ç»ƒé€»è¾‘
        # ä¸ºäº†æ¼”ç¤ºç›®çš„ï¼Œæˆ‘ä»¬è¿”å›æ¨¡æ‹Ÿçš„è®­ç»ƒç»“æœ

        logger.info(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ {model_name}...")

        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´

        # æ¨¡æ‹Ÿçš„è®­ç»ƒæŒ‡æ ‡
        training_metrics = {
            "train_accuracy": 0.72,
            "validation_accuracy": 0.68,
            "train_loss": 0.45,
            "validation_loss": 0.52,
            "training_time": 120,  # ç§’
            "epochs": 100,
            "learning_rate": 0.001,
        }

        logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒéªŒè¯å‡†ç¡®ç‡: {training_metrics['validation_accuracy']:.2%}")

        return {
            "success": True,
            "metrics": training_metrics,
            "model": None,  # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡
            "training_data_size": 1000,  # è®­ç»ƒæ•°æ®å¤§å°
            "features_used": 25,  # ä½¿ç”¨çš„ç‰¹å¾æ•°é‡
        }

    def _get_latest_model_version(self, model_name: str) -> Optional[str]:
        """
        è·å–æ¨¡å‹çš„æœ€æ–°ç‰ˆæœ¬

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            Optional[str]: æœ€æ–°ç‰ˆæœ¬å·
        """
        try:
            latest_versions = self.mlflow_client.get_latest_versions(name=model_name)
            if latest_versions:
                # è¿”å›ç‰ˆæœ¬å·æœ€å¤§çš„ç‰ˆæœ¬
                return str(max(int(v.version) for v in latest_versions))
            return None
        except Exception as e:
            logger.error(f"è·å–æœ€æ–°æ¨¡å‹ç‰ˆæœ¬å¤±è´¥: {e}")
            return None

    async def generate_comparison_report(
        self,
        model_name: str,
        old_version: str,
        new_version: str,
        retrain_result: Dict[str, Any],
    ) -> str:
        """
        ç”Ÿæˆæ–°æ—§æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š

        Args:
            model_name: æ¨¡å‹åç§°
            old_version: æ—§ç‰ˆæœ¬
            new_version: æ–°ç‰ˆæœ¬
            retrain_result: é‡è®­ç»ƒç»“æœ

        Returns:
            str: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        logger.info(f"ç”Ÿæˆæ¨¡å‹å¯¹æ¯”æŠ¥å‘Š: {model_name} v{old_version} vs v{new_version}")

        # è·å–æ—§æ¨¡å‹æ€§èƒ½æ•°æ®
        old_performance = await self.evaluate_model_performance(model_name, old_version)

        # è·å–æ–°æ¨¡å‹çš„è®­ç»ƒæŒ‡æ ‡
        new_metrics = retrain_result.get("training_metrics", {})

        # ç”ŸæˆæŠ¥å‘Šå†…å®¹
        report_time = datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S")

        report_content = f"""# æ¨¡å‹é‡è®­ç»ƒå¯¹æ¯”æŠ¥å‘Š

**æ¨¡å‹åç§°**: {model_name}
**ç”Ÿæˆæ—¶é—´**: {report_time}
**æŠ¥å‘Šç±»å‹**: è‡ªåŠ¨é‡è®­ç»ƒå¯¹æ¯”åˆ†æ

---

## ğŸ“Š é‡è®­ç»ƒè§¦å‘ä¿¡æ¯

### è§¦å‘åŸå› 
- **å½“å‰æ¨¡å‹ç‰ˆæœ¬**: v{old_version}
- **å½“å‰å‡†ç¡®ç‡**: {old_performance['accuracy']:.2%}
- **å‡†ç¡®ç‡é˜ˆå€¼**: {self.accuracy_threshold:.2%}
- **è¯„ä¼°å‘¨æœŸ**: {self.evaluation_window_days} å¤©
- **é¢„æµ‹æ•°é‡**: {old_performance['total_predictions']}

### æ€§èƒ½è¶‹åŠ¿
- **æœ€è¿‘è¶‹åŠ¿**: {old_performance['recent_trend']['trend']}
- **éœ€è¦é‡è®­ç»ƒ**: {'æ˜¯' if old_performance['needs_retrain'] else 'å¦'}

---

## ğŸ”„ é‡è®­ç»ƒç»“æœ

### æ–°æ¨¡å‹ä¿¡æ¯
- **æ–°ç‰ˆæœ¬**: v{new_version}
- **è®­ç»ƒçŠ¶æ€**: {'æˆåŠŸ' if retrain_result['success'] else 'å¤±è´¥'}
- **MLflowè¿è¡ŒID**: {retrain_result.get('run_id', 'N/A')}
- **æ¨¡å‹é˜¶æ®µ**: Stagingï¼ˆå¾…äººå·¥å®¡æ ¸ï¼‰

### è®­ç»ƒæŒ‡æ ‡
"""

        if new_metrics:
            report_content += f"""
| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| è®­ç»ƒå‡†ç¡®ç‡ | {new_metrics.get('train_accuracy', 'N/A'):.2%} |
| éªŒè¯å‡†ç¡®ç‡ | {new_metrics.get('validation_accuracy', 'N/A'):.2%} |
| è®­ç»ƒæŸå¤± | {new_metrics.get('train_loss', 'N/A'):.3f} |
| éªŒè¯æŸå¤± | {new_metrics.get('validation_loss', 'N/A'):.3f} |
| è®­ç»ƒæ—¶é—´ | {new_metrics.get('training_time', 'N/A')} ç§’ |
| è®­ç»ƒè½®æ•° | {new_metrics.get('epochs', 'N/A')} |
| å­¦ä¹ ç‡ | {new_metrics.get('learning_rate', 'N/A')} |
"""

        report_content += f"""

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ

### æ—§æ¨¡å‹è¡¨ç° (v{old_version})
- **æ•´ä½“å‡†ç¡®ç‡**: {old_performance['accuracy']:.2%}
- **é¢„æµ‹æ€»æ•°**: {old_performance['total_predictions']}
- **å¹³å‡ç½®ä¿¡åº¦**: {old_performance['avg_confidence']:.3f if old_performance['avg_confidence'] else 'N/A'}
- **è¯„ä¼°æ—¶æ®µ**: {old_performance['earliest_prediction']} è‡³ {old_performance['latest_prediction']}

### æ–°æ¨¡å‹è¡¨ç° (v{new_version})
- **éªŒè¯å‡†ç¡®ç‡**: {new_metrics.get('validation_accuracy', 0):.2%}
- **è®­ç»ƒæ•°æ®é‡**: {retrain_result.get('training_data_size', 'N/A')}
- **ä½¿ç”¨ç‰¹å¾æ•°**: {retrain_result.get('features_used', 'N/A')}

### å¯¹æ¯”åˆ†æ
"""

        if new_metrics.get("validation_accuracy"):
            improvement = (
                new_metrics["validation_accuracy"] - old_performance["accuracy"]
            )
            improvement_pct = (
                (improvement / old_performance["accuracy"]) * 100
                if old_performance["accuracy"] > 0
                else 0
            )

            report_content += f"""
- **å‡†ç¡®ç‡æ”¹è¿›**: {improvement:+.4f} ({improvement_pct:+.1f}%)
- **æ”¹è¿›çŠ¶æ€**: {'æ˜¾è‘—æ”¹è¿›' if improvement > 0.05 else 'è½»å¾®æ”¹è¿›' if improvement > 0 else 'éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–'}
"""

        report_content += """

---

## ğŸš€ éƒ¨ç½²å»ºè®®

### è‡ªåŠ¨è¯„ä¼°ç»“æœ
"""

        if (
            retrain_result["success"]
            and new_metrics.get("validation_accuracy", 0) > old_performance["accuracy"]
        ):
            report_content += """
âœ… **å»ºè®®éƒ¨ç½²**: æ–°æ¨¡å‹æ€§èƒ½ä¼˜äºå½“å‰æ¨¡å‹
- æ–°æ¨¡å‹å·²è®¾ç½®ä¸º Staging é˜¶æ®µ
- å»ºè®®è¿›è¡Œäººå·¥å®¡æ ¸åæ¨å¹¿åˆ°ç”Ÿäº§ç¯å¢ƒ
- å¯ä»¥è€ƒè™‘ A/B æµ‹è¯•éªŒè¯æ•ˆæœ
"""
        else:
            report_content += """
âš ï¸ **éœ€è¦äººå·¥å®¡æ ¸**: æ–°æ¨¡å‹æ€§èƒ½éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°
- æ–°æ¨¡å‹æš‚æ—¶ä¿æŒåœ¨ Staging é˜¶æ®µ
- å»ºè®®è¯¦ç»†åˆ†ææ€§èƒ½å·®å¼‚åŸå› 
- å¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°æˆ–æ•°æ®
"""

        report_content += f"""

### æ¨èæ“ä½œæ­¥éª¤
1. **æ€§èƒ½éªŒè¯**: åœ¨æµ‹è¯•ç¯å¢ƒä¸­éªŒè¯æ–°æ¨¡å‹æ•ˆæœ
2. **A/Bæµ‹è¯•**: å¯¹æ¯”æ–°æ—§æ¨¡å‹åœ¨çœŸå®åœºæ™¯ä¸­çš„è¡¨ç°
3. **é€æ­¥éƒ¨ç½²**: å…ˆéƒ¨ç½²åˆ°éƒ¨åˆ†æµé‡ï¼Œè§‚å¯Ÿæ•ˆæœ
4. **ç›‘æ§æŒ‡æ ‡**: å¯†åˆ‡å…³æ³¨å‡†ç¡®ç‡ã€ç½®ä¿¡åº¦ç­‰å…³é”®æŒ‡æ ‡
5. **å›æ»šå‡†å¤‡**: å¦‚æœ‰é—®é¢˜ï¼ŒåŠæ—¶å›æ»šåˆ°æ—§ç‰ˆæœ¬

---

## ğŸ“‹ æŠ€æœ¯ç»†èŠ‚

**MLflowä¿¡æ¯**:
- è·Ÿè¸ªURI: {MLFLOW_TRACKING_URI}
- å®éªŒåç§°: {model_name}_retrain
- æ¨¡å‹æ³¨å†Œè¡¨: {model_name}

**ç³»ç»Ÿé…ç½®**:
- å‡†ç¡®ç‡é˜ˆå€¼: {self.accuracy_threshold:.2%}
- æœ€å°é¢„æµ‹è¦æ±‚: {self.min_predictions_required}
- è¯„ä¼°çª—å£: {self.evaluation_window_days} å¤©

---

*æœ¬æŠ¥å‘Šç”±è‡ªåŠ¨é‡è®­ç»ƒç³»ç»Ÿç”Ÿæˆ*
"""

        # ä¿å­˜æŠ¥å‘Š
        report_filename = f"retrain_comparison_{model_name}_v{old_version}_to_v{new_version}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}.md"
        report_path = self.output_dir / report_filename

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        logger.info(f"å¯¹æ¯”æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return str(report_path)

    async def run_evaluation_cycle(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è¯„ä¼°å’Œé‡è®­ç»ƒå‘¨æœŸ

        Returns:
            Dict[str, Any]: æ‰§è¡Œç»“æœ
        """
        logger.info("å¼€å§‹è‡ªåŠ¨é‡è®­ç»ƒè¯„ä¼°å‘¨æœŸ")

        results = {
            "start_time": datetime.utcnow(),
            "models_evaluated": 0,
            "models_needing_retrain": 0,
            "retraining_triggered": 0,
            "retraining_successful": 0,
            "reports_generated": 0,
            "model_results": [],
            "errors": [],
        }

        try:
            # è·å–éœ€è¦è¯„ä¼°çš„æ¨¡å‹
            models_to_evaluate = await self.get_models_for_evaluation()
            results["models_evaluated"] = len(models_to_evaluate)

            if not models_to_evaluate:
                logger.warning("æœªæ‰¾åˆ°éœ€è¦è¯„ä¼°çš„æ¨¡å‹")
                return results

            # é€ä¸ªè¯„ä¼°æ¨¡å‹
            for model_info in models_to_evaluate:
                model_name = model_info["model_name"]
                model_version = model_info["model_version"]

                try:
                    logger.info(f"è¯„ä¼°æ¨¡å‹: {model_name} v{model_version}")

                    # è¯„ä¼°æ€§èƒ½
                    performance = await self.evaluate_model_performance(
                        model_name, model_version
                    )

                    model_result = {
                        "model_name": model_name,
                        "model_version": model_version,
                        "performance": performance,
                        "retrain_triggered": False,
                        "retrain_successful": False,
                        "new_version": None,
                        "report_path": None,
                    }

                    # å¦‚æœéœ€è¦é‡è®­ç»ƒ
                    if performance["needs_retrain"]:
                        results["models_needing_retrain"] += 1
                        logger.info(f"è§¦å‘é‡è®­ç»ƒ: {model_name} v{model_version}")

                        # æ‰§è¡Œé‡è®­ç»ƒ
                        retrain_result = await self.trigger_model_retraining(
                            model_name, model_version, performance
                        )

                        model_result["retrain_triggered"] = True
                        results["retraining_triggered"] += 1

                        if retrain_result["success"]:
                            results["retraining_successful"] += 1
                            model_result["retrain_successful"] = True
                            model_result["new_version"] = retrain_result["new_version"]

                            # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
                            try:
                                report_path = await self.generate_comparison_report(
                                    model_name,
                                    model_version,
                                    retrain_result["new_version"],
                                    retrain_result,
                                )
                                model_result["report_path"] = report_path
                                results["reports_generated"] += 1
                            except Exception as e:
                                logger.error(f"ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šå¤±è´¥: {e}")
                                results["errors"].append(
                                    f"Report generation failed for {model_name}: {e}"
                                )

                        model_result["retrain_result"] = retrain_result

                    results["model_results"].append(model_result)

                except Exception as e:
                    logger.error(f"å¤„ç†æ¨¡å‹ {model_name} å¤±è´¥: {e}")
                    results["errors"].append(f"Model {model_name}: {e}")

        except Exception as e:
            logger.error(f"è¯„ä¼°å‘¨æœŸæ‰§è¡Œå¤±è´¥: {e}")
            results["errors"].append(f"Cycle execution failed: {e}")

        results["end_time"] = datetime.utcnow()
        results["duration_seconds"] = (
            results["end_time"] - results["start_time"]
        ).total_seconds()

        logger.info(
            f"è¯„ä¼°å‘¨æœŸå®Œæˆ: "
            f"è¯„ä¼°{results['models_evaluated']}ä¸ªæ¨¡å‹, "
            f"{results['models_needing_retrain']}ä¸ªéœ€è¦é‡è®­ç»ƒ, "
            f"{results['retraining_successful']}ä¸ªé‡è®­ç»ƒæˆåŠŸ"
        )

        return results


@click.command()
@click.option("--threshold", default=0.45, help="å‡†ç¡®ç‡é˜ˆå€¼")
@click.option("--min-predictions", default=50, help="æœ€å°é¢„æµ‹æ•°é‡è¦æ±‚")
@click.option("--window-days", default=30, help="è¯„ä¼°çª—å£å¤©æ•°")
@click.option("--model-name", help="æŒ‡å®šæ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰")
@click.option("--dry-run", is_flag=True, help="è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…é‡è®­ç»ƒ")
@click.option("--verbose", is_flag=True, help="è¯¦ç»†è¾“å‡º")
def main(
    threshold: float,
    min_predictions: int,
    window_days: int,
    model_name: str,
    dry_run: bool,
    verbose: bool,
):
    """è‡ªåŠ¨é‡è®­ç»ƒç®¡é“ä¸»å…¥å£"""

    # é…ç½®æ—¥å¿—çº§åˆ«
    if verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    async def run():
        async with AutoRetrainPipeline(
            accuracy_threshold=threshold,
            min_predictions_required=min_predictions,
            evaluation_window_days=window_days,
        ) as pipeline:
            click.echo("ğŸš€ å¯åŠ¨è‡ªåŠ¨é‡è®­ç»ƒç®¡é“")
            click.echo(f"   å‡†ç¡®ç‡é˜ˆå€¼: {threshold:.2%}")
            click.echo(f"   æœ€å°é¢„æµ‹æ•°: {min_predictions}")
            click.echo(f"   è¯„ä¼°çª—å£: {window_days} å¤©")
            click.echo(f"   è¯•è¿è¡Œæ¨¡å¼: {'æ˜¯' if dry_run else 'å¦'}")

            if model_name:
                click.echo(f"   æŒ‡å®šæ¨¡å‹: {model_name}")

                # å•æ¨¡å‹è¯„ä¼°
                performance = await pipeline.evaluate_model_performance(model_name)

                click.echo("\nğŸ“Š æ¨¡å‹æ€§èƒ½è¯„ä¼°:")
                click.echo(f"   å‡†ç¡®ç‡: {performance['accuracy']:.2%}")
                click.echo(f"   é¢„æµ‹æ•°é‡: {performance['total_predictions']}")
                click.echo(f"   éœ€è¦é‡è®­ç»ƒ: {'æ˜¯' if performance['needs_retrain'] else 'å¦'}")
                click.echo(f"   åŸå› : {performance['reason']}")

                if performance["needs_retrain"] and not dry_run:
                    click.echo("\nğŸ”„ å¼€å§‹é‡è®­ç»ƒ...")
                    retrain_result = await pipeline.trigger_model_retraining(
                        model_name, performance.get("model_version"), performance
                    )

                    if retrain_result["success"]:
                        click.echo(f"âœ… é‡è®­ç»ƒæˆåŠŸ! æ–°ç‰ˆæœ¬: v{retrain_result['new_version']}")

                        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
                        report_path = await pipeline.generate_comparison_report(
                            model_name,
                            performance.get("model_version", "unknown"),
                            retrain_result["new_version"],
                            retrain_result,
                        )
                        click.echo(f"ğŸ“„ å¯¹æ¯”æŠ¥å‘Š: {report_path}")
                    else:
                        click.echo(f"âŒ é‡è®­ç»ƒå¤±è´¥: {retrain_result['error']}")

            else:
                # å…¨é‡è¯„ä¼°å‘¨æœŸ
                if dry_run:
                    click.echo("\nğŸ” è¯•è¿è¡Œæ¨¡å¼ - ä»…è¯„ä¼°ï¼Œä¸æ‰§è¡Œé‡è®­ç»ƒ")

                results = await pipeline.run_evaluation_cycle()

                click.echo("\nğŸ“Š æ‰§è¡Œç»“æœ:")
                click.echo(f"   è¯„ä¼°æ¨¡å‹æ•°: {results['models_evaluated']}")
                click.echo(f"   éœ€è¦é‡è®­ç»ƒ: {results['models_needing_retrain']}")
                click.echo(f"   è§¦å‘é‡è®­ç»ƒ: {results['retraining_triggered']}")
                click.echo(f"   é‡è®­ç»ƒæˆåŠŸ: {results['retraining_successful']}")
                click.echo(f"   ç”ŸæˆæŠ¥å‘Šæ•°: {results['reports_generated']}")
                click.echo(f"   æ‰§è¡Œæ—¶é—´: {results['duration_seconds']:.1f} ç§’")

                if results["errors"]:
                    click.echo("\nâš ï¸ é”™è¯¯ä¿¡æ¯:")
                    for error in results["errors"]:
                        click.echo(f"   - {error}")

                # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
                if verbose and results["model_results"]:
                    click.echo("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
                    for model_result in results["model_results"]:
                        click.echo(
                            f"   ğŸ¤– {model_result['model_name']} v{model_result['model_version']}:"
                        )
                        click.echo(
                            f"      å‡†ç¡®ç‡: {model_result['performance']['accuracy']:.2%}"
                        )
                        click.echo(
                            f"      é‡è®­ç»ƒ: {'æ˜¯' if model_result['retrain_triggered'] else 'å¦'}"
                        )
                        if model_result["report_path"]:
                            click.echo(f"      æŠ¥å‘Š: {model_result['report_path']}")

    # è¿è¡Œå¼‚æ­¥å‡½æ•°
    asyncio.run(run())


if __name__ == "__main__":
    main()
