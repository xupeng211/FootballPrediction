#!/usr/bin/env python3
"""
ç®€å•æ–‡ä»¶é‡æ„å·¥å…· - æŒ‰åŠŸèƒ½æ‹†åˆ†é•¿æµ‹è¯•æ–‡ä»¶
"""

import os
import re
import ast


def analyze_test_file(filepath):
    """åˆ†ææµ‹è¯•æ–‡ä»¶ç»“æ„"""
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            content = f.read()

        # è§£æAST
        tree = ast.parse(content)

        # æ”¶é›†æ‰€æœ‰å‡½æ•°å’Œç±»
        functions = []
        classes = []

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                functions.append(
                    {
                        "name": node.name,
                        "lineno": node.lineno,
                        "type": "function",
                        "decorators": [
                            d.id if isinstance(d, ast.Name) else str(d) for d in node.decorator_list
                        ],
                    }
                )
            elif isinstance(node, ast.ClassDef):
                classes.append(
                    {
                        "name": node.name,
                        "lineno": node.lineno,
                        "type": "class",
                        "methods": [n.name for n in node.body if isinstance(n, ast.FunctionDef)],
                    }
                )

        return {
            "functions": functions,
            "classes": classes,
            "total_lines": len(content.split("\n")),
            "content": content,
        }
    except Exception as e:
        print(f"åˆ†ææ–‡ä»¶ {filepath} å¤±è´¥: {e}")
        return None


def split_prediction_algorithms_test():
    """æ‹†åˆ†é¢„æµ‹ç®—æ³•æµ‹è¯•æ–‡ä»¶"""
    source_file = "tests/unit/domain/test_prediction_algorithms_comprehensive.py"

    print("ğŸ”§ åˆ†æé¢„æµ‹ç®—æ³•æµ‹è¯•æ–‡ä»¶...")
    analysis = analyze_test_file(source_file)

    if not analysis:
        print("âŒ æ–‡ä»¶åˆ†æå¤±è´¥")
        return False

    print("ğŸ“Š åˆ†æç»“æœ:")
    print(f"   æ€»è¡Œæ•°: {analysis['total_lines']}")
    print(f"   å‡½æ•°æ•°: {len(analysis['functions'])}")
    print(f"   ç±»æ•°: {len(analysis['classes'])}")

    # ç®€å•æ‹†åˆ†ç­–ç•¥ï¼šæŒ‰å‡½æ•°æ•°é‡å¹³åˆ†
    total_items = len(analysis["functions"]) + len(analysis["classes"])
    items_per_file = max(5, total_items // 4)  # æœ€å¤š4ä¸ªæ–‡ä»¶ï¼Œæ¯ä¸ªè‡³å°‘5ä¸ªé¡¹

    # åˆ†ç»„
    all_items = analysis["functions"] + analysis["classes"]
    all_items.sort(key=lambda x: x["lineno"])  # æŒ‰è¡Œå·æ’åº

    groups = []
    current_group = []

    for item in all_items:
        current_group.append(item)
        if len(current_group) >= items_per_file:
            groups.append(current_group)
            current_group = []

    if current_group:  # æ·»åŠ æœ€åä¸€ç»„
        groups.append(current_group)

    # è¯»å–åŸæ–‡ä»¶å†…å®¹
    with open(source_file, "r", encoding="utf-8") as f:
        lines = f.readlines()

    # ä¸ºæ¯ä¸ªåˆ†ç»„åˆ›å»ºæ–°æ–‡ä»¶
    output_dir = "tests/unit/domain/"
    created_files = []

    for i, group in enumerate(groups):
        if group:
            # åˆ›å»ºæ–°æ–‡ä»¶
            filename = os.path.join(output_dir, f"test_prediction_algorithms_part_{i+1}.py")

            # æå–ç›¸å…³ä»£ç 
            relevant_lines = []

            # æ·»åŠ æ–‡ä»¶å¤´
            relevant_lines.append('"""\n')
            relevant_lines.append(f"é¢„æµ‹ç®—æ³•æµ‹è¯• - ç¬¬{i+1}éƒ¨åˆ†\n")
            relevant_lines.append("ä»åŸæ–‡ä»¶ test_prediction_algorithms_comprehensive.py æ‹†åˆ†\n")
            relevant_lines.append(f'åˆ›å»ºæ—¶é—´: {__import__("datetime").datetime.now()}\n')
            relevant_lines.append('"""\n\n')

            # æ·»åŠ åŸºç¡€å¯¼å…¥
            relevant_lines.append("import pytest\n")
            relevant_lines.append("from unittest.mock import Mock, patch\n\n")

            # æå–ç›¸å…³å‡½æ•°å’Œç±»
            for item in group:
                start_line = item["lineno"] - 1
                if start_line < len(lines):
                    # ç®€å•æå–ï¼šä»å‡½æ•°å¼€å§‹åˆ°ä¸‹ä¸€ä¸ªå‡½æ•°æˆ–æ–‡ä»¶ç»“æŸ
                    end_line = len(lines)
                    for next_item in all_items:
                        if next_item["lineno"] > item["lineno"]:
                            end_line = next_item["lineno"] - 1
                            break

                    relevant_lines.extend(lines[start_line:end_line])
                    relevant_lines.append("\n")

            # å†™å…¥æ–‡ä»¶
            os.makedirs(os.path.dirname(filename), exist_ok=True)
            with open(filename, "w", encoding="utf-8") as f:
                f.writelines(relevant_lines)

            created_files.append(filename)
            print(f"âœ… åˆ›å»ºæ–‡ä»¶: {filename} ({len(relevant_lines)} è¡Œ)")

    # å¤‡ä»½åŸæ–‡ä»¶
    backup_file = source_file + ".backup"
    os.rename(source_file, backup_file)
    print(f"ğŸ“¦ åŸæ–‡ä»¶å¤‡ä»½è‡³: {backup_file}")

    print("\nğŸ“Š æ‹†åˆ†æ€»ç»“:")
    print(f"   åŸæ–‡ä»¶: {source_file} ({analysis['total_lines']} è¡Œ)")
    print(f"   æ‹†åˆ†æ–‡ä»¶æ•°: {len(created_files)}")
    print("   åˆ›å»ºæ–‡ä»¶:")
    for f in created_files:
        lines_count = len(open(f, "r", encoding="utf-8").readlines())
        print(f"     - {os.path.basename(f)} ({lines_count} è¡Œ)")

    return True


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç®€å•é‡æ„...")

    # å¤„ç†é¢„æµ‹ç®—æ³•æµ‹è¯•æ–‡ä»¶
    if split_prediction_algorithms_test():
        print("âœ… é¢„æµ‹ç®—æ³•æµ‹è¯•æ–‡ä»¶é‡æ„å®Œæˆ")
    else:
        print("âŒ é¢„æµ‹ç®—æ³•æµ‹è¯•æ–‡ä»¶é‡æ„å¤±è´¥")

    print("\nğŸ¯ å»ºè®®ä¸‹ä¸€æ­¥:")
    print("   1. éªŒè¯æ‹†åˆ†åçš„æ–‡ä»¶è¯­æ³•æ­£ç¡®")
    print("   2. è¿è¡Œæµ‹è¯•ç¡®ä¿åŠŸèƒ½æ­£å¸¸")
    print("   3. å¤„ç†ä¸‹ä¸€ä¸ªé•¿æ–‡ä»¶")


if __name__ == "__main__":
    main()
