#!/usr/bin/env python3
"""
FBref èµ›äº‹å‘ç°è€… (League Discovery)
é¦–å¸­æ•°æ®æ¶æ„å¸ˆä¸“ç”¨å·¥å…·

Purpose: ä»FBrefè‡ªåŠ¨å‘ç°æ‰€æœ‰å¯ç”¨èµ›äº‹ï¼Œå®ç°"åœ°æ¯¯å¼è¦†ç›–"
ç›®æ ‡: å‘ç°å¹¶é‡‡é›†FBrefä¸Šå­˜åœ¨çš„æ¯ä¸€åœºæ¯”èµ›

Strategy:
1. è®¿é—® https://fbref.com/en/comps/ è·å–èµ›äº‹æ€»ç´¢å¼•
2. è§£ææ‰€æœ‰è¡¨æ ¼ (Big 5, Domestic, International, Cups)
3. æå–èµ›äº‹åç§°å’Œé“¾æ¥
4. å­˜å…¥æ•°æ®åº“leaguesè¡¨
"""

import asyncio
import logging
import re
import sys
from pathlib import Path
from typing import List, Dict, Optional
from urllib.parse import urljoin

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import pandas as pd
from bs4 import BeautifulSoup
import requests
from curl_cffi import requests as curl_requests
from sqlalchemy import create_engine, text

# æ•°æ®åº“è¿æ¥
DB_URL = (
    "postgresql://postgres:postgres-dev-password@localhost:5432/football_prediction"
)
engine = create_engine(DB_URL)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class FBrefLeagueDiscovery:
    """FBrefèµ›äº‹å‘ç°å™¨"""

    BASE_URL = "https://fbref.com"
    COMP_INDEX_URL = "https://fbref.com/en/comps/"

    def __init__(self):
        self.session = curl_requests.Session(impersonate="chrome")
        self.discovered_leagues = []
        self.existing_leagues = set()

    def load_existing_leagues(self):
        """åŠ è½½æ•°æ®åº“ä¸­å·²å­˜åœ¨çš„è”èµ›"""
        with engine.connect() as conn:
            result = conn.execute(
                text(
                    """
                SELECT fbref_url, name FROM leagues
                WHERE fbref_url IS NOT NULL
            """
                )
            )
            self.existing_leagues = {row[0] for row in result.fetchall() if row[0]}

        logger.info(f"å·²åŠ è½½ {len(self.existing_leagues)} ä¸ªå·²å­˜åœ¨è”èµ›")

    def fetch_comp_index_page(self) -> Optional[BeautifulSoup]:
        """è·å–FBrefèµ›äº‹ç´¢å¼•é¡µé¢"""
        logger.info(f"ğŸ” è®¿é—®FBrefèµ›äº‹æ€»ç´¢å¼•: {self.COMP_INDEX_URL}")

        try:
            response = self.session.get(self.COMP_INDEX_URL, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, "html.parser")
            logger.info(f"âœ… æˆåŠŸè·å–é¡µé¢ï¼Œå†…å®¹é•¿åº¦: {len(response.text):,} å­—ç¬¦")

            return soup

        except Exception as e:
            logger.error(f"âŒ è·å–é¡µé¢å¤±è´¥: {e}")
            return None

    def parse_league_tables(self, soup: BeautifulSoup) -> List[Dict]:
        """è§£ææ‰€æœ‰è”èµ›è¡¨æ ¼"""
        logger.info("ğŸ† å¼€å§‹è§£æè”èµ›è¡¨æ ¼...")

        discovered = []

        # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼
        tables = soup.find_all("table")

        # æ¯ä¸ªè¡¨æ ¼å¯¹åº”ä¸€ä¸ªåˆ†ç±»
        table_categories = [
            "Big 5 European Leagues",
            "Domestic Leagues",
            "International Leagues",
            "Club Cups",
            "Youth Leagues",
            "Other Competitions",
        ]

        for idx, table in enumerate(tables):
            # è·å–è¡¨æ ¼æ ‡é¢˜
            category = "Unknown"
            try:
                # æŸ¥æ‰¾è¡¨æ ¼å‰é¢çš„æ ‡é¢˜
                prev_header = table.find_previous(["h2", "h3", "h4"])
                if prev_header:
                    category = prev_header.get_text(strip=True)
                elif idx < len(table_categories):
                    category = table_categories[idx]
            except:
                pass

            logger.info(f"ğŸ“Š è§£æè¡¨æ ¼ {idx + 1}/{len(tables)}: {category}")

            # è§£æè¡¨æ ¼è¡Œ
            try:
                rows = table.find_all("tr")[1:]  # è·³è¿‡è¡¨å¤´

                for row in rows:
                    cells = row.find_all(["td", "th"])

                    if len(cells) < 2:
                        continue

                    # æå–è”èµ›åç§°å’Œé“¾æ¥
                    link_cell = cells[0]
                    link_tag = link_cell.find("a")

                    if not link_tag or not link_tag.get("href"):
                        continue

                    league_name = link_tag.get_text(strip=True)
                    league_path = link_tag.get("href")

                    # æå–å›½å®¶/åœ°åŒº - å°è¯•å¤šä¸ªä½ç½®
                    country = ""
                    if len(cells) > 1:
                        # å›½å®¶å¯èƒ½åœ¨å‰å‡ åˆ—
                        for i in range(1, min(len(cells), 4)):
                            cell_text = cells[i].get_text(strip=True)
                            if cell_text and len(cell_text) > 1 and len(cell_text) < 30:
                                # è¿‡æ»¤æ‰æ•°å­—å’Œéå›½å®¶ä¿¡æ¯
                                if not cell_text.replace(".", "").isdigit():
                                    country = cell_text
                                    break

                    # åˆ¤æ–­çº§åˆ«
                    tier = self._determine_tier(league_name, country)

                    # ä½¿ç”¨æ­£ç¡®çš„URL - å¯¼èˆªåˆ°å½“å‰èµ›å­£é¡µé¢è€Œä¸æ˜¯å†å²é¡µé¢
                    if "/history/" in league_path:
                        # ä»å†å²é¡µé¢URLæå–comp IDï¼Œæ„å»ºæ­£ç¡®çš„URL
                        # ä¾‹å¦‚: /en/comps/9/history/Premier-League-Seasons
                        # æå–comp ID (9) å’Œè·¯å¾„ (Premier-League)
                        match = re.search(
                            r"/comps/(\d+)/history/([^/]+)-Seasons", league_path
                        )
                        if match:
                            comp_id = match.group(1)
                            comp_name = match.group(2)
                            league_path = f"/en/comps/{comp_id}/schedule/{comp_name}-Scores-and-Fixtures"
                    else:
                        # å¦‚æœä¸æ˜¯å†å²é¡µé¢ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦è½¬æ¢
                        if league_path.endswith("-Stats"):
                            league_path = league_path.replace(
                                "-Stats", "-Scores-and-Fixtures"
                            )

                    # ç”ŸæˆFBref URL
                    fbref_url = urljoin(self.BASE_URL, league_path)

                    # å»é‡æ£€æŸ¥
                    if fbref_url in self.existing_leagues:
                        logger.debug(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨è”èµ›: {league_name}")
                        continue

                    # å­˜å‚¨å‘ç°ç»“æœ
                    league_info = {
                        "name": league_name,
                        "country": country,
                        "category": category,
                        "tier": tier,
                        "fbref_url": fbref_url,
                        "fbref_path": league_path,
                    }

                    discovered.append(league_info)
                    self.existing_leagues.add(fbref_url)

            except Exception as e:
                logger.error(f"âŒ è§£æè¡¨æ ¼å¤±è´¥: {e}")
                continue

        logger.info(f"âœ… å‘ç° {len(discovered)} ä¸ªæ–°è”èµ›")
        return discovered

    def _determine_tier(self, league_name: str, country: str) -> str:
        """åˆ¤æ–­è”èµ›çº§åˆ«"""
        name_lower = league_name.lower()

        # ä¸€çº§è”èµ›ï¼ˆé¡¶çº§è”èµ›ï¼‰
        if any(
            keyword in name_lower
            for keyword in [
                "premier",
                "la liga",
                "bundesliga",
                "serie a",
                "ligue 1",
                "championship",
                "laLiga",
                "primera division",
                "primeira liga",
                "eredivisie",
                "sÃºperliga",
                "superliga",
            ]
        ):
            return "1st"

        # äºŒçº§è”èµ›
        if any(
            keyword in name_lower
            for keyword in [
                "championship",
                "segunda",
                "2.",
                "2nd",
                "tier 2",
                "bundesliga 2",
                "serie b",
                "ligue 2",
            ]
        ):
            return "2nd"

        # ä¸‰çº§è”èµ›
        if any(
            keyword in name_lower
            for keyword in ["3.", "tier 3", "third division", "tercera", "serie c"]
        ):
            return "3rd"

        # æ¯èµ›
        if any(
            keyword in name_lower
            for keyword in ["cup", "fa cup", "copa", "dfb", "coppa", "taÃ§a"]
        ):
            return "Cup"

        # æ´²é™…èµ›äº‹
        if any(
            keyword in name_lower
            for keyword in [
                "champions",
                "europa",
                "conference",
                "copa Libertadores",
                "afc",
                "caf",
                "concacaf",
            ]
        ):
            return "International"

        return "Other"

    def save_to_database(self, leagues: List[Dict]) -> int:
        """å°†å‘ç°çš„è”èµ›å­˜å…¥æ•°æ®åº“"""
        if not leagues:
            logger.warning("âš ï¸ æ²¡æœ‰è”èµ›éœ€è¦ä¿å­˜")
            return 0

        logger.info(f"ğŸ’¾ ä¿å­˜ {len(leagues)} ä¸ªè”èµ›åˆ°æ•°æ®åº“...")

        saved_count = 0

        with engine.begin() as conn:
            for league in leagues:
                try:
                    # å°è¯•æ’å…¥æ–°è”èµ›
                    conn.execute(
                        text(
                            """
                        INSERT INTO leagues (
                            name, country, category, tier,
                            fbref_url, is_active, created_at, updated_at
                        ) VALUES (
                            :name, :country, :category, :tier,
                            :fbref_url, true, NOW(), NOW()
                        )
                    """
                        ),
                        {
                            "name": league["name"],
                            "country": league["country"],
                            "category": league["category"],
                            "tier": league["tier"],
                            "fbref_url": league["fbref_url"],
                        },
                    )

                    saved_count += 1

                except Exception as e:
                    # å¦‚æœå·²å­˜åœ¨ï¼ˆå¹¶å‘æƒ…å†µï¼‰ï¼Œè·³è¿‡
                    if "duplicate key" not in str(e).lower():
                        logger.warning(f"âš ï¸ ä¿å­˜è”èµ›å¤±è´¥ {league['name']}: {e}")

        logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ä¸ªè”èµ›")
        return saved_count

    def get_statistics(self) -> Dict:
        """è·å–è”èµ›ç»Ÿè®¡ä¿¡æ¯"""
        with engine.connect() as conn:
            # æ€»æ•°
            result = conn.execute(
                text("SELECT COUNT(*) FROM leagues WHERE fbref_url IS NOT NULL")
            )
            total = result.fetchone()[0]

            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            result = conn.execute(
                text(
                    """
                SELECT category, COUNT(*) as count
                FROM leagues
                WHERE fbref_url IS NOT NULL
                GROUP BY category
                ORDER BY count DESC
            """
                )
            )
            category_stats = {row[0]: row[1] for row in result.fetchall()}

            # æŒ‰å›½å®¶ç»Ÿè®¡
            result = conn.execute(
                text(
                    """
                SELECT country, COUNT(*) as count
                FROM leagues
                WHERE fbref_url IS NOT NULL
                  AND country != ''
                GROUP BY country
                ORDER BY count DESC
                LIMIT 10
            """
                )
            )
            country_stats = {row[0]: row[1] for row in result.fetchall()}

        return {
            "total_leagues": total,
            "category_distribution": category_stats,
            "top_countries": country_stats,
        }

    def run_discovery(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„è”èµ›å‘ç°æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨FBrefèµ›äº‹å‘ç°å™¨")
        logger.info("=" * 80)

        # Step 1: åŠ è½½å·²å­˜åœ¨è”èµ›
        self.load_existing_leagues()

        # Step 2: è·å–ç´¢å¼•é¡µé¢
        soup = self.fetch_comp_index_page()
        if not soup:
            return {"error": "Failed to fetch page"}

        # Step 3: è§£æè”èµ›è¡¨æ ¼
        discovered = self.parse_league_tables(soup)

        # Step 4: ä¿å­˜åˆ°æ•°æ®åº“
        saved_count = self.save_to_database(discovered)

        # Step 5: è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = self.get_statistics()

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ èµ›äº‹å‘ç°å®Œæˆ!")
        logger.info("=" * 80)

        logger.info(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"  æ€»è”èµ›æ•°: {stats['total_leagues']:,}")
        logger.info(f"  æ–°å¢è”èµ›: {saved_count}")

        logger.info(f"\nğŸ† åˆ†ç±»åˆ†å¸ƒ:")
        for category, count in stats["category_distribution"].items():
            logger.info(f"  {category:30s}: {count:3d}")

        logger.info(f"\nğŸŒ Top 10 å›½å®¶:")
        for country, count in stats["top_countries"].items():
            logger.info(f"  {country:20s}: {count:3d}")

        logger.info("=" * 80)

        return {
            "discovered": len(discovered),
            "saved": saved_count,
            "statistics": stats,
        }


async def main():
    """ä¸»å‡½æ•°"""
    discovery = FBrefLeagueDiscovery()
    result = discovery.run_discovery()

    logger.info(f"\nâœ… ç¨‹åºæ‰§è¡Œå®Œæˆ")
    logger.info(f"   å‘ç°: {result.get('discovered', 0)} ä¸ªè”èµ›")
    logger.info(f"   ä¿å­˜: {result.get('saved', 0)} ä¸ªè”èµ›")

    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
