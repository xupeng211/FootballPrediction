#!/usr/bin/env python3
"""
é¢„æµ‹è¡¨åˆå§‹åŒ–è„šæœ¬
åˆ›å»º match_predictions è¡¨ç”¨äºŽå­˜å‚¨é¢„æµ‹ç»“æžœ

åŠŸèƒ½:
1. åˆ›å»º match_predictions è¡¨
2. è®¾ç½®å¤–é”®çº¦æŸ
3. åˆ›å»ºå¿…è¦çš„ç´¢å¼•
4. æ’å…¥æµ‹è¯•æ•°æ®(å¯é€‰)

ä½œè€…: Full Stack Automation Engineer
åˆ›å»ºæ—¶é—´: 2025-01-10
ç‰ˆæœ¬: 1.0.0 - Phase 4 Daily Automation
"""

import asyncio
import sys
from pathlib import Path
from datetime import datetime
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.database.async_manager import get_db_session, initialize_database
from sqlalchemy import text, Index
import uuid

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


async def create_predictions_table():
    """åˆ›å»ºé¢„æµ‹ç»“æžœè¡¨"""

    # SQLåˆ›å»ºè¡¨è¯­å¥
    create_table_sql = """
    CREATE TABLE IF NOT EXISTS match_predictions (
        id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        match_id INTEGER NOT NULL REFERENCES matches(id) ON DELETE CASCADE,

        -- é¢„æµ‹ç»“æžœ
        prediction VARCHAR(10) NOT NULL CHECK (prediction IN ('Home', 'Draw', 'Away')),
        confidence DECIMAL(5,4) NOT NULL CHECK (confidence >= 0.0 AND confidence <= 1.0),
        probabilities JSONB NOT NULL,  -- {"Home": 0.55, "Draw": 0.25, "Away": 0.20}

        -- æ¨¡åž‹ä¿¡æ¯
        model_version VARCHAR(50) NOT NULL DEFAULT 'v1.0.0',
        feature_count INTEGER NOT NULL DEFAULT 0,
        missing_features INTEGER NOT NULL DEFAULT 0,

        -- å…ƒæ•°æ®
        processing_time_ms DECIMAL(10,3),  -- å¤„ç†æ—¶é—´(æ¯«ç§’)
        prediction_source VARCHAR(50) NOT NULL DEFAULT 'daily_automation',  -- é¢„æµ‹æ¥æº

        -- æ—¶é—´æˆ³
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
    );
    """

    # åˆ›å»ºç´¢å¼•
    create_indexes_sql = [
        # åŒ¹é…IDç´¢å¼• (ç”¨äºŽæŸ¥æ‰¾ç‰¹å®šæ¯”èµ›çš„é¢„æµ‹)
        "CREATE INDEX IF NOT EXISTS idx_match_predictions_match_id ON match_predictions(match_id);",

        # åˆ›å»ºæ—¶é—´ç´¢å¼• (ç”¨äºŽæŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢)
        "CREATE INDEX IF NOT EXISTS idx_match_predictions_created_at ON match_predictions(created_at);",

        # æ¨¡åž‹ç‰ˆæœ¬ç´¢å¼• (ç”¨äºŽæŒ‰æ¨¡åž‹ç‰ˆæœ¬æŸ¥è¯¢)
        "CREATE INDEX IF NOT EXISTS idx_match_predictions_model_version ON match_predictions(model_version);",

        # é¢„æµ‹ç»“æžœç´¢å¼• (ç”¨äºŽç»Ÿè®¡åˆ†æž)
        "CREATE INDEX IF NOT EXISTS idx_match_predictions_prediction ON match_predictions(prediction);",

        # å¤åˆç´¢å¼•: æ¯”èµ›æ—¶é—´ + åˆ›å»ºæ—¶é—´ (ç”¨äºŽæŸ¥æ‰¾ç‰¹å®šæ—¥æœŸçš„é¢„æµ‹)
        """CREATE INDEX IF NOT EXISTS idx_match_predictions_match_created
           ON match_predictions(match_id, created_at DESC);""",

        # ç½®ä¿¡åº¦ç´¢å¼• (ç”¨äºŽæŸ¥æ‰¾é«˜ç½®ä¿¡åº¦é¢„æµ‹)
        "CREATE INDEX IF NOT EXISTS idx_match_predictions_confidence ON match_predictions(confidence DESC);",
    ]

    # è§¦å‘å™¨: è‡ªåŠ¨æ›´æ–° updated_at å­—æ®µ (æ‹†åˆ†ä¸ºå•ç‹¬è¯­å¥)
    create_trigger_sqls = [
        """
        CREATE OR REPLACE FUNCTION update_match_predictions_updated_at()
        RETURNS TRIGGER AS $$
        BEGIN
            NEW.updated_at = NOW();
            RETURN NEW;
        END;
        $$ language 'plpgsql';
        """,
        """
        DROP TRIGGER IF EXISTS trigger_update_match_predictions_updated_at ON match_predictions;
        """,
        """
        CREATE TRIGGER trigger_update_match_predictions_updated_at
            BEFORE UPDATE ON match_predictions
            FOR EACH ROW
            EXECUTE FUNCTION update_match_predictions_updated_at();
        """
    ]

    try:
        async with get_db_session() as session:
            # 1. åˆ›å»ºè¡¨
            logger.info("ðŸ”¨ åˆ›å»º match_predictions è¡¨...")
            await session.execute(text(create_table_sql))

            # 2. åˆ›å»ºç´¢å¼•
            logger.info("ðŸ“Š åˆ›å»ºç´¢å¼•...")
            for index_sql in create_indexes_sql:
                await session.execute(text(index_sql))

            # 3. åˆ›å»ºè§¦å‘å™¨
            logger.info("âš¡ åˆ›å»ºæ›´æ–°æ—¶é—´è§¦å‘å™¨...")
            for trigger_sql in create_trigger_sqls:
                await session.execute(text(trigger_sql))

            # 4. æäº¤æ‰€æœ‰æ›´æ”¹
            await session.commit()

            logger.info("âœ… match_predictions è¡¨åˆ›å»ºæˆåŠŸ!")

            # 5. éªŒè¯è¡¨ç»“æž„
            await verify_table_structure(session)

            return True

    except Exception as e:
        logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
        await session.rollback()
        return False


async def verify_table_structure(session):
    """éªŒè¯è¡¨ç»“æž„"""
    logger.info("ðŸ” éªŒè¯è¡¨ç»“æž„...")

    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    check_table_sql = """
    SELECT EXISTS (
        SELECT FROM information_schema.tables
        WHERE table_name = 'match_predictions'
    );
    """

    result = await session.execute(text(check_table_sql))
    table_exists = result.scalar()

    if not table_exists:
        raise Exception("è¡¨åˆ›å»ºå¤±è´¥")

    # æ£€æŸ¥åˆ—ä¿¡æ¯
    columns_sql = """
    SELECT column_name, data_type, is_nullable, column_default
    FROM information_schema.columns
    WHERE table_name = 'match_predictions'
    ORDER BY ordinal_position;
    """

    result = await session.execute(text(columns_sql))
    columns = result.fetchall()

    logger.info("ðŸ“‹ è¡¨ç»“æž„:")
    for col in columns:
        logger.info(f"   {col.column_name}: {col.data_type} "
                   f"(nullable: {col.is_nullable}, default: {col.column_default})")

    # æ£€æŸ¥ç´¢å¼•
    indexes_sql = """
    SELECT indexname, indexdef
    FROM pg_indexes
    WHERE tablename = 'match_predictions'
    ORDER BY indexname;
    """

    result = await session.execute(text(indexes_sql))
    indexes = result.fetchall()

    logger.info("ðŸ“Š ç´¢å¼•ä¿¡æ¯:")
    for idx in indexes:
        logger.info(f"   {idx.indexname}")

    # æ£€æŸ¥å¤–é”®çº¦æŸ
    constraints_sql = """
    SELECT constraint_name, constraint_type
    FROM information_schema.table_constraints
    WHERE table_name = 'match_predictions';
    """

    result = await session.execute(text(constraints_sql))
    constraints = result.fetchall()

    logger.info("ðŸ”— çº¦æŸä¿¡æ¯:")
    for cons in constraints:
        logger.info(f"   {cons.constraint_name}: {cons.constraint_type}")


async def insert_sample_predictions():
    """æ’å…¥ç¤ºä¾‹é¢„æµ‹æ•°æ® (å¯é€‰)"""
    logger.info("ðŸŽ¯ æ’å…¥ç¤ºä¾‹é¢„æµ‹æ•°æ®...")

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å·²å®Œæˆçš„æ¯”èµ›
    find_matches_sql = """
    SELECT id, home_team_name, away_team_name, match_date
    FROM matches
    WHERE status = 'completed'
    AND home_score IS NOT NULL
    AND away_score IS NOT NULL
    LIMIT 5;
    """

    try:
        async with get_db_session() as session:
            result = await session.execute(text(find_matches_sql))
            matches = result.fetchall()

            if not matches:
                logger.info("âš ï¸ æ²¡æœ‰æ‰¾åˆ°å·²å®Œæˆçš„æ¯”èµ›ï¼Œè·³è¿‡ç¤ºä¾‹æ•°æ®æ’å…¥")
                return

            for match in matches:
                # æ¨¡æ‹Ÿé¢„æµ‹ç»“æžœ
                sample_prediction = {
                    'match_id': str(match.id),
                    'prediction': 'Home',  # å‡è®¾ä¸»é˜ŸèŽ·èƒœ
                    'confidence': 0.65,
                    'probabilities': {"Home": 0.65, "Draw": 0.25, "Away": 0.10},
                    'model_version': 'v1.0.0',
                    'feature_count': 14,
                    'missing_features': 0,
                    'processing_time_ms': 150.5,
                    'prediction_source': 'sample_data'
                }

                insert_sql = """
                INSERT INTO match_predictions (
                    match_id, prediction, confidence, probabilities,
                    model_version, feature_count, missing_features,
                    processing_time_ms, prediction_source
                ) VALUES (
                    :match_id, :prediction, :confidence, :probabilities,
                    :model_version, :feature_count, :missing_features,
                    :processing_time_ms, :prediction_source
                );
                """

                await session.execute(text(insert_sql), sample_prediction)

            await session.commit()
            logger.info(f"âœ… æˆåŠŸæ’å…¥ {len(matches)} æ¡ç¤ºä¾‹é¢„æµ‹æ•°æ®")

    except Exception as e:
        logger.error(f"âŒ æ’å…¥ç¤ºä¾‹æ•°æ®å¤±è´¥: {e}")
        await session.rollback()


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ðŸš€ å¼€å§‹åˆå§‹åŒ–é¢„æµ‹è¡¨...")

    try:
        # 0. åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨
        logger.info("ðŸ”§ åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨...")
        initialize_database()

        # 1. åˆ›å»ºè¡¨å’Œç´¢å¼•
        success = await create_predictions_table()

        if not success:
            logger.error("âŒ é¢„æµ‹è¡¨åˆå§‹åŒ–å¤±è´¥")
            return False

        # 2. å¯é€‰: æ’å…¥ç¤ºä¾‹æ•°æ®
        # insert_sample_predictions()

        logger.info("ðŸŽ‰ é¢„æµ‹è¡¨åˆå§‹åŒ–å®Œæˆ!")
        return True

    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)