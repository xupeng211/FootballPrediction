#!/usr/bin/env python3
"""
ç‰¹å¾ç”Ÿæˆè„šæœ¬ / Feature Generation Script

è¯¥è„šæœ¬ä»Silverå±‚æ•°æ®åº“åŠ è½½æ¯”èµ›æ•°æ®ï¼Œè®¡ç®—æœºå™¨å­¦ä¹ æ‰€éœ€çš„ç‰¹å¾ï¼Œå¹¶ä¿å­˜ä¸ºCSVæ–‡ä»¶ã€‚

This script loads match data from Silver layer database, calculates features for machine learning,
and saves them as a CSV file.

ä½¿ç”¨æ–¹æ³• / Usage:
    python scripts/generate_features.py
"""

import asyncio
import logging
import os
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv

# å°è¯•åŠ è½½.envæ–‡ä»¶
env_files = [
    project_root / ".env",
    project_root / ".env.local",
    project_root / ".env.development",
]

for env_file in env_files:
    if env_file.exists():
        load_dotenv(env_file)
        break
else:
    pass

# å¯¼å…¥æ¨¡å—
try:
    import pandas as pd
    import psycopg2
    from psycopg2.extras import RealDictCursor
    from src.features.simple_feature_calculator import (
        SimpleFeatureCalculator,
        load_data_from_database,
        save_features_to_csv,
    )
except ImportError:
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class FeatureGenerator:
    """ç‰¹å¾ç”Ÿæˆå™¨."""

    def __init__(self):
        """åˆå§‹åŒ–ç‰¹å¾ç”Ÿæˆå™¨."""
        self.matches_df = None
        self.features_df = None
        self.calculator = None

    def load_data(self):
        """ä»æ•°æ®åº“åŠ è½½æ¯”èµ›æ•°æ®."""
        logger.info("=" * 60)
        logger.info("ğŸ” å¼€å§‹åŠ è½½æ¯”èµ›æ•°æ®")
        logger.info("=" * 60)

        try:
            # ä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ DATABASE_URL
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")  # Dockeré‡Œæ˜¯ dbï¼Œä¸æ˜¯localhost
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                db_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            logger.info(
                f"ä½¿ç”¨æ•°æ®åº“è¿æ¥: {db_url.split('@')[1] if '@' in db_url else db_url}"
            )

            # Pandas éœ€è¦åŒæ­¥é©±åŠ¨ï¼Œç§»é™¤ asyncpg
            if "+asyncpg" in db_url:
                db_url = db_url.replace("+asyncpg", "")
                logger.info("å·²å°†asyncpgé©±åŠ¨æ›¿æ¢ä¸ºpsycopg2ä»¥é€‚é…Pandas")

            # è¿æ¥æ•°æ®åº“å¹¶æŸ¥è¯¢æ•°æ®
            conn = psycopg2.connect(db_url)

            query = """
            SELECT
                m.id as match_id,
                m.home_team_id,
                m.away_team_id,
                m.home_score,
                m.away_score,
                m.status,
                m.match_date,
                t1.name as home_team_name,
                t2.name as away_team_name
            FROM matches m
            JOIN teams t1 ON m.home_team_id = t1.id
            JOIN teams t2 ON m.away_team_id = t2.id
            WHERE m.status = 'FINISHED'
            ORDER BY m.match_date ASC
            """

            logger.info("æ‰§è¡ŒSQLæŸ¥è¯¢...")
            self.matches_df = pd.read_sql_query(query, conn)
            conn.close()

            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.matches_df)} æ¡æ¯”èµ›è®°å½•")

            # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
            logger.info(
                f"ğŸ“… æ•°æ®æ—¶é—´èŒƒå›´: {self.matches_df['match_date'].min()} åˆ° {self.matches_df['match_date'].max()}"
            )
            logger.info(
                f"ğŸ† æ¶‰åŠçƒé˜Ÿæ•°: {len(set(self.matches_df['home_team_id'].unique()) | set(self.matches_df['away_team_id'].unique()))}"
            )

            # æ˜¾ç¤ºå‰å‡ è¡Œæ•°æ®
            logger.info("ğŸ“Š æ•°æ®é¢„è§ˆ:")

            return True

        except Exception:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False

    def calculate_features(self):
        """è®¡ç®—ç‰¹å¾."""
        logger.info("=" * 60)
        logger.info("âš™ï¸  å¼€å§‹è®¡ç®—ç‰¹å¾")
        logger.info("=" * 60)

        try:
            # åˆ›å»ºç‰¹å¾è®¡ç®—å™¨
            self.calculator = SimpleFeatureCalculator(self.matches_df)

            # ç”Ÿæˆç‰¹å¾æ•°æ®é›†
            logger.info("ğŸ”„ ç”Ÿæˆç‰¹å¾æ•°æ®é›†...")
            self.features_df = self.calculator.generate_features_dataset()

            logger.info(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆï¼Œç”Ÿæˆäº† {len(self.features_df)} æ¡ç‰¹å¾è®°å½•")

            # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
            logger.info("ğŸ“ˆ ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")

            return True

        except Exception:
            logger.error(f"âŒ ç‰¹å¾è®¡ç®—å¤±è´¥: {e}")
            return False

    def validate_features(self):
        """éªŒè¯ç‰¹å¾æ•°æ®."""
        logger.info("=" * 60)
        logger.info("ğŸ” å¼€å§‹éªŒè¯ç‰¹å¾æ•°æ®")
        logger.info("=" * 60)

        try:
            is_valid = self.calculator.validate_features(self.features_df)

            if is_valid:
                logger.info("âœ… ç‰¹å¾æ•°æ®éªŒè¯é€šè¿‡")

                # æ£€æŸ¥ç¬¬ä¸€åœºæ¯”èµ›çš„ç‰¹å¾ï¼ˆåº”è¯¥æ²¡æœ‰å†å²æ•°æ®ï¼‰
                first_match = self.features_df.iloc[0]
                logger.info("ğŸ“Š ç¬¬ä¸€åœºæ¯”èµ›ç‰¹å¾éªŒè¯:")
                logger.info(f"   ä¸»é˜Ÿæœ€è¿‘5åœºç§¯åˆ†: {first_match['home_last_5_points']}")
                logger.info(f"   å®¢é˜Ÿæœ€è¿‘5åœºç§¯åˆ†: {first_match['away_last_5_points']}")
                logger.info(
                    f"   å†å²äº¤é”‹ä¸»é˜Ÿè·èƒœæ¬¡æ•°: {first_match['h2h_last_3_home_wins']}"
                )

                # æ£€æŸ¥åç»­æ¯”èµ›çš„ç‰¹å¾
                if len(self.features_df) >= 10:
                    tenth_match = self.features_df.iloc[9]  # ç¬¬10åœºæ¯”èµ›
                    logger.info("ğŸ“Š ç¬¬ååœºæ¯”èµ›ç‰¹å¾éªŒè¯:")
                    logger.info(
                        f"   ä¸»é˜Ÿæœ€è¿‘5åœºç§¯åˆ†: {tenth_match['home_last_5_points']}"
                    )
                    logger.info(
                        f"   å®¢é˜Ÿæœ€è¿‘5åœºç§¯åˆ†: {tenth_match['away_last_5_points']}"
                    )
                    logger.info(
                        f"   å†å²äº¤é”‹ä¸»é˜Ÿè·èƒœæ¬¡æ•°: {tenth_match['h2h_last_3_home_wins']}"
                    )

                return True
            else:
                logger.error("âŒ ç‰¹å¾æ•°æ®éªŒè¯å¤±è´¥")
                return False

        except Exception:
            logger.error(f"âŒ ç‰¹å¾éªŒè¯å¤±è´¥: {e}")
            return False

    def save_to_database(self):
        """ä¿å­˜ç‰¹å¾æ•°æ®åˆ°æ•°æ®åº“."""
        logger.info("=" * 60)
        logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜ç‰¹å¾åˆ°æ•°æ®åº“")
        logger.info("=" * 60)

        try:
            import json
            from sqlalchemy import create_engine

            # è·å–æ•°æ®åº“è¿æ¥
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                db_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            # Pandaséœ€è¦åŒæ­¥é©±åŠ¨ï¼Œç§»é™¤asyncpg
            if "+asyncpg" in db_url:
                db_url = db_url.replace("+asyncpg", "")

            # åˆ›å»ºSQLAlchemyå¼•æ“
            engine = create_engine(db_url)

            # å‡†å¤‡æ‰¹é‡æ’å…¥æ•°æ® - é€‚é…å®é™…è¡¨ç»“æ„
            batch_data = []

            logger.info(f"å¼€å§‹å‡†å¤‡ {len(self.features_df)} æ¡ç‰¹å¾è®°å½•...")

            for index, row in self.features_df.iterrows():
                try:
                    # å‡†å¤‡ç‰¹å¾æ•°æ® - åªåŒ…å«å®é™…è¡¨ç»“æ„ä¸­çš„å­—æ®µ
                    feature_record = {
                        "match_id": int(row["match_id"]),
                        "feature_data": json.dumps(
                            {
                                "home_team_id": int(row["home_team_id"]),
                                "away_team_id": int(row["away_team_id"]),
                                "match_date": str(row["match_date"]),
                                "match_result": int(row["match_result"]),
                                "home_last_5_points": float(row["home_last_5_points"]),
                                "away_last_5_points": float(row["away_last_5_points"]),
                                "home_last_5_avg_goals": float(
                                    row["home_last_5_avg_goals"]
                                ),
                                "away_last_5_avg_goals": float(
                                    row["away_last_5_avg_goals"]
                                ),
                                "home_last_5_goal_diff": float(
                                    row["home_last_5_goal_diff"]
                                ),
                                "away_last_5_goal_diff": float(
                                    row["away_last_5_goal_diff"]
                                ),
                                "home_win_streak": int(row["home_win_streak"]),
                                "away_win_streak": int(row["away_win_streak"]),
                                "home_last_5_win_rate": float(
                                    row["home_last_5_win_rate"]
                                ),
                                "away_last_5_win_rate": float(
                                    row["away_last_5_win_rate"]
                                ),
                                "home_rest_days": int(row["home_rest_days"]),
                                "away_rest_days": int(row["away_rest_days"]),
                                "h2h_last_3_home_wins": int(
                                    row["h2h_last_3_home_wins"]
                                ),
                            }
                        ),
                    }
                    batch_data.append(feature_record)

                    # æ¯50æ¡è®°å½•æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if (index + 1) % 50 == 0:
                        logger.info(
                            f"å·²å‡†å¤‡ {index + 1}/{len(self.features_df)} æ¡è®°å½•..."
                        )

                except Exception:
                    logger.error(f"å‡†å¤‡ç¬¬ {index} æ¡è®°å½•å¤±è´¥: {e}")
                    continue

            logger.info(f"å¼€å§‹æ‰¹é‡æ’å…¥ {len(batch_data)} æ¡ç‰¹å¾è®°å½•åˆ°æ•°æ®åº“...")

            # æ‰¹é‡æ’å…¥åˆ°æ•°æ®åº“ - ä½¿ç”¨pandas to_sqlç›´æ¥æ’å…¥
            features_df = pd.DataFrame(batch_data)

            # æ·»åŠ æ—¶é—´æˆ³å’Œå¿…éœ€å­—æ®µ
            from datetime import datetime

            features_df["created_at"] = datetime.now()
            features_df["updated_at"] = datetime.now()
            features_df["feature_type"] = "match_features"  # æ·»åŠ å¿…éœ€çš„feature_typeå­—æ®µ
            features_df["team_id"] = None  # æ·»åŠ å¯é€‰çš„team_idå­—æ®µ

            # åªåŒ…å«è¡¨ä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
            features_df = features_df[
                [
                    "match_id",
                    "team_id",
                    "feature_type",
                    "feature_data",
                    "created_at",
                    "updated_at",
                ]
            ]

            # ä½¿ç”¨pandasçš„to_sqlæ‰¹é‡æ’å…¥
            features_df.to_sql("features", engine, if_exists="append", index=False)

            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(batch_data)} æ¡ç‰¹å¾è®°å½•åˆ°æ•°æ®åº“")

            logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(batch_data)} æ¡ç‰¹å¾è®°å½•åˆ°æ•°æ®åº“")
            return True

        except Exception:
            logger.error(f"âŒ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
            import traceback

            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False

    def save_dataset(self, filepath: str = "data/dataset_v1.csv"):
        """ä¿å­˜æ•°æ®é›†."""
        logger.info("=" * 60)
        logger.info("ğŸ’¾ å¼€å§‹ä¿å­˜æ•°æ®é›†")
        logger.info("=" * 60)

        try:
            # ä½¿ç”¨ç‰¹å¾è®¡ç®—å™¨çš„ä¿å­˜æ–¹æ³•
            save_features_to_csv(self.features_df, filepath)

            # éªŒè¯æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if os.path.exists(filepath):
                file_size = os.path.getsize(filepath)
                logger.info(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ° {filepath}")
                logger.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")

                # è¯»å–å¹¶éªŒè¯ä¿å­˜çš„æ–‡ä»¶
                saved_df = pd.read_csv(filepath)
                logger.info(
                    f"ğŸ“Š éªŒè¯ä¿å­˜çš„æ–‡ä»¶: {saved_df.shape[0]} è¡Œ, {saved_df.shape[1]} åˆ—"
                )

                return True
            else:
                logger.error(f"âŒ æ–‡ä»¶æœªåˆ›å»º: {filepath}")
                return False

        except Exception:
            logger.error(f"âŒ ä¿å­˜æ•°æ®é›†å¤±è´¥: {e}")
            return False

    def generate_summary_report(self):
        """ç”Ÿæˆç‰¹å¾æ‘˜è¦æŠ¥å‘Š."""
        logger.info("=" * 60)
        logger.info("ğŸ“‹ ç‰¹å¾ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š")
        logger.info("=" * 60)

        try:
            if self.features_df is None:
                logger.error("âŒ æ²¡æœ‰ç‰¹å¾æ•°æ®å¯ç”¨äºç”ŸæˆæŠ¥å‘Š")
                return

            # åŸºæœ¬ç»Ÿè®¡
            total_matches = len(self.features_df)
            home_wins = len(self.features_df[self.features_df["match_result"] == 1])
            away_wins = len(self.features_df[self.features_df["match_result"] == 2])
            draws = len(self.features_df[self.features_df["match_result"] == 0])

            logger.info("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
            logger.info(f"   æ€»æ¯”èµ›æ•°: {total_matches}")
            logger.info(
                f"   ä¸»é˜Ÿè·èƒœ: {home_wins} ({home_wins / total_matches * 100:.1f}%)"
            )
            logger.info(
                f"   å®¢é˜Ÿè·èƒœ: {away_wins} ({away_wins / total_matches * 100:.1f}%)"
            )
            logger.info(f"   å¹³å±€: {draws} ({draws / total_matches * 100:.1f}%)")

            # ç‰¹å¾ç»Ÿè®¡
            logger.info("ğŸ“ˆ ç‰¹å¾ç»Ÿè®¡:")
            logger.info(
                f"   ä¸»é˜Ÿè¿‘æœŸç§¯åˆ†å‡å€¼: {self.features_df['home_last_5_points'].mean():.2f}"
            )
            logger.info(
                f"   å®¢é˜Ÿè¿‘æœŸç§¯åˆ†å‡å€¼: {self.features_df['away_last_5_points'].mean():.2f}"
            )
            logger.info(
                f"   ä¸»é˜Ÿè¿‘æœŸè¿›çƒå‡å€¼: {self.features_df['home_last_5_avg_goals'].mean():.2f}"
            )
            logger.info(
                f"   å®¢é˜Ÿè¿‘æœŸè¿›çƒå‡å€¼: {self.features_df['away_last_5_avg_goals'].mean():.2f}"
            )

            # æ•°æ®è´¨é‡æ£€æŸ¥
            zero_history_matches = len(
                self.features_df[
                    (self.features_df["home_last_5_points"] == 0)
                    & (self.features_df["away_last_5_points"] == 0)
                ]
            )
            logger.info("ğŸ” æ•°æ®è´¨é‡:")
            logger.info(
                f"   æ— å†å²è®°å½•çš„æ¯”èµ›: {zero_history_matches} ({zero_history_matches / total_matches * 100:.1f}%)"
            )

        except Exception:
            logger.error(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")

    async def run(self, output_path: str = "data/dataset_v1.csv"):
        """è¿è¡Œå®Œæ•´çš„ç‰¹å¾ç”Ÿæˆæµç¨‹."""
        logger.info("ğŸš€ å¼€å§‹ç‰¹å¾ç”Ÿæˆæµç¨‹")
        start_time = datetime.now()

        try:
            # 1. åŠ è½½æ•°æ®
            if not self.load_data():
                return False

            # 2. è®¡ç®—ç‰¹å¾
            if not self.calculate_features():
                return False

            # 3. éªŒè¯ç‰¹å¾
            if not self.validate_features():
                return False

            # 4. ä¿å­˜åˆ°æ•°æ®åº“ (æ–°å¢)
            if not self.save_to_database():
                logger.warning("âš ï¸ ä¿å­˜åˆ°æ•°æ®åº“å¤±è´¥ï¼Œä½†ç»§ç»­ä¿å­˜CSVæ–‡ä»¶")

            # 5. ä¿å­˜æ•°æ®é›†
            if not self.save_dataset(output_path):
                return False

            # 6. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            self.generate_summary_report()

            end_time = datetime.now()
            duration = end_time - start_time

            logger.info("=" * 60)
            logger.info("ğŸ‰ ç‰¹å¾ç”Ÿæˆæµç¨‹å®Œæˆï¼")
            logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration}")
            logger.info(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_path}")
            logger.info("ğŸ—„ï¸  æ•°æ®åº“: features è¡¨å·²æ›´æ–°")
            logger.info("=" * 60)

            return True

        except Exception:
            logger.error(f"ğŸ’¥ ç‰¹å¾ç”Ÿæˆæµç¨‹å¤±è´¥: {e}")
            return False


async def main():
    """ä¸»å‡½æ•°."""
    logger.info("ğŸ¯ ç‰¹å¾ç”Ÿæˆå™¨å¯åŠ¨")

    try:
        generator = FeatureGenerator()
        success = await generator.run()

        if success:
            logger.info("âœ… ç‰¹å¾ç”ŸæˆæˆåŠŸï¼æ•°æ®é›†å·²å‡†å¤‡å¥½ç”¨äºæ¨¡å‹è®­ç»ƒã€‚")
            sys.exit(0)
        else:
            logger.error("âŒ ç‰¹å¾ç”Ÿæˆå¤±è´¥ï¼")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("â¹ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œç‰¹å¾ç”Ÿæˆåœæ­¢")
        sys.exit(1)
    except Exception:
        logger.error(f"ğŸ’¥ ç‰¹å¾ç”Ÿæˆå¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
