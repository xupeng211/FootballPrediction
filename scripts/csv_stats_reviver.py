#!/usr/bin/env python3
"""
ğŸš€ CSVæ•°æ®å¤æ´»è„šæœ¬ - CSV Stats Revival Script
é¦–å¸­æ•°æ®ä¿®å¤å®˜ (Chief Data Remediation Officer)

åŸºäºCSVæ–‡ä»¶ä¸­çš„xGæ•°æ®ä¿®å¤æ•°æ®åº“ä¸­çš„ç©ºstatså­—æ®µã€‚

ä½œè€…: Chief Data Remediation Officer
ç‰ˆæœ¬: v1.0.0
åˆ›å»ºæ—¶é—´: 2025-12-02
"""

import asyncio
import csv
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import asyncpg
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/csv_stats_reviver.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class CSVStatsReviver:
    """åŸºäºCSVçš„æ•°æ®å¤æ´»å™¨"""

    def __init__(self):
        self.revived_count = 0
        self.failed_count = 0
        self.skipped_count = 0
        self.start_time = datetime.now()

        # æ•°æ®åº“è¿æ¥
        self.database_url = os.getenv('DATABASE_URL', 'postgresql://postgres:postgres-dev-password@localhost:5432/football_prediction')

        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_dir = project_root / "data" / "fbref"

        # CSVæ–‡ä»¶åˆ—è¡¨
        self.csv_files = list(self.data_dir.glob("*_all_seasons_matches.csv"))

        logger.info(f"ğŸ“ å‘ç° {len(self.csv_files)} ä¸ªCSVæ–‡ä»¶")

    async def get_database_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return await asyncpg.connect(self.database_url)

    def parse_csv_match_data(self, csv_file: Path) -> List[Dict]:
        """è§£æCSVæ–‡ä»¶ä¸­çš„æ¯”èµ›æ•°æ®"""
        matches = []

        try:
            df = pd.read_csv(csv_file)

            for _, row in df.iterrows():
                # è·³è¿‡ç©ºè¡Œ
                if pd.isna(row['Home']) or pd.isna(row['Away']):
                    continue

                match_data = {
                    'home_team': str(row['Home']).strip(),
                    'away_team': str(row['Away']).strip(),
                    'date': str(row['Date']).strip(),
                    'time': str(row['Time']).strip() if 'Time' in row and not pd.isna(row['Time']) else '',
                    'score': str(row['Score']).strip() if 'Score' in row and not pd.isna(row['Score']) else '',
                    'venue': str(row['Venue']).strip() if 'Venue' in row and not pd.isna(row['Venue']) else '',
                    'attendance': str(row['Attendance']).strip() if 'Attendance' in row and not pd.isna(row['Attendance']) else '',
                    'referee': str(row['Referee']).strip() if 'Referee' in row and not pd.isna(row['Referee']) else '',
                    'match_report': str(row['Match Report']).strip() if 'Match Report' in row and not pd.isna(row['Match Report']) else '',
                    'week': str(row['Wk']).strip() if 'Wk' in row and not pd.isna(row['Wk']) else '',
                }

                # è§£æxGæ•°æ®
                xg_home = None
                xg_away = None

                if 'xG' in row and not pd.isna(row['xG']):
                    try:
                        xg_home = float(row['xG'])
                    except (ValueError, TypeError):
                        pass

                if 'xG.1' in row and not pd.isna(row['xG.1']):
                    try:
                        xg_away = float(row['xG.1'])
                    except (ValueError, TypeError):
                        pass

                # åˆ›å»ºstatså­—æ®µ
                stats = {}
                if xg_home is not None:
                    stats['xg_home'] = str(round(xg_home, 2))
                if xg_away is not None:
                    stats['xg_away'] = str(round(xg_away, 2))

                # åˆ›å»ºmetadataå­—æ®µ
                metadata = {}
                if match_data['referee']:
                    metadata['referee'] = match_data['referee']
                if match_data['venue']:
                    metadata['venue'] = match_data['venue']
                if match_data['attendance'] and match_data['attendance'] != '':
                    try:
                        attendance = int(match_data['attendance'].replace(',', ''))
                        if attendance > 0:
                            metadata['attendance'] = attendance
                    except (ValueError, TypeError):
                        pass
                if match_data['match_report']:
                    metadata['match_report_url'] = match_data['match_report']

                matches.append({
                    'home_team': match_data['home_team'],
                    'away_team': match_data['away_team'],
                    'date': match_data['date'],
                    'stats': json.dumps(stats) if stats else '{}',
                    'metadata': json.dumps(metadata) if metadata else '{}',
                    'has_xg': len(stats) > 0
                })

        except Exception as e:
            logger.error(f"âŒ è§£æCSVæ–‡ä»¶å¤±è´¥ {csv_file}: {e}")

        return matches

    async def find_matching_database_records(self, conn, csv_matches: List[Dict]) -> List[Tuple]:
        """åœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾åŒ¹é…çš„è®°å½•"""
        matching_records = []

        for csv_match in csv_matches:
            try:
                # æŸ¥è¯¢åŒ¹é…çš„æ•°æ®åº“è®°å½•
                query = """
                    SELECT m.id, m.home_team_id, m.away_team_id, t1.name as home_name, t2.name as away_name
                    FROM matches m
                    JOIN teams t1 ON m.home_team_id = t1.id
                    JOIN teams t2 ON m.away_team_id = t2.id
                    WHERE m.data_source = 'fbref'
                    AND (m.stats = '{}' OR m.stats IS NULL)
                    AND DATE(m.match_date) = $1
                    AND (t1.name ILIKE $2 OR t1.short_name ILIKE $2)
                    AND (t2.name ILIKE $3 OR t2.short_name ILIKE $3)
                    LIMIT 1
                """

                # å¤„ç†æ—¥æœŸæ ¼å¼
                csv_date = csv_match['date']
                if len(csv_date) == 10:  # YYYY-MM-DD format
                    pass  # ç›´æ¥ä½¿ç”¨
                else:
                    # å°è¯•è§£æå…¶ä»–æ ¼å¼
                    pass

                result = await conn.fetchrow(
                    query,
                    csv_date,
                    f"%{csv_match['home_team']}%",
                    f"%{csv_match['away_team']}%"
                )

                if result:
                    matching_records.append((
                        result['id'],
                        csv_match['stats'],
                        csv_match['metadata'],
                        csv_match['has_xg'],
                        result['home_name'],
                        result['away_name']
                    ))

            except Exception as e:
                logger.error(f"âŒ æŸ¥æ‰¾åŒ¹é…è®°å½•å¤±è´¥: {e}")

        return matching_records

    async def revive_database_records(self, conn, matching_records: List[Tuple]):
        """æ›´æ–°æ•°æ®åº“è®°å½•"""
        for record_id, stats, metadata, has_xg, home_name, away_name in matching_records:
            try:
                # æ›´æ–°æ•°æ®åº“è®°å½•
                update_query = """
                    UPDATE matches
                    SET stats = $1,
                        match_metadata = COALESCE(match_metadata, '{}'::jsonb) || $2::jsonb,
                        data_completeness = $3,
                        updated_at = NOW()
                    WHERE id = $4
                """

                completeness = 'complete' if has_xg else 'partial'

                await conn.execute(
                    update_query,
                    stats,
                    metadata,
                    completeness,
                    record_id
                )

                self.revived_count += 1

                if self.revived_count % 100 == 0:
                    logger.info(f"âœ… å·²ä¿®å¤ {self.revived_count} æ¡è®°å½•...")

                if self.revived_count % 10 == 0:
                    logger.info(f"ğŸ”§ ä¿®å¤è®°å½•: {home_name} vs {away_name}")

            except Exception as e:
                logger.error(f"âŒ æ›´æ–°è®°å½•å¤±è´¥ ID {record_id}: {e}")
                self.failed_count += 1

    async def run_revival_process(self):
        """æ‰§è¡Œå®Œæ•´çš„CSVæ•°æ®å¤æ´»æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨CSVæ•°æ®å¤æ´»æµç¨‹")
        logger.info(f"ğŸ“Š å¤„ç† {len(self.csv_files)} ä¸ªCSVæ–‡ä»¶")

        conn = await self.get_database_connection()

        try:
            total_csv_matches = 0

            # å¤„ç†æ¯ä¸ªCSVæ–‡ä»¶
            for csv_file in self.csv_files:
                logger.info(f"ğŸ“ å¤„ç†æ–‡ä»¶: {csv_file.name}")

                # è§£æCSVæ•°æ®
                csv_matches = self.parse_csv_match_data(csv_file)
                total_csv_matches += len(csv_matches)

                logger.info(f"ğŸ“Š ä» {csv_file.name} è§£æå‡º {len(csv_matches)} æ¡æ¯”èµ›è®°å½•")

                # æŸ¥æ‰¾åŒ¹é…çš„æ•°æ®åº“è®°å½•
                matching_records = await self.find_matching_database_records(conn, csv_matches)

                logger.info(f"ğŸ¯ æ‰¾åˆ° {len(matching_records)} æ¡åŒ¹é…çš„æ•°æ®åº“è®°å½•")

                # æ›´æ–°æ•°æ®åº“è®°å½•
                await self.revive_database_records(conn, matching_records)

            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            self.generate_final_report(total_csv_matches, len(self.csv_files))

        finally:
            await conn.close()

    def generate_final_report(self, total_csv_matches: int, files_processed: int):
        """ç”Ÿæˆæœ€ç»ˆä¿®å¤æŠ¥å‘Š"""
        end_time = datetime.now()
        duration = end_time - self.start_time

        report = f"""
ğŸ‰ CSVæ•°æ®å¤æ´»ä¿®å¤å®ŒæˆæŠ¥å‘Š
=====================================
ä¿®å¤æ—¶é—´: {self.start_time} ~ {end_time}
æ€»è€—æ—¶: {duration}
å¤„ç†æ–‡ä»¶: {files_processed} ä¸ªCSVæ–‡ä»¶
CSVæ€»è®°å½•: {total_csv_matches} æ¡

ä¿®å¤ç»“æœ:
âœ… æˆåŠŸä¿®å¤: {self.revived_count} æ¡
âŒ ä¿®å¤å¤±è´¥: {self.failed_count} æ¡
âš ï¸ è·³è¿‡è®°å½•: {self.skipped_count} æ¡

å¤„ç†é€Ÿåº¦: {self.revived_count/duration.total_seconds():.1f} è®°å½•/ç§’

çŠ¶æ€: {'âœ… ä¿®å¤æˆåŠŸ' if self.revived_count > 100 else 'âš ï¸ éƒ¨åˆ†æˆåŠŸ' if self.revived_count > 0 else 'âŒ éœ€è¦è¿›ä¸€æ­¥å¤„ç†'}
=====================================
        """

        logger.info(report)

        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        try:
            with open('/tmp/csv_revival_report.txt', 'w', encoding='utf-8') as f:
                f.write(report)
        except Exception as e:
            logger.error(f"âŒ æ— æ³•å†™å…¥æŠ¥å‘Šæ–‡ä»¶: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("""
ğŸš€ CSVæ•°æ®å¤æ´»è„šæœ¬ - CSV Stats Revival Tool
=====================================
é¦–å¸­æ•°æ®ä¿®å¤å®˜ (Chief Data Remediation Officer)
ç‰ˆæœ¬: v1.0.0

ä¿®å¤ç›®æ ‡: åŸºäºCSVæ–‡ä»¶ä¸­çš„xGæ•°æ®ä¿®å¤æ•°æ®åº“ç©ºstatså­—æ®µ
ä¿®å¤æ–¹æ³•: è§£æCSVæ–‡ä»¶ï¼ŒåŒ¹é…æ•°æ®åº“è®°å½•ï¼Œæ›´æ–°statså­—æ®µ
é¢„æœŸç»“æœ: å°†å¤§é‡ç©ºæ•°æ®å¤æ´»ä¸ºåŒ…å«xGæ•°æ®çš„å®Œæ•´è®°å½•

å¼€å§‹æ—¶é—´: {0}
=====================================
""".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))

    # åˆ›å»ºä¿®å¤å™¨å®ä¾‹
    reviver = CSVStatsReviver()

    try:
        # æ‰§è¡Œå¤æ´»æµç¨‹
        await reviver.run_revival_process()

        print("\nğŸ‰ CSVæ•°æ®å¤æ´»æµç¨‹å·²å®Œæˆï¼")
        print("ğŸ“Š è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: /tmp/csv_stats_reviver.log")
        print("ğŸ“„ æœ€ç»ˆæŠ¥å‘Š: /tmp/csv_revival_report.txt")

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ä¿®å¤æµç¨‹")
    except Exception as e:
        logger.error(f"âŒ ä¿®å¤æµç¨‹å¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())