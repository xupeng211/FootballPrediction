#!/usr/bin/env python3
"""
å…¨å†å²æ•°æ®å›å¡«è„šæœ¬ - å®‰å…¨åŠ å›ºç‰ˆ
Full Historical Data Backfill Script - Security Hardened Edition

# Strategy: Newest -> Oldest | Concurrency: 4 (Safe Mode)
ä½¿ç”¨ Super Greedy é‡‡é›†å™¨å¯¹è¿‡å» 5 å¹´ (2020-2025) çš„å…¨çƒæ ¸å¿ƒèµ›äº‹è¿›è¡Œåœ°æ¯¯å¼é‡‡é›†ã€‚
å…·å¤‡æ–­ç‚¹ç»­ä¼ ã€æ™ºèƒ½é£æ§ã€429é¿éšœç­‰ä¼ä¸šçº§å®‰å…¨ç‰¹æ€§ã€‚

Security Features:
- ğŸ›¡ï¸ æ™ºèƒ½é£æ§é™çº§ (4å¹¶å‘ + 1-3ç§’å»¶è¿Ÿ)
- ğŸ”„ å€’åºå›å¡«ç­–ç•¥ (ä¼˜å…ˆè¿‘æœŸé«˜ä»·å€¼æ•°æ®)
- ğŸš¨ æ™ºèƒ½429é¿éšœ (è‡ªåŠ¨å†·å´+é‡è¯•)
- â¯ï¸ æ–­ç‚¹ç»­ä¼ æœºåˆ¶ (æ”¯æŒéšæ—¶ä¸­æ–­/ç»§ç»­)
- ğŸ“Š å®æ—¶è¿›åº¦ç›‘æ§
- ğŸ”§ ç¡¬ç¼–ç è¡¥ä¸æœºåˆ¶

Author: DevOps & Security Engineer
Version: 2.1.0 Security Hardened Edition
Date: 2025-01-08
"""

import asyncio
import json
import logging
import sys
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from random import uniform

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("backfill_full_history.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è¿›åº¦æ¡
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    logger.warning("âš ï¸ tqdmæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º")

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from collectors.fotmob_api_collector import FotMobAPICollector
    from database.async_manager import get_db_session, initialize_database
    from database.models.match import Match
    from sqlalchemy import text  # ğŸ”§ ä¿®å¤: å¯¼å…¥ text å‡½æ•°
    COLLECTOR_AVAILABLE = True
except ImportError as e:
    logger.error(f"âŒ æ— æ³•å¯¼å…¥é‡‡é›†å™¨æ¨¡å—: {e}")
    COLLECTOR_AVAILABLE = False

# é…ç½®å¸¸é‡
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:postgres@localhost:5432/football_prediction"
)

# ğŸ—ï¸ ç¡¬ç¼–ç è¡¥ä¸ - é«˜ä»·å€¼è”èµ› ID
HARDCODED_PATCHES = {
    "Championship": 48,      # è‹±å†  - å…³é”®æ¬¡çº§è”èµ›
    "Liga Portugal": 61,     # è‘¡è¶… - è‘¡è„ç‰™é¡¶çº§è”èµ›
}

# æ—¶é—´æœºå™¨é…ç½® - å€’åºå›å¡«ç­–ç•¥ (ä¼˜å…ˆè¿‘æœŸé«˜ä»·å€¼æ•°æ®)
YEARS_TO_BACKFILL = [2025, 2024, 2023, 2022, 2021, 2020]  # æ–° -> æ—§
CONCURRENT_LIMIT = 4  # é£æ§é™çº§ï¼šå®‰å…¨å¹¶å‘æ•°
MIN_DELAY = 1.0  # é£æ§é™çº§ï¼šæœ€å°å»¶è¿Ÿ
MAX_DELAY = 3.0  # é£æ§é™çº§ï¼šæœ€å¤§å»¶è¿Ÿ
RATE_LIMIT_COOLDOWN = 60  # 429 è§¦å‘æ—¶çš„å†·å´æ—¶é—´(ç§’)

# æ´²é™…è”èµ›é…ç½® (ç”¨äºèµ›å­£æ ¼å¼åˆ¤æ–­)
EUROPEAN_COUNTRIES = {
    "England", "Spain", "Germany", "Italy", "France",
    "Netherlands", "Portugal", "Belgium", "Scotland",
    "Turkey", "Russia", "Ukraine", "Poland", "Czech Republic",
    "Austria", "Switzerland", "Denmark", "Norway", "Sweden"
}

AMERICAN_COUNTRIES = {
    "USA", "Brazil", "Argentina", "Mexico", "Chile", "Colombia",
    "Peru", "Uruguay", "Paraguay", "Ecuador", "Bolivia", "Venezuela"
}

ASIAN_COUNTRIES = {
    "Japan", "South Korea", "China", "Australia", "Saudi Arabia",
    "UAE", "Qatar", "Iran", "Iraq", "Jordan"
}

@dataclass
class BackfillStats:
    """å›å¡«ç»Ÿè®¡ä¿¡æ¯"""
    total_matches: int = 0
    processed_matches: int = 0
    skipped_matches: int = 0
    successful_matches: int = 0
    failed_matches: int = 0
    start_time: datetime = None
    errors_by_type: dict[str, int] = None

    def __post_init__(self):
        if self.errors_by_type is None:
            self.errors_by_type = {}
        if self.start_time is None:
            self.start_time = datetime.now()

    @property
    def progress_percentage(self) -> float:
        """è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_matches == 0:
            return 0.0
        return (self.processed_matches / self.total_matches) * 100

    @property
    def success_rate(self) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        if self.processed_matches == 0:
            return 0.0
        return (self.successful_matches / self.processed_matches) * 100

    @property
    def elapsed_time(self) -> timedelta:
        """è®¡ç®—å·²ç”¨æ—¶é—´"""
        return datetime.now() - self.start_time

    def log_progress(self):
        """è®°å½•è¿›åº¦æ—¥å¿—"""
        logger.info(
            f"ğŸ“Š è¿›åº¦: {self.processed_matches}/{self.total_matches} "
            f"({self.progress_percentage:.1f}%) | "
            f"âœ… æˆåŠŸ: {self.successful_matches} | "
            f"â­ï¸ è·³è¿‡: {self.skipped_matches} | "
            f"âŒ å¤±è´¥: {self.failed_matches} | "
            f"ğŸ“ˆ æˆåŠŸç‡: {self.success_rate:.1f}% | "
            f"â±ï¸ å·²ç”¨: {self.elapsed_time}"
        )

class SeasonFormatGenerator:
    """èµ›å­£æ ¼å¼ç”Ÿæˆå™¨ - æ™ºèƒ½å¤„ç†ä¸åŒè”èµ›çš„èµ›å­£æ ¼å¼ï¼Œé¿å…é‡å¤æŠ“å–"""

    def __init__(self):
        # æ ¹æ®è”èµ›IDç²¾ç¡®åˆ†ç±»ï¼Œé¿å…é‡å¤æ ¼å¼
        self.crossover_leagues = {
            # æ¬§æ´²ä¸»è¦è·¨å¹´åˆ¶è”èµ› (8æœˆ-5æœˆ)
            47,    # Premier League (è‹±æ ¼å…°)
            42,    # Championship (è‹±æ ¼å…°)
            54,    # La Liga (è¥¿ç­ç‰™)
            82,    # Serie A (æ„å¤§åˆ©)
            100,   # Bundesliga (å¾·å›½)
            354,   # Ligue 1 (æ³•å›½)
            127,   # Eredivisie (è·å…°)
            5,     # Champions League (æ¬§æ´²å† å†›è”èµ›)
            61,    # Europa League (æ¬§æ´²è”èµ›)
            57,    # Conference League (æ¬§æ´²åä¼šè”èµ›)
            364,   # Primeira Liga (è‘¡è„ç‰™)
            381,   # Scottish Premiership (è‹æ ¼å…°)
            # å…¶ä»–æ¬§æ´²è”èµ›
            144, 196, 312, 125, 399, 155, 406, 345, 71, 2, 59, 116, 419
        }

        self.single_year_leagues = {
            # å—ç¾å•å¹´åˆ¶è”èµ› (é€šå¸¸åœ¨å¹´å†…è¿›è¡Œ)
            268,   # BrasileirÃ£o (å·´è¥¿)
            326,   # Argentine Primera DivisiÃ³n (é˜¿æ ¹å»·)
            377,   # Colombian Liga (å“¥ä¼¦æ¯”äºš)
            313,   # Chilean Primera DivisiÃ³n (æ™ºåˆ©)
            # ä¸­åŒ—ç¾è”èµ›
            34,    # MLS (ç¾å›½èŒä¸šå¤§è”ç›Ÿ)
            194,   # Liga MX (å¢¨è¥¿å“¥)
            # äºšæ´²è”èµ›
            372,   # J1 League (æ—¥æœ¬)
            70,    # K League 1 (éŸ©å›½)
            # å…¶ä»–å•å¹´åˆ¶è”èµ›
            126, 210, 96, 408, 311, 398, 310, 306, 348, 421, 338, 433, 392, 103, 171
        }

    def generate_season_string(self, year: int, league_info: dict[str, Any]) -> list[str]:
        """
        æ ¹æ®è”èµ›ä¿¡æ¯æ™ºèƒ½ç”Ÿæˆå”¯ä¸€çš„èµ›å­£æ ¼å¼ï¼Œé¿å…é‡å¤æŠ“å–

        Args:
            year: èµ›å­£å¹´ä»½
            league_info: è”èµ›ä¿¡æ¯å­—å…¸

        Returns:
            List[str]: åŒ…å«å”¯ä¸€èµ›å­£æ ¼å¼çš„åˆ—è¡¨ï¼ˆä¸å†æ˜¯å¤šä¸ªæ ¼å¼ï¼‰
        """
        league_id = league_info.get("id", 0)
        country = league_info.get("country", "")
        league_type = league_info.get("type", "league")

        # 1. ä¼˜å…ˆæ ¹æ®ç²¾ç¡®çš„è”èµ›IDåˆ†ç±»
        if league_id in self.crossover_leagues:
            return [f"{year}/{year + 1}"]  # è·¨å¹´åˆ¶ï¼š2023/2024
        elif league_id in self.single_year_leagues:
            return [str(year)]  # å•å¹´åˆ¶ï¼š2023

        # 2. æ ¹æ®å›½å®¶åˆ†ç±»ï¼ˆå¤‡ç”¨é€»è¾‘ï¼‰
        if country in EUROPEAN_COUNTRIES:
            if league_type == "league":
                return [f"{year}/{year + 1}"]  # æ¬§æ´²è”èµ›å¤šä¸ºè·¨å¹´åˆ¶
            else:  # æ¯èµ›
                return [str(year)]  # æ¯èµ›å¤šä¸ºå•å¹´åˆ¶

        elif country in AMERICAN_COUNTRIES:
            return [str(year)]  # ç¾æ´²å¤šä¸ºå•å¹´åˆ¶

        elif country in ASIAN_COUNTRIES:
            if country in ["Japan", "South Korea"]:
                return [f"{year}/{year + 1}"]  # æ—¥éŸ©è·¨å¹´åˆ¶
            else:
                return [str(year)]  # å…¶ä»–äºšæ´²å•å¹´åˆ¶

        # 3. é»˜è®¤ç­–ç•¥ï¼šä¼˜å…ˆä½¿ç”¨è·¨å¹´åˆ¶ï¼ˆæ¬§æ´²è”èµ›è¾ƒå¤šï¼‰
        logger.debug(f"æœªåˆ†ç±»è”èµ› {league_id} ({country})ï¼Œé»˜è®¤ä½¿ç”¨è·¨å¹´åˆ¶æ ¼å¼")
        return [f"{year}/{year + 1}"]

class IndustrialBackfillEngine:
    """å·¥ä¸šçº§å›å¡«å¼•æ“"""

    def __init__(self):
        self.stats = BackfillStats()
        self.semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)
        self.collector = None
        self.processed_match_ids: set[str] = set()
        self.league_cache: dict[int, dict[str, Any]] = {}

    async def initialize(self):
        """åˆå§‹åŒ–å›å¡«å¼•æ“"""
        logger.info("ğŸš€ åˆå§‹åŒ–å·¥ä¸šçº§å›å¡«å¼•æ“...")

        if not COLLECTOR_AVAILABLE:
            raise RuntimeError("âŒ é‡‡é›†å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")

        # åˆå§‹åŒ–æ•°æ®åº“
        initialize_database()

        # åˆå§‹åŒ–é‡‡é›†å™¨
        self.collector = FotMobAPICollector(
            max_concurrent=CONCURRENT_LIMIT,
            timeout=60,
            max_retries=3,
            base_delay=1.0,
            enable_proxy=False,  # å›å¡«æ—¶ç¦ç”¨ä»£ç†ä»¥æé«˜é€Ÿåº¦
            enable_jitter=True
        )

        await self.collector.initialize()

        # é¢„åŠ è½½å·²å¤„ç†è¿‡çš„æ¯”èµ›ID
        await self._preload_processed_matches()

        logger.info("âœ… å·¥ä¸šçº§å›å¡«å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def _preload_processed_matches(self):
        """é¢„åŠ è½½å·²å¤„ç†çš„æ¯”èµ›ID"""
        logger.info("ğŸ“‹ é¢„åŠ è½½å·²å¤„ç†çš„æ¯”èµ›ID...")

        try:
            async with get_db_session() as session:
                result = await session.execute(
                    text("SELECT id FROM matches WHERE status = 'finished'")
                )
                matches = result.fetchall()
                self.processed_match_ids = {match[0] for match in matches}
                logger.info(f"âœ… å·²åŠ è½½ {len(self.processed_match_ids)} ä¸ªå·²å¤„ç†æ¯”èµ›ID")

        except Exception as e:
            logger.warning(f"âš ï¸ é¢„åŠ è½½æ¯”èµ›IDå¤±è´¥ï¼Œå°†è·³è¿‡æ–­ç‚¹ç»­ä¼ : {e}")
            self.processed_match_ids = set()

    async def load_league_config(self) -> list[dict[str, Any]]:
        """åŠ è½½è”èµ›é…ç½®å¹¶åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸"""
        logger.info("ğŸ“‹ åŠ è½½è”èµ›é…ç½®...")

        config_path = project_root / "config" / "target_leagues.json"

        if not config_path.exists():
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return []

        try:
            with open(config_path, encoding='utf-8') as f:
                config = json.load(f)

            leagues = config.get("leagues", [])
            logger.info(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½äº† {len(leagues)} ä¸ªè”èµ›")

            # ğŸ—ï¸ åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸
            patched_leagues = self._apply_hardcoded_patches(leagues)
            logger.info(f"ğŸ”§ åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸å: {len(patched_leagues)} ä¸ªè”èµ›")

            return patched_leagues

        except Exception as e:
            logger.error(f"âŒ åŠ è½½è”èµ›é…ç½®å¤±è´¥: {e}")
            return []

    def _apply_hardcoded_patches(self, leagues: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸"""
        logger.info("ğŸ”§ åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸...")

        existing_names = {league.get("name") for league in leagues}
        existing_ids = {league.get("id") for league in leagues}

        for league_name, league_id in HARDCODED_PATCHES.items():
            if league_name not in existing_names and league_id not in existing_ids:
                # æ·»åŠ ç¡¬ç¼–ç çš„è”èµ›
                patch_league = {
                    "name": league_name,
                    "id": league_id,
                    "tier": 2,  # é»˜è®¤ä¸ºäºŒçº§è”èµ›
                    "country": "England" if league_name == "Championship" else "Portugal",
                    "type": "league",
                    "source": "hardcoded_patch"
                }
                leagues.append(patch_league)
                logger.info(f"ğŸ”§ æ·»åŠ ç¡¬ç¼–ç è¡¥ä¸è”èµ›: {league_name} (ID: {league_id})")
            else:
                logger.debug(f"â„¹ï¸ è”èµ›å·²å­˜åœ¨ï¼Œè·³è¿‡è¡¥ä¸: {league_name}")

        return leagues

    async def fetch_league_matches(self, league_id: int, season: str) -> list[str]:
        """è·å–è”èµ›æŒ‡å®šèµ›å­£çš„æ¯”èµ›IDåˆ—è¡¨ - ä½¿ç”¨FotMob fixtures API"""
        try:
            logger.debug(f"ğŸ” è·å–è”èµ› {league_id} èµ›å­£ {season} çš„æ¯”èµ›åˆ—è¡¨...")

            # ä½¿ç”¨FotMob fixtures API
            league_url = f"https://www.fotmob.com/api/leagues?id={league_id}&timezone=Europe/London"

            # ä½¿ç”¨é‡‡é›†å™¨å‘é€è¯·æ±‚
            data, status = await self.collector._make_request(league_url, f"league_{league_id}")

            if status.name != "SUCCESS" or not data:
                logger.warning(f"âš ï¸ è”èµ› {league_id} APIè¯·æ±‚å¤±è´¥: {status}")
                return []

            # ä»fixtures.allMatchesæå–æ¯”èµ›æ•°æ®
            matches_data = []
            if "fixtures" in data:
                matches_data = data["fixtures"].get("allMatches", [])
                logger.info(f"âœ… ä»fixtures.allMatchesæ‰¾åˆ°: {len(matches_data)}åœºæ¯”èµ›")

            if not matches_data:
                logger.warning(f"âš ï¸ è”èµ› {league_id} èµ›å­£ {season}: æœªæ‰¾åˆ°æ¯”èµ›æ•°æ®")
                return []

            # æå–çº¯æ•°å­—æ¯”èµ›ID
            match_ids = []
            for match in matches_data:
                if not isinstance(match, dict):
                    continue

                match_id = match.get("id")
                if not match_id:
                    match_id = match.get("matchId") or match.get("match_id")

                if match_id:
                    clean_id = str(match_id).strip()
                    if clean_id.isdigit():
                        match_ids.append(clean_id)
                    else:
                        logger.warning(f"âš ï¸ è·³è¿‡éæ•°å­—ID: {clean_id}")

            await asyncio.sleep(uniform(0.1, 0.3))  # ç½‘ç»œå»¶è¿Ÿ

            if match_ids:
                logger.info(f"âœ… è”èµ› {league_id} èµ›å­£ {season}: æ‰¾åˆ° {len(match_ids)} åœºæ¯”èµ›")
                return match_ids
            else:
                logger.warning(f"âš ï¸ è”èµ› {league_id} èµ›å­£ {season}: æœªæ‰¾åˆ°æœ‰æ•ˆæ¯”èµ›ID")
                return []

        except Exception as e:
            logger.error(f"âŒ è·å–è”èµ› {league_id} èµ›å­£ {season} æ¯”èµ›åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def process_match(self, match_id: str, league_info: dict[str, Any]) -> bool:
        """å¤„ç†å•ä¸ªæ¯”èµ› - æ™ºèƒ½429é¿éšœç‰ˆ"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘
            try:
                # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
                if match_id in self.processed_match_ids:
                    logger.debug(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨æ¯”èµ›: {match_id}")
                    self.stats.skipped_matches += 1
                    return True

                # æ™ºèƒ½é‡‡é›†æ¯”èµ›æ•°æ® (å«429é¿éšœ)
                logger.debug(f"ğŸ”„ æ­£åœ¨é‡‡é›†: {match_id}")
                match_data = await self._collect_with_429_protection(match_id)

                if match_data:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await self._save_match_data(match_data, league_info)
                    self.stats.successful_matches += 1
                    logger.debug(f"âœ… æˆåŠŸå¤„ç†: {match_id}")
                    return True
                else:
                    self.stats.failed_matches += 1
                    self.stats.errors_by_type["collection_failed"] = self.stats.errors_by_type.get("collection_failed", 0) + 1
                    return False

            except Exception as e:
                self.stats.failed_matches += 1
                error_type = type(e).__name__
                self.stats.errors_by_type[error_type] = self.stats.errors_by_type.get(error_type, 0) + 1
                logger.error(f"âŒ å¤„ç†æ¯”èµ› {match_id} å¤±è´¥: {e}")
                return False

            finally:
                # å®‰å…¨æµé‡æ§åˆ¶
                await asyncio.sleep(uniform(MIN_DELAY, MAX_DELAY))

    async def _collect_with_429_protection(self, match_id: str):
        """æ™ºèƒ½429é¿éšœçš„æ•°æ®é‡‡é›†æ–¹æ³•"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # å°è¯•é‡‡é›†æ•°æ®
                return await self.collector.collect_match_details(match_id)

            except Exception as e:
                error_str = str(e).lower()

                # æ£€æŸ¥æ˜¯å¦ä¸º429é”™è¯¯
                if "429" in error_str or "too many requests" in error_str or "rate limit" in error_str:
                    logger.warning(f"âš ï¸ Rate Limit Hit! Cooling down for {RATE_LIMIT_COOLDOWN}s... (Attempt {attempt + 1}/{max_retries})")
                    self.stats.errors_by_type["rate_limit_429"] = self.stats.errors_by_type.get("rate_limit_429", 0) + 1

                    # å¼ºåˆ¶å†·å´
                    await asyncio.sleep(RATE_LIMIT_COOLDOWN)

                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        logger.info("ğŸ”„ Retrying after cooldown...")
                        continue
                    else:
                        logger.error(f"âŒ Max retries exceeded for {match_id} after 429 errors")
                        return None

                else:
                    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                    logger.debug(f"ğŸ”„ Non-429 error, will be handled by caller: {e}")
                    raise

        return None

    async def _save_match_data(self, match_data, league_info: dict[str, Any]):
        """ä¿å­˜æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            async with get_db_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆæŒ‰ fotmob_id æŸ¥è¯¢ï¼‰
                existing = await session.execute(
                    text("SELECT id FROM matches WHERE fotmob_id = :fotmob_id"),
                    {"fotmob_id": match_data.fotmob_id}
                )
                if existing.fetchone():
                    # æ›´æ–°ç°æœ‰è®°å½•ï¼ˆæŒ‰ fotmob_id æ›´æ–°ï¼‰
                    update_query = text("""
                    UPDATE matches SET
                        home_score = :home_score,
                        away_score = :away_score,
                        status = :status,
                        home_xg = :home_xg,
                        away_xg = :away_xg,
                        stats_json = :stats_json,
                        lineups_json = :lineups_json,
                        odds_snapshot_json = :odds_snapshot_json,
                        match_info = :match_info,
                        environment_json = :environment_json,
                        data_completeness = :data_completeness,
                        collection_time = NOW(),
                        updated_at = NOW()
                    WHERE fotmob_id = :fotmob_id
                    """)
                    await session.execute(update_query, {
                        "home_score": match_data.home_score,
                        "away_score": match_data.away_score,
                        "status": match_data.status,
                        "home_xg": match_data.xg_home,
                        "away_xg": match_data.xg_away,
                        "stats_json": json.dumps(match_data.stats_json) if match_data.stats_json else None,
                        "lineups_json": json.dumps(match_data.lineups_json) if match_data.lineups_json else None,
                        "odds_snapshot_json": json.dumps(match_data.odds_snapshot_json) if match_data.odds_snapshot_json else None,
                        "match_info": json.dumps(match_data.match_info) if match_data.match_info else None,
                        "environment_json": json.dumps(match_data.environment_json) if match_data.environment_json else None,
                        "data_completeness": "partial",
                        "fotmob_id": match_data.fotmob_id
                    })
                    logger.info(f"ğŸ“ æ›´æ–°æ¯”èµ›æ•°æ®: {match_data.fotmob_id}")
                else:
                    # æ’å…¥æ–°è®°å½•ï¼ˆä¸è®¾ç½® idï¼Œè®©å®ƒè‡ªå¢ï¼‰
                    insert_query = text("""
                    INSERT INTO matches (
                        fotmob_id, home_team_name, away_team_name,
                        home_score, away_score, status,
                        home_xg, away_xg,
                        match_time, match_date,
                        data_source, data_completeness, collection_time,
                        stats_json, lineups_json, odds_snapshot_json,
                        match_info, environment_json
                    ) VALUES (
                        :fotmob_id, :home_team_name, :away_team_name,
                        :home_score, :away_score, :status,
                        :home_xg, :away_xg,
                        :match_time, :match_date,
                        :data_source, :data_completeness, NOW(),
                        :stats_json, :lineups_json, :odds_snapshot_json,
                        :match_info, :environment_json
                    )
                    """)
                    await session.execute(insert_query, {
                        "fotmob_id": match_data.fotmob_id,
                        "home_team_name": getattr(match_data, 'home_team_name', 'Home Team'),
                        "away_team_name": getattr(match_data, 'away_team_name', 'Away Team'),
                        "home_score": match_data.home_score,
                        "away_score": match_data.away_score,
                        "status": match_data.status,
                        "home_xg": match_data.xg_home,
                        "away_xg": match_data.xg_away,
                        "match_time": match_data.match_time,
                        "match_date": match_data.match_time,
                        "data_source": "fotmob_v2",
                        "data_completeness": "partial",
                        "stats_json": json.dumps(match_data.stats_json) if match_data.stats_json else None,
                        "lineups_json": json.dumps(match_data.lineups_json) if match_data.lineups_json else None,
                        "odds_snapshot_json": json.dumps(match_data.odds_snapshot_json) if match_data.odds_snapshot_json else None,
                        "match_info": json.dumps(match_data.match_info) if match_data.match_info else None,
                        "environment_json": json.dumps(match_data.environment_json) if match_data.environment_json else None,
                    })
                    logger.info(f"ğŸ’¾ æ–°å¢æ¯”èµ›æ•°æ®: {match_data.fotmob_id}")

                await session.commit()
                self.processed_match_ids.add(match_data.fotmob_id)

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¯”èµ›æ•°æ®å¤±è´¥ {match_data.fotmob_id}: {e}")
            raise

    async def run_backfill(self):
        """è¿è¡Œå®Œæ•´å›å¡«æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨å…¨å†å²æ•°æ®å›å¡«...")

        try:
            # åŠ è½½è”èµ›é…ç½®
            leagues = await self.load_league_config()
            if not leagues:
                logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„è”èµ›é…ç½®ï¼Œé€€å‡º")
                return False

            # ç”Ÿæˆå›å¡«ä»»åŠ¡
            backfill_tasks = await self._generate_backfill_tasks(leagues)
            if not backfill_tasks:
                logger.error("âŒ æ²¡æœ‰ç”Ÿæˆå›å¡«ä»»åŠ¡ï¼Œé€€å‡º")
                return False

            self.stats.total_matches = len(backfill_tasks)
            logger.info(f"ğŸ“Š æ€»è®¡éœ€è¦å¤„ç† {self.stats.total_matches} åœºæ¯”èµ›")

            # æ‰§è¡Œå›å¡«ä»»åŠ¡
            await self._execute_backfill_tasks(backfill_tasks)

            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            await self._print_final_stats()

            return True

        except Exception as e:
            logger.error(f"âŒ å›å¡«æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return False

    async def _generate_backfill_tasks(self, leagues: list[dict[str, Any]]) -> list[tuple[str, dict[str, Any]]]:
        """ç”Ÿæˆå›å¡«ä»»åŠ¡åˆ—è¡¨ - æ™ºèƒ½èµ›å­£æ ¼å¼ï¼Œé¿å…é‡å¤æŠ“å–"""
        logger.info("ğŸ“‹ ç”Ÿæˆå›å¡«ä»»åŠ¡...")

        tasks = []
        season_generator = SeasonFormatGenerator()  # åˆ›å»ºæ™ºèƒ½èµ›å­£ç”Ÿæˆå™¨å®ä¾‹

        for league in leagues:
            league_id = league.get("id")
            league_name = league.get("name", "Unknown")

            logger.info(f"ğŸ” å¤„ç†è”èµ›: {league_name} (ID: {league_id})")

            for year in YEARS_TO_BACKFILL:
                # ç”Ÿæˆå”¯ä¸€çš„èµ›å­£æ ¼å¼ï¼ˆä¸å†é‡å¤ï¼‰
                season_formats = season_generator.generate_season_string(year, league)

                # ç”±äºç°åœ¨æ¯ä¸ªè”èµ›åªç”Ÿæˆä¸€ç§èµ›å­£æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥ç®€åŒ–é€»è¾‘
                season = season_formats[0]  # å–ç¬¬ä¸€ä¸ªï¼ˆä¹Ÿæ˜¯å”¯ä¸€çš„ï¼‰èµ›å­£æ ¼å¼

                try:
                    # è·å–è¯¥èµ›å­£çš„æ¯”èµ›åˆ—è¡¨
                    match_ids = await self.fetch_league_matches(league_id, season)

                    for match_id in match_ids:
                        tasks.append((match_id, league))

                    logger.info(f"âœ… è”èµ› {league_name} {season} èµ›å­£: {len(match_ids)} åœºæ¯”èµ›")

                except Exception as e:
                    logger.warning(f"âš ï¸ è·å–è”èµ› {league_name} èµ›å­£ {season} å¤±è´¥: {e}")
                    continue

        logger.info(f"âœ… ç”Ÿæˆå›å¡«ä»»åŠ¡: {len(tasks)} ä¸ª")
        return tasks

    async def _execute_backfill_tasks(self, tasks: list[tuple[str, dict[str, Any]]]):
        """æ‰§è¡Œå›å¡«ä»»åŠ¡"""
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå›å¡«ä»»åŠ¡...")

        # åˆ›å»ºè¿›åº¦æ¡
        if TQDM_AVAILABLE:
            pbar = tqdm(total=len(tasks), desc="å›å¡«è¿›åº¦", unit="æ¯”èµ›")

        # æ‰¹é‡å¤„ç†ä»»åŠ¡
        batch_size = 50
        processed_count = 0

        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i + batch_size]

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            batch_tasks = [
                self.process_match(match_id, league_info)
                for match_id, league_info in batch
            ]

            # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
            results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # ç»Ÿè®¡ç»“æœ
            for result in results:
                self.stats.processed_matches += 1
                processed_count += 1

                if isinstance(result, Exception):
                    logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                    self.stats.failed_matches += 1

                # æ›´æ–°è¿›åº¦æ¡
                if TQDM_AVAILABLE:
                    pbar.update(1)

            # æ¯50ä¸ªæ¯”èµ›æ‰“å°ä¸€æ¬¡è¿›åº¦
            if processed_count % 50 == 0:
                self.stats.log_progress()

        # å…³é—­è¿›åº¦æ¡
        if TQDM_AVAILABLE:
            pbar.close()

    async def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ å…¨å†å²æ•°æ®å›å¡«å®Œæˆ")
        logger.info("="*60)
        logger.info(f"ğŸ“Š æ€»æ¯”èµ›æ•°: {self.stats.total_matches}")
        logger.info(f"âœ… æˆåŠŸå¤„ç†: {self.stats.successful_matches}")
        logger.info(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨: {self.stats.skipped_matches}")
        logger.info(f"âŒ å¤„ç†å¤±è´¥: {self.stats.failed_matches}")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {self.stats.success_rate:.1f}%")
        logger.info(f"â±ï¸ æ€»ç”¨æ—¶: {self.stats.elapsed_time}")

        if self.stats.errors_by_type:
            logger.info("\nğŸš¨ é”™è¯¯ç»Ÿè®¡:")
            for error_type, count in self.stats.errors_by_type.items():
                # ç‰¹åˆ«é«˜äº®429é”™è¯¯
                if "rate_limit_429" in error_type:
                    logger.info(f"  ğŸš¨ {error_type}: {count} (è§¦å‘äº†æ™ºèƒ½å†·å´)")
                else:
                    logger.info(f"  ğŸ“Š {error_type}: {count}")

            # é£æ§çŠ¶æ€æŠ¥å‘Š
            rate_429_count = self.stats.errors_by_type.get("rate_limit_429", 0)
            if rate_429_count > 0:
                total_429_cooldown = rate_429_count * RATE_LIMIT_COOLDOWN
                logger.info("\nğŸ›¡ï¸ é£æ§æŠ¥å‘Š:")
                logger.info(f"  429è§¦å‘æ¬¡æ•°: {rate_429_count}")
                logger.info(f"  æ€»å†·å´æ—¶é—´: {total_429_cooldown//60}åˆ†{total_429_cooldown%60}ç§’")
                logger.info(f"  å¹³å‡å¤„ç†é€Ÿåº¦: ~{self.stats.successful_matches / max(1, (self.stats.elapsed_time.total_seconds() - total_429_cooldown) / 3600):.0f}åœº/å°æ—¶ (ä¸å«å†·å´æ—¶é—´)")
            else:
                logger.info("\nğŸ›¡ï¸ é£æ§æŠ¥å‘Š: æœªè§¦å‘429é™åˆ¶ï¼Œå®‰å…¨è¿è¡Œ")

        logger.info("="*60)
        logger.info("ğŸ‰ å…¨å†å²æ•°æ®å›å¡«ä»»åŠ¡å®Œæˆ!")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.collector:
            await self.collector.close()
        logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ›¡ï¸ å¯åŠ¨å®‰å…¨åŠ å›ºç‰ˆå…¨å†å²æ•°æ®å›å¡«è„šæœ¬")
    logger.info(f"ğŸ“Š ç­–ç•¥é…ç½®: å€’åºå›å¡« ({YEARS_TO_BACKFILL[0]} -> {YEARS_TO_BACKFILL[-1]})")
    logger.info(f"ğŸ”’ é£æ§è®¾ç½®: {CONCURRENT_LIMIT}å¹¶å‘ + {MIN_DELAY}-{MAX_DELAY}ç§’å»¶è¿Ÿ")
    logger.info(f"ğŸš¨ 429é¿éšœ: {RATE_LIMIT_COOLDOWN}ç§’è‡ªåŠ¨å†·å´ + 3æ¬¡é‡è¯•")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    if not os.getenv("DATABASE_URL"):
        os.environ["DATABASE_URL"] = DATABASE_URL

    # åˆ›å»ºå›å¡«å¼•æ“
    engine = IndustrialBackfillEngine()

    try:
        # åˆå§‹åŒ–
        await engine.initialize()

        # æ‰§è¡Œå›å¡«
        success = await engine.run_backfill()

        if success:
            logger.info("âœ… å®‰å…¨åŠ å›ºç‰ˆå…¨å†å²æ•°æ®å›å¡«ä»»åŠ¡æˆåŠŸå®Œæˆ!")
            sys.exit(0)
        else:
            logger.error("âŒ å®‰å…¨åŠ å›ºç‰ˆå…¨å†å²æ•°æ®å›å¡«ä»»åŠ¡å¤±è´¥!")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        engine.stats.log_progress()
        sys.exit(130)

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)

    finally:
        await engine.cleanup()

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())
