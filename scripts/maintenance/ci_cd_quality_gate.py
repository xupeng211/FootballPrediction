#!/usr/bin/env python3
"""
CI/CDè´¨é‡é—¨ç¦ç³»ç»Ÿ
CI/CD Quality Gate System

è‡ªåŠ¨åŒ–è´¨é‡æ£€æŸ¥å’Œé—¨ç¦æ§åˆ¶ï¼Œç¡®ä¿åªæœ‰ç¬¦åˆè´¨é‡æ ‡å‡†çš„ä»£ç æ‰èƒ½é€šè¿‡CI/CDæµæ°´çº¿ã€‚

ä½œè€…: Claude AI Assistant
ç‰ˆæœ¬: v1.0
åˆ›å»ºæ—¶é—´: 2025-11-03
"""

import json
import sys
import subprocess
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))


class GateResult(Enum):
    """é—¨ç¦ç»“æœæšä¸¾"""

    PASS = "pass"
    FAIL = "fail"
    WARN = "warn"


class QualityLevel(Enum):
    """è´¨é‡ç­‰çº§æšä¸¾"""

    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class QualityMetric:
    """è´¨é‡æŒ‡æ ‡æ•°æ®ç»“æ„"""

    name: str
    value: float
    threshold: float
    unit: str
    status: GateResult
    level: QualityLevel
    message: str


@dataclass
class GateReport:
    """é—¨ç¦æŠ¥å‘Šæ•°æ®ç»“æ„"""

    timestamp: str
    overall_result: GateResult
    total_metrics: int
    passed_metrics: int
    failed_metrics: int
    warning_metrics: int
    metrics: list[QualityMetric]
    recommendations: list[str]
    summary: dict[str, Any]


class QualityGate:
    """è´¨é‡é—¨ç¦æ§åˆ¶å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.timestamp = datetime.now().isoformat()

        # è´¨é‡æ ‡å‡†é…ç½®
        self.quality_thresholds = {
            # æµ‹è¯•è¦†ç›–ç‡é˜ˆå€¼
            "min_coverage": {
                "critical": 80.0,  # å…³é”®æ¨¡å—è¦†ç›–ç‡
                "high": 60.0,  # é«˜ä¼˜å…ˆçº§æ¨¡å—è¦†ç›–ç‡
                "medium": 40.0,  # ä¸­ç­‰ä¼˜å…ˆçº§æ¨¡å—è¦†ç›–ç‡
                "low": 20.0,  # æœ€ä½è¦†ç›–ç‡è¦æ±‚
            },
            # æµ‹è¯•é€šè¿‡ç‡é˜ˆå€¼
            "min_pass_rate": {
                "critical": 100.0,  # å…³é”®æµ‹è¯•å¿…é¡»100%é€šè¿‡
                "high": 95.0,  # é«˜ä¼˜å…ˆçº§æµ‹è¯•é€šè¿‡ç‡
                "medium": 90.0,  # ä¸­ç­‰ä¼˜å…ˆçº§æµ‹è¯•é€šè¿‡ç‡
                "low": 80.0,  # æœ€ä½é€šè¿‡ç‡è¦æ±‚
            },
            # ä»£ç è´¨é‡è¯„åˆ†
            "min_quality_score": {
                "critical": 90.0,  # å…³é”®æ¨¡å—è´¨é‡è¯„åˆ†
                "high": 80.0,  # é«˜ä¼˜å…ˆçº§æ¨¡å—è´¨é‡è¯„åˆ†
                "medium": 70.0,  # ä¸­ç­‰ä¼˜å…ˆçº§æ¨¡å—è´¨é‡è¯„åˆ†
                "low": 60.0,  # æœ€ä½è´¨é‡è¯„åˆ†è¦æ±‚
            },
            # æ€§èƒ½åŸºå‡†
            "max_test_execution_time": {
                "critical": 60.0,  # å…³é”®æµ‹è¯•æœ€é•¿æ‰§è¡Œæ—¶é—´(ç§’)
                "high": 180.0,  # é«˜ä¼˜å…ˆçº§æµ‹è¯•æ‰§è¡Œæ—¶é—´
                "medium": 300.0,  # ä¸­ç­‰ä¼˜å…ˆçº§æµ‹è¯•æ‰§è¡Œæ—¶é—´
                "low": 600.0,  # æœ€é•¿å¯æ¥å—æ‰§è¡Œæ—¶é—´
            },
            # å®‰å…¨æ£€æŸ¥
            "max_security_issues": {
                "critical": 0,  # å…³é”®å®‰å…¨é—®é¢˜å¿…é¡»ä¸º0
                "high": 1,  # é«˜ä¼˜å…ˆçº§å®‰å…¨é—®é¢˜ä¸Šé™
                "medium": 3,  # ä¸­ç­‰ä¼˜å…ˆçº§å®‰å…¨é—®é¢˜ä¸Šé™
                "low": 5,  # æœ€å¤šå…è®¸çš„ä½ä¼˜å…ˆçº§å®‰å…¨é—®é¢˜
            },
        }

    def run_test_checks(self) -> list[QualityMetric]:
        """è¿è¡Œæµ‹è¯•ç›¸å…³æ£€æŸ¥"""
        metrics = []

        try:
            # è¿è¡Œpytestå¹¶æ”¶é›†è¦†ç›–ç‡
            result = subprocess.run(
                ["pytest", "--cov=src", "--cov-report=json", "--tb=short"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=600,
            )

            # è¯»å–è¦†ç›–ç‡æŠ¥å‘Š
            coverage_file = self.project_root / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file) as f:
                    coverage_data = json.load(f)
                    total_coverage = coverage_data.get("totals", {}).get(
                        "percent_covered", 0.0
                    )

                # è¦†ç›–ç‡æ£€æŸ¥
                coverage_metric = self._evaluate_metric(
                    "test_coverage",
                    total_coverage,
                    self.quality_thresholds["min_coverage"]["low"],
                    "%",
                    QualityLevel.MEDIUM,
                )
                metrics.append(coverage_metric)

            # è§£æpytestè¾“å‡ºè·å–æµ‹è¯•ç»“æœ
            test_output = result.stdout
            # æå–æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
            lines = test_output.split("\n")
            for line in lines:
                # è§£ææµ‹è¯•ç»“æœè¡Œ
                parts = line.split()
                passed = failed = errors = 0

                for part in parts:
                    if part.endswith("passed"):
                        passed = int(part.replace("passed", ""))
                    elif part.endswith("failed"):
                        failed = int(part.replace("failed", ""))
                    elif part.endswith("error"):
                        errors = int(part.replace("error", ""))

                total_tests = passed + failed + errors
                pass_rate = (passed / total_tests * 100) if total_tests > 0 else 0.0

                # æµ‹è¯•é€šè¿‡ç‡æ£€æŸ¥
                pass_rate_metric = self._evaluate_metric(
                    "test_pass_rate",
                    pass_rate,
                    self.quality_thresholds["min_pass_rate"]["low"],
                    "%",
                    QualityLevel.HIGH,
                )
                metrics.append(pass_rate_metric)

                # æµ‹è¯•æ‰§è¡Œæ—¶é—´æ£€æŸ¥
                execution_time = self._extract_execution_time(test_output)
                time_metric = self._evaluate_metric(
                    "test_execution_time",
                    execution_time,
                    self.quality_thresholds["max_test_execution_time"]["low"],
                    "s",
                    QualityLevel.MEDIUM,
                    reverse=True,  # æ—¶é—´è¶ŠçŸ­è¶Šå¥½
                )
                metrics.append(time_metric)
                break

        except subprocess.TimeoutExpired:
            metrics.append(
                QualityMetric(
                    name="test_execution_timeout",
                    value=999.0,
                    threshold=600.0,
                    unit="s",
                    status=GateResult.FAIL,
                    level=QualityLevel.CRITICAL,
                    message="æµ‹è¯•æ‰§è¡Œè¶…æ—¶(>10åˆ†é’Ÿ)",
                )
            )
        except Exception as e:
            metrics.append(
                QualityMetric(
                    name="test_check_error",
                    value=0.0,
                    threshold=0.0,
                    unit="",
                    status=GateResult.FAIL,
                    level=QualityLevel.CRITICAL,
                    message=f"æµ‹è¯•æ£€æŸ¥å¤±è´¥: {str(e)}",
                )
            )

        return metrics

    def run_code_quality_checks(self) -> list[QualityMetric]:
        """è¿è¡Œä»£ç è´¨é‡æ£€æŸ¥"""
        metrics = []

        try:
            # è¿è¡ŒRuffä»£ç æ£€æŸ¥
            result = subprocess.run(
                ["ruff", "check", "src/", "--output-format=json"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
            )

            if result.stdout:
                try:
                    ruff_issues = json.loads(result.stdout)
                    issue_count = len(ruff_issues)

                    # ä»£ç é—®é¢˜æ•°é‡æ£€æŸ¥
                    issues_metric = self._evaluate_metric(
                        "code_issues",
                        100.0 - issue_count,  # è½¬æ¢ä¸ºåˆ†æ•°ï¼Œé—®é¢˜è¶Šå°‘è¶Šå¥½
                        80.0,  # æœŸæœ›è‡³å°‘80åˆ†(å³ä¸è¶…è¿‡20ä¸ªé—®é¢˜)
                        "score",
                        QualityLevel.MEDIUM,
                    )
                    metrics.append(issues_metric)

                except json.JSONDecodeError:
                    pass

            # è¿è¡Œç±»å‹æ£€æŸ¥
            mypy_result = subprocess.run(
                ["mypy", "src/", "--show-error-codes"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
            )

            mypy_errors = mypy_result.stdout.count("error:")
            mypy_metric = self._evaluate_metric(
                "type_errors", 100.0 - mypy_errors, 90.0, "score", QualityLevel.HIGH
            )
            metrics.append(mypy_metric)

        except Exception as e:
            metrics.append(
                QualityMetric(
                    name="code_quality_check_error",
                    value=0.0,
                    threshold=0.0,
                    unit="",
                    status=GateResult.FAIL,
                    level=QualityLevel.CRITICAL,
                    message=f"ä»£ç è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}",
                )
            )

        return metrics

    def run_security_checks(self) -> list[QualityMetric]:
        """è¿è¡Œå®‰å…¨æ£€æŸ¥"""
        metrics = []

        try:
            # è¿è¡Œbanditå®‰å…¨æ‰«æ
            result = subprocess.run(
                ["bandit", "-r", "src/", "-f", "json"],
                cwd=self.project_root,
                capture_output=True,
                text=True,
            )

            if result.stdout:
                try:
                    bandit_data = json.loads(result.stdout)
                    high_severity_issues = len(
                        [
                            i
                            for i in bandit_data.get("results", [])
                            if i.get("issue_severity") == "HIGH"
                        ]
                    )

                    # å®‰å…¨é—®é¢˜æ£€æŸ¥
                    security_metric = self._evaluate_metric(
                        "security_issues",
                        100.0 - high_severity_issues,
                        95.0,
                        "score",
                        QualityLevel.CRITICAL,
                    )
                    metrics.append(security_metric)

                except json.JSONDecodeError:
                    pass

        except FileNotFoundError:
            # banditæœªå®‰è£…ï¼Œæ·»åŠ è­¦å‘Š
            metrics.append(
                QualityMetric(
                    name="security_check_unavailable",
                    value=0.0,
                    threshold=0.0,
                    unit="",
                    status=GateResult.WARN,
                    level=QualityLevel.MEDIUM,
                    message="å®‰å…¨æ£€æŸ¥å·¥å…·banditæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install bandit",
                )
            )
        except Exception as e:
            metrics.append(
                QualityMetric(
                    name="security_check_error",
                    value=0.0,
                    threshold=0.0,
                    unit="",
                    status=GateResult.FAIL,
                    level=QualityLevel.CRITICAL,
                    message=f"å®‰å…¨æ£€æŸ¥å¤±è´¥: {str(e)}",
                )
            )

        return metrics

    def _evaluate_metric(
        self,
        name: str,
        value: float,
        threshold: float,
        unit: str,
        level: QualityLevel,
        reverse: bool = False,
    ) -> QualityMetric:
        """è¯„ä¼°å•ä¸ªè´¨é‡æŒ‡æ ‡"""
        if reverse:
            # å¯¹äºæ—¶é—´ç­‰æŒ‡æ ‡ï¼Œå€¼è¶Šå°è¶Šå¥½
            status = GateResult.PASS if value <= threshold else GateResult.FAIL
        else:
            # å¯¹äºè¦†ç›–ç‡ç­‰æŒ‡æ ‡ï¼Œå€¼è¶Šå¤§è¶Šå¥½
            status = GateResult.PASS if value >= threshold else GateResult.FAIL

        if status == GateResult.FAIL:
            if level in [QualityLevel.CRITICAL, QualityLevel.HIGH]:
                message = f"{name} {value}{unit} ä½äºè¦æ±‚é˜ˆå€¼ {threshold}{unit}"
            else:
                status = GateResult.WARN  # ä¸­ä½ä¼˜å…ˆçº§å¤±è´¥é™çº§ä¸ºè­¦å‘Š
                message = f"{name} {value}{unit} ä½äºæ¨èå€¼ {threshold}{unit}"
        else:
            message = f"{name} {value}{unit} ç¬¦åˆè¦æ±‚"

        return QualityMetric(
            name=name,
            value=value,
            threshold=threshold,
            unit=unit,
            status=status,
            level=level,
            message=message,
        )

    def _extract_execution_time(self, test_output: str) -> float:
        """ä»pytestè¾“å‡ºä¸­æå–æµ‹è¯•æ‰§è¡Œæ—¶é—´"""
        for line in test_output.split("\n"):
            if "seconds" in line and "=" in line:
                try:
                    time_part = line.split("=")[1].strip()
                    return float(time_part.split()[0])
                except (IndexError, ValueError):
                    continue
        return 0.0

    def evaluate_quality_gate(self) -> GateReport:
        """æ‰§è¡Œè´¨é‡é—¨ç¦è¯„ä¼°"""

        all_metrics = []

        # è¿è¡Œå„ç±»è´¨é‡æ£€æŸ¥
        test_metrics = self.run_test_checks()
        all_metrics.extend(test_metrics)

        quality_metrics = self.run_code_quality_checks()
        all_metrics.extend(quality_metrics)

        security_metrics = self.run_security_checks()
        all_metrics.extend(security_metrics)

        # ç»Ÿè®¡ç»“æœ
        passed = len([m for m in all_metrics if m.status == GateResult.PASS])
        failed = len([m for m in all_metrics if m.status == GateResult.FAIL])
        warning = len([m for m in all_metrics if m.status == GateResult.WARN])

        # ç¡®å®šæ€»ä½“ç»“æœ
        critical_failures = [
            m
            for m in all_metrics
            if m.status == GateResult.FAIL and m.level == QualityLevel.CRITICAL
        ]
        overall_result = (
            GateResult.FAIL
            if critical_failures
            else GateResult.WARN
            if failed > 0
            else GateResult.PASS
        )

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_recommendations(all_metrics)

        # ç”Ÿæˆæ‘˜è¦
        summary = self._generate_summary(all_metrics)

        return GateReport(
            timestamp=self.timestamp,
            overall_result=overall_result,
            total_metrics=len(all_metrics),
            passed_metrics=passed,
            failed_metrics=failed,
            warning_metrics=warning,
            metrics=all_metrics,
            recommendations=recommendations,
            summary=summary,
        )

    def _generate_recommendations(self, metrics: list[QualityMetric]) -> list[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        failed_metrics = [m for m in metrics if m.status == GateResult.FAIL]
        warning_metrics = [m for m in metrics if m.status == GateResult.WARN]

        if failed_metrics:
            critical_failures = [
                m for m in failed_metrics if m.level == QualityLevel.CRITICAL
            ]
            if critical_failures:
                recommendations.append("ğŸš¨ **å…³é”®é—®é¢˜å¿…é¡»ä¿®å¤**:")
                for metric in critical_failures:
                    recommendations.append(f"   - {metric.message}")

            high_failures = [m for m in failed_metrics if m.level == QualityLevel.HIGH]
            if high_failures:
                recommendations.append("âš ï¸ **é«˜ä¼˜å…ˆçº§é—®é¢˜**:")
                for metric in high_failures:
                    recommendations.append(f"   - {metric.message}")

        if warning_metrics:
            recommendations.append("ğŸ’¡ **æ”¹è¿›å»ºè®®**:")
            for metric in warning_metrics:
                recommendations.append(f"   - {metric.message}")

        if not failed_metrics and not warning_metrics:
            recommendations.append("ğŸ‰ **æ‰€æœ‰è´¨é‡æ£€æŸ¥é€šè¿‡ï¼** ä»£ç è´¨é‡ç¬¦åˆCI/CDè¦æ±‚ã€‚")

        return recommendations

    def _generate_summary(self, metrics: list[QualityMetric]) -> dict[str, Any]:
        """ç”Ÿæˆè´¨é‡æ‘˜è¦"""
        summary = {
            "health_score": 0.0,
            "critical_issues": 0,
            "high_issues": 0,
            "medium_issues": 0,
            "low_issues": 0,
            "categories": {
                "testing": {"pass": 0, "fail": 0, "warn": 0},
                "quality": {"pass": 0, "fail": 0, "warn": 0},
                "security": {"pass": 0, "fail": 0, "warn": 0},
            },
        }

        for metric in metrics:
            # æŒ‰ä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
            if (
                metric.level == QualityLevel.CRITICAL
                and metric.status == GateResult.FAIL
            ):
                summary["critical_issues"] += 1
            elif metric.level == QualityLevel.HIGH and metric.status == GateResult.FAIL:
                summary["high_issues"] += 1
            elif metric.level == QualityLevel.MEDIUM and metric.status in [
                GateResult.FAIL,
                GateResult.WARN,
            ]:
                summary["medium_issues"] += 1
            elif metric.level == QualityLevel.LOW and metric.status in [
                GateResult.FAIL,
                GateResult.WARN,
            ]:
                summary["low_issues"] += 1

            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category = (
                "testing"
                if "test" in metric.name
                else "security"
                if "security" in metric.name
                else "quality"
            )
            summary["categories"][category][metric.status.value] += 1

        # è®¡ç®—å¥åº·è¯„åˆ†
        total_metrics = len(metrics)
        if total_metrics > 0:
            passed_percentage = (
                len([m for m in metrics if m.status == GateResult.PASS]) / total_metrics
            ) * 100
            warning_penalty = (
                len([m for m in metrics if m.status == GateResult.WARN]) / total_metrics
            ) * 10
            failure_penalty = (
                len([m for m in metrics if m.status == GateResult.FAIL]) / total_metrics
            ) * 30
            summary["health_score"] = max(
                0, passed_percentage - warning_penalty - failure_penalty
            )

        return summary

    def export_report(
        self, report: GateReport, output_file: Path | None = None
    ) -> Path:
        """å¯¼å‡ºè´¨é‡é—¨ç¦æŠ¥å‘Š"""
        if output_file is None:
            output_file = (
                self.project_root
                / "reports"
                / "quality_gate"
                / f"quality_gate_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )

        output_file.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        report_dict = asdict(report)
        report_dict["overall_result"] = report.overall_result.value
        report_dict["metrics"] = [
            {
                **asdict(metric),
                "status": metric.status.value,
                "level": metric.level.value,
            }
            for metric in report.metrics
        ]

        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(report_dict, f, indent=2, ensure_ascii=False)

        return output_file


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="CI/CDè´¨é‡é—¨ç¦ç³»ç»Ÿ")
    parser.add_argument("--project-root", type=Path, help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
    parser.add_argument("--output-file", type=Path, help="æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--strict", action="store_true", help="ä¸¥æ ¼æ¨¡å¼ï¼Œä»»ä½•è­¦å‘Šéƒ½ä¼šå¯¼è‡´é—¨ç¦å¤±è´¥"
    )

    args = parser.parse_args()

    # åˆ›å»ºè´¨é‡é—¨ç¦å®ä¾‹
    project_root = args.project_root or Path(__file__).parent.parent.parent
    gate = QualityGate(project_root)

    try:
        # æ‰§è¡Œè´¨é‡é—¨ç¦è¯„ä¼°
        report = gate.evaluate_quality_gate()

        # å¯¼å‡ºæŠ¥å‘Š
        gate.export_report(report, args.output_file)

        # æ˜¾ç¤ºç»“æœæ‘˜è¦

        # æ˜¾ç¤ºå…³é”®é—®é¢˜
        critical_issues = [
            m
            for m in report.metrics
            if m.status == GateResult.FAIL and m.level == QualityLevel.CRITICAL
        ]
        if critical_issues:
            for _metric in critical_issues:
                pass

        # æ˜¾ç¤ºå»ºè®®
        if report.recommendations:
            for _rec in report.recommendations:
                pass

        # è®¾ç½®é€€å‡ºç 
        if report.overall_result == GateResult.FAIL:
            sys.exit(1)
        elif report.overall_result == GateResult.WARN and args.strict:
            sys.exit(1)
        else:
            sys.exit(0)

    except KeyboardInterrupt:
        sys.exit(130)
    except Exception:
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
