#!/usr/bin/env python3
"""
Phase 7 Week 4 éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆæ‰©å±•å™¨
Phase 7 Week 4 Deployment Preparation and CI/CD Integration Expander

åŸºäºå‰ä¸‰å‘¨çš„æµ‹è¯•æˆæœï¼Œå‡†å¤‡ç”Ÿäº§éƒ¨ç½²å’Œä¼˜åŒ–CI/CDæµæ°´çº¿
"""

import ast
import json
import subprocess
import yaml
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set

class Phase7DeploymentCicdExpander:
    """Phase 7 Week 4 éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆæ‰©å±•å™¨"""

    def __init__(self):
        self.start_time = datetime.now()
        self.analysis_results = {}
        self.deployment_files_created = []

    def expand_deployment_cicd_integration(self) -> Dict:
        """æ‰©å±•éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆ"""
        print("ğŸš€ å¼€å§‹Phase 7 Week 4: éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆ")
        print("=" * 60)
        print("ğŸ¯ ç›®æ ‡: å‡†å¤‡ç”Ÿäº§éƒ¨ç½²å’Œä¼˜åŒ–CI/CDæµæ°´çº¿")
        print("ğŸ“Š é˜¶æ®µ: Week 4 - éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆ")
        print("=" * 60)

        # 1. åˆ†æå½“å‰éƒ¨ç½²å’ŒCI/CDçŠ¶æ€
        print("\nğŸ“‹ åˆ†æå½“å‰éƒ¨ç½²å’ŒCI/CDçŠ¶æ€...")
        deployment_analysis = self._analyze_deployment_status()
        self.analysis_results['deployment_analysis'] = deployment_analysis

        # 2. ä¼˜åŒ–CI/CDæµæ°´çº¿
        print("\nğŸ”§ ä¼˜åŒ–CI/CDæµæ°´çº¿...")
        cicd_optimization = self._optimize_cicd_pipeline()
        self.analysis_results['cicd_optimization'] = cicd_optimization

        # 3. åˆ›å»ºéƒ¨ç½²é…ç½®
        print("\nğŸ“¦ åˆ›å»ºéƒ¨ç½²é…ç½®...")
        deployment_config = self._create_deployment_configurations()
        self.analysis_results['deployment_config'] = deployment_config

        # 4. å»ºç«‹ç›‘æ§å’Œå‘Šè­¦
        print("\nğŸ“Š å»ºç«‹ç›‘æ§å’Œå‘Šè­¦...")
        monitoring_setup = self._setup_monitoring_alerting()
        self.analysis_results['monitoring_setup'] = monitoring_setup

        # 5. ç”Ÿæˆç”Ÿäº§éƒ¨ç½²æ–‡æ¡£
        print("\nğŸ“„ ç”Ÿæˆç”Ÿäº§éƒ¨ç½²æ–‡æ¡£...")
        deployment_docs = self._generate_deployment_documentation()
        self.analysis_results['deployment_docs'] = deployment_docs

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        elapsed_time = (datetime.now() - self.start_time).total_seconds()

        final_result = {
            'success': True,
            'phase': 'Phase 7 Week 4',
            'elapsed_time': f"{elapsed_time:.1f}s",
            'analysis_results': self.analysis_results,
            'summary': {
                'deployment_status': deployment_analysis['deployment_readiness'],
                'cicd_optimizations': cicd_optimization['optimizations_count'],
                'deployment_configs_created': deployment_config['configs_created'],
                'monitoring_components': monitoring_setup['monitoring_components'],
                'documentation_pages': deployment_docs['docs_generated']
            },
            'recommendations': self._generate_deployment_recommendations()
        }

        print("\nğŸ‰ Phase 7 Week 4 éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆå®Œæˆ:")
        print(f"   éƒ¨ç½²å‡†å¤‡çŠ¶æ€: {final_result['summary']['deployment_status']}")
        print(f"   CI/CDä¼˜åŒ–é¡¹: {final_result['summary']['cicd_optimizations']} ä¸ª")
        print(f"   éƒ¨ç½²é…ç½®æ–‡ä»¶: {final_result['summary']['deployment_configs_created']} ä¸ª")
        print(f"   ç›‘æ§ç»„ä»¶: {final_result['summary']['monitoring_components']} ä¸ª")
        print(f"   æ–‡æ¡£é¡µé¢: {final_result['summary']['documentation_pages']} ä¸ª")
        print(f"   æ‰§è¡Œæ—¶é—´: {final_result['elapsed_time']}")
        print("   çŠ¶æ€: âœ… æˆåŠŸ")

        print("\nğŸ“‹ ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
        for i, step in enumerate(final_result['recommendations'][:3], 1):
            print(f"   {i}. {step}")

        # ä¿å­˜æŠ¥å‘Š
        self._save_report(final_result)

        return final_result

    def _analyze_deployment_status(self) -> Dict:
        """åˆ†æå½“å‰éƒ¨ç½²çŠ¶æ€"""
        try:
            deployment_files = {
                'Dockerfile': Path('Dockerfile'),
                'docker-compose.yml': Path('docker-compose.yml'),
                'docker-compose.prod.yml': Path('docker-compose.prod.yml'),
                '.env.example': Path('.env.example'),
                'requirements/requirements.lock': Path('requirements/requirements.lock'),
                'Makefile': Path('Makefile'),
                '.github/workflows': Path('.github/workflows')
            }

            existing_files = {}
            for name, path in deployment_files.items():
                if path.exists():
                    existing_files[name] = {
                        'exists': True,
                        'size': path.stat().st_size if path.is_file() else 0,
                        'modified': datetime.fromtimestamp(path.stat().st_mtime).isoformat()
                    }
                else:
                    existing_files[name] = {'exists': False}

            # æ£€æŸ¥CI/CDå·¥ä½œæµ
            ci_workflows = []
            if Path('.github/workflows').exists():
                ci_workflows = list(Path('.github/workflows').glob('*.yml'))
                ci_workflows.extend(list(Path('.github/workflows').glob('*.yaml')))

            # è®¡ç®—éƒ¨ç½²å‡†å¤‡åº¦
            readiness_score = sum(1 for info in existing_files.values() if info['exists'])
            max_score = len(existing_files)
            readiness_percentage = (readiness_score / max_score) * 100 if max_score > 0 else 0

            deployment_readiness = "ç”Ÿäº§å°±ç»ª" if readiness_percentage >= 80 else "éœ€è¦å®Œå–„" if readiness_percentage >= 60 else "åŸºç¡€ä¸è¶³"

            print(f"   ğŸ” å‘ç°éƒ¨ç½²æ–‡ä»¶: {readiness_score}/{max_score}")
            print(f"   ğŸ” CI/CDå·¥ä½œæµ: {len(ci_workflows)} ä¸ª")

            return {
                'deployment_readiness': deployment_readiness,
                'readiness_score': f"{readiness_percentage:.1f}%",
                'existing_files': existing_files,
                'ci_workflows_count': len(ci_workflows),
                'ci_workflows': [str(wf.name) for wf in ci_workflows]
            }

        except Exception as e:
            return {
                'deployment_readiness': 'åˆ†æå¤±è´¥',
                'error': str(e),
                'status': 'analysis_failed'
            }

    def _optimize_cicd_pipeline(self) -> Dict:
        """ä¼˜åŒ–CI/CDæµæ°´çº¿"""
        optimizations = []
        workflows_created = []

        try:
            # 1. åˆ›å»ºå¢å¼ºçš„æµ‹è¯•å·¥ä½œæµ
            enhanced_test_workflow = self._create_enhanced_test_workflow()
            workflow_file = Path('.github/workflows/enhanced-test.yml')
            workflow_file.parent.mkdir(parents=True, exist_ok=True)

            with open(workflow_file, 'w', encoding='utf-8') as f:
                yaml.dump(enhanced_test_workflow, f, default_flow_style=False, allow_unicode=True)

            workflows_created.append(str(workflow_file))
            optimizations.append("å¢å¼ºçš„æµ‹è¯•å·¥ä½œæµ")

            # 2. åˆ›å»ºéƒ¨ç½²å·¥ä½œæµ
            deployment_workflow = self._create_deployment_workflow()
            deploy_workflow_file = Path('.github/workflows/deploy.yml')

            with open(deploy_workflow_file, 'w', encoding='utf-8') as f:
                yaml.dump(deployment_workflow, f, default_flow_style=False, allow_unicode=True)

            workflows_created.append(str(deploy_workflow_file))
            optimizations.append("è‡ªåŠ¨éƒ¨ç½²å·¥ä½œæµ")

            # 3. åˆ›å»ºç›‘æ§å·¥ä½œæµ
            monitoring_workflow = self._create_monitoring_workflow()
            monitor_workflow_file = Path('.github/workflows/monitor.yml')

            with open(monitor_workflow_file, 'w', encoding='utf-8') as f:
                yaml.dump(monitoring_workflow, f, default_flow_style=False, allow_unicode=True)

            workflows_created.append(str(monitor_workflow_file))
            optimizations.append("ç›‘æ§å‘Šè­¦å·¥ä½œæµ")

            # 4. åˆ›å»ºè´¨é‡é—¨ç¦å·¥ä½œæµ
            quality_gate_workflow = self._create_quality_gate_workflow()
            quality_workflow_file = Path('.github/workflows/quality-gate.yml')

            with open(quality_workflow_file, 'w', encoding='utf-8') as f:
                yaml.dump(quality_gate_workflow, f, default_flow_style=False, allow_unicode=True)

            workflows_created.append(str(quality_workflow_file))
            optimizations.append("è´¨é‡é—¨ç¦å·¥ä½œæµ")

            print(f"   ğŸ“ åˆ›å»ºCI/CDå·¥ä½œæµ: {len(workflows_created)} ä¸ª")

            return {
                'optimizations_count': len(optimizations),
                'optimizations': optimizations,
                'workflows_created': workflows_created,
                'status': 'cicd_optimized'
            }

        except Exception as e:
            return {
                'optimizations_count': 0,
                'error': str(e),
                'status': 'optimization_failed'
            }

    def _create_enhanced_test_workflow(self) -> Dict:
        """åˆ›å»ºå¢å¼ºçš„æµ‹è¯•å·¥ä½œæµ"""
        return {
            'name': 'Enhanced Testing Pipeline',
            'on': {
                'push': {'branches': ['main', 'develop']},
                'pull_request': {'branches': ['main']},
                'schedule': [{'cron': '0 2 * * *'}]  # æ¯æ—¥2ç‚¹è¿è¡Œ
            },
            'jobs': {
                'test-matrix': {
                    'runs-on': 'ubuntu-latest',
                    'strategy': {
                        'matrix': {
                            'python-version': ['3.11', '3.12'],
                            'test-type': ['unit', 'integration', 'e2e']
                        }
                    },
                    'steps': [
                        {'uses': 'actions/checkout@v4'},
                        {'name': 'Set up Python ${{ matrix.python-version }}',
                         'uses': 'actions/setup-python@v4',
                         'with': {'python-version': '${{ matrix.python-version }}'}},
                        {'name': 'Cache dependencies',
                         'uses': 'actions/cache@v3',
                         'with': {
                             'path': '~/.cache/pip',
                             'key': '${{ runner.os }}-pip-${{ hashFiles("**/requirements*.lock") }}',
                             'restore-keys': '${{ runner.os }}-pip-'
                         }},
                        {'name': 'Install dependencies',
                         'run': 'pip install -r requirements/requirements.lock'},
                        {'name': 'Run Phase 7 Test Coverage',
                         'run': 'python3 scripts/phase7_test_coverage_expander.py'},
                        {'name': 'Run Database Tests',
                         'run': 'python3 scripts/phase7_database_test_expander.py'},
                        {'name': 'Run Business Logic Tests',
                         'run': 'python3 scripts/phase7_business_logic_test_expander.py'},
                        {'name': 'Run tests with coverage',
                         'run': 'pytest tests/${{ matrix.test-type }}/ --cov=src --cov-report=xml --cov-report=html'},
                        {'name': 'Upload coverage to Codecov',
                         'uses': 'codecov/codecov-action@v3',
                         'with': {'file': './coverage.xml'}}
                    ]
                }
            }
        }

    def _create_deployment_workflow(self) -> Dict:
        """åˆ›å»ºéƒ¨ç½²å·¥ä½œæµ"""
        return {
            'name': 'Deployment Pipeline',
            'on': {
                'push': {'branches': ['main']},
                'workflow_dispatch': {'inputs': {
                    'environment': {
                        'description': 'Deployment environment',
                        'required': True,
                        'default': 'staging',
                        'type': 'choice',
                        'options': ['staging', 'production']
                    }
                }}
            },
            'jobs': {
                'deploy': {
                    'runs-on': 'ubuntu-latest',
                    'environment': '${{ github.event.inputs.environment || ''staging'' }}',
                    'steps': [
                        {'uses': 'actions/checkout@v4'},
                        {'name': 'Set up Docker Buildx',
                         'uses': 'docker/setup-buildx-action@v3'},
                        {'name': 'Login to Container Registry',
                         'uses': 'docker/login-action@v3',
                         'with': {
                             'registry': 'ghcr.io',
                             'username': '${{ github.actor }}',
                             'password': '${{ secrets.GITHUB_TOKEN }}'
                         }},
                        {'name': 'Build and push Docker image',
                         'uses': 'docker/build-push-action@v5',
                         'with': {
                             'context': '.',
                             'push': True,
                             'tags': 'ghcr.io/${{ github.repository }}:${{ github.sha }}',
                             'cache-from': 'type=gha',
                             'cache-to': 'type=gha,mode=max'
                         }},
                        {'name': 'Deploy to ${{ github.event.inputs.environment || ''staging'' }}',
                         'run': '''
                           echo "ğŸš€ Deploying to ${ github.event.inputs.environment || ''staging'' }"
                           docker-compose -f docker-compose.${{ github.event.inputs.environment || ''staging'' }}.yml up -d
                           docker-compose ps
                         '''}
                    ]
                }
            }
        }

    def _create_monitoring_workflow(self) -> Dict:
        """åˆ›å»ºç›‘æ§å·¥ä½œæµ"""
        return {
            'name': 'System Monitoring',
            'on': {
                'schedule': [{'cron': '*/5 * * * *'}],  # æ¯5åˆ†é’Ÿ
                'workflow_dispatch': {}
            },
            'jobs': {
                'health-check': {
                    'runs-on': 'ubuntu-latest',
                    'steps': [
                        {'uses': 'actions/checkout@v4'},
                        {'name': 'Check Application Health',
                         'run': '''
                           curl -f http://football-prediction.local/health || exit 1
                           echo "âœ… Application health check passed"
                         '''},
                        {'name': 'Check Database Connection',
                         'run': '''
                           docker-compose exec -T db pg_isready -U postgres || exit 1
                           echo "âœ… Database connection check passed"
                         '''},
                        {'name': 'Check Redis Connection',
                         'run': '''
                           docker-compose exec -T redis redis-cli ping || exit 1
                           echo "âœ… Redis connection check passed"
                         '''},
                        {'name': 'Run Quality Checks',
                         'run': '''
                           python3 scripts/quality_guardian.py --check-only
                           echo "âœ… Quality checks passed"
                         '''}
                    ]
                }
            }
        }

    def _create_quality_gate_workflow(self) -> Dict:
        """åˆ›å»ºè´¨é‡é—¨ç¦å·¥ä½œæµ"""
        return {
            'name': 'Quality Gate',
            'on': {
                'pull_request': {'branches': ['main']},
                'push': {'branches': ['main']}
            },
            'jobs': {
                'quality-checks': {
                    'runs-on': 'ubuntu-latest',
                    'steps': [
                        {'uses': 'actions/checkout@v4'},
                        {'name': 'Run Code Quality Checks',
                         'run': '''
                           make lint
                           make type-check
                           echo "âœ… Code quality checks passed"
                         '''},
                        {'name': 'Security Scan',
                         'run': '''
                           make security-check
                           echo "âœ… Security scan passed"
                         '''},
                        {'name': 'Test Coverage Gate',
                         'run': '''
                           make coverage
                           COVERAGE=$(python3 -c "import json; print(json.load(open('coverage.json'))['totals']['percent_covered'])")
                           if (( $(echo "$COVERAGE < 80" | bc -l) )); then
                             echo "âŒ Coverage $COVERAGE% is below 80% threshold"
                             exit 1
                           fi
                           echo "âœ… Coverage $COVERAGE% meets quality gate"
                         '''},
                        {'name': 'Performance Tests',
                         'run': '''
                           make test-performance
                           echo "âœ… Performance tests passed"
                         '''}
                    ]
                }
            }
        }

    def _create_deployment_configurations(self) -> Dict:
        """åˆ›å»ºéƒ¨ç½²é…ç½®"""
        configs_created = []

        try:
            # 1. åˆ›å»ºç”Ÿäº§ç¯å¢ƒDocker Compose
            prod_compose = self._create_production_docker_compose()
            prod_compose_file = Path('docker-compose.prod.yml')

            with open(prod_compose_file, 'w', encoding='utf-8') as f:
                yaml.dump(prod_compose, f, default_flow_style=False, allow_unicode=True)

            configs_created.append(str(prod_compose_file))

            # 2. åˆ›å»ºKubernetesé…ç½®
            k8s_configs = self._create_kubernetes_configurations()
            k8s_dir = Path('deploy/k8s')
            k8s_dir.mkdir(parents=True, exist_ok=True)

            for config_name, config_content in k8s_configs.items():
                config_file = k8s_dir / f"{config_name}.yaml"
                with open(config_file, 'w', encoding='utf-8') as f:
                    yaml.dump(config_content, f, default_flow_style=False, allow_unicode=True)
                configs_created.append(str(config_file))

            # 3. åˆ›å»ºç¯å¢ƒé…ç½®
            env_configs = self._create_environment_configs()
            env_dir = Path('environments')
            env_dir.mkdir(exist_ok=True)

            for env_name, env_content in env_configs.items():
                env_file = env_dir / f".env.{env_name}"
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.write(env_content)
                configs_created.append(str(env_file))

            print(f"   ğŸ“ åˆ›å»ºéƒ¨ç½²é…ç½®: {len(configs_created)} ä¸ª")

            return {
                'configs_created': len(configs_created),
                'configs': configs_created,
                'status': 'configs_created'
            }

        except Exception as e:
            return {
                'configs_created': 0,
                'error': str(e),
                'status': 'config_creation_failed'
            }

    def _create_production_docker_compose(self) -> Dict:
        """åˆ›å»ºç”Ÿäº§ç¯å¢ƒDocker Composeé…ç½®"""
        return {
            'version': '3.8',
            'services': {
                'app': {
                    'image': 'ghcr.io/xupeng211/football-prediction:latest',
                    'ports': ['8000:8000'],
                    'environment': [
                        'ENV=production',
                        'DEBUG=false',
                        'LOG_LEVEL=INFO'
                    ],
                    'depends_on': ['db', 'redis'],
                    'restart': 'unless-stopped',
                    'deploy': {
                        'replicas': 3,
                        'resources': {
                            'limits': {
                                'cpus': '1.0',
                                'memory': '1G'
                            }
                        }
                    },
                    'healthcheck': {
                        'test': ['CMD', 'curl', '-f', 'http://localhost:8000/health'],
                        'interval': '30s',
                        'timeout': '10s',
                        'retries': 3
                    }
                },
                'db': {
                    'image': 'postgres:15',
                    'environment': [
                        'POSTGRES_DB=football_prediction_prod',
                        'POSTGRES_USER=postgres',
                        'POSTGRES_PASSWORD=${DB_PASSWORD}'
                    ],
                    'volumes': ['postgres_data_prod:/var/lib/postgresql/data'],
                    'restart': 'unless-stopped',
                    'deploy': {
                        'resources': {
                            'limits': {
                                'cpus': '0.5',
                                'memory': '512M'
                            }
                        }
                    }
                },
                'redis': {
                    'image': 'redis:7-alpine',
                    'restart': 'unless-stopped',
                    'deploy': {
                        'resources': {
                            'limits': {
                                'cpus': '0.2',
                                'memory': '256M'
                            }
                        }
                    }
                },
                'nginx': {
                    'image': 'nginx:alpine',
                    'ports': ['80:80', '443:443'],
                    'volumes': ['./nginx/nginx.conf:/etc/nginx/nginx.conf:ro'],
                    'depends_on': ['app'],
                    'restart': 'unless-stopped'
                }
            },
            'volumes': {
                'postgres_data_prod': {'driver': 'local'}
            },
            'networks': {
                'frontend': None,
                'backend': None
            }
        }

    def _create_kubernetes_configurations(self) -> Dict[str, Dict]:
        """åˆ›å»ºKubernetesé…ç½®"""
        return {
            'namespace': {
                'apiVersion': 'v1',
                'kind': 'Namespace',
                'metadata': {'name': 'football-prediction'}
            },
            'deployment': {
                'apiVersion': 'apps/v1',
                'kind': 'Deployment',
                'metadata': {
                    'name': 'football-prediction',
                    'namespace': 'football-prediction'
                },
                'spec': {
                    'replicas': 3,
                    'selector': {'matchLabels': {'app': 'football-prediction'}},
                    'template': {
                        'metadata': {'labels': {'app': 'football-prediction'}},
                        'spec': {
                            'containers': [{
                                'name': 'app',
                                'image': 'ghcr.io/xupeng211/football-prediction:latest',
                                'ports': [{'containerPort': 8000}],
                                'env': [
                                    {'name': 'ENV', 'value': 'production'},
                                    {'name': 'DB_HOST', 'value': 'postgres-service'},
                                    {'name': 'REDIS_HOST', 'value': 'redis-service'}
                                ],
                                'resources': {
                                    'requests': {
                                        'cpu': '100m',
                                        'memory': '256Mi'
                                    },
                                    'limits': {
                                        'cpu': '1000m',
                                        'memory': '1Gi'
                                    }
                                }
                            }]
                        }
                    }
                }
            }
        }

    def _create_environment_configs(self) -> Dict[str, str]:
        """åˆ›å»ºç¯å¢ƒé…ç½®"""
        return {
            'production': '''# Production Environment Configuration
ENV=production
DEBUG=false
LOG_LEVEL=INFO
DATABASE_URL=postgresql://postgres:${DB_PASSWORD}@postgres:5432/football_prediction_prod
REDIS_URL=redis://redis:6379/0
SECRET_KEY=${SECRET_KEY}
CORS_ORIGINS=https://football-prediction.com
MONITORING_ENABLED=true
''',
            'staging': '''# Staging Environment Configuration
ENV=staging
DEBUG=true
LOG_LEVEL=DEBUG
DATABASE_URL=postgresql://postgres:postgres@postgres:5432/football_prediction_staging
REDIS_URL=redis://redis:6379/0
SECRET_KEY=staging-secret-key-change-in-production
CORS_ORIGINS=https://staging.football-prediction.com
MONITORING_ENABLED=true
'''
        }

    def _setup_monitoring_alerting(self) -> Dict:
        """å»ºç«‹ç›‘æ§å’Œå‘Šè­¦"""
        monitoring_components = []

        try:
            # 1. åˆ›å»ºPrometheusé…ç½®
            prometheus_config = self._create_prometheus_config()
            prometheus_file = Path('monitoring/prometheus.yml')
            prometheus_file.parent.mkdir(parents=True, exist_ok=True)

            with open(prometheus_file, 'w', encoding='utf-8') as f:
                yaml.dump(prometheus_config, f, default_flow_style=False, allow_unicode=True)

            monitoring_components.append('Prometheusé…ç½®')

            # 2. åˆ›å»ºGrafanaä»ªè¡¨æ¿
            grafana_dashboard = self._create_grafana_dashboard()
            grafana_file = Path('monitoring/grafana/dashboards/football-prediction.json')
            grafana_file.parent.mkdir(parents=True, exist_ok=True)

            with open(grafana_file, 'w', encoding='utf-8') as f:
                json.dump(grafana_dashboard, f, indent=2)

            monitoring_components.append('Grafanaä»ªè¡¨æ¿')

            # 3. åˆ›å»ºå‘Šè­¦è§„åˆ™
            alert_rules = self._create_alert_rules()
            alert_file = Path('monitoring/alerts/rules.yml')

            with open(alert_file, 'w', encoding='utf-8') as f:
                yaml.dump(alert_rules, f, default_flow_style=False, allow_unicode=True)

            monitoring_components.append('å‘Šè­¦è§„åˆ™')

            print(f"   ğŸ“Š åˆ›å»ºç›‘æ§ç»„ä»¶: {len(monitoring_components)} ä¸ª")

            return {
                'monitoring_components': len(monitoring_components),
                'components': monitoring_components,
                'status': 'monitoring_setup'
            }

        except Exception as e:
            return {
                'monitoring_components': 0,
                'error': str(e),
                'status': 'monitoring_setup_failed'
            }

    def _create_prometheus_config(self) -> Dict:
        """åˆ›å»ºPrometheusé…ç½®"""
        return {
            'global': {
                'scrape_interval': '15s',
                'evaluation_interval': '15s'
            },
            'rule_files': ['alert.yml'],
            'scrape_configs': [
                {
                    'job_name': 'football-prediction',
                    'static_configs': [{'targets': ['app:8000']}],
                    'metrics_path': '/metrics',
                    'scrape_interval': '5s'
                },
                {
                    'job_name': 'postgres',
                    'static_configs': [{'targets': ['postgres-exporter:9187']}]
                },
                {
                    'job_name': 'redis',
                    'static_configs': [{'targets': ['redis-exporter:9121']}]
                }
            ],
            'alerting': {
                'alertmanagers': [{'static_configs': [{'targets': ['alertmanager:9093']}]}]
            }
        }

    def _create_grafana_dashboard(self) -> Dict:
        """åˆ›å»ºGrafanaä»ªè¡¨æ¿"""
        return {
            'dashboard': {
                'title': 'Football Prediction System Dashboard',
                'panels': [
                    {
                        'title': 'Application Health',
                        'type': 'stat',
                        'targets': [{'expr': 'up{job="football-prediction"}'}]
                    },
                    {
                        'title': 'Request Rate',
                        'type': 'graph',
                        'targets': [{'expr': 'rate(http_requests_total[5m])'}]
                    },
                    {
                        'title': 'Response Time',
                        'type': 'graph',
                        'targets': [{'expr': 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))'}]
                    },
                    {
                        'title': 'Error Rate',
                        'type': 'singlestat',
                        'targets': [{'expr': 'rate(http_requests_total{status=~"5.."}[5m])'}]
                    }
                ],
                'time': {'from': 'now-1h', 'to': 'now'},
                'refresh': '5s'
            }
        }

    def _create_alert_rules(self) -> Dict:
        """åˆ›å»ºå‘Šè­¦è§„åˆ™"""
        return {
            'groups': [
                {
                    'name': 'football-prediction-alerts',
                    'rules': [
                        {
                            'alert': 'ApplicationDown',
                            'expr': 'up{job="football-prediction"} == 0',
                            'for': '1m',
                            'labels': {'severity': 'critical'},
                            'annotations': {
                                'summary': 'Football Prediction application is down',
                                'description': 'Application has been down for more than 1 minute.'
                            }
                        },
                        {
                            'alert': 'HighErrorRate',
                            'expr': 'rate(http_requests_total{status=~"5.."}[5m]) > 0.1',
                            'for': '5m',
                            'labels': {'severity': 'warning'},
                            'annotations': {
                                'summary': 'High error rate detected',
                                'description': 'Error rate is above 10% for more than 5 minutes.'
                            }
                        },
                        {
                            'alert': 'HighResponseTime',
                            'expr': 'histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1',
                            'for': '5m',
                            'labels': {'severity': 'warning'},
                            'annotations': {
                                'summary': 'High response time detected',
                                'description': '95th percentile response time is above 1 second.'
                            }
                        }
                    ]
                }
            ]
        }

    def _generate_deployment_documentation(self) -> Dict:
        """ç”Ÿæˆéƒ¨ç½²æ–‡æ¡£"""
        docs_generated = []

        try:
            # 1. åˆ›å»ºéƒ¨ç½²æŒ‡å—
            deployment_guide = self._create_deployment_guide()
            guide_file = Path('docs/deployment/DEPLOYMENT_GUIDE.md')
            guide_file.parent.mkdir(parents=True, exist_ok=True)

            with open(guide_file, 'w', encoding='utf-8') as f:
                f.write(deployment_guide)

            docs_generated.append('éƒ¨ç½²æŒ‡å—')

            # 2. åˆ›å»ºè¿ç»´æ‰‹å†Œ
            ops_manual = self._create_operations_manual()
            ops_file = Path('docs/operations/OPERATIONS_MANUAL.md')
            ops_file.parent.mkdir(parents=True, exist_ok=True)

            with open(ops_file, 'w', encoding='utf-8') as f:
                f.write(ops_manual)

            docs_generated.append('è¿ç»´æ‰‹å†Œ')

            # 3. åˆ›å»ºæ•…éšœæ’é™¤æŒ‡å—
            troubleshooting_guide = self._create_troubleshooting_guide()
            trouble_file = Path('docs/troubleshooting/TROUBLESHOOTING.md')
            trouble_file.parent.mkdir(parents=True, exist_ok=True)

            with open(trouble_file, 'w', encoding='utf-8') as f:
                f.write(troubleshooting_guide)

            docs_generated.append('æ•…éšœæ’é™¤æŒ‡å—')

            print(f"   ğŸ“„ ç”Ÿæˆæ–‡æ¡£: {len(docs_generated)} ä¸ª")

            return {
                'docs_generated': len(docs_generated),
                'documents': docs_generated,
                'status': 'documentation_generated'
            }

        except Exception as e:
            return {
                'docs_generated': 0,
                'error': str(e),
                'status': 'documentation_failed'
            }

    def _create_deployment_guide(self) -> str:
        """åˆ›å»ºéƒ¨ç½²æŒ‡å—"""
        return '''# Football Prediction System éƒ¨ç½²æŒ‡å—

## æ¦‚è¿°
æœ¬æ–‡æ¡£æä¾›äº†Football Predictionç³»ç»Ÿçš„å®Œæ•´éƒ¨ç½²æŒ‡å—ï¼ŒåŒ…æ‹¬å¼€å‘ã€æµ‹è¯•å’Œç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²æ­¥éª¤ã€‚

## ç¯å¢ƒè¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- CPU: æœ€å°‘2æ ¸å¿ƒï¼Œæ¨è4æ ¸å¿ƒ
- å†…å­˜: æœ€å°‘4GBï¼Œæ¨è8GB
- å­˜å‚¨: æœ€å°‘20GBå¯ç”¨ç©ºé—´
- æ“ä½œç³»ç»Ÿ: Linux (Ubuntu 20.04+), macOS, Windows 10+

### è½¯ä»¶è¦æ±‚
- Docker 20.10+
- Docker Compose 2.0+
- Python 3.11+ (ç”¨äºæœ¬åœ°å¼€å‘)
- Git 2.30+

## å¿«é€Ÿéƒ¨ç½²

### 1. å…‹éš†ä»“åº“
```bash
git clone https://github.com/xupeng211/FootballPrediction.git
cd FootballPrediction
```

### 2. ç¯å¢ƒé…ç½®
```bash
# å¤åˆ¶ç¯å¢ƒé…ç½®
cp .env.example .env

# ç¼–è¾‘ç¯å¢ƒå˜é‡
vim .env
```

### 3. å¯åŠ¨æœåŠ¡
```bash
# å¼€å‘ç¯å¢ƒ
docker-compose up -d

# ç”Ÿäº§ç¯å¢ƒ
docker-compose -f docker-compose.prod.yml up -d
```

### 4. éªŒè¯éƒ¨ç½²
```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health
```

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### 1. ç¯å¢ƒå‡†å¤‡
```bash
# åˆ›å»ºç”Ÿäº§ç¯å¢ƒé…ç½®
cp environments/.env.production .env

# è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡
export DB_PASSWORD="your-secure-password"
export SECRET_KEY="your-secret-key"
```

### 2. å¯åŠ¨ç”Ÿäº§æœåŠ¡
```bash
# ä½¿ç”¨ç”Ÿäº§é…ç½®å¯åŠ¨
docker-compose -f docker-compose.prod.yml up -d

# åˆå§‹åŒ–æ•°æ®åº“
docker-compose -f docker-compose.prod.yml exec app python -m alembic upgrade head
```

### 3. é…ç½®åå‘ä»£ç†
```bash
# å¯åŠ¨Nginx
docker-compose -f docker-compose.prod.yml up -d nginx
```

## ç›‘æ§å’Œç»´æŠ¤

### åº”ç”¨ç›‘æ§
- Grafana: http://localhost:3000
- Prometheus: http://localhost:9090
- AlertManager: http://localhost:9093

### æ—¥å¿—æŸ¥çœ‹
```bash
# æŸ¥çœ‹åº”ç”¨æ—¥å¿—
docker-compose logs -f app

# æŸ¥çœ‹æ‰€æœ‰æœåŠ¡æ—¥å¿—
docker-compose logs -f
```

### æ•°æ®å¤‡ä»½
```bash
# æ•°æ®åº“å¤‡ä»½
docker-compose exec db pg_dump -U postgres football_prediction_prod > backup.sql

# æ¢å¤æ•°æ®åº“
docker-compose exec -T db psql -U postgres football_prediction_prod < backup.sql
```

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜
1. **æœåŠ¡å¯åŠ¨å¤±è´¥**: æ£€æŸ¥ç«¯å£å ç”¨å’Œé…ç½®æ–‡ä»¶
2. **æ•°æ®åº“è¿æ¥å¤±è´¥**: éªŒè¯æ•°æ®åº“æœåŠ¡çŠ¶æ€å’Œè¿æ¥é…ç½®
3. **å†…å­˜ä¸è¶³**: è°ƒæ•´Dockerå®¹å™¨èµ„æºé™åˆ¶

### å¥åº·æ£€æŸ¥
```bash
# åº”ç”¨å¥åº·æ£€æŸ¥
curl -f http://localhost:8000/health

# æ•°æ®åº“è¿æ¥æ£€æŸ¥
docker-compose exec db pg_isready -U postgres

# Redisè¿æ¥æ£€æŸ¥
docker-compose exec redis redis-cli ping
```

---

*æ›´æ–°æ—¶é—´: 2025-10-30*
'''

    def _create_operations_manual(self) -> str:
        """åˆ›å»ºè¿ç»´æ‰‹å†Œ"""
        return '''# Football Prediction System è¿ç»´æ‰‹å†Œ

## æ¦‚è¿°
æœ¬æ‰‹å†Œæä¾›Football Predictionç³»ç»Ÿçš„æ—¥å¸¸è¿ç»´æŒ‡å—ï¼ŒåŒ…æ‹¬ç›‘æ§ã€ç»´æŠ¤ã€æ•…éšœå¤„ç†ç­‰æ“ä½œã€‚

## ç³»ç»Ÿæ¶æ„

### æœåŠ¡ç»„ä»¶
- **App**: FastAPIåº”ç”¨æœåŠ¡ (ç«¯å£8000)
- **DB**: PostgreSQLæ•°æ®åº“ (ç«¯å£5432)
- **Redis**: ç¼“å­˜æœåŠ¡ (ç«¯å£6379)
- **Nginx**: åå‘ä»£ç† (ç«¯å£80/443)

### æ•°æ®æµ
```
ç”¨æˆ·è¯·æ±‚ â†’ Nginx â†’ FastAPI App â†’ PostgreSQL/Redis
                â†“
            æ—¥å¿— â†’ ç›‘æ§ â†’ å‘Šè­¦
```

## æ—¥å¸¸è¿ç»´

### æœåŠ¡ç®¡ç†
```bash
# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# é‡å¯æœåŠ¡
docker-compose restart app

# æ›´æ–°æœåŠ¡
docker-compose pull && docker-compose up -d
```

### æ—¥å¿—ç®¡ç†
```bash
# å®æ—¶æŸ¥çœ‹åº”ç”¨æ—¥å¿—
docker-compose logs -f app

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
docker-compose logs app | grep ERROR

# æ¸…ç†æ—§æ—¥å¿—
docker system prune -f
```

### æ€§èƒ½ç›‘æ§

#### ç³»ç»ŸæŒ‡æ ‡
- CPUä½¿ç”¨ç‡ < 80%
- å†…å­˜ä½¿ç”¨ç‡ < 85%
- ç£ç›˜ä½¿ç”¨ç‡ < 90%
- ç½‘ç»œå»¶è¿Ÿ < 100ms

#### åº”ç”¨æŒ‡æ ‡
- å“åº”æ—¶é—´ < 1s (95th percentile)
- é”™è¯¯ç‡ < 5%
- å¯ç”¨æ€§ > 99.9%
- å¹¶å‘ç”¨æˆ·æ•°ç›‘æ§

### æ•°æ®åº“ç»´æŠ¤

#### å®šæœŸä»»åŠ¡
```bash
# æ•°æ®åº“å¤‡ä»½ (æ¯æ—¥)
0 2 * * * docker-compose exec db pg_dump -U postgres football_prediction_prod > /backup/$(date +\%Y\%m\%d).sql

# æ¸…ç†æ—§å¤‡ä»½ (æ¯å‘¨)
0 3 * * 0 find /backup -name "*.sql" -mtime +7 -delete

# æ•°æ®åº“ä¼˜åŒ– (æ¯æœˆ)
0 4 1 * * docker-compose exec db psql -U postgres -d football_prediction_prod -c "VACUUM ANALYZE;"
```

#### æ€§èƒ½è°ƒä¼˜
```sql
-- æŸ¥çœ‹æ…¢æŸ¥è¯¢
SELECT query, mean_time, calls FROM pg_stat_statements ORDER BY mean_time DESC LIMIT 10;

-- æŸ¥çœ‹ç´¢å¼•ä½¿ç”¨æƒ…å†µ
SELECT schemaname, tablename, attname, n_distinct, correlation FROM pg_stats;

-- é‡å»ºç´¢å¼•
REINDEX DATABASE football_prediction_prod;
```

### ç¼“å­˜ç®¡ç†

#### Redisç›‘æ§
```bash
# æŸ¥çœ‹Redisä¿¡æ¯
docker-compose exec redis redis-cli info

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
docker-compose exec redis redis-cli info memory

# æ¸…ç†è¿‡æœŸé”®
docker-compose exec redis redis-cli FLUSHEXPIRED
```

#### ç¼“å­˜ç­–ç•¥
- çƒ­æ•°æ®TTL: 1å°æ—¶
- é¢„æµ‹ç»“æœTTL: 30åˆ†é’Ÿ
- ç”¨æˆ·ä¼šè¯TTL: 24å°æ—¶

## å®‰å…¨ç®¡ç†

### è®¿é—®æ§åˆ¶
- å®šæœŸæ›´æ–°å¯†ç 
- ä½¿ç”¨å¼ºå¯†ç ç­–ç•¥
- å¯ç”¨åŒå› ç´ è®¤è¯
- é™åˆ¶ç®¡ç†å‘˜æƒé™

### æ•°æ®å®‰å…¨
```bash
# æ•°æ®åŠ å¯†
docker-compose exec db psql -U postgres -c "CREATE EXTENSION IF NOT EXISTS pgcrypto;"

# å®šæœŸå®‰å…¨æ‰«æ
docker run --rm -v $(pwd):/app clair-scanner:latest /app
```

### ç½‘ç»œå®‰å…¨
```bash
# é˜²ç«å¢™é…ç½®
ufw allow 80
ufw allow 443
ufw deny 5432  # ç¦æ­¢å¤–éƒ¨è®¿é—®æ•°æ®åº“
ufw enable
```

## å‘Šè­¦å¤„ç†

### å‘Šè­¦çº§åˆ«
- **Critical**: ç«‹å³å“åº” (1å°æ—¶å†…)
- **Warning**: å·¥ä½œæ—¶é—´å“åº” (4å°æ—¶å†…)
- **Info**: å®šæœŸæ£€æŸ¥ (24å°æ—¶å†…)

### å‘Šè­¦æµç¨‹
1. æ¥æ”¶å‘Šè­¦é€šçŸ¥
2. ç¡®è®¤å‘Šè­¦è¯¦æƒ…
3. æ‰§è¡Œåº”æ€¥é¢„æ¡ˆ
4. è®°å½•å¤„ç†è¿‡ç¨‹
5. åˆ†ææ ¹æœ¬åŸå› 

### åº”æ€¥é¢„æ¡ˆ

#### åº”ç”¨å®•æœº
```bash
# å¿«é€Ÿé‡å¯
docker-compose restart app

# æ£€æŸ¥æ—¥å¿—
docker-compose logs app --tail=100

# å›æ»šç‰ˆæœ¬
docker-compose down && docker-compose up -d
```

#### æ•°æ®åº“æ•…éšœ
```bash
# åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®åº“
# ä¿®æ”¹è¿æ¥é…ç½®
docker-compose restart app

# æ¢å¤ä¸»æ•°æ®åº“
# ä»å¤‡ä»½æ¢å¤æ•°æ®
```

## å®¹é‡è§„åˆ’

### èµ„æºè¯„ä¼°
- ç”¨æˆ·å¢é•¿ç‡: 10%/æœˆ
- æ•°æ®å¢é•¿ç‡: 5%/æœˆ
- å³°å€¼å¹¶å‘: 1000ç”¨æˆ·

### æ‰©å®¹ç­–ç•¥
- æ°´å¹³æ‰©å±•: å¢åŠ åº”ç”¨å®ä¾‹
- å‚ç›´æ‰©å±•: å¢åŠ å•å®ä¾‹èµ„æº
- æ•°æ®åº“åˆ†ç‰‡: æŒ‰ä¸šåŠ¡æ¨¡å—åˆ†ç¦»

## ç‰ˆæœ¬å‘å¸ƒ

### å‘å¸ƒæµç¨‹
1. ä»£ç å®¡æŸ¥
2. è‡ªåŠ¨åŒ–æµ‹è¯•
3. é¢„å‘å¸ƒéªŒè¯
4. ç”Ÿäº§å‘å¸ƒ
5. ç›‘æ§éªŒè¯

### å›æ»šç­–ç•¥
```bash
# å¿«é€Ÿå›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬
docker-compose down
docker pull ghcr.io/xupeng211/football-prediction:previous-tag
docker-compose up -d
```

---

*æ›´æ–°æ—¶é—´: 2025-10-30*
'''

    def _create_troubleshooting_guide(self) -> str:
        """åˆ›å»ºæ•…éšœæ’é™¤æŒ‡å—"""
        return '''# Football Prediction System æ•…éšœæ’é™¤æŒ‡å—

## æ¦‚è¿°
æœ¬æŒ‡å—æä¾›Football Predictionç³»ç»Ÿå¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆã€‚

## å¿«é€Ÿè¯Šæ–­

### å¥åº·æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# health_check.sh

echo "ğŸ” ç³»ç»Ÿå¥åº·æ£€æŸ¥..."

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
echo "ğŸ“Š æ£€æŸ¥æœåŠ¡çŠ¶æ€:"
docker-compose ps

# æ£€æŸ¥åº”ç”¨å¥åº·
echo "ğŸ¥ æ£€æŸ¥åº”ç”¨å¥åº·:"
curl -s http://localhost:8000/health || echo "âŒ åº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥"

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "ğŸ—„ï¸ æ£€æŸ¥æ•°æ®åº“è¿æ¥:"
docker-compose exec -T db pg_isready -U postgres || echo "âŒ æ•°æ®åº“è¿æ¥å¤±è´¥"

# æ£€æŸ¥Redisè¿æ¥
echo "ğŸ”´ æ£€æŸ¥Redisè¿æ¥:"
docker-compose exec -T redis redis-cli ping || echo "âŒ Redisè¿æ¥å¤±è´¥"

echo "âœ… å¥åº·æ£€æŸ¥å®Œæˆ"
```

## å¸¸è§é—®é¢˜

### 1. åº”ç”¨æ— æ³•å¯åŠ¨

#### ç—‡çŠ¶
- Dockerå®¹å™¨å¯åŠ¨å¤±è´¥
- å¥åº·æ£€æŸ¥å¤±è´¥
- æ— æ³•è®¿é—®åº”ç”¨

#### è¯Šæ–­æ­¥éª¤
```bash
# æŸ¥çœ‹å®¹å™¨çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹å¯åŠ¨æ—¥å¿—
docker-compose logs app

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep :8000

# æ£€æŸ¥ç¯å¢ƒå˜é‡
docker-compose config
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# é‡å¯æœåŠ¡
docker-compose down && docker-compose up -d

# æ¸…ç†å¹¶é‡å»º
docker-compose down -v
docker system prune -f
docker-compose up -d

# æ£€æŸ¥é…ç½®æ–‡ä»¶
docker-compose config
```

### 2. æ•°æ®åº“è¿æ¥é—®é¢˜

#### ç—‡çŠ¶
- åº”ç”¨æ—¥å¿—æ˜¾ç¤ºæ•°æ®åº“è¿æ¥é”™è¯¯
- æ•°æ®åº“æŸ¥è¯¢å¤±è´¥
- æ— æ³•è¿æ¥åˆ°æ•°æ®åº“

#### è¯Šæ–­æ­¥éª¤
```bash
# æ£€æŸ¥æ•°æ®åº“å®¹å™¨çŠ¶æ€
docker-compose ps db

# æ£€æŸ¥æ•°æ®åº“æ—¥å¿—
docker-compose logs db

# æµ‹è¯•æ•°æ®åº“è¿æ¥
docker-compose exec db psql -U postgres -d football_prediction -c "SELECT 1;"

# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker network ls
docker network inspect football-prediction_default
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# é‡å¯æ•°æ®åº“
docker-compose restart db

# æ£€æŸ¥æ•°æ®åº“é…ç½®
docker-compose exec db cat /var/lib/postgresql/data/postgresql.conf

# é‡å»ºæ•°æ®åº“
docker-compose down -v
docker-compose up -d db
docker-compose exec app python -m alembic upgrade head
```

### 3. Redisè¿æ¥é—®é¢˜

#### ç—‡çŠ¶
- ç¼“å­˜æ“ä½œå¤±è´¥
- ä¼šè¯ä¸¢å¤±
- æ€§èƒ½ä¸‹é™

#### è¯Šæ–­æ­¥éª¤
```bash
# æ£€æŸ¥Rediså®¹å™¨
docker-compose ps redis

# æµ‹è¯•Redisè¿æ¥
docker-compose exec redis redis-cli ping

# æ£€æŸ¥Rediså†…å­˜ä½¿ç”¨
docker-compose exec redis redis-cli info memory

# æŸ¥çœ‹Redisæ—¥å¿—
docker-compose logs redis
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# é‡å¯Redis
docker-compose restart redis

# æ¸…ç†Rediså†…å­˜
docker-compose exec redis redis-cli FLUSHALL

# æ£€æŸ¥Redisé…ç½®
docker-compose exec redis redis-cli CONFIG GET "*"
```

### 4. æ€§èƒ½é—®é¢˜

#### ç—‡çŠ¶
- å“åº”æ—¶é—´æ…¢
- CPUä½¿ç”¨ç‡é«˜
- å†…å­˜ä½¿ç”¨ç‡é«˜

#### è¯Šæ–­æ­¥éª¤
```bash
# æ£€æŸ¥ç³»ç»Ÿèµ„æº
docker stats

# æŸ¥çœ‹åº”ç”¨æ€§èƒ½æŒ‡æ ‡
curl http://localhost:8000/metrics

# åˆ†ææ…¢æŸ¥è¯¢
docker-compose exec db psql -U postgres -d football_prediction -c "
SELECT query, mean_time, calls
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;"

# æ£€æŸ¥ç¼“å­˜å‘½ä¸­ç‡
docker-compose exec redis redis-cli info stats | grep keyspace
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# æ‰©å®¹åº”ç”¨å®ä¾‹
docker-compose up -d --scale app=3

# ä¼˜åŒ–æ•°æ®åº“
docker-compose exec db psql -U postgres -d football_prediction -c "VACUUM ANALYZE;"

# æ¸…ç†ç¼“å­˜
docker-compose exec redis redis-cli FLUSHDB
```

### 5. å†…å­˜æ³„æ¼

#### ç—‡çŠ¶
- å†…å­˜ä½¿ç”¨æŒç»­å¢é•¿
- åº”ç”¨é‡å¯åå†…å­˜ä¸‹é™
- OOMé”™è¯¯

#### è¯Šæ–­æ­¥éª¤
```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"

# æŸ¥çœ‹å†…å­˜è¯¦æƒ…
docker-compose exec app cat /proc/meminfo

# æ£€æŸ¥Pythonå†…å­˜ä½¿ç”¨
docker-compose exec app python -c "
import psutil
import os
process = psutil.Process(os.getpid())
print(f'Memory: {process.memory_info().rss / 1024 / 1024:.2f} MB')
"
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# é‡å¯åº”ç”¨
docker-compose restart app

# è®¾ç½®å†…å­˜é™åˆ¶
# åœ¨docker-compose.ymlä¸­æ·»åŠ :
# deploy:
#   resources:
#     limits:
#       memory: 1G

# ç›‘æ§å†…å­˜ä½¿ç”¨
docker stats --no-stream
```

### 6. ç£ç›˜ç©ºé—´ä¸è¶³

#### ç—‡çŠ¶
- ç£ç›˜ä½¿ç”¨ç‡ > 90%
- æ— æ³•å†™å…¥æ—¥å¿—
- æ•°æ®åº“æ“ä½œå¤±è´¥

#### è¯Šæ–­æ­¥éª¤
```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨
df -h

# æŸ¥çœ‹å¤§æ–‡ä»¶
find / -type f -size +1G 2>/dev/null

# æ£€æŸ¥Dockerç©ºé—´ä½¿ç”¨
docker system df

# æŸ¥çœ‹æ—¥å¿—å¤§å°
du -sh /var/lib/docker/containers/*
```

#### è§£å†³æ–¹æ¡ˆ
```bash
# æ¸…ç†Docker
docker system prune -a -f

# æ¸…ç†æ—¥å¿—
docker-compose exec app find /app/logs -name "*.log" -mtime +7 -delete

# æ¸…ç†æ•°æ®åº“
docker-compose exec db psql -U postgres -d football_prediction -c "VACUUM FULL;"
```

## æ—¥å¿—åˆ†æ

### åº”ç”¨æ—¥å¿—æ ¼å¼
```
2025-10-30 03:00:00 [INFO] Request: GET /api/predictions - Status: 200 - Time: 0.05s
2025-10-30 03:00:01 [ERROR] Database connection failed: connection timeout
2025-10-30 03:00:02 [WARNING] Cache miss for key: prediction_123
```

### æ—¥å¿—åˆ†æå‘½ä»¤
```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
docker-compose logs app | grep ERROR

# ç»Ÿè®¡HTTPçŠ¶æ€ç 
docker-compose logs app | grep -o "Status: [0-9]*" | sort | uniq -c

# æŸ¥çœ‹æ…¢è¯·æ±‚
docker-compose logs app | grep "Time: [0-9]*\." | awk '$NF > 1.0'

# å®æ—¶ç›‘æ§æ—¥å¿—
docker-compose logs -f app | grep -E "(ERROR|WARNING)"
```

## æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åº“ä¼˜åŒ–
```sql
-- åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY idx_predictions_created_at ON predictions(created_at);

-- åˆ†ææŸ¥è¯¢æ€§èƒ½
EXPLAIN ANALYZE SELECT * FROM predictions WHERE created_at > NOW() - INTERVAL '1 day';

-- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
ANALYZE predictions;
```

### åº”ç”¨ä¼˜åŒ–
```python
# ç¼“å­˜ä¼˜åŒ–
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_prediction(match_id: int):
    # ç¼“å­˜é¢„æµ‹ç»“æœ
    pass

# è¿æ¥æ± ä¼˜åŒ–
DATABASE_CONFIG = {
    "pool_size": 20,
    "max_overflow": 30,
    "pool_timeout": 30,
    "pool_recycle": 3600
}
```

## åº”æ€¥å“åº”

### ä¸¥é‡æ•…éšœå¤„ç†æµç¨‹
1. **ç«‹å³å“åº”** (5åˆ†é’Ÿå†…)
   - ç¡®è®¤æ•…éšœèŒƒå›´
   - å¯åŠ¨åº”æ€¥é¢„æ¡ˆ
   - é€šçŸ¥ç›¸å…³äººå‘˜

2. **æ•…éšœéš”ç¦»** (30åˆ†é’Ÿå†…)
   - éš”ç¦»æ•…éšœç»„ä»¶
   - å¯ç”¨å¤‡ç”¨æœåŠ¡
   - ä¿ç•™æ•…éšœç°åœº

3. **å¿«é€Ÿä¿®å¤** (2å°æ—¶å†…)
   - åº”ç”¨å¿«é€Ÿä¿®å¤
   - éªŒè¯ä¿®å¤æ•ˆæœ
   - æ¢å¤æ­£å¸¸æœåŠ¡

4. **æ ¹å› åˆ†æ** (24å°æ—¶å†…)
   - åˆ†ææ•…éšœåŸå› 
   - åˆ¶å®šé¢„é˜²æªæ–½
   - æ›´æ–°è¿ç»´æ–‡æ¡£

### è”ç³»æ–¹å¼
- **æŠ€æœ¯è´Ÿè´£äºº**: [è”ç³»æ–¹å¼]
- **è¿ç»´å›¢é˜Ÿ**: [è”ç³»æ–¹å¼]
- **äº§å“è´Ÿè´£äºº**: [è”ç³»æ–¹å¼]

---

*æ›´æ–°æ—¶é—´: 2025-10-30*
'''

    def _generate_deployment_recommendations(self) -> List[str]:
        """ç”Ÿæˆéƒ¨ç½²ç›¸å…³å»ºè®®"""
        return [
            "ğŸš€ æ‰§è¡Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²éªŒè¯å’Œæµ‹è¯•",
            "ğŸ“Š å»ºç«‹å®Œæ•´çš„ç›‘æ§å‘Šè­¦ä½“ç³»",
            "ğŸ”§ åˆ¶å®šå®šæœŸç»´æŠ¤å’Œå¤‡ä»½è®¡åˆ’",
            "ğŸ“š å®Œå–„è¿ç»´æ–‡æ¡£å’Œåº”æ€¥é¢„æ¡ˆ",
            "ğŸ›¡ï¸ å¼ºåŒ–å®‰å…¨é˜²æŠ¤å’Œè®¿é—®æ§åˆ¶",
            "âš¡ ä¼˜åŒ–æ€§èƒ½å’Œèµ„æºä½¿ç”¨æ•ˆç‡",
            "ğŸ”„ å»ºç«‹ç‰ˆæœ¬å‘å¸ƒå’Œå›æ»šæµç¨‹"
        ]

    def _save_report(self, result: Dict):
        """ä¿å­˜æŠ¥å‘Š"""
        report_file = Path(f'phase7_deployment_cicd_expansion_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Phase 7 Week 4 éƒ¨ç½²å‡†å¤‡å’ŒCI/CDé›†æˆæ‰©å±•å™¨")
    print("=" * 60)

    expander = Phase7DeploymentCicdExpander()
    result = expander.expand_deployment_cicd_integration()

    return result

if __name__ == '__main__':
    main()