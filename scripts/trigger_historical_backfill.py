#!/usr/bin/env python3
"""
å†å²æ•°æ®å›æº¯é‡‡é›†è„šæœ¬ / Historical Data Backfill Collection Script

è¯¥è„šæœ¬è¯»å–æ•°æ®æºé…ç½®ï¼Œç”Ÿæˆè¿‡å»3å¹´çš„å†å²æ—¥æœŸåˆ—è¡¨ï¼Œ
å¹¶é€šè¿‡Celeryè§¦å‘å¤§è§„æ¨¡çš„FotMobå†å²æ•°æ®é‡‡é›†ä»»åŠ¡ã€‚

This script reads data source configuration, generates historical date lists for the past 3 years,
and triggers large-scale FotMob historical data collection tasks via Celery.

ä½¿ç”¨æ–¹æ³• / Usage:
    python scripts/trigger_historical_backfill.py [--dry-run]

å‚æ•° / Arguments:
    --dry-run: åªç”Ÿæˆæ—¥æœŸåˆ—è¡¨ï¼Œä¸å®é™…è§¦å‘ä»»åŠ¡

æ³¨æ„äº‹é¡¹ / Notes:
- è¯¥è„šæœ¬ä¼šç”Ÿæˆå¤§é‡çš„Celeryä»»åŠ¡ï¼Œè¯·ç¡®ä¿Workeræœ‰è¶³å¤Ÿçš„å¤„ç†èƒ½åŠ›
- å»ºè®®åˆ†æ‰¹æ‰§è¡Œï¼Œé¿å…å¯¹APIé€ æˆè¿‡å¤§å‹åŠ›
- å†å²æ•°æ®é‡‡é›†å¯¹äºæœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒè‡³å…³é‡è¦
"""

import asyncio
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv

# å°è¯•åŠ è½½.envæ–‡ä»¶
env_files = [
    project_root / ".env",
    project_root / ".env.local",
    project_root / ".env.development",
]

for env_file in env_files:
    if env_file.exists():
        load_dotenv(env_file)
        break

import json
from src.tasks.celery_app import celery_app


def load_data_source_config() -> Dict[str, Any]:
    """åŠ è½½æ•°æ®æºé…ç½®"""
    try:
        config_path = project_root / "src" / "config" / "data_sources.json"
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æˆ˜ç•¥é…ç½®: {config_path}")
        logger.info(f"ğŸ“‹ é…ç½®ç‰ˆæœ¬: {config.get('version', 'unknown')}")
        logger.info(f"ğŸ¯ é‡‡é›†ç­–ç•¥: {config.get('collection_strategy', 'unknown')}")
        return config
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ•°æ®æºé…ç½®å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "version": "1.0.0",
            "collection_strategy": "high_value_focus",
            "backfill": {
                "years": 3,
                "days_per_season": 30,
                "target_leagues": ["PL", "PD", "BL1", "SA", "FL1"]
            },
            "fotmob": {
                "rate_limit": {
                    "requests_per_minute": 10,
                    "delay_between_requests": 6
                }
            }
        }


def generate_historical_dates(config: Dict[str, Any]) -> List[str]:
    """ç”Ÿæˆå†å²æ—¥æœŸåˆ—è¡¨"""
    strategic_settings = config.get('strategic_settings', {})
    years = strategic_settings.get('backfill_seasons', 3)
    days_per_season = 20  # æ¯èµ›å­£é‡‡æ ·20å¤©ï¼Œå¹³è¡¡è¦†ç›–é¢å’Œæ•ˆç‡
    target_leagues = config.get('target_leagues', [])

    current_year = datetime.now().year

    logger.info(f"ğŸ¯ å¼€å§‹ç”Ÿæˆ {years} å¹´å†å²æ•°æ®å›æº¯é…ç½®")
    logger.info(f"   - å½“å‰å¹´ä»½: {current_year}")
    logger.info(f"   - å›æº¯å¹´ä»½æ•°: {years}")
    logger.info(f"   - æ¯èµ›å­£é‡‡æ ·å¤©æ•°: {days_per_season}")
    logger.info(f"   - ç›®æ ‡è”èµ›æ•°é‡: {len(target_leagues)}")

    # ç”Ÿæˆå¹´ä»½åˆ—è¡¨
    target_years = [current_year - i for i in range(years)]
    logger.info(f"   - ç›®æ ‡å¹´ä»½: {target_years}")

    # ä¸ºæ¯å¹´ç”Ÿæˆé‡‡æ ·æ—¥æœŸ
    all_dates = []

    for year in target_years:
        season_dates = generate_season_dates(year, days_per_season)
        all_dates.extend(season_dates)
        logger.info(f"âœ… {year}å¹´ç”Ÿæˆ {len(season_dates)} ä¸ªæ—¥æœŸ")

    # å»é‡å¹¶æ’åº
    unique_dates = list(set(all_dates))
    unique_dates.sort()

    logger.info(f"ğŸ“… ç”Ÿæˆçš„å†å²æ—¥æœŸèŒƒå›´:")
    logger.info(f"   - æœ€æ—©: {unique_dates[0] if unique_dates else 'None'}")
    logger.info(f"   - æœ€æ™š: {unique_dates[-1] if unique_dates else 'None'}")
    logger.info(f"   - æ€»æ—¥æœŸæ•°: {len(unique_dates)}")
    logger.info(f"   - æ¯èµ›å­£å¹³å‡æ—¥æœŸæ•°: {len(unique_dates) // years}")

    return unique_dates


def generate_season_dates(year: int, days_per_season: int) -> List[str]:
    """ä¸ºæŒ‡å®šèµ›å­£ç”Ÿæˆé‡‡æ ·æ—¥æœŸ"""
    dates = []

    # å®šä¹‰èµ›å­£å¤§è‡´æ—¶é—´èŒƒå›´
    season_start_month = 8  # 8æœˆå¼€å§‹
    season_end_month = 5   # æ¬¡å¹´5æœˆç»“æŸ

    # ç”Ÿæˆèµ›å­£èµ·å§‹æ—¥æœŸ
    season_start = datetime(year, season_start_month, 1)
    season_end = datetime(year + 1, season_end_month, 31)

    # è®¡ç®—æ€»å¤©æ•°
    total_days = (season_end - season_start).days
    logger.info(f"   - {year}èµ›å­£: {season_start.strftime('%Y-%m-%d')} åˆ° {season_end.strftime('%Y-%m-%d')}")
    logger.info(f"   - èµ›å­£æ€»å¤©æ•°: {total_days}")

    # è®¡ç®—é‡‡æ ·é—´éš”
    if total_days <= days_per_season:
        # å¦‚æœèµ›å­£å¤©æ•°å°‘äºç›®æ ‡å¤©æ•°ï¼Œé‡‡æ ·æ‰€æœ‰å¤©
        interval_days = 1
    else:
        # è®¡ç®—é‡‡æ ·é—´éš”
        interval_days = total_days // days_per_season

    logger.info(f"   - é‡‡æ ·é—´éš”: {interval_days}å¤©")

    # ç”Ÿæˆé‡‡æ ·æ—¥æœŸ
    current_date = season_start
    while current_date <= season_end:
        date_str = current_date.strftime('%Y%m%d')
        dates.append(date_str)
        current_date += timedelta(days=interval_days)

    return dates


async def trigger_collection_tasks(dates: List[str], dry_run: bool = False) -> int:
    """è§¦å‘é‡‡é›†ä»»åŠ¡"""
    # ä»é…ç½®ä¸­è·å–é€Ÿç‡é™åˆ¶
    rate_limit = 6  # æ¯ä¸ªä»»åŠ¡é—´éš”6ç§’ï¼Œé¿å…API 429é”™è¯¯

    logger.info(f"ğŸš€ å¼€å§‹å†å²æ•°æ®å›æº¯é‡‡é›†ï¼Œå…± {len(dates)} ä¸ªæ—¥æœŸ")
    logger.info(f"âš ï¸ å¯ç”¨é€Ÿç‡èŠ‚æµ: æ¯ä¸ªä»»åŠ¡é—´éš” {rate_limit} ç§’ï¼Œé¿å… API 429 é”™è¯¯")

    if dry_run:
        logger.info("ğŸ” DRY RUN æ¨¡å¼: åªæ˜¾ç¤ºå°†è¦è§¦å‘çš„ä»»åŠ¡")
        for i, date_str in enumerate(dates):
            logger.info(f"   [{i+1:3}/{len(dates)}] å°†è§¦å‘æ—¥æœŸ {date_str} çš„æ•°æ®é‡‡é›†")
        return len(dates)

    tasks_triggered = 0
    failed_tasks = 0

    for i, date_str in enumerate(dates):
        try:
            logger.info(f"ğŸ“… [{i+1:3}/{len(dates)}] è§¦å‘æ—¥æœŸ {date_str} çš„æ•°æ®é‡‡é›†")

            # è°ƒç”¨Celeryä»»åŠ¡
            task = celery_app.send_task(
                'collect_fotmob_data',
                kwargs={'date': date_str},
                queue='fotmob'
            )

            tasks_triggered += 1
            logger.info(f"âœ… ä»»åŠ¡å·²æäº¤: {task.id}")

            # é€Ÿç‡é™åˆ¶ï¼šç­‰å¾…ä¸€æ®µæ—¶é—´å†è§¦å‘ä¸‹ä¸€ä¸ªä»»åŠ¡
            if i < len(dates) - 1:  # æœ€åä¸€ä¸ªä»»åŠ¡ä¸éœ€è¦ç­‰å¾…
                logger.info(f"â±ï¸ é€Ÿç‡é™åˆ¶: ç­‰å¾… {rate_limit} ç§’...")
                await asyncio.sleep(rate_limit)

        except Exception as e:
            logger.error(f"âŒ è§¦å‘æ—¥æœŸ {date_str} çš„é‡‡é›†ä»»åŠ¡å¤±è´¥: {e}")
            failed_tasks += 1
            continue

    logger.info(f"ğŸ‰ å†å²æ•°æ®å›æº¯é‡‡é›†ä»»åŠ¡è§¦å‘å®Œæˆï¼")
    logger.info(f"ğŸ“Š é‡‡é›†ä»»åŠ¡ç»Ÿè®¡: {'total_dates': len(dates), 'successful_tasks': tasks_triggered, 'failed_tasks': failed_tasks, 'success_rate': tasks_triggered / len(dates) * 100 if dates else 0}")
    logger.info(f"ğŸ“‹ ä¸‹ä¸€æ­¥æ“ä½œ:")
    logger.info(f"   1. ç›‘æ§é‡‡é›†è¿›åº¦: docker-compose logs -f worker | grep -i fotmob")
    logger.info(f"   2. è¿è¡Œ ETL å¤„ç†: docker-compose exec app python scripts/run_etl_silver.py")
    logger.info(f"   3. è§¦å‘å®Œæ•´ç®¡é“: docker-compose exec worker celery -A src.tasks.celery_app call complete_data_pipeline")

    return tasks_triggered


def print_collection_summary(config: Dict[str, Any], dates: List[str]):
    """æ‰“å°é‡‡é›†æ‘˜è¦ä¿¡æ¯"""
    strategic_settings = config.get('strategic_settings', {})
    target_leagues = config.get('target_leagues', [])

    print("=" * 80)
    print("ğŸ¯ å†å²æ•°æ®å›æº¯é‡‡é›†è®¡åˆ’")
    print("=" * 80)

    print(f"ğŸ“Š é‡‡é›†ç­–ç•¥: {strategic_settings.get('collection_strategy', 'unknown')}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {strategic_settings.get('backfill_seasons', 3)} å¹´")
    print(f"ğŸ“ˆ ç›®æ ‡è”èµ›æ•°é‡: {len(target_leagues)}")
    print(f"ğŸ“‹ æ€»é‡‡æ ·æ—¥æœŸ: {len(dates)} ä¸ª")

    # æ˜¾ç¤ºæ ¸å¿ƒè”èµ›
    tier1_leagues = [league['name'] for league in target_leagues if league.get('type') == 'Tier1']
    tier2_leagues = [league['name'] for league in target_leagues if league.get('type') == 'Tier2']
    cup_leagues = [league['name'] for league in target_leagues if league.get('type') == 'Cup']

    if tier1_leagues:
        print(f"ğŸ† æ ¸å¿ƒè”èµ›: {', '.join(tier1_leagues[:3])}{'...' if len(tier1_leagues) > 3 else ''}")
    if tier2_leagues:
        print(f"ğŸ“ˆ æ¬¡çº§è”èµ›: {', '.join(tier2_leagues[:2])}{'...' if len(tier2_leagues) > 2 else ''}")
    if cup_leagues:
        print(f"ğŸ… æ¯èµ›: {', '.join(cup_leagues)}")

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    year_stats = {}
    for date_str in dates:
        year = date_str[:4]
        year_stats[year] = year_stats.get(year, 0) + 1

    print(f"ğŸ“… æŒ‰å¹´ä»½åˆ†å¸ƒ:")
    for year in sorted(year_stats.keys(), reverse=True):
        print(f"   {year}å¹´: {year_stats[year]} ä¸ªæ—¥æœŸ")

    rate_limit = 6
    print(f"âš™ï¸  é€Ÿç‡é™åˆ¶: {rate_limit} ç§’/ä»»åŠ¡")
    print(f"â±ï¸  é¢„è®¡æ€»æ—¶é•¿: {len(dates) * rate_limit / 60:.1f} åˆ†é’Ÿ")
    print(f"ğŸ“ˆ é¢„æœŸæ•°æ®é‡: çº¦ {len(dates) * 20} - {len(dates) * 50} åœºæ¯”èµ›")
    print("=" * 80)


def validate_environment():
    """éªŒè¯æ‰§è¡Œç¯å¢ƒ"""
    try:
        # ç®€å•çš„ç¯å¢ƒéªŒè¯
        import os
        database_url = os.getenv("DATABASE_URL")
        redis_url = os.getenv("REDIS_URL")

        logger.info("âœ… ç¯å¢ƒé…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"   - Database URL: {'å·²é…ç½®' if database_url else 'æœªé…ç½®'}")
        logger.info(f"   - Redis URL: {'å·²é…ç½®' if redis_url else 'æœªé…ç½®'}")
        logger.info(f"   - Celeryè¿æ¥: {'å¯ç”¨' if database_url and redis_url else 'ä¸å¯ç”¨'}")
        return True
    except Exception as e:
        logger.error(f"âŒ ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å†å²æ•°æ®å›æº¯é‡‡é›†è„šæœ¬')
    parser.add_argument('--dry-run', action='store_true', help='åªç”Ÿæˆæ—¥æœŸåˆ—è¡¨ï¼Œä¸å®é™…è§¦å‘ä»»åŠ¡')
    args = parser.parse_args()

    logger.info("ğŸ¯ å†å²æ•°æ®å›æº¯é‡‡é›†å¯åŠ¨")
    logger.info("=" * 80)

    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        logger.error("âŒ ç¯å¢ƒéªŒè¯å¤±è´¥ï¼Œé€€å‡ºæ‰§è¡Œ")
        return 1

    # åŠ è½½é…ç½®
    config = load_data_source_config()
    if not config:
        logger.error("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæ‰§è¡Œ")
        return 1

    # ç”Ÿæˆå†å²æ—¥æœŸ
    dates = generate_historical_dates(config)
    if not dates:
        logger.error("âŒ æ²¡æœ‰ç”Ÿæˆå†å²æ—¥æœŸï¼Œé€€å‡ºæ‰§è¡Œ")
        return 1

    # æ‰“å°é‡‡é›†æ‘˜è¦
    print_collection_summary(config, dates)

    # ç¡®è®¤æ‰§è¡Œ
    if not args.dry_run:
        try:
            response = input("\nâ“ ç¡®è®¤è¦æ‰§è¡Œå¤§è§„æ¨¡å†å²æ•°æ®é‡‡é›†å—ï¼Ÿè¿™å°†è§¦å‘å¤§é‡Celeryä»»åŠ¡ [y/N]: ")
            if response.lower() not in ['y', 'yes', 'æ˜¯']:
                logger.info("âŒ ç”¨æˆ·å–æ¶ˆæ‰§è¡Œ")
                return 0
        except KeyboardInterrupt:
            logger.info("âŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            return 0

    # æ‰§è¡Œé‡‡é›†
    try:
        tasks_triggered = await trigger_collection_tasks(dates, args.dry_run)

        if args.dry_run:
            logger.info(f"ğŸ” DRY RUN å®Œæˆ: å°†è§¦å‘ {tasks_triggered} ä¸ªä»»åŠ¡")
        else:
            logger.info(f"ğŸš€ æ‰§è¡Œå®Œæˆ: æˆåŠŸè§¦å‘ {tasks_triggered} ä¸ªä»»åŠ¡")

            # æä¾›åç»­æ“ä½œæŒ‡å¯¼
            print("\nğŸ“‹ åç»­æ“ä½œå»ºè®®:")
            print("1. ç›‘æ§ä»»åŠ¡æ‰§è¡Œ: docker-compose logs -f worker")
            print("2. æ£€æŸ¥é‡‡é›†è¿›åº¦: docker-compose exec db psql -U postgres -d football_prediction -c \"SELECT COUNT(*) FROM raw_match_data WHERE created_at > NOW() - INTERVAL '1 hour';\"")
            print("3. æŸ¥çœ‹ä»»åŠ¡é˜Ÿåˆ—: docker-compose exec worker celery -A src.tasks.celery_app inspect active")
            print("4. è¿è¡ŒETLå¤„ç†: docker-compose exec app python scripts/run_etl_silver.py")

        return 0

    except Exception as e:
        logger.error(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)