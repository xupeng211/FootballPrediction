#!/usr/bin/env python3
"""
åœ°æ¯¯å¼è¦†ç›–æ•°æ®é‡‡é›†è„šæœ¬ / Comprehensive Coverage Data Collection Script

ğŸ¯ æˆ˜ç•¥å˜æ›´ï¼šæ‰§è¡Œ"åœ°æ¯¯å¼è¦†ç›–"ç­–ç•¥
- æ—¶é—´èŒƒå›´ï¼š2022å¹´1æœˆ1æ—¥ åˆ° ä»Šå¤©
- é‡‡é›†æ–¹å¼ï¼šè¿ç»­æ—¥æœŸï¼Œä¸€å¤©éƒ½ä¸è·³è¿‡
- åŒ…å«èµ›äº‹ï¼šæ‰€æœ‰è”èµ› + å›½é™…æ¯èµ› + å‹è°Šèµ›
- ä¸å†éšæœºé‡‡æ ·ï¼Œä¸å†è·³è¿‡ä¼‘èµ›æœŸ

ğŸš€ Strategic Change: "Comprehensive Coverage" Strategy
- Time Range: 2022-01-01 to Today
- Collection Method: Continuous dates, no skipping
- Include: All leagues + International cups + Friendlies
- No random sampling, no rest day skipping

ä½¿ç”¨æ–¹æ³• / Usage:
    python scripts/trigger_historical_backfill.py [--dry-run]

å‚æ•° / Arguments:
    --dry-run: åªæ˜¾ç¤ºè®¡åˆ’ï¼Œä¸å®é™…è§¦å‘ä»»åŠ¡

æ³¨æ„äº‹é¡¹ / Notes:
- è¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡æ•°æ®é‡‡é›†ä»»åŠ¡ï¼Œé¢„è®¡éœ€è¦æ•°å°æ—¶å®Œæˆ
- 5-10ç§’éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸäººè¡Œä¸ºï¼Œæ›´åŠ å®‰å…¨
- å°†é‡‡é›†åŒ…æ‹¬ä¼‘èµ›æœŸåœ¨å†…çš„æ¯ä¸€å¤©çš„æ•°æ®
- å›½é™…å‹è°Šèµ›æ˜¯é«˜ä»·å€¼æ•°æ®æºï¼Œä¸å®¹å¿½è§†
"""

import asyncio
import logging
import os
import sys
import random
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv

# å°è¯•åŠ è½½.envæ–‡ä»¶
env_files = [
    project_root / ".env",
    project_root / ".env.local",
    project_root / ".env.development",
]

for env_file in env_files:
    if env_file.exists():
        load_dotenv(env_file)
        break

import json
from src.tasks.celery_app import celery_app


def load_data_source_config() -> dict[str, Any]:
    """åŠ è½½æ•°æ®æºé…ç½®"""
    try:
        config_path = project_root / "src" / "config" / "data_sources.json"
        with open(config_path, encoding="utf-8") as f:
            config = json.load(f)
        logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®æˆ˜ç•¥é…ç½®: {config_path}")
        logger.info(f"ğŸ“‹ é…ç½®ç‰ˆæœ¬: {config.get('version', 'unknown')}")
        logger.info(f"ğŸ¯ é‡‡é›†ç­–ç•¥: {config.get('collection_strategy', 'unknown')}")
        return config
    except Exception:
        logger.error(f"âŒ åŠ è½½æ•°æ®æºé…ç½®å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤é…ç½®
        return {
            "version": "1.0.0",
            "collection_strategy": "high_value_focus",
            "backfill": {
                "years": 3,
                "days_per_season": 30,
                "target_leagues": ["PL", "PD", "BL1", "SA", "FL1"],
            },
            "fotmob": {
                "rate_limit": {"requests_per_minute": 10, "delay_between_requests": 6}
            },
        }


def generate_comprehensive_dates(config: dict[str, Any]) -> list[str]:
    """ç”Ÿæˆåœ°æ¯¯å¼è¦†ç›–çš„è¿ç»­æ—¥æœŸåˆ—è¡¨"""
    strategic_settings = config.get("strategic_settings", {})

    # ğŸ¯ åœ°æ¯¯å¼è¦†ç›–ç­–ç•¥å‚æ•°
    start_date_str = strategic_settings.get("start_date", "20220101")
    end_date_str = strategic_settings.get("end_date", "today")
    skip_rest_days = strategic_settings.get("skip_rest_days", False)

    target_leagues = config.get("target_leagues", [])

    # è§£ææ—¥æœŸèŒƒå›´
    if end_date_str == "today":
        end_date = datetime.now()
    else:
        end_date = datetime.strptime(end_date_str, "%Y%m%d")

    start_date = datetime.strptime(start_date_str, "%Y%m%d")

    logger.info("ğŸ¯ åœ°æ¯¯å¼è¦†ç›–æ•°æ®é‡‡é›†ç­–ç•¥")
    logger.info(
        f"   - æ—¶é—´èŒƒå›´: {start_date.strftime('%Y-%m-%d')} åˆ° {end_date.strftime('%Y-%m-%d')}"
    )
    logger.info(f"   - è·³è¿‡ä¼‘èµ›æœŸ: {'æ˜¯' if skip_rest_days else 'å¦ï¼ˆåœ°æ¯¯å¼è¦†ç›–ï¼‰'}")
    logger.info(f"   - ç›®æ ‡è”èµ›æ•°é‡: {len(target_leagues)}")

    # è¯†åˆ«å›½é™…èµ›äº‹
    international_leagues = [
        league for league in target_leagues if league.get("type") == "International"
    ]
    domestic_leagues = [
        league for league in target_leagues if league.get("type") != "International"
    ]

    logger.info(f"   - å›½å†…è”èµ›: {len(domestic_leagues)} ä¸ª")
    logger.info(f"   - å›½é™…èµ›äº‹: {len(international_leagues)} ä¸ª")

    if international_leagues:
        logger.info(
            f"   - å›½é™…èµ›äº‹åŒ…æ‹¬: {', '.join([league['name'] for league in international_leagues])}"
        )

    # ç”Ÿæˆè¿ç»­æ—¥æœŸåˆ—è¡¨ï¼ˆåœ°æ¯¯å¼è¦†ç›–ï¼‰
    all_dates = []
    current_date = start_date

    while current_date <= end_date:
        date_str = current_date.strftime("%Y%m%d")
        all_dates.append(date_str)
        current_date += timedelta(days=1)

    logger.info("ğŸ“… åœ°æ¯¯å¼è¦†ç›–æ—¥æœŸç»Ÿè®¡:")
    logger.info(f"   - æœ€æ—©æ—¥æœŸ: {all_dates[0] if all_dates else 'None'}")
    logger.info(f"   - æœ€æ™šæ—¥æœŸ: {all_dates[-1] if all_dates else 'None'}")
    logger.info(f"   - æ€»å¤©æ•°: {len(all_dates)} å¤©")

    # è®¡ç®—é¢„è®¡æ‰§è¡Œæ—¶é—´
    min_delay = 5
    max_delay = 10
    avg_delay = (min_delay + max_delay) / 2
    estimated_minutes = len(all_dates) * avg_delay / 60
    estimated_hours = estimated_minutes / 60

    logger.info("â±ï¸ é¢„è®¡æ‰§è¡Œæ—¶é—´:")
    logger.info(f"   - å»¶è¿ŸèŒƒå›´: {min_delay}-{max_delay} ç§’/ä»»åŠ¡")
    logger.info(
        f"   - é¢„è®¡æ€»æ—¶é•¿: {estimated_minutes:.1f} åˆ†é’Ÿ ({estimated_hours:.1f} å°æ—¶)"
    )

    return all_dates


async def trigger_comprehensive_collection(
    dates: list[str], dry_run: bool = False
) -> int:
    """è§¦å‘åœ°æ¯¯å¼è¦†ç›–é‡‡é›†ä»»åŠ¡"""
    # ğŸ¯ åœ°æ¯¯å¼è¦†ç›–ç­–ç•¥ï¼š5-10ç§’éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸäººè¡Œä¸º
    min_delay = 5
    max_delay = 10

    logger.info("ğŸš€ å¯åŠ¨åœ°æ¯¯å¼è¦†ç›–æ•°æ®é‡‡é›†")
    logger.info(f"ğŸ“… é‡‡é›†æ—¥æœŸèŒƒå›´: {len(dates)} å¤©è¿ç»­è¦†ç›–")
    logger.info(f"â±ï¸ å»¶è¿Ÿç­–ç•¥: {min_delay}-{max_delay} ç§’éšæœºå»¶è¿Ÿï¼ˆæ¨¡æ‹ŸçœŸäººè¡Œä¸ºï¼‰")
    logger.info("ğŸ¯ ä¸è·³è¿‡ä¼‘èµ›æœŸ: ç¡®ä¿æ•°æ®å®Œæ•´æ€§")

    if dry_run:
        logger.info("ğŸ” DRY RUN æ¨¡å¼: æ˜¾ç¤ºåœ°æ¯¯å¼è¦†ç›–è®¡åˆ’")
        for i, date_str in enumerate(dates[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            logger.info(f"   [{i + 1:3}/{len(dates)}] é‡‡é›†æ—¥æœŸ: {date_str}")
        if len(dates) > 10:
            logger.info(f"   ... è¿˜æœ‰ {len(dates) - 10} ä¸ªæ—¥æœŸ")
        return len(dates)

    tasks_triggered = 0
    failed_tasks = 0

    for i, date_str in enumerate(dates):
        try:
            # æ ¼å¼åŒ–è¿›åº¦æ˜¾ç¤º
            progress = (i + 1) / len(dates) * 100
            logger.info(
                f"ğŸ“… [{i + 1:4}/{len(dates)}] ({progress:5.1f}%) é‡‡é›† {date_str}"
            )

            # è°ƒç”¨Celeryä»»åŠ¡è§¦å‘FotMobæ•°æ®é‡‡é›†
            task = celery_app.send_task(
                "collect_fotmob_data",
                kwargs={"date": date_str},
                queue="fotmob",
                priority=5,  # ä¸­ç­‰ä¼˜å…ˆçº§
            )

            tasks_triggered += 1
            logger.info(f"âœ… ä»»åŠ¡æäº¤æˆåŠŸ: {task.id}")

            # ğŸ¯ åœ°æ¯¯å¼è¦†ç›–å»¶è¿Ÿç­–ç•¥ï¼šéšæœºå»¶è¿Ÿ5-10ç§’
            if i < len(dates) - 1:  # æœ€åä¸€ä¸ªä»»åŠ¡ä¸éœ€è¦ç­‰å¾…
                delay = random.uniform(min_delay, max_delay)
                logger.info(f"â±ï¸ éšæœºå»¶è¿Ÿ {delay:.1f} ç§’...")
                await asyncio.sleep(delay)

        except Exception:
            logger.error(f"âŒ æ—¥æœŸ {date_str} é‡‡é›†å¤±è´¥: {e}")
            failed_tasks += 1
            # å¤±è´¥æ—¶ä¹Ÿæ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿ç»­å¤±è´¥å†²å‡»API
            await asyncio.sleep(random.uniform(2, 4))
            continue

    # æœ€ç»ˆç»Ÿè®¡æŠ¥å‘Š
    success_rate = (tasks_triggered / len(dates)) * 100 if dates else 0
    logger.info("ğŸ‰ åœ°æ¯¯å¼è¦†ç›–é‡‡é›†ä»»åŠ¡è§¦å‘å®Œæˆï¼")
    logger.info("ğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
    logger.info(f"   - æ€»æ—¥æœŸæ•°: {len(dates)}")
    logger.info(f"   - æˆåŠŸä»»åŠ¡: {tasks_triggered}")
    logger.info(f"   - å¤±è´¥ä»»åŠ¡: {failed_tasks}")
    logger.info(f"   - æˆåŠŸç‡: {success_rate:.1f}%")

    return tasks_triggered


def print_comprehensive_summary(config: dict[str, Any], dates: list[str]):
    """æ‰“å°åœ°æ¯¯å¼è¦†ç›–é‡‡é›†æ‘˜è¦"""
    strategic_settings = config.get("strategic_settings", {})
    target_leagues = config.get("target_leagues", [])

    print("=" * 80)
    print("ğŸ¯ åœ°æ¯¯å¼è¦†ç›–æ•°æ®é‡‡é›†æˆ˜ç•¥")
    print("=" * 80)

    print(
        f"ğŸ“Š é‡‡é›†ç­–ç•¥: {strategic_settings.get('collection_strategy', 'comprehensive_coverage')}"
    )
    print(
        f"ğŸ“… æ—¶é—´èŒƒå›´: {strategic_settings.get('start_date', '20220101')} åˆ° {strategic_settings.get('end_date', 'today')}"
    )
    print("ğŸ¯ è¦†ç›–æ–¹å¼: è¿ç»­æ—¥æœŸï¼Œä¸è·³è¿‡ä¼‘èµ›æœŸ")
    print(f"ğŸ“‹ æ€»å¤©æ•°: {len(dates)} å¤©")

    # è”èµ›åˆ†ç±»ç»Ÿè®¡
    tier1_leagues = [
        league["name"] for league in target_leagues if league.get("type") == "Tier1"
    ]
    tier2_leagues = [
        league["name"] for league in target_leagues if league.get("type") == "Tier2"
    ]
    cup_leagues = [
        league["name"] for league in target_leagues if league.get("type") == "Cup"
    ]
    international_leagues = [
        league["name"]
        for league in target_leagues
        if league.get("type") == "International"
    ]
    asian_leagues = [
        league["name"] for league in target_leagues if league.get("type") == "Asia"
    ]
    american_leagues = [
        league["name"] for league in target_leagues if league.get("type") == "America"
    ]

    print("\nğŸ† ç›®æ ‡èµ›äº‹åˆ†ç±»:")
    if tier1_leagues:
        print(f"   ğŸ¥‡ é¡¶çº§è”èµ›: {', '.join(tier1_leagues)}")
    if tier2_leagues:
        print(f"   ğŸ¥ˆ æ¬¡çº§è”èµ›: {', '.join(tier2_leagues)}")
    if cup_leagues:
        print(f"   ğŸ… æ¯èµ›: {', '.join(cup_leagues)}")
    if international_leagues:
        print(f"   ğŸŒ å›½é™…èµ›äº‹: {', '.join(international_leagues)}")
    if asian_leagues:
        print(f"   ğŸ® äºšæ´²è”èµ›: {', '.join(asian_leagues)}")
    if american_leagues:
        print(f"   âš½ ç¾æ´²è”èµ›: {', '.join(american_leagues)}")

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    year_stats = {}
    month_stats = {}
    for date_str in dates:
        year = date_str[:4]
        month = date_str[4:6]
        year_stats[year] = year_stats.get(year, 0) + 1
        month_stats[f"{year}-{month}"] = month_stats.get(f"{year}-{month}", 0) + 1

    print("\nğŸ“… æŒ‰å¹´ä»½åˆ†å¸ƒ:")
    for year in sorted(year_stats.keys()):
        print(f"   {year}å¹´: {year_stats[year]} å¤©")

    print("\nâ±ï¸ æ‰§è¡Œå‚æ•°:")
    print("   - å»¶è¿Ÿç­–ç•¥: 5-10ç§’éšæœºå»¶è¿Ÿ")
    print(f"   - é¢„è®¡æ—¶é•¿: {len(dates) * 7.5 / 60:.1f} åˆ†é’Ÿ")
    print("   - å¹¶å‘ä»»åŠ¡: 1ä¸ªï¼ˆé¡ºåºæ‰§è¡Œä¿å®‰å…¨ï¼‰")

    print("\nğŸ“ˆ é¢„æœŸæ”¶ç›Š:")
    print("   - æ•°æ®å®Œæ•´æ€§: 100%æ— é—´æ–­è¦†ç›–")
    print(f"   - é¢„æœŸæ¯”èµ›æ•°: çº¦ {len(dates) * 15} - {len(dates) * 40} åœº")
    print("   - åŒ…å«å‹è°Šèµ›: é«˜ä»·å€¼è®­ç»ƒæ•°æ®")
    print("   - å›½é™…æ¯èµ›: é‡å¤§èµ›äº‹æ•°æ®å…¨è¦†ç›–")

    print("=" * 80)


def validate_environment():
    """éªŒè¯æ‰§è¡Œç¯å¢ƒ"""
    try:
        # ç®€å•çš„ç¯å¢ƒéªŒè¯
        import os

        database_url = os.getenv("DATABASE_URL")
        redis_url = os.getenv("REDIS_URL")

        logger.info("âœ… ç¯å¢ƒé…ç½®éªŒè¯é€šè¿‡")
        logger.info(f"   - Database URL: {'å·²é…ç½®' if database_url else 'æœªé…ç½®'}")
        logger.info(f"   - Redis URL: {'å·²é…ç½®' if redis_url else 'æœªé…ç½®'}")
        logger.info(
            f"   - Celeryè¿æ¥: {'å¯ç”¨' if database_url and redis_url else 'ä¸å¯ç”¨'}"
        )
        return True
    except Exception:
        logger.error(f"âŒ ç¯å¢ƒéªŒè¯å¤±è´¥: {e}")
        return False


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å†å²æ•°æ®å›æº¯é‡‡é›†è„šæœ¬")
    parser.add_argument(
        "--dry-run", action="store_true", help="åªç”Ÿæˆæ—¥æœŸåˆ—è¡¨ï¼Œä¸å®é™…è§¦å‘ä»»åŠ¡"
    )
    args = parser.parse_args()

    logger.info("ğŸ¯ å†å²æ•°æ®å›æº¯é‡‡é›†å¯åŠ¨")
    logger.info("=" * 80)

    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        logger.error("âŒ ç¯å¢ƒéªŒè¯å¤±è´¥ï¼Œé€€å‡ºæ‰§è¡Œ")
        return 1

    # åŠ è½½é…ç½®
    config = load_data_source_config()
    if not config:
        logger.error("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæ‰§è¡Œ")
        return 1

    # ç”Ÿæˆåœ°æ¯¯å¼è¦†ç›–æ—¥æœŸ
    dates = generate_comprehensive_dates(config)
    if not dates:
        logger.error("âŒ æ²¡æœ‰ç”Ÿæˆåœ°æ¯¯å¼è¦†ç›–æ—¥æœŸï¼Œé€€å‡ºæ‰§è¡Œ")
        return 1

    # æ‰“å°åœ°æ¯¯å¼è¦†ç›–æ‘˜è¦
    print_comprehensive_summary(config, dates)

    # ç¡®è®¤æ‰§è¡Œ
    if not args.dry_run:
        try:
            print("\nâš ï¸  åœ°æ¯¯å¼è¦†ç›–é‡‡é›†ç¡®è®¤")
            print(
                f"ğŸ“… å°†é‡‡é›† {len(dates)} å¤©çš„æ•°æ®ï¼Œé¢„è®¡éœ€è¦ {len(dates) * 7.5 / 60:.1f} åˆ†é’Ÿ"
            )
            response = input(
                "â“ ç¡®è®¤è¦æ‰§è¡Œåœ°æ¯¯å¼è¦†ç›–æ•°æ®é‡‡é›†å—ï¼Ÿè¿™å°†è§¦å‘å¤§é‡Celeryä»»åŠ¡ [y/N]: "
            )
            if response.lower() not in ["y", "yes", "æ˜¯"]:
                logger.info("âŒ ç”¨æˆ·å–æ¶ˆæ‰§è¡Œ")
                return 0
        except KeyboardInterrupt:
            logger.info("âŒ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
            return 0

    # æ‰§è¡Œåœ°æ¯¯å¼è¦†ç›–é‡‡é›†
    try:
        tasks_triggered = await trigger_comprehensive_collection(dates, args.dry_run)

        if args.dry_run:
            logger.info(f"ğŸ” DRY RUN å®Œæˆ: å°†è§¦å‘ {tasks_triggered} ä¸ªåœ°æ¯¯å¼è¦†ç›–ä»»åŠ¡")
        else:
            logger.info(f"ğŸš€ åœ°æ¯¯å¼è¦†ç›–æ‰§è¡Œå®Œæˆ: æˆåŠŸè§¦å‘ {tasks_triggered} ä¸ªä»»åŠ¡")

            # æä¾›åç»­æ“ä½œæŒ‡å¯¼
            print("\nğŸ“‹ åœ°æ¯¯å¼è¦†ç›–åç»­æ“ä½œå»ºè®®:")
            print("1. ğŸ“Š å®æ—¶ç›‘æ§: docker-compose logs -f worker | grep -i fotmob")
            print(
                "2. ğŸ” æ£€æŸ¥è¿›åº¦: docker-compose exec db psql -U postgres -d football_prediction -c \"SELECT COUNT(*) FROM raw_match_data WHERE created_at > NOW() - INTERVAL '1 hour';\""
            )
            print(
                "3. ğŸ“‹ æŸ¥çœ‹é˜Ÿåˆ—: docker-compose exec worker celery -A src.tasks.celery_app inspect active"
            )
            print(
                "4. âš¡ ETLå¤„ç†: docker-compose exec app python scripts/run_etl_silver.py"
            )
            print(
                "5. ğŸ“ˆ è´¨é‡å®¡è®¡: docker-compose exec app python scripts/audit_data_quality.py"
            )

        return 0

    except Exception:
        logger.error(f"âŒ åœ°æ¯¯å¼è¦†ç›–æ‰§è¡Œå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
