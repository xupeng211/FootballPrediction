#!/usr/bin/env python3
"""
ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–è„šæœ¬
System Performance Optimization Script

åŸºäºç›‘æ§æ•°æ®è¿›è¡Œå…¨é¢çš„æ€§èƒ½è°ƒä¼˜ï¼ŒåŒ…æ‹¬ï¼š
- APIå“åº”æ—¶é—´ä¼˜åŒ–ï¼ˆç›®æ ‡<200msï¼‰
- å¹¶å‘å¤„ç†èƒ½åŠ›æå‡ï¼ˆç›®æ ‡100+å¹¶å‘è¯·æ±‚ï¼‰
- ç¼“å­˜ç­–ç•¥ä¼˜åŒ–ï¼ˆç›®æ ‡90%+å‘½ä¸­ç‡ï¼‰
- æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
- å†…å­˜å’ŒCPUä½¿ç”¨ä¼˜åŒ–

Author: Claude AI Assistant
Date: 2025-11-03
Version: 1.0.0
"""

import asyncio
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import subprocess
import psutil
import yaml

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.config import get_settings
from src.core.logging import get_logger

logger = get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: datetime
    api_response_time: float  # ms
    request_count: int
    concurrent_connections: int
    memory_usage: float  # MB
    cpu_usage: float  # %
    db_connections: int
    cache_hit_rate: float  # %
    error_rate: float  # %
    throughput: float  # requests/second


@dataclass
class OptimizationTarget:
    """ä¼˜åŒ–ç›®æ ‡æ•°æ®ç±»"""
    name: str
    current_value: float
    target_value: float
    unit: str
    priority: int  # 1-5, 1ä¸ºæœ€é«˜
    description: str


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœæ•°æ®ç±»"""
    target_name: str
    before_value: float
    after_value: float
    improvement_percentage: float
    success: bool
    details: str


class SystemPerformanceOptimizer:
    """ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ€§èƒ½ä¼˜åŒ–å™¨"""
        self.settings = get_settings()
        self.project_root = project_root
        self.optimization_history: List[OptimizationResult] = []

        # æ€§èƒ½ç›‘æ§ç«¯ç‚¹
        self.prometheus_url = "http://localhost:9090"
        self.grafana_url = "http://localhost:3000"

        # å½“å‰æ€§èƒ½æŒ‡æ ‡
        self.current_metrics: Optional[PerformanceMetrics] = None

        # ä¼˜åŒ–é…ç½®
        self.optimization_config = self._load_optimization_config()

        logger.info("ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_optimization_config(self) -> dict:
        """åŠ è½½ä¼˜åŒ–é…ç½®"""
        config_path = self.project_root / "config" / "monitoring" / "optimization.yml"

        default_config = {
            "targets": {
                "api_response_time": {"target": 200, "unit": "ms", "priority": 1},
                "concurrent_requests": {"target": 100, "unit": "count", "priority": 2},
                "cache_hit_rate": {"target": 90, "unit": "%", "priority": 2},
                "memory_usage": {"target": 512, "unit": "MB", "priority": 3},
                "cpu_usage": {"target": 70, "unit": "%", "priority": 3},
                "db_connections": {"target": 20, "unit": "count", "priority": 2},
                "error_rate": {"target": 1, "unit": "%", "priority": 1},
                "throughput": {"target": 50, "unit": "req/s", "priority": 2}
            },
            "optimizations": {
                "api": {
                    "enable_compression": True,
                    "enable_caching": True,
                    "connection_timeout": 30,
                    "keep_alive_timeout": 5,
                    "max_concurrent_requests": 100
                },
                "database": {
                    "pool_size": 20,
                    "max_overflow": 30,
                    "pool_timeout": 30,
                    "pool_recycle": 3600,
                    "echo": False
                },
                "cache": {
                    "redis_max_connections": 50,
                    "default_ttl": 3600,
                    "max_memory_policy": "allkeys-lru"
                },
                "application": {
                    "workers": 4,
                    "worker_class": "uvicorn.workers.UvicornWorker",
                    "worker_connections": 1000,
                    "max_requests": 1000,
                    "max_requests_jitter": 100
                }
            }
        }

        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    user_config = yaml.safe_load(f)
                    # åˆå¹¶é…ç½®
                    default_config.update(user_config)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½ä¼˜åŒ–é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")

        return default_config

    async def collect_current_metrics(self) -> PerformanceMetrics:
        """æ”¶é›†å½“å‰æ€§èƒ½æŒ‡æ ‡"""
        logger.info("æ­£åœ¨æ”¶é›†å½“å‰æ€§èƒ½æŒ‡æ ‡...")

        try:
            # ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
            memory_info = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)

            # æ¨¡æ‹ŸAPIå“åº”æ—¶é—´æµ‹è¯•
            api_response_time = await self._test_api_response_time()

            # è·å–æ•°æ®åº“è¿æ¥æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
            db_connections = await self._get_database_connections()

            # è·å–ç¼“å­˜å‘½ä¸­ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
            cache_hit_rate = await self._get_cache_hit_rate()

            # è·å–å¹¶å‘è¿æ¥æ•°ï¼ˆæ¨¡æ‹Ÿï¼‰
            concurrent_connections = await self._get_concurrent_connections()

            # è®¡ç®—ååé‡å’Œé”™è¯¯ç‡ï¼ˆæ¨¡æ‹Ÿï¼‰
            throughput, error_rate = await self._calculate_throughput_and_errors()

            metrics = PerformanceMetrics(
                timestamp=datetime.now(),
                api_response_time=api_response_time,
                request_count=100,  # æ¨¡æ‹Ÿæ•°æ®
                concurrent_connections=concurrent_connections,
                memory_usage=memory_info.used / 1024 / 1024,  # MB
                cpu_usage=cpu_percent,
                db_connections=db_connections,
                cache_hit_rate=cache_hit_rate,
                error_rate=error_rate,
                throughput=throughput
            )

            self.current_metrics = metrics
            logger.info(f"æ€§èƒ½æŒ‡æ ‡æ”¶é›†å®Œæˆ: APIå“åº”æ—¶é—´={api_response_time:.2f}ms, å†…å­˜ä½¿ç”¨={metrics.memory_usage:.2f}MB")

            return metrics

        except Exception as e:
            logger.error(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            raise

    async def _test_api_response_time(self) -> float:
        """æµ‹è¯•APIå“åº”æ—¶é—´"""
        try:
            # ä½¿ç”¨curlæµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹
            cmd = ["curl", "-o", "/dev/null", "-s", "-w", "%{time_total}", "http://localhost:8000/health"]
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)

            if result.returncode == 0:
                response_time_seconds = float(result.stdout.strip())
                return response_time_seconds * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            else:
                logger.warning("æ— æ³•æµ‹è¯•APIå“åº”æ—¶é—´ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return 250.0  # æ¨¡æ‹Ÿæ•°æ®

        except Exception as e:
            logger.warning(f"APIå“åº”æ—¶é—´æµ‹è¯•å¤±è´¥: {e}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return 250.0  # æ¨¡æ‹Ÿæ•°æ®

    async def _get_database_connections(self) -> int:
        """è·å–æ•°æ®åº“è¿æ¥æ•°"""
        try:
            # è¿™é‡Œåº”è¯¥è¿æ¥åˆ°æ•°æ®åº“è·å–å®é™…è¿æ¥æ•°
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return 8
        except Exception as e:
            logger.warning(f"è·å–æ•°æ®åº“è¿æ¥æ•°å¤±è´¥: {e}")
            return 8

    async def _get_cache_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        try:
            # è¿™é‡Œåº”è¯¥è¿æ¥åˆ°Redisè·å–å®é™…å‘½ä¸­ç‡
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return 75.0
        except Exception as e:
            logger.warning(f"è·å–ç¼“å­˜å‘½ä¸­ç‡å¤±è´¥: {e}")
            return 75.0

    async def _get_concurrent_connections(self) -> int:
        """è·å–å¹¶å‘è¿æ¥æ•°"""
        try:
            # è¿™é‡Œåº”è¯¥ä»åº”ç”¨æœåŠ¡å™¨è·å–å®é™…å¹¶å‘è¿æ¥æ•°
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return 25
        except Exception as e:
            logger.warning(f"è·å–å¹¶å‘è¿æ¥æ•°å¤±è´¥: {e}")
            return 25

    async def _calculate_throughput_and_errors(self) -> Tuple[float, float]:
        """è®¡ç®—ååé‡å’Œé”™è¯¯ç‡"""
        try:
            # è¿™é‡Œåº”è¯¥åŸºäºå®é™…è¯·æ±‚æ—¥å¿—è®¡ç®—
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
            return 30.0, 2.0  # 30 req/s, 2% error rate
        except Exception as e:
            logger.warning(f"è®¡ç®—ååé‡å’Œé”™è¯¯ç‡å¤±è´¥: {e}")
            return 30.0, 2.0

    def identify_optimization_targets(self) -> List[OptimizationTarget]:
        """è¯†åˆ«ä¼˜åŒ–ç›®æ ‡"""
        if not self.current_metrics:
            raise ValueError("è¯·å…ˆæ”¶é›†å½“å‰æ€§èƒ½æŒ‡æ ‡")

        targets = []
        metrics = self.current_metrics
        config_targets = self.optimization_config["targets"]

        # APIå“åº”æ—¶é—´
        if metrics.api_response_time > config_targets["api_response_time"]["target"]:
            targets.append(OptimizationTarget(
                name="api_response_time",
                current_value=metrics.api_response_time,
                target_value=config_targets["api_response_time"]["target"],
                unit=config_targets["api_response_time"]["unit"],
                priority=config_targets["api_response_time"]["priority"],
                description="APIå“åº”æ—¶é—´ä¼˜åŒ–"
            ))

        # å¹¶å‘è¯·æ±‚æ•°
        if metrics.concurrent_connections < config_targets["concurrent_requests"]["target"]:
            targets.append(OptimizationTarget(
                name="concurrent_requests",
                current_value=metrics.concurrent_connections,
                target_value=config_targets["concurrent_requests"]["target"],
                unit=config_targets["concurrent_requests"]["unit"],
                priority=config_targets["concurrent_requests"]["priority"],
                description="å¹¶å‘å¤„ç†èƒ½åŠ›æå‡"
            ))

        # ç¼“å­˜å‘½ä¸­ç‡
        if metrics.cache_hit_rate < config_targets["cache_hit_rate"]["target"]:
            targets.append(OptimizationTarget(
                name="cache_hit_rate",
                current_value=metrics.cache_hit_rate,
                target_value=config_targets["cache_hit_rate"]["target"],
                unit=config_targets["cache_hit_rate"]["unit"],
                priority=config_targets["cache_hit_rate"]["priority"],
                description="ç¼“å­˜ç­–ç•¥ä¼˜åŒ–"
            ))

        # å†…å­˜ä½¿ç”¨
        if metrics.memory_usage > config_targets["memory_usage"]["target"]:
            targets.append(OptimizationTarget(
                name="memory_usage",
                current_value=metrics.memory_usage,
                target_value=config_targets["memory_usage"]["target"],
                unit=config_targets["memory_usage"]["unit"],
                priority=config_targets["memory_usage"]["priority"],
                description="å†…å­˜ä½¿ç”¨ä¼˜åŒ–"
            ))

        # CPUä½¿ç”¨ç‡
        if metrics.cpu_usage > config_targets["cpu_usage"]["target"]:
            targets.append(OptimizationTarget(
                name="cpu_usage",
                current_value=metrics.cpu_usage,
                target_value=config_targets["cpu_usage"]["target"],
                unit=config_targets["cpu_usage"]["unit"],
                priority=config_targets["cpu_usage"]["priority"],
                description="CPUä½¿ç”¨ç‡ä¼˜åŒ–"
            ))

        # é”™è¯¯ç‡
        if metrics.error_rate > config_targets["error_rate"]["target"]:
            targets.append(OptimizationTarget(
                name="error_rate",
                current_value=metrics.error_rate,
                target_value=config_targets["error_rate"]["target"],
                unit=config_targets["error_rate"]["unit"],
                priority=config_targets["error_rate"]["priority"],
                description="é”™è¯¯ç‡é™ä½"
            ))

        # æŒ‰ä¼˜å…ˆçº§æ’åº
        targets.sort(key=lambda x: x.priority)

        logger.info(f"è¯†åˆ«åˆ° {len(targets)} ä¸ªä¼˜åŒ–ç›®æ ‡")
        return targets

    async def optimize_api_response_time(self) -> OptimizationResult:
        """ä¼˜åŒ–APIå“åº”æ—¶é—´"""
        logger.info("å¼€å§‹ä¼˜åŒ–APIå“åº”æ—¶é—´...")

        before_value = self.current_metrics.api_response_time
        target_value = self.optimization_config["targets"]["api_response_time"]["target"]

        try:
            # 1. å¯ç”¨Gzipå‹ç¼©
            await self._enable_gzip_compression()

            # 2. ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± 
            await self._optimize_database_pool()

            # 3. å¯ç”¨å“åº”ç¼“å­˜
            await self._enable_response_caching()

            # 4. ä¼˜åŒ–å¼‚æ­¥ä»»åŠ¡å¤„ç†
            await self._optimize_async_tasks()

            # ç­‰å¾…ä¼˜åŒ–ç”Ÿæ•ˆ
            await asyncio.sleep(5)

            # æµ‹é‡ä¼˜åŒ–åçš„å“åº”æ—¶é—´
            after_value = await self._test_api_response_time()

            improvement = ((before_value - after_value) / before_value) * 100
            success = after_value <= target_value

            result = OptimizationResult(
                target_name="api_response_time",
                before_value=before_value,
                after_value=after_value,
                improvement_percentage=improvement,
                success=success,
                details=f"APIå“åº”æ—¶é—´ä» {before_value:.2f}ms ä¼˜åŒ–åˆ° {after_value:.2f}msï¼Œæ”¹å–„ {improvement:.2f}%"
            )

            self.optimization_history.append(result)
            logger.info(f"APIå“åº”æ—¶é—´ä¼˜åŒ–å®Œæˆ: {result.details}")

            return result

        except Exception as e:
            logger.error(f"APIå“åº”æ—¶é—´ä¼˜åŒ–å¤±è´¥: {e}")
            return OptimizationResult(
                target_name="api_response_time",
                before_value=before_value,
                after_value=before_value,
                improvement_percentage=0.0,
                success=False,
                details=f"ä¼˜åŒ–å¤±è´¥: {str(e)}"
            )

    async def _enable_gzip_compression(self):
        """å¯ç”¨Gzipå‹ç¼©"""
        logger.info("å¯ç”¨Gzipå‹ç¼©...")

        # æ›´æ–°FastAPIé…ç½®ï¼Œå¯ç”¨Gzipä¸­é—´ä»¶
        config_updates = {
            "middleware": {
                "gzip": {
                    "enabled": True,
                    "minimum_size": 1024
                }
            }
        }

        await self._update_application_config(config_updates)

    async def _optimize_database_pool(self):
        """ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± """
        logger.info("ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± ...")

        db_config = self.optimization_config["optimizations"]["database"]
        config_updates = {
            "database": {
                "pool_size": db_config["pool_size"],
                "max_overflow": db_config["max_overflow"],
                "pool_timeout": db_config["pool_timeout"],
                "pool_recycle": db_config["pool_recycle"]
            }
        }

        await self._update_application_config(config_updates)

    async def _enable_response_caching(self):
        """å¯ç”¨å“åº”ç¼“å­˜"""
        logger.info("å¯ç”¨å“åº”ç¼“å­˜...")

        config_updates = {
            "cache": {
                "enabled": True,
                "default_ttl": self.optimization_config["optimizations"]["cache"]["default_ttl"]
            }
        }

        await self._update_application_config(config_updates)

    async def _optimize_async_tasks(self):
        """ä¼˜åŒ–å¼‚æ­¥ä»»åŠ¡å¤„ç†"""
        logger.info("ä¼˜åŒ–å¼‚æ­¥ä»»åŠ¡å¤„ç†...")

        config_updates = {
            "async": {
                "max_concurrent_tasks": 100,
                "task_timeout": 30
            }
        }

        await self._update_application_config(config_updates)

    async def optimize_concurrent_requests(self) -> OptimizationResult:
        """ä¼˜åŒ–å¹¶å‘å¤„ç†èƒ½åŠ›"""
        logger.info("å¼€å§‹ä¼˜åŒ–å¹¶å‘å¤„ç†èƒ½åŠ›...")

        before_value = self.current_metrics.concurrent_connections
        target_value = self.optimization_config["targets"]["concurrent_requests"]["target"]

        try:
            # 1. å¢åŠ Uvicorn workeræ•°é‡
            await self._increase_worker_count()

            # 2. ä¼˜åŒ–è¿æ¥é…ç½®
            await self._optimize_connection_config()

            # 3. å¯ç”¨è¿æ¥å¤ç”¨
            await self._enable_connection_keepalive()

            # ç­‰å¾…ä¼˜åŒ–ç”Ÿæ•ˆ
            await asyncio.sleep(10)

            # æµ‹é‡ä¼˜åŒ–åçš„å¹¶å‘å¤„ç†èƒ½åŠ›
            after_value = await self._get_concurrent_connections()

            improvement = ((after_value - before_value) / before_value) * 100 if before_value > 0 else 0
            success = after_value >= target_value

            result = OptimizationResult(
                target_name="concurrent_requests",
                before_value=before_value,
                after_value=after_value,
                improvement_percentage=improvement,
                success=success,
                details=f"å¹¶å‘å¤„ç†èƒ½åŠ›ä» {before_value} æå‡åˆ° {after_value}ï¼Œæ”¹å–„ {improvement:.2f}%"
            )

            self.optimization_history.append(result)
            logger.info(f"å¹¶å‘å¤„ç†èƒ½åŠ›ä¼˜åŒ–å®Œæˆ: {result.details}")

            return result

        except Exception as e:
            logger.error(f"å¹¶å‘å¤„ç†èƒ½åŠ›ä¼˜åŒ–å¤±è´¥: {e}")
            return OptimizationResult(
                target_name="concurrent_requests",
                before_value=before_value,
                after_value=before_value,
                improvement_percentage=0.0,
                success=False,
                details=f"ä¼˜åŒ–å¤±è´¥: {str(e)}"
            )

    async def _increase_worker_count(self):
        """å¢åŠ workeræ•°é‡"""
        logger.info("å¢åŠ Uvicorn workeræ•°é‡...")

        app_config = self.optimization_config["optimizations"]["application"]

        # æ›´æ–°docker-composeé…ç½®
        docker_compose_path = self.project_root / "docker" / "docker-compose.production.yml"

        if docker_compose_path.exists():
            await self._update_docker_workers(docker_compose_path, app_config)

    async def _update_docker_workers(self, docker_compose_path: Path, app_config: dict):
        """æ›´æ–°Dockeré…ç½®ä¸­çš„workeræ•°é‡"""
        try:
            with open(docker_compose_path, 'r', encoding='utf-8') as f:
                docker_config = yaml.safe_load(f)

            # æ›´æ–°appæœåŠ¡çš„deployé…ç½®
            if 'services' in docker_config and 'app' in docker_config['services']:
                docker_config['services']['app']['deploy']['replicas'] = app_config['workers']

                with open(docker_compose_path, 'w', encoding='utf-8') as f:
                    yaml.dump(docker_config, f, default_flow_style=False, allow_unicode=True)

                logger.info(f"Docker workeræ•°é‡å·²æ›´æ–°ä¸º {app_config['workers']}")

        except Exception as e:
            logger.error(f"æ›´æ–°Docker workeré…ç½®å¤±è´¥: {e}")

    async def _optimize_connection_config(self):
        """ä¼˜åŒ–è¿æ¥é…ç½®"""
        logger.info("ä¼˜åŒ–è¿æ¥é…ç½®...")

        config_updates = {
            "connections": {
                "max_connections": 1000,
                "keep_alive_timeout": 5,
                "connection_timeout": 30
            }
        }

        await self._update_application_config(config_updates)

    async def _enable_connection_keepalive(self):
        """å¯ç”¨è¿æ¥å¤ç”¨"""
        logger.info("å¯ç”¨è¿æ¥å¤ç”¨...")

        config_updates = {
            "connections": {
                "keep_alive": True,
                "max_keep_alive_requests": 100
            }
        }

        await self._update_application_config(config_updates)

    async def optimize_cache_strategy(self) -> OptimizationResult:
        """ä¼˜åŒ–ç¼“å­˜ç­–ç•¥"""
        logger.info("å¼€å§‹ä¼˜åŒ–ç¼“å­˜ç­–ç•¥...")

        before_value = self.current_metrics.cache_hit_rate
        target_value = self.optimization_config["targets"]["cache_hit_rate"]["target"]

        try:
            # 1. ä¼˜åŒ–Redisé…ç½®
            await self._optimize_redis_config()

            # 2. å®æ–½æ™ºèƒ½ç¼“å­˜ç­–ç•¥
            await self._implement_smart_caching()

            # 3. ä¼˜åŒ–ç¼“å­˜é”®è®¾è®¡
            await self._optimize_cache_keys()

            # ç­‰å¾…ä¼˜åŒ–ç”Ÿæ•ˆ
            await asyncio.sleep(5)

            # æµ‹é‡ä¼˜åŒ–åçš„ç¼“å­˜å‘½ä¸­ç‡
            after_value = await self._get_cache_hit_rate()

            improvement = ((after_value - before_value) / before_value) * 100 if before_value > 0 else 0
            success = after_value >= target_value

            result = OptimizationResult(
                target_name="cache_hit_rate",
                before_value=before_value,
                after_value=after_value,
                improvement_percentage=improvement,
                success=success,
                details=f"ç¼“å­˜å‘½ä¸­ç‡ä» {before_value:.2f}% æå‡åˆ° {after_value:.2f}%ï¼Œæ”¹å–„ {improvement:.2f}%"
            )

            self.optimization_history.append(result)
            logger.info(f"ç¼“å­˜ç­–ç•¥ä¼˜åŒ–å®Œæˆ: {result.details}")

            return result

        except Exception as e:
            logger.error(f"ç¼“å­˜ç­–ç•¥ä¼˜åŒ–å¤±è´¥: {e}")
            return OptimizationResult(
                target_name="cache_hit_rate",
                before_value=before_value,
                after_value=before_value,
                improvement_percentage=0.0,
                success=False,
                details=f"ä¼˜åŒ–å¤±è´¥: {str(e)}"
            )

    async def _optimize_redis_config(self):
        """ä¼˜åŒ–Redisé…ç½®"""
        logger.info("ä¼˜åŒ–Redisé…ç½®...")

        cache_config = self.optimization_config["optimizations"]["cache"]

        # æ›´æ–°Redisé…ç½®
        redis_config = {
            "maxmemory": "256mb",
            "maxmemory-policy": cache_config["max_memory_policy"],
            "save": "900 1 300 10 60 10000",
            "appendonly": "yes",
            "appendfsync": "everysec"
        }

        await self._update_redis_config(redis_config)

    async def _implement_smart_caching(self):
        """å®æ–½æ™ºèƒ½ç¼“å­˜ç­–ç•¥"""
        logger.info("å®æ–½æ™ºèƒ½ç¼“å­˜ç­–ç•¥...")

        # å®æ–½åŸºäºè®¿é—®æ¨¡å¼çš„æ™ºèƒ½ç¼“å­˜
        smart_cache_config = {
            "strategies": {
                "prediction_results": {"ttl": 1800, "max_size": 1000},
                "team_stats": {"ttl": 3600, "max_size": 500},
                "match_data": {"ttl": 900, "max_size": 2000},
                "user_data": {"ttl": 7200, "max_size": 100}
            }
        }

        await self._update_application_config({"cache": smart_cache_config})

    async def _optimize_cache_keys(self):
        """ä¼˜åŒ–ç¼“å­˜é”®è®¾è®¡"""
        logger.info("ä¼˜åŒ–ç¼“å­˜é”®è®¾è®¡...")

        # å®æ–½åˆ†å±‚ç¼“å­˜é”®ç­–ç•¥
        cache_key_config = {
            "key_patterns": {
                "user": "user:{user_id}:{data_type}",
                "match": "match:{match_id}:{data_type}",
                "team": "team:{team_id}:{season}:{data_type}",
                "prediction": "prediction:{user_id}:{match_id}:{model_version}"
            }
        }

        await self._update_application_config({"cache": cache_key_config})

    async def _update_application_config(self, config_updates: dict):
        """æ›´æ–°åº”ç”¨é…ç½®"""
        logger.info(f"æ›´æ–°åº”ç”¨é…ç½®: {config_updates}")

        # è¿™é‡Œåº”è¯¥å®é™…æ›´æ–°é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªæ˜¯è®°å½•é…ç½®æ›´æ–°

        config_path = self.project_root / "config" / "performance.json"

        try:
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    existing_config = json.load(f)
            else:
                existing_config = {}

            # åˆå¹¶é…ç½®
            existing_config.update(config_updates)

            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(existing_config, f, indent=2, ensure_ascii=False)

            logger.info("åº”ç”¨é…ç½®æ›´æ–°å®Œæˆ")

        except Exception as e:
            logger.error(f"æ›´æ–°åº”ç”¨é…ç½®å¤±è´¥: {e}")

    async def _update_redis_config(self, redis_config: dict):
        """æ›´æ–°Redisé…ç½®"""
        logger.info(f"æ›´æ–°Redisé…ç½®: {redis_config}")

        # è¿™é‡Œåº”è¯¥å®é™…æ›´æ–°Redisé…ç½®æ–‡ä»¶
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªæ˜¯è®°å½•é…ç½®æ›´æ–°

        redis_config_path = self.project_root / "config" / "monitoring" / "redis.yml"

        try:
            with open(redis_config_path, 'w', encoding='utf-8') as f:
                yaml.dump(redis_config, f, default_flow_style=False, allow_unicode=True)

            logger.info("Redisé…ç½®æ›´æ–°å®Œæˆ")

        except Exception as e:
            logger.error(f"æ›´æ–°Redisé…ç½®å¤±è´¥: {e}")

    async def run_optimization_cycle(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„ä¼˜åŒ–å‘¨æœŸ"""
        logger.info("å¼€å§‹ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å‘¨æœŸ...")

        try:
            # 1. æ”¶é›†å½“å‰æ€§èƒ½æŒ‡æ ‡
            initial_metrics = await self.collect_current_metrics()

            # 2. è¯†åˆ«ä¼˜åŒ–ç›®æ ‡
            optimization_targets = self.identify_optimization_targets()

            if not optimization_targets:
                logger.info("å½“å‰æ€§èƒ½æŒ‡æ ‡å·²è¾¾åˆ°æ‰€æœ‰ç›®æ ‡ï¼Œæ— éœ€ä¼˜åŒ–")
                return {
                    "status": "optimal",
                    "message": "æ‰€æœ‰æ€§èƒ½æŒ‡æ ‡å·²è¾¾åˆ°ç›®æ ‡",
                    "metrics": asdict(initial_metrics),
                    "optimization_results": []
                }

            logger.info(f"è¯†åˆ«åˆ° {len(optimization_targets)} ä¸ªä¼˜åŒ–ç›®æ ‡ï¼Œå¼€å§‹ä¼˜åŒ–...")

            # 3. æ‰§è¡Œä¼˜åŒ–
            optimization_results = []

            for target in optimization_targets:
                if target.name == "api_response_time":
                    result = await self.optimize_api_response_time()
                    optimization_results.append(result)

                elif target.name == "concurrent_requests":
                    result = await self.optimize_concurrent_requests()
                    optimization_results.append(result)

                elif target.name == "cache_hit_rate":
                    result = await self.optimize_cache_strategy()
                    optimization_results.append(result)

                # åœ¨æ¯æ¬¡ä¼˜åŒ–åç­‰å¾…ç”Ÿæ•ˆ
                await asyncio.sleep(3)

            # 4. æ”¶é›†ä¼˜åŒ–åçš„æ€§èƒ½æŒ‡æ ‡
            final_metrics = await self.collect_current_metrics()

            # 5. ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            optimization_report = self._generate_optimization_report(
                initial_metrics, final_metrics, optimization_targets, optimization_results
            )

            logger.info("ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å‘¨æœŸå®Œæˆ")

            return {
                "status": "completed",
                "message": f"æ€§èƒ½ä¼˜åŒ–å®Œæˆï¼Œå¤„ç†äº† {len(optimization_targets)} ä¸ªä¼˜åŒ–ç›®æ ‡",
                "initial_metrics": asdict(initial_metrics),
                "final_metrics": asdict(final_metrics),
                "optimization_targets": [asdict(target) for target in optimization_targets],
                "optimization_results": [asdict(result) for result in optimization_results],
                "report": optimization_report
            }

        except Exception as e:
            logger.error(f"ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å¤±è´¥: {e}")
            return {
                "status": "failed",
                "message": f"ä¼˜åŒ–è¿‡ç¨‹å‡ºç°é”™è¯¯: {str(e)}",
                "error": str(e)
            }

    def _generate_optimization_report(
        self,
        initial_metrics: PerformanceMetrics,
        final_metrics: PerformanceMetrics,
        targets: List[OptimizationTarget],
        results: List[OptimizationResult]
    ) -> dict:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""

        successful_optimizations = [r for r in results if r.success]
        failed_optimizations = [r for r in results if not r.success]

        report = {
            "summary": {
                "total_targets": len(targets),
                "successful_optimizations": len(successful_optimizations),
                "failed_optimizations": len(failed_optimizations),
                "overall_success_rate": (len(successful_optimizations) / len(targets)) * 100 if targets else 0
            },
            "performance_improvements": {
                "api_response_time": {
                    "before": initial_metrics.api_response_time,
                    "after": final_metrics.api_response_time,
                    "improvement": ((initial_metrics.api_response_time - final_metrics.api_response_time) / initial_metrics.api_response_time) * 100 if initial_metrics.api_response_time > 0 else 0
                },
                "cache_hit_rate": {
                    "before": initial_metrics.cache_hit_rate,
                    "after": final_metrics.cache_hit_rate,
                    "improvement": ((final_metrics.cache_hit_rate - initial_metrics.cache_hit_rate) / initial_metrics.cache_hit_rate) * 100 if initial_metrics.cache_hit_rate > 0 else 0
                },
                "throughput": {
                    "before": initial_metrics.throughput,
                    "after": final_metrics.throughput,
                    "improvement": ((final_metrics.throughput - initial_metrics.throughput) / initial_metrics.throughput) * 100 if initial_metrics.throughput > 0 else 0
                },
                "concurrent_connections": {
                    "before": initial_metrics.concurrent_connections,
                    "after": final_metrics.concurrent_connections,
                    "improvement": ((final_metrics.concurrent_connections - initial_metrics.concurrent_connections) / initial_metrics.concurrent_connections) * 100 if initial_metrics.concurrent_connections > 0 else 0
                }
            },
            "detailed_results": [asdict(result) for result in results],
            "recommendations": self._generate_recommendations(final_metrics, results)
        }

        return report

    def _generate_recommendations(self, final_metrics: PerformanceMetrics, results: List[OptimizationResult]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºæœ€ç»ˆæ€§èƒ½æŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if final_metrics.api_response_time > 200:
            recommendations.append("å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–APIå“åº”æ—¶é—´ï¼Œè€ƒè™‘å¯ç”¨CDNæˆ–æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–")

        if final_metrics.memory_usage > 512:
            recommendations.append("å»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œè€ƒè™‘å¯ç”¨å†…å­˜åˆ†æå™¨è¯†åˆ«å†…å­˜æ³„æ¼")

        if final_metrics.cpu_usage > 70:
            recommendations.append("å»ºè®®ä¼˜åŒ–CPUä½¿ç”¨ï¼Œè€ƒè™‘å¯ç”¨æ€§èƒ½åˆ†æå™¨è¯†åˆ«CPUç“¶é¢ˆ")

        if final_metrics.cache_hit_rate < 90:
            recommendations.append("å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–ç¼“å­˜ç­–ç•¥ï¼Œè€ƒè™‘é¢„çƒ­ç¼“å­˜æˆ–è°ƒæ•´TTLè®¾ç½®")

        if final_metrics.error_rate > 1:
            recommendations.append("å»ºè®®é™ä½é”™è¯¯ç‡ï¼Œæ£€æŸ¥æ—¥å¿—è¯†åˆ«å¸¸è§é”™è¯¯æ¨¡å¼")

        # åŸºäºä¼˜åŒ–ç»“æœç”Ÿæˆå»ºè®®
        failed_optimizations = [r for r in results if not r.success]
        if failed_optimizations:
            recommendations.append(f"æœ‰ {len(failed_optimizations)} ä¸ªä¼˜åŒ–ç›®æ ‡æœªè¾¾æˆï¼Œå»ºè®®è¿›è¡Œæ·±å…¥åˆ†æ")

        return recommendations

    def save_optimization_report(self, report: dict) -> str:
        """ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"performance_optimization_report_{timestamp}.json"

        # ä¿å­˜åˆ°reportsç›®å½•
        reports_dir = self.project_root / "reports"
        performance_reports_dir = reports_dir / "performance"
        performance_reports_dir.mkdir(parents=True, exist_ok=True)

        report_path = performance_reports_dir / report_filename

        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"æ€§èƒ½ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
            return str(report_path)

        except Exception as e:
            logger.error(f"ä¿å­˜ä¼˜åŒ–æŠ¥å‘Šå¤±è´¥: {e}")
            raise

    def print_optimization_summary(self, report: dict):
        """æ‰“å°ä¼˜åŒ–æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸš€ ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š")
        print("="*80)

        summary = report.get("summary", {})
        print(f"ğŸ“Š ä¼˜åŒ–ç›®æ ‡æ€»æ•°: {summary.get('total_targets', 0)}")
        print(f"âœ… æˆåŠŸä¼˜åŒ–: {summary.get('successful_optimizations', 0)}")
        print(f"âŒ å¤±è´¥ä¼˜åŒ–: {summary.get('failed_optimizations', 0)}")
        print(f"ğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {summary.get('overall_success_rate', 0):.1f}%")

        print("\nğŸ¯ æ€§èƒ½æ”¹å–„æƒ…å†µ:")
        improvements = report.get("performance_improvements", {})

        for metric, improvement in improvements.items():
            before = improvement.get("before", 0)
            after = improvement.get("after", 0)
            change = improvement.get("improvement", 0)

            print(f"  {metric}: {before:.2f} â†’ {after:.2f} ({change:+.2f}%)")

        print("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        recommendations = report.get("recommendations", [])
        for i, rec in enumerate(recommendations, 1):
            print(f"  {i}. {rec}")

        print("\n" + "="*80)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è¶³çƒé¢„æµ‹ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å·¥å…·")
    print("="*50)

    optimizer = SystemPerformanceOptimizer()

    try:
        # è¿è¡Œä¼˜åŒ–å‘¨æœŸ
        report = await optimizer.run_optimization_cycle()

        if report["status"] == "completed":
            print("\nâœ… æ€§èƒ½ä¼˜åŒ–å®Œæˆ!")

            # ä¿å­˜æŠ¥å‘Š
            report_path = optimizer.save_optimization_report(report)
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

            # æ‰“å°æ‘˜è¦
            optimizer.print_optimization_summary(report)

        elif report["status"] == "optimal":
            print("\nğŸ‰ å½“å‰ç³»ç»Ÿæ€§èƒ½å·²è¾¾åˆ°æ‰€æœ‰ç›®æ ‡!")

        else:
            print(f"\nâŒ æ€§èƒ½ä¼˜åŒ–å¤±è´¥: {report.get('message', 'æœªçŸ¥é”™è¯¯')}")

    except KeyboardInterrupt:
        print("\nâš ï¸ ä¼˜åŒ–è¿‡ç¨‹è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"æ€§èƒ½ä¼˜åŒ–ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    asyncio.run(main())