#!/usr/bin/env python3
"""
MLOpsé¦–å¸­æ¶æ„å¸ˆä¸“ç”¨ - æ¯æ—¥æ•°æ®ç®¡é“ç¼–æ’å™¨
å®Œå…¨è‡ªåŠ¨åŒ–çš„ç«¯åˆ°ç«¯æ•°æ®å¤„ç†æµç¨‹
"""

import asyncio
import subprocess
import logging
import sys
import re
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Dict, List, Optional

# å¯¼å…¥æˆ‘ä»¬çš„æ¨¡å—
sys.path.append("/home/user/projects/FootballPrediction/src")
from ml_ops.auto_entity_resolver import AutoEntityResolver


def parse_postgres_output(output: str) -> int:
    """
    è§£æPostgreSQLå‘½ä»¤è¾“å‡ºï¼Œæå–æ•°å­—

    æ”¯æŒæ ¼å¼:
    - "202" (çº¯æ•°å­—)
    - "total_matches \n---------------\n           202\n(1 row)"
    - "UPDATE 202"
    - "INSERT 0 202"
    - "DELETE 202"
    """
    try:
        # æ–¹æ³•1: å°è¯•ç›´æ¥è½¬æ¢çº¯æ•°å­—
        stripped = output.strip()
        if stripped.isdigit():
            return int(stripped)

        # æ–¹æ³•2: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰æ•°å­—
        numbers = re.findall(r"\d+", output)
        if numbers:
            # å¯¹äºUPDATE/INSERTæ ¼å¼ï¼Œé€šå¸¸ç¬¬äºŒä¸ªæ•°å­—æ˜¯å—å½±å“çš„è¡Œæ•°
            if "UPDATE" in output.upper() or "INSERT" in output.upper():
                return int(numbers[-1]) if len(numbers) > 1 else int(numbers[0])
            else:
                # å¯¹äºå…¶ä»–æ ¼å¼ï¼Œå–ç¬¬ä¸€ä¸ªåˆç†çš„æ•°å­—ï¼ˆæ’é™¤è¡Œå·ç­‰ï¼‰
                for num in numbers:
                    if int(num) > 0 and int(num) < 100000:  # åˆç†çš„æ•°æ®èŒƒå›´
                        return int(num)
                return int(numbers[0])

        # æ–¹æ³•3: å¦‚æœéƒ½å¤±è´¥äº†ï¼Œæ‰“å°è­¦å‘Šå¹¶è¿”å›0
        logger.warning(f"âš ï¸ æ— æ³•è§£æPostgreSQLè¾“å‡º: {repr(output[:100])}")
        return 0

    except Exception as e:
        logger.warning(f"âš ï¸ PostgreSQLè¾“å‡ºè§£æå¼‚å¸¸: {e}, è¾“å‡º: {repr(output[:100])}")
        return 0


logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("logs/daily_pipeline.log"), logging.StreamHandler()],
)
logger = logging.getLogger(__name__)


class DailyPipelineOrchestrator:
    """æ¯æ—¥ç®¡é“ç¼–æ’å™¨"""

    def __init__(self):
        self.pipeline_start = datetime.now()
        self.stage_results = {}
        self.pipeline_stats = {
            "total_stages": 5,
            "completed_stages": 0,
            "failed_stages": 0,
            "new_teams_detected": 0,
            "matches_processed": 0,
            "features_generated": 0,
            "training_set_updated": False,
        }

    async def run_stage_extraction(self) -> bool:
        """Stage 1: æ•°æ®æå–"""
        logger.info("ğŸš€ Stage 1: æ•°æ®æå– (Data Extraction)")
        stage_start = datetime.now()

        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ•°æ®é‡‡é›†å™¨
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿé‡‡é›†è¿‡ç¨‹
            logger.info("ğŸ“¡ å¯åŠ¨æ•°æ®é‡‡é›†å™¨...")

            # æ¨¡æ‹Ÿé‡‡é›†æ˜¨æ—¥æ•°æ®
            target_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
            logger.info(f"ğŸ¯ ç›®æ ‡æ—¥æœŸ: {target_date}")

            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨ï¼š
            # from src.data.collectors.fotmob_browser_v2 import FotmobBrowserScraperV2
            # scraper = FotmobBrowserScraperV2()
            # await scraper.collect_daily_data(target_date)

            # æ¨¡æ‹Ÿé‡‡é›†ç»“æœ
            simulated_matches = [
                {
                    "date": target_date,
                    "home": "Team A",
                    "away": "Team B",
                    "score": "2-1",
                },
                {
                    "date": target_date,
                    "home": "Team C",
                    "away": "Team D",
                    "score": "1-1",
                },
            ]

            # ä¿å­˜æ¨¡æ‹Ÿæ•°æ®ç”¨äºåç»­å¤„ç†
            output_dir = Path("data/daily_extraction")
            output_dir.mkdir(parents=True, exist_ok=True)

            output_file = output_dir / f"daily_matches_{target_date}.json"
            with open(output_file, "w") as f:
                json.dump(simulated_matches, f, indent=2)

            duration = (datetime.now() - stage_start).total_seconds()
            self.stage_results["extraction"] = {
                "status": "success",
                "duration": duration,
                "matches_collected": len(simulated_matches),
                "target_date": target_date,
                "output_file": str(output_file),
            }

            logger.info(f"âœ… æ•°æ®æå–å®Œæˆ: {len(simulated_matches)} åœºæ¯”èµ›")
            logger.info(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®æå–å¤±è´¥: {e}")
            self.stage_results["extraction"] = {
                "status": "failed",
                "error": str(e),
                "duration": (datetime.now() - stage_start).total_seconds(),
            }
            return False

    async def run_stage_loading(self) -> bool:
        """Stage 2: æ•°æ®åŠ è½½"""
        logger.info("ğŸ“¥ Stage 2: æ•°æ®åŠ è½½ (Data Loading)")
        stage_start = datetime.now()

        try:
            logger.info("ğŸ”„ å¯åŠ¨æ•°æ®åŠ è½½æµç¨‹...")

            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒç”¨æ•°æ®ä¿å­˜å™¨
            # from src.database.fbref_database_saver import FBrefDatabaseSaver
            # saver = FBrefDatabaseSaver()
            # await saver.save_daily_data()

            # æ¨¡æ‹ŸåŠ è½½æ•°æ®åº“
            cmd = [
                "docker-compose",
                "exec",
                "db",
                "psql",
                "-U",
                "postgres",
                "-d",
                "football_prediction",
                "-c",
                """
                SELECT COUNT(*) as total_matches
                FROM matches
                WHERE DATE(created_at) = CURRENT_DATE - INTERVAL '1 day'
                """,
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                matches_loaded = parse_postgres_output(result.stdout)
                logger.info(f"ğŸ“Š PostgreSQLè¾“å‡ºè§£æ: {matches_loaded} åœºæ¯”èµ›")
            else:
                matches_loaded = 0
                logger.warning(f"âš ï¸ æ— æ³•æŸ¥è¯¢åŠ è½½çš„åŒ¹é…æ•°: {result.stderr}")

            duration = (datetime.now() - stage_start).total_seconds()
            self.stage_results["loading"] = {
                "status": "success",
                "duration": duration,
                "matches_loaded": matches_loaded,
            }

            self.pipeline_stats["matches_processed"] = matches_loaded
            logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {matches_loaded} åœºæ¯”èµ›")
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
            self.stage_results["loading"] = {
                "status": "failed",
                "error": str(e),
                "duration": (datetime.now() - stage_start).total_seconds(),
            }
            return False

    async def run_stage_transformation(self) -> bool:
        """Stage 3: æ•°æ®è½¬æ¢ (è‡ªåŠ¨å®ä½“è§£æ)"""
        logger.info("ğŸ”„ Stage 3: æ•°æ®è½¬æ¢ (Auto Entity Resolution)")
        stage_start = datetime.now()

        try:
            logger.info("ğŸ” å¯åŠ¨è‡ªåŠ¨å®ä½“è§£æ...")

            # åˆå§‹åŒ–è‡ªåŠ¨å®ä½“è§£æå™¨
            resolver = AutoEntityResolver()
            await resolver.load_existing_teams()

            # è·å–æ˜¨æ—¥æ¯”èµ›ä¸­çš„æ–°çƒé˜Ÿ (ä½¿ç”¨match_dateæ›¿ä»£created_at)
            cmd = [
                "docker-compose",
                "exec",
                "db",
                "psql",
                "-U",
                "postgres",
                "-d",
                "football_prediction",
                "-tAc",
                """
                SELECT DISTINCT home_team_name, away_team_name
                FROM view_match_features
                WHERE DATE(match_date) >= CURRENT_DATE - INTERVAL '7 day'
                """,
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.warning(f"âš ï¸ è·å–çƒé˜Ÿåˆ—è¡¨å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥: {result.stderr}")
                # é™çº§ç­–ç•¥ï¼šä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è¿›è¡Œå®ä½“è§£ææ¼”ç¤º
                team_list = ["Manchester United", "Liverpool", "Chelsea", "Arsenal"]
                logger.info(f"ğŸ“Š ä½¿ç”¨æ¨¡æ‹Ÿçƒé˜Ÿåˆ—è¡¨: {len(team_list)} ä¸ªçƒé˜Ÿ")
            else:
                # è§£æçƒé˜Ÿåˆ—è¡¨
                team_names = set()
                for line in result.stdout.strip().split("\n"):
                    if line.strip():
                        parts = [p.strip() for p in line.split("|")]
                        if len(parts) >= 2:
                            team_names.add(parts[0])
                            team_names.add(parts[1])

                team_list = list(team_names)
                logger.info(f"ğŸ“Š å‘ç° {len(team_list)} ä¸ªå”¯ä¸€çƒé˜Ÿ")

            # æ‰§è¡Œå®ä½“è§£æ
            resolution_results = await resolver.resolve_team_list(team_list)

            duration = (datetime.now() - stage_start).total_seconds()
            self.stage_results["transformation"] = {
                "status": "success",
                "duration": duration,
                "teams_processed": len(team_list),
                "resolution_stats": resolution_results["stats"],
            }

            self.pipeline_stats["new_teams_detected"] = resolution_results["stats"][
                "new_teams_detected"
            ]
            logger.info(
                f"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {resolution_results['stats']['new_teams_detected']} ä¸ªæ–°çƒé˜Ÿ"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è½¬æ¢å¤±è´¥: {e}")
            self.stage_results["transformation"] = {
                "status": "failed",
                "error": str(e),
                "duration": (datetime.now() - stage_start).total_seconds(),
            }
            return False

    async def run_stage_feature_engineering(self) -> bool:
        """Stage 4: ç‰¹å¾å·¥ç¨‹"""
        logger.info("âš™ï¸  Stage 4: ç‰¹å¾å·¥ç¨‹ (Feature Engineering)")
        stage_start = datetime.now()

        try:
            logger.info("ğŸ”§ å¯åŠ¨ç‰¹å¾å·¥ç¨‹æµç¨‹...")

            # è°ƒç”¨ç‰¹å¾æ„å»ºå™¨
            cmd = [sys.executable, "scripts/build_v1_dataset.py"]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {result.stderr}")
                return False

            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            dataset_path = Path("data/training_sets/v1_dataset.csv")
            if dataset_path.exists():
                import pandas as pd

                df = pd.read_csv(dataset_path)
                features_count = len([col for col in df.columns if "avg_" in col])
                total_rows = len(df)
            else:
                features_count = 0
                total_rows = 0

            duration = (datetime.now() - stage_start).total_seconds()
            self.stage_results["feature_engineering"] = {
                "status": "success",
                "duration": duration,
                "features_generated": features_count,
                "total_rows": total_rows,
                "dataset_path": str(dataset_path),
            }

            self.pipeline_stats["features_generated"] = features_count
            logger.info(
                f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: {total_rows} è¡Œæ•°æ®, {features_count} ä¸ªç‰¹å¾"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ ç‰¹å¾å·¥ç¨‹å¤±è´¥: {e}")
            self.stage_results["feature_engineering"] = {
                "status": "failed",
                "error": str(e),
                "duration": (datetime.now() - stage_start).total_seconds(),
            }
            return False

    async def run_stage_export(self) -> bool:
        """Stage 5: æ•°æ®å¯¼å‡º"""
        logger.info("ğŸ“¤ Stage 5: æ•°æ®å¯¼å‡º (Export)")
        stage_start = datetime.now()

        try:
            logger.info("ğŸ’¾ å¯åŠ¨æ•°æ®å¯¼å‡ºæµç¨‹...")

            # åˆ›å»ºç‰ˆæœ¬åŒ–çš„è®­ç»ƒæ•°æ®é›†
            version = datetime.now().strftime("%Y%m%d_%H%M%S")
            export_dir = Path("data/training_sets")
            export_dir.mkdir(parents=True, exist_ok=True)

            # å¤åˆ¶æœ€æ–°çš„æ•°æ®é›†
            source_dataset = export_dir / "v1_dataset.csv"
            if source_dataset.exists():
                versioned_dataset = export_dir / f"training_set_v1_{version}.csv"

                import shutil

                shutil.copy2(source_dataset, versioned_dataset)

                # åˆ›å»ºç¬¦å·é“¾æ¥åˆ°æœ€æ–°ç‰ˆæœ¬
                latest_link = export_dir / "latest_training_set.csv"
                if latest_link.exists():
                    latest_link.unlink()
                latest_link.symlink_to(versioned_dataset.name)

                duration = (datetime.now() - stage_start).total_seconds()
                self.stage_results["export"] = {
                    "status": "success",
                    "duration": duration,
                    "version": version,
                    "exported_file": str(versioned_dataset),
                    "latest_link": str(latest_link),
                }

                self.pipeline_stats["training_set_updated"] = True
                logger.info(f"âœ… æ•°æ®å¯¼å‡ºå®Œæˆ: {versioned_dataset}")
                logger.info(f"ğŸ”— æœ€æ–°ç‰ˆæœ¬é“¾æ¥: {latest_link}")
                return True
            else:
                logger.error("âŒ æºæ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨")
                return False

        except Exception as e:
            logger.error(f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {e}")
            self.stage_results["export"] = {
                "status": "failed",
                "error": str(e),
                "duration": (datetime.now() - stage_start).total_seconds(),
            }
            return False

    async def run_pipeline(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„æ¯æ—¥ç®¡é“"""
        logger.info("ğŸš€ å¯åŠ¨æ¯æ—¥æ•°æ®ç®¡é“")
        logger.info(f"ğŸ“… ç®¡é“å¯åŠ¨æ—¶é—´: {self.pipeline_start}")

        stages = [
            ("extraction", self.run_stage_extraction),
            ("loading", self.run_stage_loading),
            ("transformation", self.run_stage_transformation),
            ("feature_engineering", self.run_stage_feature_engineering),
            ("export", self.run_stage_export),
        ]

        success = True

        for stage_name, stage_func in stages:
            logger.info(f"ğŸ”„ æ‰§è¡Œé˜¶æ®µ: {stage_name}")

            try:
                stage_success = await stage_func()
                if stage_success:
                    self.pipeline_stats["completed_stages"] += 1
                    logger.info(f"âœ… é˜¶æ®µ {stage_name} å®Œæˆ")
                else:
                    self.pipeline_stats["failed_stages"] += 1
                    logger.error(f"âŒ é˜¶æ®µ {stage_name} å¤±è´¥")
                    # ç»§ç»­æ‰§è¡Œå…¶ä»–é˜¶æ®µï¼Œä¸è¦å› ä¸ºä¸€ä¸ªé˜¶æ®µå¤±è´¥å°±åœæ­¢
                    success = False

            except Exception as e:
                logger.error(f"ğŸ’¥ é˜¶æ®µ {stage_name} å¼‚å¸¸: {e}")
                self.pipeline_stats["failed_stages"] += 1
                success = False

        # ç”Ÿæˆç®¡é“æŠ¥å‘Š
        await self.generate_pipeline_report()

        total_duration = (datetime.now() - self.pipeline_start).total_seconds()

        logger.info("=" * 60)
        logger.info("ğŸ‰ æ¯æ—¥æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.1f}ç§’")
        logger.info(
            f"âœ… å®Œæˆé˜¶æ®µ: {self.pipeline_stats['completed_stages']}/{self.pipeline_stats['total_stages']}"
        )
        logger.info(f"âŒ å¤±è´¥é˜¶æ®µ: {self.pipeline_stats['failed_stages']}")
        logger.info(f"ğŸ†• æ–°çƒé˜Ÿ: {self.pipeline_stats['new_teams_detected']}")
        logger.info(f"âš½ å¤„ç†æ¯”èµ›: {self.pipeline_stats['matches_processed']}")
        logger.info(f"ğŸ”§ ç‰¹å¾æ•°é‡: {self.pipeline_stats['features_generated']}")
        logger.info(
            f"ğŸ“Š è®­ç»ƒé›†æ›´æ–°: {'æ˜¯' if self.pipeline_stats['training_set_updated'] else 'å¦'}"
        )

        return success

    async def generate_pipeline_report(self):
        """ç”Ÿæˆç®¡é“æ‰§è¡ŒæŠ¥å‘Š"""
        report_data = {
            "pipeline_start": self.pipeline_start.isoformat(),
            "pipeline_end": datetime.now().isoformat(),
            "stage_results": self.stage_results,
            "pipeline_stats": self.pipeline_stats,
        }

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        report_path = Path("logs/daily_pipeline_report.json")
        report_path.parent.mkdir(exist_ok=True)

        try:
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ç®¡é“æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜ç®¡é“æŠ¥å‘Šå¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        orchestrator = DailyPipelineOrchestrator()
        success = await orchestrator.run_pipeline()

        if success:
            logger.info("âœ… æ¯æ—¥æ•°æ®ç®¡é“æ‰§è¡ŒæˆåŠŸ")
            return 0
        else:
            logger.error("âŒ æ¯æ—¥æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ ç®¡é“æ‰§è¡Œå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
