#!/usr/bin/env python3
"""
è¶³çƒé¢„æµ‹è‡ªåŠ¨åŒ–ç®¡é“ / Football Prediction Automated Pipeline

è¯¥è„šæœ¬è‡ªåŠ¨æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹ï¼š
1. æ•°æ®åŒæ­¥ï¼šä»å¤–éƒ¨APIè·å–æœ€æ–°æ¯”èµ›æ•°æ®
2. æ•°æ®æ¸…æ´—ï¼šETLè¿‡ç¨‹å°†æ•°æ®å†™å…¥Silverå±‚
3. ç‰¹å¾æ›´æ–°ï¼šé‡æ–°è®¡ç®—ç‰¹å¾ç”Ÿæˆè®­ç»ƒæ•°æ®é›†
4. æ¨¡å‹é‡è®­ï¼šä½¿ç”¨æœ€æ–°æ•°æ®é‡æ–°è®­ç»ƒXGBoostæ¨¡å‹
5. é¢„æµ‹æœªæ¥ï¼šå¯¹æœªæ¥7å¤©çš„æ¯”èµ›è¿›è¡Œé¢„æµ‹

This script automates the complete data processing pipeline:
1. Data Sync: Fetch latest match data from external APIs
2. Data Cleaning: ETL process to write data to Silver layer
3. Feature Update: Recalculate features to generate training dataset
4. Model Retrain: Retrain XGBoost model with latest data
5. Future Predictions: Predict matches for the next 7 days

ä½¿ç”¨æ–¹æ³• / Usage:
    python scripts/daily_pipeline.py

å®šæ—¶ä»»åŠ¡è®¾ç½® / Cron Setup:
    # æ¯å¤©å‡Œæ™¨3ç‚¹è¿è¡Œ
    0 3 * * * /usr/bin/python3 /path/to/FootballPrediction/scripts/daily_pipeline.py >> /path/to/logs/pipeline.log 2>&1
"""

import asyncio
import logging
import os
import sys
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv

env_files = [
    project_root / ".env",
    project_root / ".env.local",
    project_root / ".env.development",
]

for env_file in env_files:
    if env_file.exists():
        load_dotenv(env_file)
        break

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    import pandas as pd
    import numpy as np
    import xgboost as xgb
    import json
    from src.data.collectors.fixtures_collector import FixturesCollector
    from src.data.processing.football_data_cleaner import FootballDataCleaner
    from src.features.simple_feature_calculator import SimpleFeatureCalculator
    from src.database.async_manager import get_db_session, initialize_database
    from src.database.models.raw_data import RawMatchData
    from src.database.models.match import Match
    from sqlalchemy import select
except ImportError:
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - [PIPELINE] - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DailyPipeline:
    """æ¯æ—¥è‡ªåŠ¨åŒ–ç®¡é“."""

    def __init__(self):
        """åˆå§‹åŒ–ç®¡é“."""
        self.start_time = datetime.now()
        self.pipeline_steps = 5
        self.current_step = 0
        self.errors = []

    def _get_target_seasons(self) -> list[int]:
        """
        è·å–ç›®æ ‡èµ›å­£åˆ—è¡¨.

        æ ¹æ®å½“å‰æ—¥æœŸæ™ºèƒ½åˆ¤æ–­å½“å‰èµ›å­£å’Œä¸Šä¸€èµ›å­£ï¼š
        - è¶³çƒèµ›å­£é€šå¸¸è·¨å¹´ï¼Œä»8æœˆå¼€å§‹åˆ°æ¬¡å¹´5æœˆç»“æŸ
        - å¦‚æœå½“å‰æœˆä»½ >= 7ï¼Œå½“å‰èµ›å­£ = å½“å‰å¹´ä»½
        - å¦‚æœå½“å‰æœˆä»½ < 7ï¼Œå½“å‰èµ›å­£ = å»å¹´å¹´ä»½
        - è¿”å› [å½“å‰èµ›å­£, ä¸Šä¸€èµ›å­£] ä»¥ä¿è¯æ•°æ®å®Œæ•´æ€§

        Returns:
            list[int]: ç›®æ ‡èµ›å­£åˆ—è¡¨ [current_season, previous_season]
        """
        current_date = datetime.now()
        current_month = current_date.month
        current_year = current_date.year

        # åˆ¤æ–­å½“å‰èµ›å­£
        if current_month >= 7:
            # 7æœˆåŠä»¥åï¼Œå½“å‰èµ›å­£ = å½“å‰å¹´ä»½
            current_season = current_year
        else:
            # 6æœˆåŠä»¥å‰ï¼Œå½“å‰èµ›å­£ = å»å¹´å¹´ä»½
            current_season = current_year - 1

        # è®¡ç®—ä¸Šä¸€èµ›å­£
        previous_season = current_season - 1

        target_seasons = [current_season, previous_season]

        logger.info("ğŸ—“ï¸  æ™ºèƒ½èµ›å­£åˆ¤æ–­:")
        logger.info(f"    å½“å‰æ—¥æœŸ: {current_date.strftime('%Y-%m-%d')}")
        logger.info(f"    å½“å‰èµ›å­£: {current_season}")
        logger.info(f"    ä¸Šä¸€èµ›å­£: {previous_season}")
        logger.info(f"    ç›®æ ‡èµ›å­£åˆ—è¡¨: {target_seasons}")

        return target_seasons

    def log_step(self, step_name: str, status: str = "START"):
        """è®°å½•ç®¡é“æ­¥éª¤.

        Args:
            step_name: æ­¥éª¤åç§°
            status: çŠ¶æ€ (START/COMPLETED/FAILED)
        """
        if status == "START":
            self.current_step += 1
            logger.info(
                f"[{self.current_step}/{self.pipeline_steps}] {step_name} - å¼€å§‹"
            )
        elif status == "COMPLETED":
            logger.info(
                f"[{self.current_step}/{self.pipeline_steps}] {step_name} - âœ… å®Œæˆ"
            )
        elif status == "FAILED":
            logger.error(
                f"[{self.current_step}/{self.pipeline_steps}] {step_name} - âŒ å¤±è´¥"
            )

    async def step_1_data_sync(self) -> bool:
        """æ­¥éª¤1ï¼šæ•°æ®åŒæ­¥ - è·å–æœ€æ–°æ¯”èµ›æ•°æ®ï¼ˆæ”¯æŒå¤šèµ›å­£æ™ºèƒ½é‡‡é›†ï¼‰."""
        step_name = "æ•°æ®åŒæ­¥ (Data Sync)"
        self.log_step(step_name, "START")

        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            initialize_database()
            logger.info("æ•°æ®åº“è¿æ¥åˆå§‹åŒ–æˆåŠŸ")

            # è·å–ç›®æ ‡èµ›å­£åˆ—è¡¨
            target_seasons = self._get_target_seasons()

            # åˆ›å»ºæ•°æ®é‡‡é›†å™¨
            collector = FixturesCollector(data_source="football_api")

            # å®šä¹‰ç›®æ ‡è”èµ›
            target_leagues = [
                "PL",
                "PD",
                "BL1",
                "SA",
                "FL1",
            ]  # æ¬§æ´²äº”å¤§è”èµ›ï¼šè‹±è¶…ã€è¥¿ç”²ã€å¾·ç”²ã€æ„ç”²ã€æ³•ç”²

            total_records_collected = 0
            total_success = 0
            total_errors = 0

            logger.info(f"ğŸ† å¼€å§‹å¤šèµ›å­£æ•°æ®é‡‡é›†ï¼Œç›®æ ‡è”èµ›: {target_leagues}")
            logger.info(f"ğŸ“… ç›®æ ‡èµ›å­£: {target_seasons}")

            # éå†æ¯ä¸ªèµ›å­£è¿›è¡Œé‡‡é›†
            for season in target_seasons:
                logger.info(f"ğŸ”„ æ­£åœ¨é‡‡é›† {season} èµ›å­£æ•°æ®...")

                try:
                    # é‡‡é›†å½“å‰èµ›å­£çš„æ‰€æœ‰è”èµ›æ•°æ®
                    season_result = await collector.collect_fixtures(
                        leagues=target_leagues, season=season
                    )

                    if season_result.success:
                        season_records = season_result.data.get("records_collected", 0)
                        total_records_collected += season_records
                        total_success += 1

                        logger.info(
                            f"âœ… {season} èµ›å­£æ•°æ®é‡‡é›†æˆåŠŸï¼Œæ”¶é›†åˆ° {season_records} æ¡è®°å½•"
                        )

                        # å¦‚æœæœ‰è¯¦ç»†çš„è”èµ›ç»Ÿè®¡ä¿¡æ¯ï¼Œä¹Ÿè®°å½•ä¸‹æ¥
                        if "league_stats" in season_result.data:
                            league_stats = season_result.data["league_stats"]
                            logger.info(f"ğŸ“Š {season} èµ›å­£è”èµ›ç»Ÿè®¡:")
                            for league, stats in league_stats.items():
                                logger.info(f"    - {league}: {stats}")
                    else:
                        total_errors += 1
                        error_msg = season_result.error or "æœªçŸ¥é”™è¯¯"
                        logger.error(f"âŒ {season} èµ›å­£æ•°æ®é‡‡é›†å¤±è´¥: {error_msg}")
                        self.errors.append(f"{season}èµ›å­£æ•°æ®é‡‡é›†å¤±è´¥: {error_msg}")

                except Exception:
                    total_errors += 1
                    logger.error(f"âŒ {season} èµ›å­£æ•°æ®é‡‡é›†å¼‚å¸¸: {e}")
                    self.errors.append(f"{season}èµ›å­£æ•°æ®é‡‡é›†å¼‚å¸¸: {str(e)}")

            # è¯„ä¼°æ•´ä½“é‡‡é›†ç»“æœ
            logger.info("=" * 60)
            logger.info("ğŸ“Š å¤šèµ›å­£é‡‡é›†ç»Ÿè®¡æ‘˜è¦")
            logger.info("=" * 60)
            logger.info(f"ğŸ¯ ç›®æ ‡èµ›å­£æ•°: {len(target_seasons)}")
            logger.info(f"âœ… æˆåŠŸé‡‡é›†èµ›å­£æ•°: {total_success}")
            logger.info(f"âŒ å¤±è´¥é‡‡é›†èµ›å­£æ•°: {total_errors}")
            logger.info(f"ğŸ“„ æ€»è®°å½•æ”¶é›†æ•°: {total_records_collected}")

            # åˆ¤æ–­æ•´ä½“æ˜¯å¦æˆåŠŸ
            if total_success > 0:
                self.log_step(step_name, "COMPLETED")
                logger.info(f"æ•°æ®åŒæ­¥æˆåŠŸï¼Œå…±é‡‡é›†åˆ° {total_records_collected} æ¡è®°å½•")
                return True
            else:
                self.log_step(step_name, "FAILED")
                self.errors.append("æ‰€æœ‰èµ›å­£æ•°æ®é‡‡é›†å‡å¤±è´¥")
                return False

        except Exception:
            self.log_step(step_name, "FAILED")
            error_msg = str(e)
            self.errors.append(f"æ•°æ®åŒæ­¥å¼‚å¸¸: {error_msg}")
            logger.error(f"æ•°æ®åŒæ­¥å¼‚å¸¸: {e}")
            return False

    async def step_2_data_cleaning(self) -> bool:
        """æ­¥éª¤2ï¼šæ•°æ®æ¸…æ´— - ETLåˆ°Silverå±‚."""
        step_name = "æ•°æ®æ¸…æ´— (ETL)"
        self.log_step(step_name, "START")

        try:
            # è¿™é‡Œå¤ç”¨ETLè„šæœ¬çš„é€»è¾‘
            from scripts.run_etl_silver import SilverETLProcessor

            processor = SilverETLProcessor()
            success = await processor.run_etl()

            if success:
                self.log_step(step_name, "COMPLETED")
                logger.info("æ•°æ®æ¸…æ´—å®Œæˆï¼Œæ•°æ®å·²å†™å…¥Silverå±‚")
                return True
            else:
                self.log_step(step_name, "FAILED")
                self.errors.append("æ•°æ®æ¸…æ´—å¤±è´¥")
                return False

        except Exception:
            self.log_step(step_name, "FAILED")
            error_msg = str(e)
            self.errors.append(f"æ•°æ®æ¸…æ´—å¼‚å¸¸: {error_msg}")
            logger.error(f"æ•°æ®æ¸…æ´—å¼‚å¸¸: {e}")
            return False

    def step_3_feature_generation(self) -> bool:
        """æ­¥éª¤3ï¼šç‰¹å¾ç”Ÿæˆ - æ›´æ–°ç‰¹å¾æ•°æ®é›†."""
        step_name = "ç‰¹å¾ç”Ÿæˆ (Feature Generation)"
        self.log_step(step_name, "START")

        try:
            # ä»æ•°æ®åº“åŠ è½½Silverå±‚æ•°æ®
            logger.info("ä»Silverå±‚æ•°æ®åº“åŠ è½½æ¯”èµ›æ•°æ®...")

            import psycopg2

            # ä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ DATABASE_URL
            db_url = os.getenv("DATABASE_URL")
            if not db_url:
                # å›é€€é€»è¾‘ï¼šä½¿ç”¨å•ç‹¬çš„ç¯å¢ƒå˜é‡
                db_user = os.getenv("POSTGRES_USER", "postgres")
                db_password = os.getenv("POSTGRES_PASSWORD", "football_prediction_2024")
                db_host = os.getenv("DB_HOST", "db")  # Dockeré‡Œæ˜¯ dbï¼Œä¸æ˜¯localhost
                db_port = os.getenv("DB_PORT", "5432")
                db_name = os.getenv("POSTGRES_DB", "football_prediction")
                db_url = f"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}"

            # Pandas éœ€è¦åŒæ­¥é©±åŠ¨ï¼Œç§»é™¤ asyncpg
            if "+asyncpg" in db_url:
                db_url = db_url.replace("+asyncpg", "")

            conn = psycopg2.connect(db_url)
            query = """
            SELECT
                m.id as match_id,
                m.home_team_id,
                m.away_team_id,
                m.home_score,
                m.away_score,
                m.status,
                m.match_date,
                t1.name as home_team_name,
                t2.name as away_team_name
            FROM matches m
            JOIN teams t1 ON m.home_team_id = t1.id
            JOIN teams t2 ON m.away_team_id = t2.id
            ORDER BY m.match_date ASC
            """

            matches_df = pd.read_sql_query(query, conn)
            conn.close()

            logger.info(f"åŠ è½½äº† {len(matches_df)} æ¡æ¯”èµ›è®°å½•")

            # ç”Ÿæˆç‰¹å¾
            calculator = SimpleFeatureCalculator(matches_df)
            features_df = calculator.generate_features_dataset()

            # ä¿å­˜ç‰¹å¾æ•°æ®
            features_df.to_csv("data/dataset_v1.csv", index=False)

            self.log_step(step_name, "COMPLETED")
            logger.info(f"ç‰¹å¾ç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆ {len(features_df)} æ¡ç‰¹å¾è®°å½•")
            return True

        except Exception:
            self.log_step(step_name, "FAILED")
            error_msg = str(e)
            self.errors.append(f"ç‰¹å¾ç”Ÿæˆå¼‚å¸¸: {error_msg}")
            logger.error(f"ç‰¹å¾ç”Ÿæˆå¼‚å¸¸: {e}")
            return False

    def step_4_model_training(self) -> bool:
        """æ­¥éª¤4ï¼šæ¨¡å‹è®­ç»ƒ - é‡è®­XGBoostæ¨¡å‹."""
        step_name = "æ¨¡å‹è®­ç»ƒ (Model Training)"
        self.log_step(step_name, "START")

        try:
            # åŠ è½½ç‰¹å¾æ•°æ®
            df = pd.read_csv("data/dataset_v1.csv")
            df["match_date"] = pd.to_datetime(df["match_date"])
            df = df.sort_values("match_date").reset_index(drop=True)

            # å®šä¹‰ç‰¹å¾åˆ—ï¼ˆåŒ…å«æ–°çš„é«˜çº§ç‰¹å¾ï¼‰
            feature_columns = [
                # åŸºç¡€ç‰¹å¾
                "home_team_id",
                "away_team_id",
                "home_last_5_points",
                "away_last_5_points",
                "home_last_5_avg_goals",
                "away_last_5_avg_goals",
                "h2h_last_3_home_wins",
                # é«˜çº§ç‰¹å¾ - ä½“èƒ½ã€å®åŠ›ã€å£«æ°”
                "home_last_5_goal_diff",
                "away_last_5_goal_diff",
                "home_win_streak",
                "away_win_streak",
                "home_last_5_win_rate",
                "away_last_5_win_rate",
                "home_rest_days",
                "away_rest_days",
            ]

            # å‡†å¤‡æ•°æ®
            X = df[feature_columns].copy()
            y = df["match_result"].copy()

            # æ—¶é—´åºåˆ—åˆ‡åˆ†
            split_point = int(len(X) * 0.8)
            X_train = X[:split_point]
            X_test = X[split_point:]
            y_train = y[:split_point]
            y_test = y[split_point:]

            # è®­ç»ƒXGBoostæ¨¡å‹
            params = {
                "objective": "multi:softmax",
                "num_class": 3,
                "max_depth": 6,
                "learning_rate": 0.1,
                "n_estimators": 100,
                "random_state": 42,
                "eval_metric": "mlogloss",
                "use_label_encoder": False,
            }

            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train)

            # è¯„ä¼°æ¨¡å‹
            y_pred = model.predict(X_test)
            accuracy = (y_pred == y_test).mean()

            # ä¿å­˜æ¨¡å‹
            os.makedirs("models", exist_ok=True)
            model.save_model("models/football_model_v1.json")

            # ä¿å­˜å…ƒæ•°æ®
            import json

            metadata = {
                "model_version": "v1",
                "training_date": datetime.now().isoformat(),
                "feature_names": feature_columns,
                "target_classes": ["å¹³å±€", "ä¸»é˜Ÿèƒœ", "å®¢é˜Ÿèƒœ"],
                "training_samples": len(X_train),
                "test_samples": len(X_test),
                "test_accuracy": float(accuracy),
                "num_features": len(feature_columns),
            }

            with open(
                "models/football_model_v1_metadata.json", "w", encoding="utf-8"
            ) as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            self.log_step(step_name, "COMPLETED")
            logger.info(
                f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œæµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy * 100:.2f}%)"
            )
            return True

        except Exception:
            self.log_step(step_name, "FAILED")
            error_msg = str(e)
            self.errors.append(f"æ¨¡å‹è®­ç»ƒå¼‚å¸¸: {error_msg}")
            logger.error(f"æ¨¡å‹è®­ç»ƒå¼‚å¸¸: {e}")
            return False

    def step_5_predict_future(self) -> bool:
        """æ­¥éª¤5ï¼šæœªæ¥é¢„æµ‹ - é¢„æµ‹æœªæ¥7å¤©çš„æ¯”èµ›."""
        step_name = "æœªæ¥é¢„æµ‹ (Future Predictions)"
        self.log_step(step_name, "START")

        try:
            # åŠ è½½æ¨¡å‹
            model = xgb.XGBClassifier()
            model.load_model("models/football_model_v1.json")

            # åŠ è½½å…ƒæ•°æ®
            with open("models/football_model_v1_metadata.json", encoding="utf-8") as f:
                metadata = json.load(f)

            feature_names = metadata["feature_names"]
            result_names = {0: "å¹³å±€", 1: "ä¸»é˜Ÿèƒœ", 2: "å®¢é˜Ÿèƒœ"}

            # ä»æ•°æ®åº“è·å–æœªæ¥æ¯”èµ›
            import psycopg2

            db_config = {
                "host": os.getenv("DB_HOST", "localhost"),
                "port": int(os.getenv("DB_PORT", 5432)),
                "database": os.getenv("DB_NAME", "football_prediction"),
                "user": os.getenv("DB_USER", "postgres"),
                "password": os.getenv("DB_PASSWORD", "postgres-dev-password"),
            }

            conn = psycopg2.connect(**db_config)

            # æŸ¥è¯¢æœªæ¥7å¤©å†…çš„æ¯”èµ›
            now = datetime.now()
            future_date = now + timedelta(days=7)

            query = """
            SELECT
                m.id,
                m.home_team_id,
                m.away_team_id,
                m.match_date,
                m.status,
                t1.name as home_team_name,
                t2.name as away_team_name,
                COALESCE(m.home_score, 0) as home_score,
                COALESCE(m.away_score, 0) as away_score
            FROM matches m
            JOIN teams t1 ON m.home_team_id = t1.id
            JOIN teams t2 ON m.away_team_id = t2.id
            WHERE m.match_date >= %s AND m.match_date <= %s AND m.status != 'FINISHED'
            ORDER BY m.match_date ASC
            """

            future_matches_df = pd.read_sql_query(
                query, conn, params=[now, future_date]
            )
            conn.close()

            if len(future_matches_df) == 0:
                logger.info("âš ï¸  æœªæ‰¾åˆ°æœªæ¥7å¤©å†…çš„æ¯”èµ›")
                logger.info("å¯èƒ½åŸå› :")
                logger.info("  1. å½“å‰èµ›å­£å·²ç»“æŸ")
                logger.info("  2. æ•°æ®åº“ä¸­æœªå®‰æ’æœªæ¥æ¯”èµ›")
                logger.info("  3. æ‰€æœ‰æœªæ¥æ¯”èµ›å·²å®Œæˆæˆ–å–æ¶ˆ")
                self.log_step(step_name, "COMPLETED")
                return True

            # ä¸ºæœªæ¥æ¯”èµ›ç”Ÿæˆç‰¹å¾
            # éœ€è¦ä»å†å²æ•°æ®è®¡ç®—ç‰¹å¾
            all_matches_df = pd.read_csv("data/dataset_v1.csv")
            all_matches_df["match_date"] = pd.to_datetime(all_matches_df["match_date"])

            # åˆå¹¶å†å²æ•°æ®å’Œæœªæ¥æ¯”èµ›æ•°æ®
            combined_df = pd.concat(
                [
                    all_matches_df[["home_team_id", "away_team_id", "match_date"]],
                    future_matches_df[["home_team_id", "away_team_id", "match_date"]],
                ],
                ignore_index=True,
            )
            combined_df = combined_df.sort_values("match_date").reset_index(drop=True)

            # è®¡ç®—ç‰¹å¾
            calculator = SimpleFeatureCalculator(all_matches_df)

            # ä¸ºæ¯åœºæœªæ¥æ¯”èµ›è®¡ç®—ç‰¹å¾
            future_features = []
            for _, match in future_matches_df.iterrows():
                try:
                    # ä½¿ç”¨calculatedæ–¹æ³•è®¡ç®—ç‰¹å¾
                    match.to_dict()

                    # è®¡ç®—ä¸»é˜Ÿè¿‘æœŸæˆ˜ç»©
                    home_points, home_avg_goals = (
                        calculator.calculate_team_recent_stats(
                            match["home_team_id"], match["match_date"]
                        )
                    )

                    # è®¡ç®—å®¢é˜Ÿè¿‘æœŸæˆ˜ç»©
                    away_points, away_avg_goals = (
                        calculator.calculate_team_recent_stats(
                            match["away_team_id"], match["match_date"]
                        )
                    )

                    # è®¡ç®—å†å²äº¤é”‹
                    h2h_wins = calculator.calculate_h2h_stats(
                        match["home_team_id"],
                        match["away_team_id"],
                        match["match_date"],
                    )

                    features = {
                        "home_team_id": match["home_team_id"],
                        "away_team_id": match["away_team_id"],
                        "home_last_5_points": home_points,
                        "away_last_5_points": away_points,
                        "home_last_5_avg_goals": home_avg_goals,
                        "away_last_5_avg_goals": away_avg_goals,
                        "h2h_last_3_home_wins": h2h_wins,
                        "match_date": match["match_date"],
                        "home_team_name": match["home_team_name"],
                        "away_team_name": match["away_team_name"],
                    }
                    future_features.append(features)

                except Exception:
                    logger.warning(f"è®¡ç®—æ¯”èµ› {match['id']} ç‰¹å¾å¤±è´¥: {e}")
                    continue

            if len(future_features) == 0:
                logger.warning("âš ï¸  æ— æ³•ä¸ºæœªæ¥æ¯”èµ›è®¡ç®—ç‰¹å¾")
                self.log_step(step_name, "COMPLETED")
                return True

            # è¿›è¡Œé¢„æµ‹
            features_df = pd.DataFrame(future_features)
            X_future = features_df[feature_names]
            predictions = model.predict(X_future)
            probabilities = model.predict_proba(X_future)

            # è¾“å‡ºé¢„æµ‹ç»“æœ
            logger.info("=" * 60)
            logger.info("ğŸ”® æœªæ¥æ¯”èµ›é¢„æµ‹ç»“æœ")
            logger.info("=" * 60)

            for i, (_, match) in enumerate(future_matches_df.iterrows()):
                pred = predictions[i]
                probs = probabilities[i]

                match_date = match["match_date"].strftime("%Y-%m-%d")
                home_team = match["home_team_name"]
                away_team = match["away_team_name"]

                logger.info(f"[{match_date}] {home_team} (ä¸») vs {away_team} (å®¢)")
                logger.info(f"é¢„æµ‹: {result_names[pred]}")
                logger.info(
                    f"æ¦‚ç‡: å¹³å±€ {probs[0]:.1%} | ä¸»èƒœ {probs[1]:.1%} | å®¢èƒœ {probs[2]:.1%}"
                )
                logger.info("-" * 50)

            logger.info(f"ğŸ“Š å…±é¢„æµ‹ {len(future_features)} åœºæœªæ¥æ¯”èµ›")

            self.log_step(step_name, "COMPLETED")
            return True

        except Exception:
            self.log_step(step_name, "FAILED")
            error_msg = str(e)
            self.errors.append(f"æœªæ¥é¢„æµ‹å¼‚å¸¸: {error_msg}")
            logger.error(f"æœªæ¥é¢„æµ‹å¼‚å¸¸: {e}")
            return False

    async def run(self) -> bool:
        """è¿è¡Œå®Œæ•´çš„ç®¡é“æµç¨‹.

        Returns:
            bool: ç®¡é“æ˜¯å¦æˆåŠŸå®Œæˆ
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ è¶³çƒé¢„æµ‹è‡ªåŠ¨åŒ–ç®¡é“å¯åŠ¨")
        logger.info(f"â° å¯åŠ¨æ—¶é—´: {self.start_time}")
        logger.info("=" * 60)

        try:
            # æ‰§è¡Œç®¡é“æ­¥éª¤
            step1_success = await self.step_1_data_sync()
            if not step1_success:
                logger.error("âŒ ç®¡é“åœ¨æ­¥éª¤ 1 å¤±è´¥")
                logger.error(f"é”™è¯¯åˆ—è¡¨: {self.errors}")
                return False

            step2_success = await self.step_2_data_cleaning()
            if not step2_success:
                logger.error("âŒ ç®¡é“åœ¨æ­¥éª¤ 2 å¤±è´¥")
                logger.error(f"é”™è¯¯åˆ—è¡¨: {self.errors}")
                return False

            step3_success = self.step_3_feature_generation()
            if not step3_success:
                logger.error("âŒ ç®¡é“åœ¨æ­¥éª¤ 3 å¤±è´¥")
                logger.error(f"é”™è¯¯åˆ—è¡¨: {self.errors}")
                return False

            step4_success = self.step_4_model_training()
            if not step4_success:
                logger.error("âŒ ç®¡é“åœ¨æ­¥éª¤ 4 å¤±è´¥")
                logger.error(f"é”™è¯¯åˆ—è¡¨: {self.errors}")
                return False

            step5_success = self.step_5_predict_future()
            if not step5_success:
                logger.error("âŒ ç®¡é“åœ¨æ­¥éª¤ 5 å¤±è´¥")
                logger.error(f"é”™è¯¯åˆ—è¡¨: {self.errors}")
                return False

            # è®¡ç®—æ€»è€—æ—¶
            end_time = datetime.now()
            duration = end_time - self.start_time

            logger.info("=" * 60)
            logger.info("ğŸ‰ ç®¡é“æ‰§è¡Œå®Œæˆï¼")
            logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration}")
            logger.info(f"âœ… æˆåŠŸæ‰§è¡Œæ‰€æœ‰ {self.pipeline_steps} ä¸ªæ­¥éª¤")
            logger.info("=" * 60)

            return True

        except Exception:
            logger.error(f"ğŸ’¥ ç®¡é“æ‰§è¡Œå¼‚å¸¸: {e}")
            self.errors.append(f"ç®¡é“å¼‚å¸¸: {str(e)}")
            return False


async def main():
    """ä¸»å‡½æ•°."""
    logger.info("ğŸˆ è¶³çƒé¢„æµ‹è‡ªåŠ¨åŒ–ç®¡é“å¯åŠ¨")

    try:
        pipeline = DailyPipeline()
        success = await pipeline.run()

        if success:
            logger.info("âœ… ç®¡é“æ‰§è¡ŒæˆåŠŸï¼")
            sys.exit(0)
        else:
            logger.error("âŒ ç®¡é“æ‰§è¡Œå¤±è´¥ï¼")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("â¹ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œç®¡é“åœæ­¢")
        sys.exit(1)
    except Exception:
        logger.error(f"ğŸ’¥ ç®¡é“å¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
