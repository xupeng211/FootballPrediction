#!/usr/bin/env python3
"""
æ–‡æ¡£è®¿é—®åˆ†æå’Œç»Ÿè®¡å·¥å…·
Documentation Analytics and Statistics Tool

è¿™ä¸ªè„šæœ¬æä¾›äº†æ–‡æ¡£è®¿é—®åˆ†æã€ä½¿ç”¨ç»Ÿè®¡å’Œè´¨é‡è¯„ä¼°åŠŸèƒ½ï¼Œ
å¸®åŠ©äº†è§£æ–‡æ¡£çš„ä½¿ç”¨æƒ…å†µå’Œæ”¹è¿›æ–¹å‘ã€‚

ä½œè€…: Claude AI Assistant
ç‰ˆæœ¬: v1.0.0
åˆ›å»ºæ—¶é—´: 2025-10-24
"""

import os
import sys
import json
import re
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from collections import Counter, defaultdict
import logging
import subprocess

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DocsAnalytics:
    """æ–‡æ¡£åˆ†æå™¨"""

    def __init__(self, docs_dir: str = "docs"):
        """
        åˆå§‹åŒ–æ–‡æ¡£åˆ†æå™¨

        Args:
            docs_dir: æ–‡æ¡£ç›®å½•è·¯å¾„
        """
        self.docs_dir = Path(docs_dir)
        self.site_dir = Path("site")
        self.analytics_data = {
            "analysis_time": datetime.now().isoformat(),
            "project_info": self._get_project_info(),
            "content_analysis": {},
            "structure_analysis": {},
            "quality_metrics": {},
            "usage_patterns": {},
            "recommendations": []
        }

    def _get_project_info(self) -> Dict[str, Any]:
        """è·å–é¡¹ç›®ä¿¡æ¯"""
        return {
            "name": self._get_project_name(),
            "description": "è¶³çƒé¢„æµ‹ç³»ç»Ÿæ–‡æ¡£",
            "version": "v2.0",
            "docs_directory": str(self.docs_dir),
            "total_size": self._get_directory_size(self.docs_dir)
        }

    def _get_project_name(self) -> str:
        """è·å–é¡¹ç›®åç§°"""
        try:
            if os.path.exists("pyproject.toml"):
                with open("pyproject.toml", 'r') as f:
                    content = f.read()
                    # ç®€å•çš„é¡¹ç›®åæå–
                    match = re.search(r'name\s*=\s*["\']([^"\']+)["\']', content)
                    if match:
                        return match.group(1)
        except Exception:
            pass
        return "FootballPrediction"

    def _get_directory_size(self, directory: Path) -> str:
        """è·å–ç›®å½•å¤§å°"""
        try:
            result = subprocess.run(
                ["du", "-sh", str(directory)],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                return result.stdout.split()[0]
        except Exception:
            pass
        return "æœªçŸ¥"

    def analyze_content(self) -> None:
        """åˆ†ææ–‡æ¡£å†…å®¹"""
        logger.info("ğŸ“Š åˆ†ææ–‡æ¡£å†…å®¹...")

        content_analysis = {
            "file_statistics": self._analyze_file_statistics(),
            "content_statistics": self._analyze_content_statistics(),
            "topic_analysis": self._analyze_topics(),
            "language_analysis": self._analyze_language(),
            "readability_analysis": self._analyze_readability()
        }

        self.analytics_data["content_analysis"] = content_analysis

    def _analyze_file_statistics(self) -> Dict[str, Any]:
        """åˆ†ææ–‡ä»¶ç»Ÿè®¡"""
        stats = {
            "total_files": 0,
            "total_size": 0,
            "file_types": defaultdict(int),
            "largest_files": [],
            "file_age_distribution": defaultdict(int)
        }

        try:
            all_files = []
            current_time = datetime.now()

            for file_path in self.docs_dir.rglob("*"):
                if file_path.is_file():
                    all_files.append(file_path)

            stats["total_files"] = len(all_files)

            # ç»Ÿè®¡æ–‡ä»¶ç±»å‹å’Œå¤§å°
            for file_path in all_files:
                size = file_path.stat().st_size
                stats["total_size"] += size
                stats["file_types"][file_path.suffix] += 1

                # ç»Ÿè®¡æ–‡ä»¶å¹´é¾„
                file_age_days = (current_time - datetime.fromtimestamp(file_path.stat().st_mtime)).days
                if file_age_days < 7:
                    stats["file_age_distribution"]["<1_week"] += 1
                elif file_age_days < 30:
                    stats["file_age_distribution"]["<1_month"] += 1
                elif file_age_days < 90:
                    stats["file_age_distribution"]["<3_months"] += 1
                else:
                    stats["file_age_distribution"][">3_months"] += 1

            # æ‰¾å‡ºæœ€å¤§çš„æ–‡ä»¶
            stats["largest_files"] = [
                {
                    "path": str(f.relative_to(self.docs_dir)),
                    "size": f.stat().st_size,
                    "size_human": self._format_size(f.stat().st_size)
                }
                for f in sorted(all_files, key=lambda x: x.stat().st_size, reverse=True)[:10]
            ]

            stats["total_size_human"] = self._format_size(stats["total_size"])

        except Exception as e:
            logger.error(f"æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {e}")

        return dict(stats)

    def _analyze_content_statistics(self) -> Dict[str, Any]:
        """åˆ†æå†…å®¹ç»Ÿè®¡"""
        stats = {
            "total_words": 0,
            "total_lines": 0,
            "total_characters": 0,
            "average_file_size": 0,
            "files_with_frontmatter": 0,
            "files_with_toc": 0,
            "files_with_code_blocks": 0,
            "files_with_images": 0,
            "files_with_tables": 0,
            "files_with_links": 0
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))
            file_sizes = []

            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    lines = content.split('\n')
                    words = content.split()

                    file_sizes.append(len(content))
                    stats["total_lines"] += len(lines)
                    stats["total_words"] += len(words)
                    stats["total_characters"] += len(content)

                    # æ£€æŸ¥æ–‡æ¡£ç»“æ„
                    if content.startswith('---'):
                        stats["files_with_frontmatter"] += 1

                    if '## ğŸ“‘ ç›®å½•' in content or '## ç›®å½•' in content:
                        stats["files_with_toc"] += 1

                    if '```' in content:
                        stats["files_with_code_blocks"] += 1

                    if '![' in content:
                        stats["files_with_images"] += 1

                    if '|' in content and '-' in content:
                        stats["files_with_tables"] += 1

                    if '[' in content and '](' in content:
                        stats["files_with_links"] += 1

                except Exception as e:
                    logger.warning(f"è¯»å–æ–‡ä»¶å¤±è´¥ {md_file}: {e}")

            if file_sizes:
                stats["average_file_size"] = sum(file_sizes) / len(file_sizes)
                stats["average_file_size_human"] = self._format_size(int(stats["average_file_size"]))

            stats["total_files_analyzed"] = len(md_files)

        except Exception as e:
            logger.error(f"å†…å®¹ç»Ÿè®¡å¤±è´¥: {e}")

        return dict(stats)

    def _analyze_topics(self) -> Dict[str, Any]:
        """åˆ†æä¸»é¢˜åˆ†å¸ƒ"""
        topics = {
            "architecture": 0,
            "development": 0,
            "testing": 0,
            "deployment": 0,
            "security": 0,
            "monitoring": 0,
            "database": 0,
            "api": 0,
            "ml": 0,
            "data": 0
        }

        topic_keywords = {
            "architecture": ["æ¶æ„", "architecture", "ç³»ç»Ÿ", "è®¾è®¡", "æ¨¡å—"],
            "development": ["å¼€å‘", "development", "ä»£ç ", "ç¼–ç¨‹", "å¼€å‘"],
            "testing": ["æµ‹è¯•", "test", "æµ‹è¯•", "å•å…ƒ", "é›†æˆ"],
            "deployment": ["éƒ¨ç½²", "deployment", "å‘å¸ƒ", "ä¸Šçº¿", "ç”Ÿäº§"],
            "security": ["å®‰å…¨", "security", "è®¤è¯", "æˆæƒ", "åŠ å¯†"],
            "monitoring": ["ç›‘æ§", "monitoring", "æ—¥å¿—", "å‘Šè­¦", "æŒ‡æ ‡"],
            "database": ["æ•°æ®åº“", "database", "db", "sql", "å­˜å‚¨"],
            "api": ["API", "api", "æ¥å£", "endpoint", "è·¯ç”±"],
            "ml": ["æœºå™¨å­¦ä¹ ", "ML", "æ¨¡å‹", "è®­ç»ƒ", "é¢„æµ‹"],
            "data": ["æ•°æ®", "data", "æ•°æ®é‡‡é›†", "å¤„ç†", "ç‰¹å¾"]
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))

            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read().lower()

                    # è®¡ç®—ä¸»é¢˜å¾—åˆ†
                    for topic, keywords in topic_keywords.items():
                        score = sum(1 for keyword in keywords if keyword in content)
                        if score > 0:
                            topics[topic] += score

                except Exception as e:
                    logger.warning(f"åˆ†æä¸»é¢˜å¤±è´¥ {md_file}: {e}")

            # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            total_topics = sum(topics.values())
            if total_topics > 0:
                topic_distribution = {
                    topic: (count / total_topics) * 100
                    for topic, count in topics.items()
                }
            else:
                topic_distribution = topics

        except Exception as e:
            logger.error(f"ä¸»é¢˜åˆ†æå¤±è´¥: {e}")
            topic_distribution = topics

        return {
            "topic_counts": topics,
            "topic_distribution": topic_distribution,
            "dominant_topic": max(topics.items(), key=lambda x: x[1])[0] if topics else None
        }

    def _analyze_language(self) -> Dict[str, Any]:
        """åˆ†æè¯­è¨€ä½¿ç”¨"""
        language_stats = {
            "chinese_characters": 0,
            "english_words": 0,
            "mixed_language_files": 0,
            "chinese_only_files": 0,
            "english_only_files": 0,
            "language_ratio": 0.0
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))

            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # è®¡ç®—ä¸­æ–‡å­—ç¬¦
                    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', content))
                    # è®¡ç®—è‹±æ–‡å•è¯
                    english_words = len(re.findall(r'\b[a-zA-Z]+\b', content))

                    language_stats["chinese_characters"] += chinese_chars
                    language_stats["english_words"] += english_words

                    if chinese_chars > 0 and english_words > 0:
                        language_stats["mixed_language_files"] += 1
                    elif chinese_chars > 0:
                        language_stats["chinese_only_files"] += 1
                    elif english_words > 0:
                        language_stats["english_only_files"] += 1

                except Exception as e:
                    logger.warning(f"è¯­è¨€åˆ†æå¤±è´¥ {md_file}: {e}")

            total = language_stats["chinese_characters"] + language_stats["english_words"]
            if total > 0:
                language_stats["language_ratio"] = {
                    "chinese": (language_stats["chinese_characters"] / total) * 100,
                    "english": (language_stats["english_words"] / total) * 100
                }

        except Exception as e:
            logger.error(f"è¯­è¨€åˆ†æå¤±è´¥: {e}")

        return language_stats

    def _analyze_readability(self) -> Dict[str, Any]:
        """åˆ†æå¯è¯»æ€§"""
        readability = {
            "average_words_per_paragraph": 0,
            "average_sentences_per_paragraph": 0,
            "average_words_per_sentence": 0,
            "headings_per_file": 0,
            "files_with_multiple_headings": 0,
            "complex_files": 0
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))
            paragraph_counts = []
            sentence_counts = []
            word_counts = []
            heading_counts = []

            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # ç»Ÿè®¡æ ‡é¢˜
                    headings = re.findall(r'^#+\s+', content, re.MULTILINE)
                    heading_counts.append(len(headings))
                    if len(headings) > 5:
                        readability["files_with_multiple_headings"] += 1

                    # ç®€å•çš„å¯è¯»æ€§åˆ†æ
                    words = content.split()
                    sentences = re.split(r'[.!?]+', content)
                    paragraphs = content.split('\n\n')

                    word_counts.append(len(words))
                    sentence_counts.append(len([s for s in sentences if s.strip()]))
                    paragraph_counts.append(len([p for p in paragraphs if p.strip()]))

                    # æ ‡è®°å¤æ‚æ–‡ä»¶ï¼ˆè¶…è¿‡500ä¸ªå•è¯ä¸”å°‘äº3ä¸ªæ ‡é¢˜ï¼‰
                    if len(words) > 500 and len(headings) < 3:
                        readability["complex_files"] += 1

                except Exception as e:
                    logger.warning(f"å¯è¯»æ€§åˆ†æå¤±è´¥ {md_file}: {e}")

            if paragraph_counts:
                readability["average_words_per_paragraph"] = sum(word_counts) / sum(paragraph_counts)
                readability["average_sentences_per_paragraph"] = sum(sentence_counts) / sum(paragraph_counts)
                readability["average_words_per_sentence"] = sum(word_counts) / sum(sentence_counts)

            if heading_counts:
                readability["headings_per_file"] = sum(heading_counts) / len(heading_counts)

        except Exception as e:
            logger.error(f"å¯è¯»æ€§åˆ†æå¤±è´¥: {e}")

        return readability

    def analyze_structure(self) -> None:
        """åˆ†ææ–‡æ¡£ç»“æ„"""
        logger.info("ğŸ—ï¸ åˆ†ææ–‡æ¡£ç»“æ„...")

        structure_analysis = {
            "directory_structure": self._analyze_directory_structure(),
            "navigation_structure": self._analyze_navigation_structure(),
            "cross_references": self._analyze_cross_references(),
            "depth_analysis": self._analyze_depth()
        }

        self.analytics_data["structure_analysis"] = structure_analysis

    def _analyze_directory_structure(self) -> Dict[str, Any]:
        """åˆ†æç›®å½•ç»“æ„"""
        structure = {
            "total_directories": 0,
            "max_depth": 0,
            "directory_distribution": defaultdict(int),
            "empty_directories": [],
            "large_directories": []
        }

        try:
            for root, dirs, files in os.walk(self.docs_dir):
                depth = root.count(os.sep) - str(self.docs_dir).count(os.sep)
                structure["max_depth"] = max(structure["max_depth"], depth)
                structure["total_directories"] += 1
                structure["directory_distribution"][f"depth_{depth}"] += 1

                # æ£€æŸ¥ç©ºç›®å½•
                if not dirs and not files:
                    structure["empty_directories"].append(root)

                # æ£€æŸ¥å¤§ç›®å½•ï¼ˆæ–‡ä»¶æ•°>10ï¼‰
                if len(files) > 10:
                    structure["large_directories"].append({
                        "path": root,
                        "file_count": len(files)
                    })

        except Exception as e:
            logger.error(f"ç›®å½•ç»“æ„åˆ†æå¤±è´¥: {e}")

        return dict(structure)

    def _analyze_navigation_structure(self) -> Dict[str, Any]:
        """åˆ†æå¯¼èˆªç»“æ„"""
        navigation = {
            "has_index": False,
            "has_readme": False,
            "nav_sections": 0,
            "nav_links": 0,
            "orphaned_files": []
        }

        try:
            # æ£€æŸ¥ä¸»è¦å¯¼èˆªæ–‡ä»¶
            index_file = self.docs_dir / "INDEX.md"
            readme_file = self.docs_dir / "README.md"

            navigation["has_index"] = index_file.exists()
            navigation["has_readme"] = readme_file.exists()

            if index_file.exists():
                with open(index_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ç»Ÿè®¡å¯¼èˆªé“¾æ¥
                links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
                navigation["nav_links"] = len(links)

                # ç»Ÿè®¡å¯¼èˆªç« èŠ‚
                sections = re.findall(r'^#+\s+', content, re.MULTILINE)
                navigation["nav_sections"] = len(sections)

            # æ£€æŸ¥å­¤ç«‹æ–‡ä»¶
            all_md_files = set(self.docs_dir.rglob("*.md"))
            linked_files = set()

            for md_file in all_md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # æŸ¥æ‰¾å†…éƒ¨é“¾æ¥
                    internal_links = re.findall(r'\[([^\]]+)\]\(([^)]+\.md)\)', content)
                    for link in internal_links:
                        linked_path = self.docs_dir / link[1]
                        if linked_path.exists():
                            linked_files.add(linked_path)

                except Exception:
                    pass

            orphaned = all_md_files - linked_files
            navigation["orphaned_files"] = [str(f.relative_to(self.docs_dir)) for f in orphaned if f != index_file]

        except Exception as e:
            logger.error(f"å¯¼èˆªç»“æ„åˆ†æå¤±è´¥: {e}")

        return navigation

    def _analyze_cross_references(self) -> Dict[str, Any]:
        """åˆ†æäº¤å‰å¼•ç”¨"""
        references = {
            "total_references": 0,
            "internal_references": 0,
            "external_references": 0,
            "reference_density": 0.0,
            "well_connected_files": [],
            "poorly_connected_files": []
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))
            file_connections = defaultdict(int)

            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
                    links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
                    references["total_references"] += len(links)

                    for link in links:
                        url = link[1]
                        if url.startswith('http'):
                            references["external_references"] += 1
                        else:
                            references["internal_references"] += 1

                    file_connections[md_file] = len(links)

                except Exception:
                    pass

            # è®¡ç®—å¼•ç”¨å¯†åº¦
            if len(md_files) > 0:
                references["reference_density"] = references["total_references"] / len(md_files)

            # æ‰¾å‡ºè¿æ¥è‰¯å¥½çš„æ–‡ä»¶ï¼ˆ>10ä¸ªé“¾æ¥ï¼‰
            references["well_connected_files"] = [
                str(f.relative_to(self.docs_dir))
                for f, count in file_connections.items()
                if count > 10
            ]

            # æ‰¾å‡ºè¿æ¥ä¸ä½³çš„æ–‡ä»¶ï¼ˆ<2ä¸ªé“¾æ¥ï¼‰
            references["poorly_connected_files"] = [
                str(f.relative_to(self.docs_dir))
                for f, count in file_connections.items()
                if count < 2 and f.name != "INDEX.md"
            ]

        except Exception as e:
            logger.error(f"äº¤å‰å¼•ç”¨åˆ†æå¤±è´¥: {e}")

        return references

    def _analyze_depth(self) -> Dict[str, Any]:
        """åˆ†ææ–‡æ¡£æ·±åº¦"""
        depth_analysis = {
            "average_depth": 0.0,
            "max_depth": 0,
            "depth_distribution": defaultdict(int),
            "deep_files": []
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))
            depths = []

            for md_file in md_files:
                # è®¡ç®—ç›¸å¯¹æ·±åº¦
                relative_depth = len(md_file.relative_to(self.docs_dir).parts) - 1
                depths.append(relative_depth)
                depth_analysis["depth_distribution"][f"depth_{relative_depth}"] += 1

                # è®°å½•æ·±å±‚æ–‡ä»¶ï¼ˆæ·±åº¦>3ï¼‰
                if relative_depth > 3:
                    depth_analysis["deep_files"].append(str(md_file.relative_to(self.docs_dir)))

            if depths:
                depth_analysis["average_depth"] = sum(depths) / len(depths)
                depth_analysis["max_depth"] = max(depths)

        except Exception as e:
            logger.error(f"æ·±åº¦åˆ†æå¤±è´¥: {e}")

        return depth_analysis

    def analyze_quality(self) -> None:
        """åˆ†ææ–‡æ¡£è´¨é‡"""
        logger.info("ğŸ“ˆ åˆ†ææ–‡æ¡£è´¨é‡...")

        quality_metrics = {
            "completeness_score": self._calculate_completeness_score(),
            "consistency_score": self._calculate_consistency_score(),
            "maintenance_score": self._calculate_maintenance_score(),
            "accessibility_score": self._calculate_accessibility_score(),
            "overall_quality_score": 0.0,
            "quality_issues": []
        }

        # è®¡ç®—æ€»åˆ†
        scores = [
            quality_metrics["completeness_score"],
            quality_metrics["consistency_score"],
            quality_metrics["maintenance_score"],
            quality_metrics["accessibility_score"]
        ]
        quality_metrics["overall_quality_score"] = sum(scores) / len(scores)

        self.analytics_data["quality_metrics"] = quality_metrics

    def _calculate_completeness_score(self) -> float:
        """è®¡ç®—å®Œæ•´æ€§å¾—åˆ†"""
        score = 0.0
        total_checks = 0

        try:
            # æ£€æŸ¥æ ¸å¿ƒæ–‡ä»¶
            core_files = [
                "INDEX.md",
                "README.md",
                "architecture/ARCHITECTURE.md",
                "reference/API_REFERENCE.md",
                "testing/TEST_IMPROVEMENT_GUIDE.md"
            ]

            for file_name in core_files:
                total_checks += 1
                if (self.docs_dir / file_name).exists():
                    score += 1.0

            # æ£€æŸ¥æ–‡æ¡£è¦†ç›–ç‡
            md_files = list(self.docs_dir.rglob("*.md"))
            if len(md_files) >= 10:  # è‡³å°‘10ä¸ªæ–‡æ¡£æ–‡ä»¶
                score += 1.0
            total_checks += 1

            # æ£€æŸ¥å¤šè¯­è¨€æ”¯æŒ
            if self.analytics_data.get("content_analysis", {}).get("language_analysis", {}).get("mixed_language_files", 0) > 0:
                score += 1.0
            total_checks += 1

        except Exception as e:
            logger.error(f"å®Œæ•´æ€§è¯„åˆ†å¤±è´¥: {e}")

        return (score / total_checks * 100) if total_checks > 0 else 0.0

    def _calculate_consistency_score(self) -> float:
        """è®¡ç®—ä¸€è‡´æ€§å¾—åˆ†"""
        score = 0.0
        total_checks = 0

        try:
            md_files = list(self.docs_dir.rglob("*.md"))

            # æ£€æŸ¥æ ‡é¢˜æ ¼å¼ä¸€è‡´æ€§
            consistent_titles = 0
            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    if re.search(r'^# ', content, re.MULTILINE):
                        consistent_titles += 1
                except Exception:
                    pass

            if len(md_files) > 0:
                title_consistency = consistent_titles / len(md_files)
                score += title_consistency
                total_checks += 1

            # æ£€æŸ¥é“¾æ¥æ ¼å¼ä¸€è‡´æ€§
            consistent_links = 0
            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
                    if links:
                        consistent_links += 1
                except Exception:
                    pass

            if len(md_files) > 0:
                link_consistency = consistent_links / len(md_files)
                score += link_consistency
                total_checks += 1

        except Exception as e:
            logger.error(f"ä¸€è‡´æ€§è¯„åˆ†å¤±è´¥: {e}")

        return (score / total_checks * 100) if total_checks > 0 else 0.0

    def _calculate_maintenance_score(self) -> float:
        """è®¡ç®—ç»´æŠ¤æ€§å¾—åˆ†"""
        score = 0.0
        total_checks = 0

        try:
            # æ£€æŸ¥æ–‡ä»¶æ›´æ–°æ—¶é—´
            md_files = list(self.docs_dir.rglob("*.md"))
            recent_files = 0
            current_time = datetime.now()

            for md_file in md_files:
                file_time = datetime.fromtimestamp(md_file.stat().st_mtime)
                if (current_time - file_time).days < 30:  # 30å¤©å†…æ›´æ–°
                    recent_files += 1

            if len(md_files) > 0:
                freshness_score = recent_files / len(md_files)
                score += freshness_score
                total_checks += 1

            # æ£€æŸ¥æ–‡æ¡£ç»“æ„æ¸…æ™°åº¦
            structure_score = min(len(self.docs_dir.rglob("*/")) / 10, 1.0)  # ç›®å½•æ•°é‡/10
            score += structure_score
            total_checks += 1

        except Exception as e:
            logger.error(f"ç»´æŠ¤æ€§è¯„åˆ†å¤±è´¥: {e}")

        return (score / total_checks * 100) if total_checks > 0 else 0.0

    def _calculate_accessibility_score(self) -> float:
        """è®¡ç®—å¯è®¿é—®æ€§å¾—åˆ†"""
        score = 0.0
        total_checks = 0

        try:
            md_files = list(self.docs_dir.rglob("*.md"))

            # æ£€æŸ¥ç›®å½•å­˜åœ¨æ€§
            files_with_toc = 0
            for md_file in md_files:
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    if '## ğŸ“‘ ç›®å½•' in content or '## ç›®å½•' in content:
                        files_with_toc += 1
                except Exception:
                    pass

            if len(md_files) > 0:
                toc_score = files_with_toc / len(md_files)
                score += toc_score
                total_checks += 1

            # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶
            if (self.docs_dir / "INDEX.md").exists():
                score += 1.0
            total_checks += 1

        except Exception as e:
            logger.error(f"å¯è®¿é—®æ€§è¯„åˆ†å¤±è´¥: {e}")

        return (score / total_checks * 100) if total_checks > 0 else 0.0

    def generate_recommendations(self) -> None:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        logger.info("ğŸ’¡ ç”Ÿæˆæ”¹è¿›å»ºè®®...")

        recommendations = []

        # åŸºäºå†…å®¹åˆ†æçš„å»ºè®®
        content_stats = self.analytics_data.get("content_analysis", {})

        if content_stats.get("content_statistics", {}).get("files_with_toc", 0) < len(list(self.docs_dir.rglob("*.md"))) * 0.5:
            recommendations.append({
                "type": "structure",
                "priority": "high",
                "title": "å¢åŠ æ–‡æ¡£ç›®å½•",
                "description": "è¶…è¿‡50%çš„æ–‡æ¡£ç¼ºå°‘ç›®å½•ï¼Œå»ºè®®ä¸ºé‡è¦æ–‡æ¡£æ·»åŠ ç›®å½•ä»¥æé«˜å¯è¯»æ€§"
            })

        if content_stats.get("content_statistics", {}).get("files_with_code_blocks", 0) < 5:
            recommendations.append({
                "type": "content",
                "priority": "medium",
                "title": "å¢åŠ ä»£ç ç¤ºä¾‹",
                "description": "æ–‡æ¡£ä¸­ç¼ºå°‘ä»£ç ç¤ºä¾‹ï¼Œå»ºè®®æ·»åŠ å®é™…çš„ä»£ç ç¤ºä¾‹å’Œç”¨æ³•è¯´æ˜"
            })

        # åŸºäºç»“æ„åˆ†æçš„å»ºè®®
        structure_stats = self.analytics_data.get("structure_analysis", {})

        orphaned_files = structure_stats.get("navigation_structure", {}).get("orphaned_files", [])
        if len(orphaned_files) > 3:
            recommendations.append({
                "type": "navigation",
                "priority": "high",
                "title": "å‡å°‘å­¤ç«‹æ–‡æ¡£",
                "description": f"å‘ç°{len(orphaned_files)}ä¸ªå­¤ç«‹æ–‡æ¡£ï¼Œå»ºè®®åœ¨ç´¢å¼•é¡µé¢æˆ–ç›¸å…³æ–‡æ¡£ä¸­æ·»åŠ é“¾æ¥"
            })

        # åŸºäºè´¨é‡åˆ†æçš„å»ºè®®
        quality_metrics = self.analytics_data.get("quality_metrics", {})

        if quality_metrics.get("overall_quality_score", 0) < 70:
            recommendations.append({
                "type": "quality",
                "priority": "high",
                "title": "æå‡æ–‡æ¡£è´¨é‡",
                "description": f"å½“å‰æ–‡æ¡£è´¨é‡å¾—åˆ†ä¸º{quality_metrics.get('overall_quality_score', 0):.1f}ï¼Œå»ºè®®ç³»ç»Ÿæ€§æ”¹è¿›æ–‡æ¡£è´¨é‡"
            })

        # åŸºäºè¯­è¨€åˆ†æçš„å»ºè®®
        language_stats = content_stats.get("language_analysis", {})
        if language_stats.get("mixed_language_files", 0) == 0:
            recommendations.append({
                "type": "internationalization",
                "priority": "medium",
                "title": "è€ƒè™‘å¤šè¯­è¨€æ”¯æŒ",
                "description": "å½“å‰æ–‡æ¡£ä¸ºå•ä¸€è¯­è¨€ï¼Œè€ƒè™‘æ·»åŠ è‹±æ–‡ç‰ˆæœ¬ä»¥æ”¯æŒå›½é™…ç”¨æˆ·"
            })

        # åŸºäºäº¤å‰å¼•ç”¨åˆ†æçš„å»ºè®®
        cross_refs = structure_stats.get("cross_references", {})
        if cross_refs.get("reference_density", 0) < 5:
            recommendations.append({
                "type": "connectivity",
                "priority": "medium",
                "title": "å¢åŠ æ–‡æ¡£é“¾æ¥",
                "description": f"æ–‡æ¡£é“¾æ¥å¯†åº¦è¾ƒä½({cross_refs.get('reference_density', 0):.1f})ï¼Œå»ºè®®å¢åŠ ç›¸å…³æ–‡æ¡£çš„äº¤å‰å¼•ç”¨"
            })

        self.analytics_data["recommendations"] = recommendations

    def _format_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024
        return f"{size_bytes:.1f} TB"

    def save_analysis(self, output_path: Optional[str] = None) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"docs-analytics-report-{timestamp}.json"

        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.analytics_data, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"ğŸ“Š åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_path}")
            return output_path

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            raise

    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        report = []
        report.append("# ğŸ“Š æ–‡æ¡£åˆ†ææŠ¥å‘Š")
        report.append("")
        report.append(f"**åˆ†ææ—¶é—´**: {self.analytics_data['analysis_time']}")
        report.append(f"**é¡¹ç›®åç§°**: {self.analytics_data['project_info']['name']}")
        report.append("")
        report.append("## ğŸ“‹ æ ¸å¿ƒæŒ‡æ ‡")
        report.append("")

        # å†…å®¹ç»Ÿè®¡
        content_stats = self.analytics_data.get("content_analysis", {})
        if content_stats:
            file_stats = content_stats.get("file_statistics", {})
            content_stats_data = content_stats.get("content_statistics", {})

            report.append("### ğŸ“„ æ–‡æ¡£ç»Ÿè®¡")
            report.append(f"- **æ€»æ–‡ä»¶æ•°**: {file_stats.get('total_files', 0)}")
            report.append(f"- **æ€»å¤§å°**: {file_stats.get('total_size_human', 'æœªçŸ¥')}")
            report.append(f"- **æ€»å­—æ•°**: {content_stats_data.get('total_words', 0):,}")
            report.append(f"- **æ€»è¡Œæ•°**: {content_stats_data.get('total_lines', 0):,}")
            report.append("")

        # ç»“æ„ç»Ÿè®¡
        structure_stats = self.analytics_data.get("structure_analysis", {})
        if structure_stats:
            nav_stats = structure_stats.get("navigation_structure", {})
            report.append("### ğŸ—ï¸ ç»“æ„åˆ†æ")
            report.append(f"- **ç´¢å¼•æ–‡ä»¶**: {'âœ…' if nav_stats.get('has_index') else 'âŒ'}")
            report.append(f"- **å¯¼èˆªé“¾æ¥**: {nav_stats.get('nav_links', 0)}")
            report.append(f"- **å­¤ç«‹æ–‡ä»¶**: {len(nav_stats.get('orphaned_files', []))}")
            report.append("")

        # è´¨é‡è¯„ä¼°
        quality_metrics = self.analytics_data.get("quality_metrics", {})
        if quality_metrics:
            report.append("### ğŸ“ˆ è´¨é‡è¯„ä¼°")
            report.append(f"- **æ•´ä½“å¾—åˆ†**: {quality_metrics.get('overall_quality_score', 0):.1f}/100")
            report.append(f"- **å®Œæ•´æ€§**: {quality_metrics.get('completeness_score', 0):.1f}/100")
            report.append(f"- **ä¸€è‡´æ€§**: {quality_metrics.get('consistency_score', 0):.1f}/100")
            report.append(f"- **ç»´æŠ¤æ€§**: {quality_metrics.get('maintenance_score', 0):.1f}/100")
            report.append(f"- **å¯è®¿é—®æ€§**: {quality_metrics.get('accessibility_score', 0):.1f}/100")
            report.append("")

        # æ”¹è¿›å»ºè®®
        recommendations = self.analytics_data.get("recommendations", [])
        if recommendations:
            report.append("## ğŸ’¡ æ”¹è¿›å»ºè®®")
            report.append("")
            for i, rec in enumerate(recommendations[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªå»ºè®®
                priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(rec.get("priority", "medium"), "ğŸŸ¡")
                report.append(f"{i}. **{priority_emoji} {rec.get('title', 'æœªçŸ¥')}** ({rec.get('priority', 'medium')})")
                report.append(f"   {rec.get('description', 'æ— æè¿°')}")
                report.append("")

        return "\n".join(report)

    def print_summary(self) -> None:
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ“Š æ–‡æ¡£åˆ†ææ‘˜è¦")
        print("="*60)

        # åŸºæœ¬ä¿¡æ¯
        print(f"é¡¹ç›®: {self.analytics_data['project_info']['name']}")
        print(f"æ–‡æ¡£ç›®å½•: {self.analytics_data['project_info']['docs_directory']}")
        print(f"åˆ†ææ—¶é—´: {self.analytics_data['analysis_time']}")
        print()

        # æ ¸å¿ƒæŒ‡æ ‡
        content_stats = self.analytics_data.get("content_analysis", {})
        if content_stats:
            file_stats = content_stats.get("file_statistics", {})
            content_data = content_stats.get("content_statistics", {})

            print("ğŸ“„ æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ€»æ–‡ä»¶æ•°: {file_stats.get('total_files', 0)}")
            print(f"  æ€»å¤§å°: {file_stats.get('total_size_human', 'æœªçŸ¥')}")
            print(f"  æ€»å­—æ•°: {content_data.get('total_words', 0):,}")
            print(f"  æ€»è¡Œæ•°: {content_data.get('total_lines', 0):,}")
            print()

        # è´¨é‡å¾—åˆ†
        quality_metrics = self.analytics_data.get("quality_metrics", {})
        if quality_metrics:
            print("ğŸ“ˆ è´¨é‡è¯„ä¼°:")
            print(f"  æ•´ä½“å¾—åˆ†: {quality_metrics.get('overall_quality_score', 0):.1f}/100")
            print(f"  å®Œæ•´æ€§: {quality_metrics.get('completeness_score', 0):.1f}/100")
            print(f"  ä¸€è‡´æ€§: {quality_metrics.get('consistency_score', 0):.1f}/100")
            print(f"  ç»´æŠ¤æ€§: {quality_metrics.get('maintenance_score', 0):.1f}/100")
            print()

        # ä¸»è¦å»ºè®®
        recommendations = self.analytics_data.get("recommendations", [])
        if recommendations:
            print("ğŸ’¡ ä¸»è¦å»ºè®®:")
            for rec in recommendations[:3]:
                priority_icon = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(rec.get("priority", "medium"), "ğŸŸ¡")
                print(f"  {priority_icon} {rec.get('title', 'æœªçŸ¥')}")
            print()

        print("="*60)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ–‡æ¡£åˆ†æå·¥å…·")
    parser.add_argument("--docs-dir", default="docs", help="æ–‡æ¡£ç›®å½•è·¯å¾„")
    parser.add_argument("--output", help="åˆ†ææŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    parser.add_argument("--summary", action="store_true", help="æ˜¾ç¤ºæ‘˜è¦æŠ¥å‘Š")
    parser.add_argument("--full-analysis", action="store_true", help="æ‰§è¡Œå®Œæ•´åˆ†æ")

    args = parser.parse_args()

    # åˆ›å»ºåˆ†æå™¨
    analyzer = DocsAnalytics(args.docs_dir)

    # æ‰§è¡Œåˆ†æ
    if args.full_analysis:
        await analyzer.analyze_content()
        await analyzer.analyze_structure()
        analyzer.analyze_quality()
        analyzer.generate_recommendations()
    else:
        # é»˜è®¤æ‰§è¡ŒåŸºç¡€åˆ†æ
        await analyzer.analyze_content()
        await analyzer.analyze_structure()
        analyzer.analyze_quality()
        analyzer.generate_recommendations()

    # ä¿å­˜åˆ†æç»“æœ
    report_path = analyzer.save_analysis(args.output)

    # æ˜¾ç¤ºæ‘˜è¦
    if args.summary:
        print(analyzer.generate_summary_report())

    # æ‰“å°æ§åˆ¶å°æ‘˜è¦
    analyzer.print_summary()

    print(f"\nğŸ“Š è¯¦ç»†åˆ†ææŠ¥å‘Š: {report_path}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())