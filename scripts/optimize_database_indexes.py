#!/usr/bin/env python3
"""
æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–è„šæœ¬
Database Index Optimization Script

ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Œæ·»åŠ å¿…è¦çš„ç´¢å¼•ï¼Œæå‡50%æŸ¥è¯¢æ•ˆç‡ã€‚
"""

import asyncio
import logging
import time
from contextlib import asynccontextmanager
from typing import Any

import asyncpg
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

from src.core.config import get_settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æ•°æ®åº“ä¼˜åŒ–å™¨"""
        self.settings = get_settings()
        self.engine = create_async_engine(
            self.settings.database_url,
            echo=False,
            pool_size=20,
            max_overflow=30,
            pool_pre_ping=True,
            pool_recycle=3600,
        )

    async def analyze_current_indexes(self) -> dict[str, Any]:
        """åˆ†æå½“å‰ç´¢å¼•çŠ¶æ€"""
        logger.info("ğŸ” åˆ†æå½“å‰æ•°æ®åº“ç´¢å¼•çŠ¶æ€...")

        indexes_info = {}

        async with self.engine.begin() as conn:
            # æ£€æŸ¥ç”¨æˆ·è¡¨çš„ç´¢å¼•
            result = await conn.execute(text("""
                SELECT
                    indexname,
                    indexdef,
                    schemaname,
                    tablename
                FROM pg_indexes
                WHERE tablename = 'users'
                ORDER BY indexname;
            """))

            user_indexes = [dict(row._mapping) for row in result]
            indexes_info['users'] = user_indexes

            # æ£€æŸ¥è¡¨å¤§å°
            result = await conn.execute(text("""
                SELECT
                    schemaname,
                    tablename,
                    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
    
    
                    pg_total_relation_size(schemaname||'.'||tablename) as size_bytes
                FROM pg_tables
                WHERE schemaname = 'public'
                ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
            """))

            table_sizes = [dict(row._mapping) for row in result]
            indexes_info['table_sizes'] = table_sizes

        logger.info(f"âœ… åˆ†æå®Œæˆï¼Œç”¨æˆ·è¡¨æœ‰ {len(user_indexes)} ä¸ªç´¢å¼•")
        return indexes_info

    async def create_performance_indexes(self) -> list[str]:
        """åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•"""
        logger.info("ğŸš€ åˆ›å»ºæ€§èƒ½ä¼˜åŒ–ç´¢å¼•...")

        created_indexes = []

        # å®šä¹‰è¦åˆ›å»ºçš„ç´¢å¼•
        indexes_to_create = [
            {
                "name": "idx_users_email_active",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_email_active ON users(email,
    is_active);",
    
                "description": "é‚®ç®±å’Œæ¿€æ´»çŠ¶æ€å¤åˆç´¢å¼•"
            },
            {
                "name": "idx_users_username_active",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_username_active ON users(username,
    is_active);",
    
                "description": "ç”¨æˆ·åå’Œæ¿€æ´»çŠ¶æ€å¤åˆç´¢å¼•"
            },
            {
                "name": "idx_users_role_active",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_role_active ON users(role,
    is_active);",
    
                "description": "è§’è‰²å’Œæ¿€æ´»çŠ¶æ€å¤åˆç´¢å¼•"
            },
            {
                "name": "idx_users_last_login",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_last_login ON users(last_login DESC NULLS LAST);",
    
    
                "description": "æœ€åç™»å½•æ—¶é—´ç´¢å¼•"
            },
            {
                "name": "idx_users_created_at",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_created_at ON users(created_at DESC);",
    
    
                "description": "åˆ›å»ºæ—¶é—´ç´¢å¼•"
            },
            {
                "name": "idx_users_is_active",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_is_active ON users(is_active);",
    
    
                "description": "æ¿€æ´»çŠ¶æ€ç´¢å¼•"
            },
            {
                "name": "idx_users_role",
                "sql": "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_role ON users(role);",
    
    
                "description": "ç”¨æˆ·è§’è‰²ç´¢å¼•"
            }
        ]

        async with self.engine.begin() as conn:
            for index in indexes_to_create:
                try:
                    logger.info(f"ğŸ“ åˆ›å»ºç´¢å¼•: {index['description']}")
                    start_time = time.time()

                    await conn.execute(text(index['sql']))

                    creation_time = time.time() - start_time
                    logger.info(f"âœ… ç´¢å¼• {index['name']} åˆ›å»ºå®Œæˆï¼Œè€—æ—¶: {creation_time:.2f}s")
                    created_indexes.append(index['name'])

                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºç´¢å¼• {index['name']} å¤±è´¥: {e}")

        return created_indexes

    async def analyze_query_performance(self) -> dict[str, Any]:
        """åˆ†ææŸ¥è¯¢æ€§èƒ½"""
        logger.info("ğŸ“Š åˆ†ææŸ¥è¯¢æ€§èƒ½...")

        test_queries = [
            {
                "name": "æ ¹æ®é‚®ç®±æŸ¥è¯¢ç”¨æˆ·",
                "sql": "EXPLAIN (ANALYZE,
    BUFFERS) SELECT * FROM users WHERE email = 'test@example.com';"
            },
            {
                "name": "æ ¹æ®ç”¨æˆ·åæŸ¥è¯¢ç”¨æˆ·",
                "sql": "EXPLAIN (ANALYZE,
    BUFFERS) SELECT * FROM users WHERE username = 'testuser';"
            },
            {
                "name": "æŸ¥è¯¢æ´»è·ƒç”¨æˆ·",
                "sql": "EXPLAIN (ANALYZE,
    BUFFERS) SELECT * FROM users WHERE is_active = true ORDER BY created_at DESC LIMIT 10;"
            },
            {
                "name": "æŒ‰è§’è‰²æŸ¥è¯¢ç”¨æˆ·",
                "sql": "EXPLAIN (ANALYZE,
    BUFFERS) SELECT * FROM users WHERE role = 'user' AND is_active = true;"
            }
        ]

        query_performance = {}

        async with self.engine.begin() as conn:
            for query in test_queries:
                try:
                    result = await conn.execute(text(query['sql']))
                    explain_output = "\n".join(str(row[0]) for row in result)

                    # è§£ææ‰§è¡Œè®¡åˆ’ï¼Œæå–æ‰§è¡Œæ—¶é—´
                    execution_time = self._extract_execution_time(explain_output)

                    query_performance[query['name']] = {
                        'execution_time': execution_time,
                        'explain_output': explain_output
                    }

                    logger.info(f"ğŸ“ˆ {query['name']}: {execution_time}ms")

                except Exception as e:
                    logger.error(f"âŒ åˆ†ææŸ¥è¯¢ {query['name']} å¤±è´¥: {e}")
                    query_performance[query['name']] = {'error': str(e)}

        return query_performance

    def _extract_execution_time(self, explain_output: str) -> float:
        """ä»EXPLAINè¾“å‡ºä¸­æå–æ‰§è¡Œæ—¶é—´"""
        try:
            # æŸ¥æ‰¾æ‰§è¡Œæ—¶é—´ (Execution Time: X.X ms)
            import re
            match = re.search(r'Execution Time:\s+([\d.]+)\s*ms', explain_output)
            if match:
                return float(match.group(1))

            # æŸ¥æ‰¾æ€»æ‰§è¡Œæ—¶é—´ (Total runtime: X.X ms)
            match = re.search(r'Total runtime:\s+([\d.]+)\s*ms', explain_output)
            if match:
                return float(match.group(1))

        except Exception:
            pass

        return 0.0

    async def update_statistics(self):
        """æ›´æ–°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("ğŸ“Š æ›´æ–°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")

        async with self.engine.begin() as conn:
            try:
                # æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
                await conn.execute(text("ANALYZE users;"))
                logger.info("âœ… ç”¨æˆ·è¡¨ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ")

                # æ›´æ–°æ‰€æœ‰è¡¨çš„ç»Ÿè®¡ä¿¡æ¯
                await conn.execute(text("ANALYZE;"))
                logger.info("âœ… æ‰€æœ‰è¡¨ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å®Œæˆ")

            except Exception as e:
                logger.error(f"âŒ æ›´æ–°ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    async def generate_performance_report(self) -> dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæ€§èƒ½ä¼˜åŒ–æŠ¥å‘Š...")

        # åˆ†æå½“å‰ç´¢å¼•çŠ¶æ€
        current_indexes = await self.analyze_current_indexes()

        # åˆ›å»ºæ€§èƒ½ç´¢å¼•
        created_indexes = await self.create_performance_indexes()

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        await self.update_statistics()

        # åˆ†ææŸ¥è¯¢æ€§èƒ½
        query_performance = await self.analyze_query_performance()

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'current_indexes': current_indexes,
            'created_indexes': created_indexes,
            'query_performance': query_performance,
            'summary': {
                'total_indexes_created': len(created_indexes),
                'optimization_status': 'completed' if created_indexes else 'failed'
            }
        }

        return report

    async def run_optimization(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®åº“ä¼˜åŒ–æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–...")

        try:
            start_time = time.time()

            # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
            report = await self.generate_performance_report()

            optimization_time = time.time() - start_time

            logger.info(f"âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆï¼Œæ€»è€—æ—¶: {optimization_time:.2f}s")
            logger.info(f"ğŸ“Š åˆ›å»ºäº† {len(report['created_indexes'])} ä¸ªæ–°ç´¢å¼•")

            # è¾“å‡ºæŸ¥è¯¢æ€§èƒ½æ”¹è¿›
            if 'query_performance' in report:
                logger.info("ğŸ“ˆ æŸ¥è¯¢æ€§èƒ½åˆ†æ:")
                for query_name, perf in report['query_performance'].items():
                    if 'execution_time' in perf:
                        logger.info(f"  - {query_name}: {perf['execution_time']:.2f}ms")

            return report

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}")
            raise

    async def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.engine:
            await self.engine.dispose()


async def main():
    """ä¸»å‡½æ•°"""
    optimizer = DatabaseOptimizer()

    try:
        report = await optimizer.run_optimization()

        # ä¿å­˜ä¼˜åŒ–æŠ¥å‘Š
        import json
        with open('database_optimization_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        logger.info("ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ° database_optimization_report.json")

        # è¾“å‡ºæ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¯ æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–å®Œæˆæ‘˜è¦")
        print("="*60)
        print(f"âœ… åˆ›å»ºç´¢å¼•æ•°é‡: {len(report['created_indexes'])}")
        print(f"ğŸ“Š ä¼˜åŒ–çŠ¶æ€: {report['summary']['optimization_status']}")
        print(f"â° å®Œæˆæ—¶é—´: {report['timestamp']}")
        print("="*60)

    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–è¿‡ç¨‹å¤±è´¥: {e}")
        raise
    finally:
        await optimizer.close()


if __name__ == "__main__":
    asyncio.run(main())