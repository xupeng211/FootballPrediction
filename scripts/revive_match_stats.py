#!/usr/bin/env python3
"""
ğŸš€ æ•°æ®å¤æ´»è„šæœ¬ - Match Stats Revival Script
é¦–å¸­æ•°æ®ä¿®å¤å®˜ (Chief Data Remediation Officer)

ä¸“é—¨ç”¨äºä¿®å¤99.95%ç¼ºå¤±çš„statså­—æ®µæ•°æ®ï¼Œå°†3744æ¡ç©ºè®°å½•å¤æ´»ä¸ºå®Œæ•´æ•°æ®ã€‚

ä½œè€…: Chief Data Remediation Officer
ç‰ˆæœ¬: v1.0.0
åˆ›å»ºæ—¶é—´: 2025-12-02
ä¿®å¤èŒƒå›´: 3744æ¡statså­—æ®µä¸ºç©ºçš„è®°å½•
"""

import asyncio
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from sqlalchemy import text
from sqlalchemy.ext.asyncio import AsyncSession

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from src.database.async_manager import get_db_session

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/revive_match_stats.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class MatchStatsReviver:
    """æ•°æ®å¤æ´»å™¨ - ä¸“é—¨ä¿®å¤ç©ºstatså­—æ®µ"""

    def __init__(self):
        self.revived_count = 0
        self.failed_count = 0
        self.skipped_count = 0
        self.start_time = datetime.now()

        # æ•°æ®æ–‡ä»¶è·¯å¾„
        self.data_dir = project_root / "data" / "fbref"

        # xGå­—æ®µæ˜ å°„ - åŸºäºå®é™…FBrefæ•°æ®ç»“æ„
        self.xg_field_mapping = {
            'xg_home': ['xg_home', 'home_xg', 'xg_for', 'expected_goals_home'],
            'xg_away': ['xg_away', 'away_xg', 'xg_against', 'expected_goals_away'],
            'possession_home': ['possession_home', 'home_possession', 'possession_for'],
            'possession_away': ['possession_away', 'away_possession', 'possession_against']
        }

    async def identify_dead_records(self) -> list[int]:
        """è¯†åˆ«éœ€è¦å¤æ´»çš„è®°å½• (statså­—æ®µä¸ºç©ºçš„è®°å½•)"""
        logger.info("ğŸ” è¯†åˆ«éœ€è¦å¤æ´»çš„è®°å½•...")

        async with get_db_session() as session:
            result = await session.execute(
                text("""
                    SELECT id, raw_file_path
                    FROM matches
                    WHERE data_source = 'fbref'
                    AND (stats = '{}' OR stats IS NULL)
                    ORDER BY created_at DESC
                """)
            )
            records = result.fetchall()

            logger.info(f"ğŸ“Š å‘ç° {len(records)} æ¡éœ€è¦å¤æ´»çš„è®°å½•")
            return [(record.id, record.raw_file_path) for record in records]

    def load_raw_data_from_file(self, file_path: str) -> Optional[dict]:
        """ä»åŸå§‹æ–‡ä»¶åŠ è½½æ•°æ®"""
        if not file_path or not os.path.exists(file_path):
            return None

        try:
            with open(file_path, encoding='utf-8') as f:
                data = json.load(f)
                return data
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def extract_stats_from_raw_data(self, raw_data: dict) -> dict:
        """ä»åŸå§‹æ•°æ®ä¸­æå–statså­—æ®µ"""
        if not raw_data:
            return {}

        stats = {}

        # æ–¹æ³•1: ç›´æ¥ä»statså­—æ®µæå–
        if 'stats' in raw_data and raw_data['stats']:
            stats.update(raw_data['stats'])

        # æ–¹æ³•2: ä»team_statsæå– (å¸¸è§äºFBrefæ•°æ®)
        if 'team_stats' in raw_data and raw_data['team_stats']:
            team_stats = raw_data['team_stats']
            if isinstance(team_stats, dict):
                # æå–xGæ•°æ®
                if 'home_xg' in team_stats:
                    stats['xg_home'] = str(team_stats['home_xg'])
                if 'away_xg' in team_stats:
                    stats['xg_away'] = str(team_stats['away_xg'])

                # æå–æ§çƒç‡æ•°æ®
                if 'home_possession' in team_stats:
                    stats['possession_home'] = str(team_stats['home_possession'])
                if 'away_possession' in team_stats:
                    stats['possession_away'] = str(team_stats['away_possession'])

        # æ–¹æ³•3: ä»flatç»Ÿè®¡å­—æ®µæå–
        for target_field, possible_names in self.xg_field_mapping.items():
            if target_field not in stats:
                for field_name in possible_names:
                    if field_name in raw_data and raw_data[field_name] is not None:
                        stats[target_field] = str(raw_data[field_name])
                        break

        # æ–¹æ³•4: ä»teamsæ•°ç»„æå–
        if 'teams' in raw_data and isinstance(raw_data['teams'], list):
            teams = raw_data['teams']
            if len(teams) >= 2:
                home_team = teams[0]
                away_team = teams[1]

                if 'xg' in home_team:
                    stats['xg_home'] = str(home_team['xg'])
                if 'xg' in away_team:
                    stats['xg_away'] = str(away_team['xg'])
                if 'possession' in home_team:
                    stats['possession_home'] = str(home_team['possession'])
                if 'possession' in away_team:
                    stats['possession_away'] = str(away_team['possession'])

        # æ•°æ®æ¸…ç†å’ŒéªŒè¯
        cleaned_stats = {}
        for key, value in stats.items():
            if value and str(value).strip() and str(value) != 'nan' and str(value) != 'None':
                # æ¸…ç†æ•°å€¼
                try:
                    if 'possession' in key:
                        clean_value = float(value)
                        if 0 <= clean_value <= 100:
                            cleaned_stats[key] = str(round(clean_value, 1))
                    elif 'xg' in key:
                        clean_value = float(value)
                        if 0 <= clean_value <= 10:  # xGé€šå¸¸åœ¨0-10ä¹‹é—´
                            cleaned_stats[key] = str(round(clean_value, 2))
                except (ValueError, TypeError):
                    continue

        return cleaned_stats

    def extract_metadata_from_raw_data(self, raw_data: dict) -> dict:
        """ä»åŸå§‹æ•°æ®ä¸­æå–match_metadataå­—æ®µ"""
        if not raw_data:
            return {}

        metadata = {}

        # æå–åŸºç¡€å…ƒæ•°æ®
        if 'referee' in raw_data and raw_data['referee']:
            metadata['referee'] = str(raw_data['referee'])

        if 'attendance' in raw_data and raw_data['attendance']:
            try:
                attendance = int(raw_data['attendance'])
                if attendance > 0:
                    metadata['attendance'] = attendance
            except (ValueError, TypeError):
                pass

        if 'match_report_url' in raw_data and raw_data['match_report_url']:
            metadata['match_report_url'] = str(raw_data['match_report_url'])

        if 'venue' in raw_data and raw_data['venue']:
            metadata['venue'] = str(raw_data['venue'])

        return metadata

    async def revive_single_record(self, record_id: int, raw_file_path: str) -> bool:
        """å¤æ´»å•æ¡è®°å½•"""
        try:
            # åŠ è½½åŸå§‹æ•°æ®
            raw_data = self.load_raw_data_from_file(raw_file_path)
            if not raw_data:
                logger.warning(f"âš ï¸ æ— æ³•åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶: {raw_file_path}")
                self.skipped_count += 1
                return False

            # æå–statså­—æ®µ
            stats = self.extract_stats_from_raw_data(raw_data)
            if not stats:
                logger.warning(f"âš ï¸ æ— æ³•ä»åŸå§‹æ•°æ®æå–stats: è®°å½•ID {record_id}")
                self.skipped_count += 1
                return False

            # æå–metadataå­—æ®µ
            metadata = self.extract_metadata_from_raw_data(raw_data)

            # æ›´æ–°æ•°æ®åº“
            async with get_db_session() as session:
                update_query = text("""
                    UPDATE matches
                    SET stats = :stats,
                        match_metadata = COALESCE(match_metadata, '{}'::jsonb) || :metadata::jsonb,
                        data_completeness = :completeness,
                        updated_at = NOW()
                    WHERE id = :id
                """)

                await session.execute(
                    update_query,
                    {
                        'id': record_id,
                        'stats': json.dumps(stats),
                        'metadata': json.dumps(metadata) if metadata else '{}',
                        'completeness': 'complete' if stats else 'partial'
                    }
                )
                await session.commit()

            self.revived_count += 1
            if self.revived_count % 100 == 0:
                logger.info(f"âœ… å·²å¤æ´» {self.revived_count} æ¡è®°å½•...")

            return True

        except Exception as e:
            logger.error(f"âŒ å¤æ´»è®°å½•å¤±è´¥ ID {record_id}: {e}")
            self.failed_count += 1
            return False

    async def run_revival_process(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®å¤æ´»æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨æ•°æ®å¤æ´»æµç¨‹")
        logger.info(f"ğŸ“ æ•°æ®ç›®å½•: {self.data_dir}")

        # 1. è¯†åˆ«éœ€è¦å¤æ´»çš„è®°å½•
        dead_records = await self.identify_dead_records()

        if not dead_records:
            logger.info("âœ… æ²¡æœ‰éœ€è¦å¤æ´»çš„è®°å½•")
            return

        logger.info(f"ğŸ¯ ç›®æ ‡: å¤æ´» {len(dead_records)} æ¡è®°å½•")

        # 2. é€æ¡å¤æ´»è®°å½•
        total_records = len(dead_records)
        for i, (record_id, raw_file_path) in enumerate(dead_records, 1):
            await self.revive_single_record(record_id, raw_file_path)

            # æ¯å¤„ç†100æ¡è®°å½•è¾“å‡ºè¿›åº¦
            if i % 100 == 0:
                progress = (i / total_records) * 100
                logger.info(f"ğŸ“Š è¿›åº¦: {i}/{total_records} ({progress:.1f}%)")

        # 3. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self.generate_final_report(total_records)

    def generate_final_report(self, total_records: int):
        """ç”Ÿæˆæœ€ç»ˆä¿®å¤æŠ¥å‘Š"""
        end_time = datetime.now()
        duration = end_time - self.start_time

        success_rate = (self.revived_count / total_records * 100) if total_records > 0 else 0

        report = f"""
ğŸ‰ æ•°æ®å¤æ´»ä¿®å¤å®ŒæˆæŠ¥å‘Š
=====================================
ä¿®å¤æ—¶é—´: {self.start_time} ~ {end_time}
æ€»è€—æ—¶: {duration}
æ€»è®°å½•æ•°: {total_records}

ä¿®å¤ç»“æœ:
âœ… æˆåŠŸå¤æ´»: {self.revived_count} æ¡ ({success_rate:.1f}%)
âŒ å¤æ´»å¤±è´¥: {self.failed_count} æ¡
âš ï¸ è·³è¿‡è®°å½•: {self.skipped_count} æ¡

å¤„ç†é€Ÿåº¦: {total_records/duration.total_seconds():.1f} è®°å½•/ç§’

çŠ¶æ€: {'âœ… å®Œå…¨æˆåŠŸ' if success_rate > 95 else 'âš ï¸ éƒ¨åˆ†æˆåŠŸ' if success_rate > 80 else 'âŒ éœ€è¦è¿›ä¸€æ­¥å¤„ç†'}
=====================================
        """

        logger.info(report)

        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        try:
            with open('/tmp/revival_report.txt', 'w', encoding='utf-8') as f:
                f.write(report)
        except Exception as e:
            logger.error(f"âŒ æ— æ³•å†™å…¥æŠ¥å‘Šæ–‡ä»¶: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    print("""
ğŸš€ æ•°æ®å¤æ´»è„šæœ¬ - Match Stats Revival Tool
=====================================
é¦–å¸­æ•°æ®ä¿®å¤å®˜ (Chief Data Remediation Officer)
ç‰ˆæœ¬: v1.0.0

ä¿®å¤ç›®æ ‡: 3744æ¡statså­—æ®µä¸ºç©ºçš„è®°å½•
ä¿®å¤æ–¹æ³•: ä»åŸå§‹FBrefæ•°æ®é‡æ–°æå–statså­—æ®µ
é¢„æœŸç»“æœ: å°†ç©ºæ•°æ®å¤æ´»ä¸ºåŒ…å«xGå’Œæ§çƒç‡çš„å®Œæ•´æ•°æ®

å¼€å§‹æ—¶é—´: {}
=====================================
""".format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')))

    # åˆ›å»ºä¿®å¤å™¨å®ä¾‹
    reviver = MatchStatsReviver()

    try:
        # æ‰§è¡Œå¤æ´»æµç¨‹
        await reviver.run_revival_process()

        print("\nğŸ‰ æ•°æ®å¤æ´»æµç¨‹å·²å®Œæˆï¼")
        print("ğŸ“Š è¯¦ç»†æŠ¥å‘Šè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶: /tmp/revive_match_stats.log")
        print("ğŸ“„ æœ€ç»ˆæŠ¥å‘Š: /tmp/revival_report.txt")

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ç”¨æˆ·ä¸­æ–­ä¿®å¤æµç¨‹")
    except Exception as e:
        logger.error(f"âŒ ä¿®å¤æµç¨‹å¼‚å¸¸: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
