#!/usr/bin/env python3
"""ç‰¹å¾é€‰æ‹©æ¼”ç¤ºè„šæœ¬
Feature Selection Demo Script.

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ™ºèƒ½ç‰¹å¾é€‰æ‹©å™¨æ¥è‡ªåŠ¨ç­›é€‰æœ€é‡è¦çš„ç‰¹å¾ï¼Œ
åŒ…æ‹¬å…±çº¿æ€§æ£€æµ‹å’ŒåŸºäºæ¨¡å‹é‡è¦æ€§çš„ç‰¹å¾æ’åºã€‚
"""

import logging
import numpy as np
import pandas as pd
from pathlib import Path
import sys
import warnings

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.ml.feature_selector import FeatureSelector, create_feature_selector
from src.ml.football_prediction_pipeline import FootballPredictionPipeline

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def generate_synthetic_football_data(n_samples=1000, n_features=50, random_state=42):
    """ç”Ÿæˆåˆæˆçš„è¶³çƒæ¯”èµ›æ•°æ®ç”¨äºæ¼”ç¤º.

    Args:
        n_samples: æ ·æœ¬æ•°é‡
        n_features: ç‰¹å¾æ•°é‡
        random_state: éšæœºç§å­

    Returns:
        ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡
    """
    np.random.seed(random_state)

    # ç”ŸæˆåŸºç¡€ç‰¹å¾
    feature_names = []

    # çƒé˜Ÿå®åŠ›ç‰¹å¾ (10ä¸ª)
    for i in range(5):
        feature_names.extend([
            f'home_team_strength_{i}',
            f'away_team_strength_{i}',
            f'strength_diff_{i}'
        ])

    # å†å²äº¤é”‹ç‰¹å¾ (8ä¸ª)
    for i in range(4):
        feature_names.extend([
            f'historical_home_wins_{i}',
            f'historical_away_wins_{i}'
        ])

    # è¿‘æœŸçŠ¶æ€ç‰¹å¾ (12ä¸ª)
    for i in range(6):
        feature_names.extend([
            f'home_recent_form_{i}',
            f'away_recent_form_{i}'
        ])

    # ç»Ÿè®¡æ•°æ®ç‰¹å¾ (15ä¸ª)
    stats_features = [
        'home_goals_scored', 'home_goals_conceded',
        'away_goals_scored', 'away_goals_conceded',
        'home_shots_on_target', 'away_shots_on_target',
        'home_possession', 'away_possession',
        'home_pass_accuracy', 'away_pass_accuracy',
        'home_fouls', 'away_fouls',
        'home_corners', 'away_corners',
        'home_yellow_cards'
    ]
    feature_names.extend(stats_features)

    # ç¯å¢ƒç‰¹å¾ (5ä¸ª)
    env_features = ['home_advantage', 'weather_condition', 'crowd_factor', 'travel_distance', 'rest_days']
    feature_names.extend(env_features)

    # ç¡®ä¿ç‰¹å¾æ•°é‡æ­£ç¡®
    feature_names = feature_names[:n_features]

    # ç”Ÿæˆç‰¹å¾çŸ©é˜µ
    X = pd.DataFrame(np.random.randn(n_samples, n_features), columns=feature_names)

    # æ·»åŠ ä¸€äº›ç›¸å…³æ€§ç‰¹å¾ï¼ˆæ¨¡æ‹Ÿå…±çº¿æ€§ï¼‰
    if n_features >= 20:
        # åˆ›å»ºé«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹
        if 'home_team_strength_0' in X.columns and 'away_team_strength_0' in X.columns:
            X['home_strength_copy'] = X['home_team_strength_0'] * 0.95 + np.random.normal(0, 0.05, n_samples)
            X['away_strength_copy'] = X['away_team_strength_0'] * 0.98 + np.random.normal(0, 0.02, n_samples)

            # æ·»åŠ è¿™äº›æ–°ç‰¹å¾åˆ°DataFrame
            additional_features = ['home_strength_copy', 'away_strength_copy']
        else:
            additional_features = []

        # åªæœ‰å½“ç›¸å…³ç‰¹å¾å­˜åœ¨æ—¶æ‰æ·»åŠ æ¯”å€¼ç‰¹å¾
        if 'home_goals_scored' in X.columns and 'away_goals_conceded' in X.columns:
            X['goals_ratio'] = X['home_goals_scored'] / (X['away_goals_conceded'] + 1)
            additional_features.append('goals_ratio')

        # æ›´æ–°ç‰¹å¾åç§°åˆ—è¡¨
        if additional_features:
            feature_names.extend(additional_features)
            X = X[feature_names]

    # ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆæ¯”èµ›ç»“æœï¼‰
    # åŸºäºå‡ ä¸ªé‡è¦ç‰¹å¾çš„çº¿æ€§ç»„åˆ
    important_features = [
        'home_team_strength_0', 'away_team_strength_0', 'strength_diff_0',
        'historical_home_wins_0', 'historical_away_wins_0',
        'home_recent_form_0', 'away_recent_form_0',
        'home_advantage'
    ]

    # ç¡®ä¿é‡è¦ç‰¹å¾å­˜åœ¨
    available_important = [f for f in important_features if f in X.columns]

    if available_important:
        # è®¡ç®—æ¯”èµ›ç»“æœçš„æ¦‚ç‡
        logit = (
            X[available_important].sum(axis=1) * 0.3 +
            np.random.normal(0, 0.5, n_samples)  # æ·»åŠ å™ªå£°
        )

        # è½¬æ¢ä¸ºæ¦‚ç‡
        prob = 1 / (1 + np.exp(-logit))

        # ç”ŸæˆäºŒå…ƒåˆ†ç±»ç»“æœ
        y = (prob > 0.5).astype(int)
    else:
        # å¦‚æœæ²¡æœ‰é‡è¦ç‰¹å¾ï¼Œéšæœºç”Ÿæˆ
        y = np.random.binomial(1, 0.5, n_samples)

    logger.info(f"ç”Ÿæˆåˆæˆæ•°æ®: {n_samples} æ ·æœ¬, {X.shape[1]} ä¸ªç‰¹å¾")
    logger.info(f"ç›®æ ‡å˜é‡åˆ†å¸ƒ: {np.bincount(y)}")

    return X, y


def demo_basic_feature_selection():
    """æ¼”ç¤ºåŸºç¡€ç‰¹å¾é€‰æ‹©åŠŸèƒ½."""
    print("\n" + "="*60)
    print("ğŸ¯ æ¼”ç¤º1: åŸºç¡€ç‰¹å¾é€‰æ‹©åŠŸèƒ½")
    print("="*60)

    # ç”Ÿæˆæ•°æ®
    X, y = generate_synthetic_football_data(n_samples=1000, n_features=30)

    # åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
    selector = create_feature_selector(
        task_type="classification",
        correlation_threshold=0.9,
        min_features=5,
        max_features=20
    )

    # æ‰§è¡Œç‰¹å¾é€‰æ‹©
    selected_features = selector.select_features(
        X, y,
        top_k=15,
        remove_collinear=True
    )

    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š ç‰¹å¾é€‰æ‹©ç»“æœ:")
    print(f"åŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_features)}")
    print(f"ç§»é™¤çš„ç‰¹å¾æ•°é‡: {len(selector.removed_features)}")

    print("\nâœ… é€‰æ‹©çš„ç‰¹å¾:")
    for i, feature in enumerate(selected_features, 1):
        print(f"  {i:2d}. {feature}")

    # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§
    if selector.feature_importance_df is not None:
        print("\nğŸ” å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾:")
        top_features = selector.feature_importance_df.head(10)
        for i, row in top_features.iterrows():
            print(f"  {row['feature']:<25} (é‡è¦æ€§: {row['importance_avg']:.4f})")

    # æ˜¾ç¤ºå…±çº¿æ€§æ£€æµ‹ç»“æœ
    if selector.correlation_matrix is not None:
        high_corr_pairs = []
        corr_matrix = selector.correlation_matrix
        for i in range(len(corr_matrix.columns)):
            for j in range(i + 1, len(corr_matrix.columns)):
                if corr_matrix.iloc[i, j] > 0.9:
                    high_corr_pairs.append((
                        corr_matrix.columns[i],
                        corr_matrix.columns[j],
                        corr_matrix.iloc[i, j]
                    ))

        if high_corr_pairs:
            print("\nâš ï¸  æ£€æµ‹åˆ°çš„é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹ (r > 0.9):")
            for feat1, feat2, corr in high_corr_pairs[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {feat1} â†” {feat2}: {corr:.3f}")
        else:
            print("\nâœ… æœªæ£€æµ‹åˆ°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")


def demo_feature_selection_pipeline():
    """æ¼”ç¤ºé›†æˆåˆ°è®­ç»ƒæµæ°´çº¿ä¸­çš„ç‰¹å¾é€‰æ‹©."""
    print("\n" + "="*60)
    print("ğŸš€ æ¼”ç¤º2: é›†æˆç‰¹å¾é€‰æ‹©çš„è®­ç»ƒæµæ°´çº¿")
    print("="*60)

    # ç”Ÿæˆæ•°æ®
    X, y = generate_synthetic_football_data(n_samples=800, n_features=25)

    # åˆ†å‰²æ•°æ®
    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    print(f"è®­ç»ƒæ•°æ®: {X_train.shape[0]} æ ·æœ¬, {X_train.shape[1]} ç‰¹å¾")
    print(f"æµ‹è¯•æ•°æ®: {X_test.shape[0]} æ ·æœ¬, {X_test.shape[1]} ç‰¹å¾")

    try:
        # åˆ›å»ºå¸¦ç‰¹å¾é€‰æ‹©çš„æµæ°´çº¿
        pipeline = FootballPredictionPipeline(
            model_name="demo_football_prediction",
            output_dir="models/demo",
            enable_feature_selection=True,
            feature_selection_params={
                "task_type": "classification",
                "correlation_threshold": 0.85,
                "min_features": 3,
                "max_features": 15
            }
        )

        # è®­ç»ƒæ¨¡å‹
        print("\nğŸƒ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        training_result = pipeline.train_model(
            X_train, y_train,
            X_test, y_test,
            model_type="xgboost",
            feature_selection_top_k=10,
            optimize_hyperparameters=False  # ä¸ºäº†æ¼”ç¤ºé€Ÿåº¦ï¼Œè·³è¿‡è¶…å‚æ•°ä¼˜åŒ–
        )

        # æ˜¾ç¤ºè®­ç»ƒç»“æœ
        print("\nğŸ“ˆ è®­ç»ƒå®Œæˆ!")
        print(f"æ¨¡å‹æ€§èƒ½: {training_result.get('metrics', {})}")

        # æ˜¾ç¤ºç‰¹å¾é€‰æ‹©ä¿¡æ¯
        feature_selection_info = training_result.get('feature_selection', {})
        if feature_selection_info.get('enabled'):
            print("\nğŸ¯ ç‰¹å¾é€‰æ‹©ç»“æœ:")
            print(f"åŸå§‹ç‰¹å¾æ•°: {feature_selection_info.get('original_features', 'N/A')}")
            print(f"é€‰æ‹©ç‰¹å¾æ•°: {feature_selection_info.get('selected_features', 'N/A')}")
            print(f"ç§»é™¤ç‰¹å¾æ•°: {feature_selection_info.get('removed_features', 'N/A')}")

            selected_features = feature_selection_info.get('selected_feature_names', [])
            if selected_features:
                print("\nâœ… æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾:")
                for i, feature in enumerate(selected_features, 1):
                    print(f"  {i:2d}. {feature}")

        # è¯„ä¼°æ¨¡å‹
        print("\nğŸ§ª è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        predictions = pipeline.predict(X_test)

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = (predictions == y_test).mean()
        print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {accuracy:.4f}")

        # æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶
        output_dir = Path("models/demo")
        if output_dir.exists():
            print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            for file in output_dir.glob("*"):
                if file.is_file():
                    print(f"  {file.name}")

    except Exception as e:
        print(f"\nâŒ æµæ°´çº¿æ¼”ç¤ºå¤±è´¥: {e}")
        logger.info("è¿™å¯èƒ½æ˜¯å› ä¸ºç¼ºå°‘æŸäº›ä¾èµ–æ¨¡å—ï¼Œè¿™æ˜¯æ­£å¸¸çš„")


def demo_feature_importance_analysis():
    """æ¼”ç¤ºç‰¹å¾é‡è¦æ€§åˆ†æ."""
    print("\n" + "="*60)
    print("ğŸ“Š æ¼”ç¤º3: ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("="*60)

    # ç”Ÿæˆå…·æœ‰æ˜ç¡®é‡è¦æ€§ç»“æ„çš„æ•°æ®
    X, y = generate_synthetic_football_data(n_samples=500, n_features=20)

    # åˆ›å»ºç‰¹å¾é€‰æ‹©å™¨
    selector = FeatureSelector(
        task_type="classification",
        correlation_threshold=0.95,
        random_state=42
    )

    # æ‰§è¡Œç‰¹å¾é€‰æ‹©
    selected_features = selector.select_features(X, y, top_k=10)

    # åˆ†æç‰¹å¾é‡è¦æ€§
    if selector.feature_importance_df is not None:
        importance_df = selector.feature_importance_df

        print("\nğŸ” ç‰¹å¾é‡è¦æ€§è¯¦ç»†åˆ†æ:")

        # æŒ‰ä¸åŒé‡è¦æ€§æŒ‡æ ‡æ’åº
        print("\n1ï¸âƒ£ æŒ‰å¹³å‡é‡è¦æ€§æ’åº:")
        avg_top = importance_df.nlargest(5, 'importance_avg')
        for i, row in avg_top.iterrows():
            print(f"   {row['feature']:<20} (å¹³å‡: {row['importance_avg']:.4f}, "
                  f"æœ€å¤§: {row['importance_max']:.4f})")

        print("\n2ï¸âƒ£ æŒ‰éšæœºæ£®æ—é‡è¦æ€§æ’åº:")
        if 'rf_importance' in importance_df.columns:
            rf_top = importance_df.nlargest(5, 'rf_importance')
            for i, row in rf_top.iterrows():
                print(f"   {row['feature']:<20} (RFé‡è¦æ€§: {row['rf_importance']:.4f})")

        print("\n3ï¸âƒ£ æŒ‰äº’ä¿¡æ¯æ’åº:")
        if 'mi_importance' in importance_df.columns:
            mi_top = importance_df.nlargest(5, 'mi_importance')
            for i, row in mi_top.iterrows():
                print(f"   {row['feature']:<20} (äº’ä¿¡æ¯: {row['mi_importance']:.4f})")

    # å°è¯•ç”Ÿæˆç‰¹å¾é‡è¦æ€§å›¾
    try:
        selector.plot_feature_importance(top_k=10, save_path="feature_importance_demo.png")
        print("\nğŸ“ˆ ç‰¹å¾é‡è¦æ€§å›¾å·²ä¿å­˜åˆ°: feature_importance_demo.png")
    except Exception as e:
        print(f"\nâš ï¸  ç»˜å›¾å¤±è´¥ï¼ˆå¯èƒ½ç¼ºå°‘matplotlibï¼‰: {e}")


def demo_collinearity_detection():
    """æ¼”ç¤ºå…±çº¿æ€§æ£€æµ‹åŠŸèƒ½."""
    print("\n" + "="*60)
    print("ğŸ”— æ¼”ç¤º4: å…±çº¿æ€§æ£€æµ‹")
    print("="*60)

    # ç”Ÿæˆå…·æœ‰é«˜ç›¸å…³æ€§çš„æ•°æ®
    np.random.seed(42)
    n_samples = 200

    # åˆ›å»ºåŸºç¡€ç‰¹å¾
    base_data = {
        'feature_A': np.random.randn(n_samples),
        'feature_B': np.random.randn(n_samples),
        'feature_C': np.random.randn(n_samples),
    }

    # åˆ›å»ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾
    base_data['feature_A_copy'] = base_data['feature_A'] * 0.97 + np.random.normal(0, 0.03, n_samples)
    base_data['feature_A_copy2'] = base_data['feature_A'] * 0.99 + np.random.normal(0, 0.01, n_samples)
    base_data['feature_B_near_duplicate'] = base_data['feature_B'] * 0.94 + np.random.normal(0, 0.06, n_samples)

    # åˆ›å»ºä¸€äº›ä¸ç›¸å…³çš„ç‰¹å¾
    base_data['independent_1'] = np.random.randn(n_samples)
    base_data['independent_2'] = np.random.randn(n_samples)
    base_data['independent_3'] = np.random.randn(n_samples)

    X = pd.DataFrame(base_data)

    # ç”Ÿæˆç›®æ ‡å˜é‡ï¼ˆåªä¸æŸäº›ç‰¹å¾ç›¸å…³ï¼‰
    y = ((base_data['feature_A'] + base_data['feature_B'] * 0.5) > 0).astype(int)

    print("ğŸ“‹ ç”Ÿæˆçš„æ•°æ®ç‰¹å¾:")
    print(f"  æ ·æœ¬æ•°: {n_samples}")
    print(f"  ç‰¹å¾æ•°: {X.shape[1]}")
    print(f"  ç›®æ ‡å˜é‡åˆ†å¸ƒ: {np.bincount(y)}")

    # å±•ç¤ºç›¸å…³æ€§çŸ©é˜µ
    print("\nğŸ“Š ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ:")
    corr_matrix = X.corr()
    for i, col in enumerate(corr_matrix.columns):
        correlations = [f"{corr_matrix.iloc[i, j]:.3f}" for j in range(len(corr_matrix.columns))]
        print(f"  {col:<20} {'  '.join(correlations)}")

    # ä½¿ç”¨ç‰¹å¾é€‰æ‹©å™¨æ£€æµ‹å…±çº¿æ€§
    selector = FeatureSelector(
        task_type="classification",
        correlation_threshold=0.9,
        random_state=42
    )

    print("\nğŸ” æ‰§è¡Œå…±çº¿æ€§æ£€æµ‹...")
    kept_features, removed_features = selector.detect_collinearity(X, y)

    print("\nâœ… å…±çº¿æ€§æ£€æµ‹ç»“æœ:")
    print(f"ä¿ç•™çš„ç‰¹å¾ ({len(kept_features)}): {kept_features}")
    print(f"ç§»é™¤çš„ç‰¹å¾ ({len(removed_features)}): {removed_features}")

    # æ˜¾ç¤ºå…·ä½“çš„é«˜ç›¸å…³æ€§å¯¹
    if selector.correlation_matrix is not None:
        high_corr_pairs = []
        for i in range(len(selector.correlation_matrix.columns)):
            for j in range(i + 1, len(selector.correlation_matrix.columns)):
                corr_val = selector.correlation_matrix.iloc[i, j]
                if corr_val > 0.9:
                    high_corr_pairs.append((
                        selector.correlation_matrix.columns[i],
                        selector.correlation_matrix.columns[j],
                        corr_val
                    ))

        if high_corr_pairs:
            print("\nâš ï¸  å‘ç°çš„é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹:")
            for feat1, feat2, corr in high_corr_pairs:
                print(f"  {feat1} â†” {feat2}: r = {corr:.4f}")
        else:
            print("\nâœ… æœªå‘ç°é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")


def main():
    """ä¸»å‡½æ•°."""
    print("ğŸš€ ç‰¹å¾é€‰æ‹©ç³»ç»Ÿæ¼”ç¤º")
    print("="*60)
    print("æœ¬æ¼”ç¤ºå°†å±•ç¤ºæ™ºèƒ½ç‰¹å¾é€‰æ‹©å™¨çš„å„ç§åŠŸèƒ½:")
    print("1. åŸºç¡€ç‰¹å¾é€‰æ‹©åŠŸèƒ½")
    print("2. é›†æˆåˆ°è®­ç»ƒæµæ°´çº¿")
    print("3. ç‰¹å¾é‡è¦æ€§åˆ†æ")
    print("4. å…±çº¿æ€§æ£€æµ‹")

    try:
        # æ¼”ç¤º1: åŸºç¡€ç‰¹å¾é€‰æ‹©
        demo_basic_feature_selection()

        # æ¼”ç¤º2: æµæ°´çº¿é›†æˆ
        demo_feature_selection_pipeline()

        # æ¼”ç¤º3: ç‰¹å¾é‡è¦æ€§åˆ†æ
        demo_feature_importance_analysis()

        # æ¼”ç¤º4: å…±çº¿æ€§æ£€æµ‹
        demo_collinearity_detection()

        print("\n" + "="*60)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        print("="*60)
        print("\nğŸ’¡ ä¸»è¦ç‰¹æ€§æ€»ç»“:")
        print("âœ… åŸºäºå¤šç§æ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§è¯„ä¼°")
        print("âœ… æ™ºèƒ½å…±çº¿æ€§æ£€æµ‹å’Œç§»é™¤")
        print("âœ… å¯é…ç½®çš„ç‰¹å¾é€‰æ‹©å‚æ•°")
        print("âœ… ä¸è®­ç»ƒæµæ°´çº¿æ— ç¼é›†æˆ")
        print("âœ… è¯¦ç»†çš„ç‰¹å¾åˆ†ææŠ¥å‘Š")
        print("âœ… ç‰¹å¾é€‰æ‹©ç»“æœæŒä¹…åŒ–")

        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        output_files = [
            "models/demo/selected_features.json",
            "models/demo/feature_selection_results.json",
            "feature_importance_demo.png"
        ]
        for file in output_files:
            if Path(file).exists():
                print(f"  âœ… {file}")
            else:
                print(f"  âŒ {file} (æœªç”Ÿæˆ)")

    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
