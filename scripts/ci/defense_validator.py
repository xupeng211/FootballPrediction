#!/usr/bin/env python3
"""
é˜²å¾¡æœºåˆ¶éªŒè¯ç³»ç»Ÿ - ç¡®ä¿é˜²å¾¡æªæ–½æœ‰æ•ˆå·¥ä½œ

è¿™ä¸ªæ¨¡å—è´Ÿè´£ï¼š
1. éªŒè¯æ–°ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹æ˜¯å¦èƒ½æ­£ç¡®è¿è¡Œ
2. æ£€æŸ¥lintè§„åˆ™æ˜¯å¦èƒ½æ•è·é¢„æœŸçš„é—®é¢˜
3. éªŒè¯pre-commité’©å­æ˜¯å¦æ­£å¸¸å·¥ä½œ
4. æµ‹è¯•CIå·¥ä½œæµæ˜¯å¦æŒ‰é¢„æœŸæ‰§è¡Œ
5. ç”Ÿæˆé˜²å¾¡æ•ˆæœè¯„ä¼°æŠ¥å‘Š

ä½œè€…ï¼šAI CI Guardian System
ç‰ˆæœ¬ï¼šv1.0.0
"""

import json
import subprocess
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

import click


class TestValidator:
    """æµ‹è¯•éªŒè¯å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.tests_dir = project_root / "tests"

    def validate_test_files(self, test_files: List[str]) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•æ–‡ä»¶æ˜¯å¦èƒ½æ­£ç¡®è¿è¡Œ"""
        results = {
            "total_tests": len(test_files),
            "passed_tests": 0,
            "failed_tests": 0,
            "test_results": {},
            "validation_time": 0,
        }

        start_time = time.time()

        for test_file in test_files:
            test_path = self.tests_dir / test_file
            if not test_path.exists():
                results["test_results"][test_file] = {
                    "status": "not_found",
                    "message": f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_path}",
                }
                results["failed_tests"] += 1
                continue

            # è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶
            test_result = self._run_single_test(test_path)
            results["test_results"][test_file] = test_result

            if test_result["status"] == "passed":
                results["passed_tests"] += 1
            else:
                results["failed_tests"] += 1

        results["validation_time"] = time.time() - start_time
        results["success_rate"] = (results["passed_tests"] / max(results["total_tests"], 1)) * 100

        return results

    def _run_single_test(self, test_path: Path) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶"""
        try:
            result = subprocess.run(
                ["pytest", str(test_path), "-v", "--tb=short"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=60,
            )

            if result.returncode == 0:
                return {
                    "status": "passed",
                    "message": "æµ‹è¯•é€šè¿‡",
                    "output": result.stdout[-500:] if result.stdout else "",
                    "test_count": self._count_tests_in_output(result.stdout),
                }
            else:
                return {
                    "status": "failed",
                    "message": "æµ‹è¯•å¤±è´¥",
                    "output": result.stdout[-500:] if result.stdout else "",
                    "error": result.stderr[-500:] if result.stderr else "",
                    "test_count": self._count_tests_in_output(result.stdout),
                }

        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "message": "æµ‹è¯•è¶…æ—¶",
                "output": "",
                "error": "æµ‹è¯•è¿è¡Œè¶…è¿‡60ç§’",
            }
        except Exception as e:
            return {
                "status": "error",
                "message": f"æµ‹è¯•è¿è¡Œé”™è¯¯: {e}",
                "output": "",
                "error": str(e),
            }

    def _count_tests_in_output(self, output: str) -> int:
        """ä»æµ‹è¯•è¾“å‡ºä¸­ç»Ÿè®¡æµ‹è¯•æ•°é‡"""
        if not output:
            return 0

        # æŸ¥æ‰¾ç±»ä¼¼ "3 passed" çš„æ¨¡å¼
        import re

        match = re.search(r"(\d+) passed", output)
        if match:
            return int(match.group(1))

        # æˆ–è€…è®¡ç®— "test_" å‡ºç°çš„æ¬¡æ•°
        return output.count("::test_")

    def validate_test_coverage(self, test_files: List[str]) -> Dict[str, Any]:
        """éªŒè¯æµ‹è¯•è¦†ç›–ç‡"""
        if not test_files:
            return {"status": "skipped", "message": "æ²¡æœ‰æµ‹è¯•æ–‡ä»¶"}

        try:
            # è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
            result = subprocess.run(
                ["pytest"]
                + [f"tests/{tf}" for tf in test_files]
                + ["--cov=src", "--cov-report=json", "--cov-report=term-missing"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=120,
            )

            coverage_data = {}
            coverage_file = self.project_root / "coverage.json"

            if coverage_file.exists():
                with open(coverage_file, "r") as f:
                    coverage_data = json.load(f)

            total_coverage = coverage_data.get("totals", {}).get("percent_covered", 0)

            return {
                "status": "completed",
                "total_coverage": total_coverage,
                "coverage_data": coverage_data.get("files", {}),
                "meets_threshold": total_coverage >= 80,
                "output": result.stdout[-500:] if result.stdout else "",
            }

        except Exception as e:
            return {
                "status": "error",
                "message": f"è¦†ç›–ç‡æ£€æŸ¥å¤±è´¥: {e}",
                "error": str(e),
            }


class LintValidator:
    """Linté…ç½®éªŒè¯å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root

    def validate_lint_configs(self, config_files: List[str]) -> Dict[str, Any]:
        """éªŒè¯linté…ç½®æ˜¯å¦æœ‰æ•ˆ"""
        results = {
            "total_configs": len(config_files),
            "valid_configs": 0,
            "invalid_configs": 0,
            "config_results": {},
        }

        for config_file in config_files:
            config_path = self.project_root / config_file

            if "pyproject.toml" in config_file or "ruff" in config_file:
                result = self._validate_ruff_config(config_path)
            elif "mypy" in config_file:
                result = self._validate_mypy_config(config_path)
            elif "bandit" in config_file:
                result = self._validate_bandit_config(config_path)
            else:
                result = self._validate_generic_config(config_path)

            results["config_results"][config_file] = result

            if result["status"] == "valid":
                results["valid_configs"] += 1
            else:
                results["invalid_configs"] += 1

        results["success_rate"] = (
            results["valid_configs"] / max(results["total_configs"], 1)
        ) * 100

        return results

    def _validate_ruff_config(self, config_path: Path) -> Dict[str, Any]:
        """éªŒè¯ruffé…ç½®"""
        if not config_path.exists():
            return {"status": "not_found", "message": f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"}

        try:
            # æµ‹è¯•ruffé…ç½®
            result = subprocess.run(
                ["ruff", "check", "--config", str(config_path), "src/", "--no-fix"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=30,
            )

            # ruffè¿”å›ç å¯èƒ½ä¸ä¸º0ï¼ˆå› ä¸ºæœ‰linté—®é¢˜ï¼‰ï¼Œä½†é…ç½®æœ‰æ•ˆ
            return {
                "status": "valid",
                "message": "Ruffé…ç½®æœ‰æ•ˆ",
                "output": result.stdout[-200:] if result.stdout else "",
                "issues_found": result.returncode != 0,
            }

        except subprocess.TimeoutExpired:
            return {"status": "timeout", "message": "Ruffæ£€æŸ¥è¶…æ—¶"}
        except FileNotFoundError:
            return {"status": "tool_not_found", "message": "Ruffå·¥å…·æœªå®‰è£…"}
        except Exception as e:
            return {"status": "error", "message": f"Ruffé…ç½®éªŒè¯å¤±è´¥: {e}"}

    def _validate_mypy_config(self, config_path: Path) -> Dict[str, Any]:
        """éªŒè¯mypyé…ç½®"""
        if not config_path.exists():
            return {"status": "not_found", "message": f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"}

        try:
            # æµ‹è¯•mypyé…ç½®
            result = subprocess.run(
                ["mypy", "--config-file", str(config_path), "src/"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=60,
            )

            return {
                "status": "valid",
                "message": "MyPyé…ç½®æœ‰æ•ˆ",
                "output": result.stdout[-200:] if result.stdout else "",
                "issues_found": result.returncode != 0,
            }

        except subprocess.TimeoutExpired:
            return {"status": "timeout", "message": "MyPyæ£€æŸ¥è¶…æ—¶"}
        except FileNotFoundError:
            return {"status": "tool_not_found", "message": "MyPyå·¥å…·æœªå®‰è£…"}
        except Exception as e:
            return {"status": "error", "message": f"MyPyé…ç½®éªŒè¯å¤±è´¥: {e}"}

    def _validate_bandit_config(self, config_path: Path) -> Dict[str, Any]:
        """éªŒè¯bandité…ç½®"""
        if not config_path.exists():
            return {"status": "not_found", "message": f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"}

        try:
            # æµ‹è¯•bandité…ç½®
            result = subprocess.run(
                ["bandit", "-c", str(config_path), "-r", "src/", "-f", "json"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=30,
            )

            return {
                "status": "valid",
                "message": "Bandité…ç½®æœ‰æ•ˆ",
                "output": result.stdout[-200:] if result.stdout else "",
                "issues_found": result.returncode != 0,
            }

        except subprocess.TimeoutExpired:
            return {"status": "timeout", "message": "Banditæ£€æŸ¥è¶…æ—¶"}
        except FileNotFoundError:
            return {"status": "tool_not_found", "message": "Banditå·¥å…·æœªå®‰è£…"}
        except Exception as e:
            return {"status": "error", "message": f"Bandité…ç½®éªŒè¯å¤±è´¥: {e}"}

    def _validate_generic_config(self, config_path: Path) -> Dict[str, Any]:
        """éªŒè¯é€šç”¨é…ç½®æ–‡ä»¶"""
        if not config_path.exists():
            return {"status": "not_found", "message": f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}"}

        try:
            # åŸºç¡€æ–‡ä»¶è¯­æ³•æ£€æŸ¥
            with open(config_path, "r", encoding="utf-8") as f:
                content = f.read()

            if config_path.suffix == ".toml":
                import tomllib

                tomllib.loads(content)
            elif config_path.suffix in [".yaml", ".yml"]:
                import yaml

                yaml.safe_load(content)
            elif config_path.suffix == ".json":
                json.loads(content)

            return {
                "status": "valid",
                "message": "é…ç½®æ–‡ä»¶è¯­æ³•æ­£ç¡®",
                "size": len(content),
            }

        except Exception as e:
            return {"status": "invalid", "message": f"é…ç½®æ–‡ä»¶è¯­æ³•é”™è¯¯: {e}"}


class PreCommitValidator:
    """Pre-commité’©å­éªŒè¯å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root

    def validate_precommit_hooks(self, precommit_files: List[str]) -> Dict[str, Any]:
        """éªŒè¯pre-commité’©å­é…ç½®"""
        if not precommit_files:
            return {"status": "skipped", "message": "æ²¡æœ‰pre-commité…ç½®"}

        config_path = self.project_root / ".pre-commit-config.yaml"
        if not config_path.exists():
            return {"status": "not_found", "message": "pre-commité…ç½®æ–‡ä»¶ä¸å­˜åœ¨"}

        try:
            # éªŒè¯é…ç½®æ–‡ä»¶è¯­æ³•
            with open(config_path, "r", encoding="utf-8") as f:
                import yaml

                config = yaml.safe_load(f)

            if not isinstance(config, dict) or "repos" not in config:
                return {"status": "invalid", "message": "pre-commité…ç½®æ ¼å¼é”™è¯¯"}

            # å°è¯•å®‰è£…å’Œè¿è¡Œpre-commit
            install_result = self._install_precommit_hooks()
            if not install_result["success"]:
                return {
                    "status": "install_failed",
                    "message": "pre-commité’©å­å®‰è£…å¤±è´¥",
                    "error": install_result["error"],
                }

            # è¿è¡Œpre-commitæ£€æŸ¥
            run_result = self._run_precommit_hooks()

            return {
                "status": "valid",
                "message": "pre-commité’©å­é…ç½®æœ‰æ•ˆ",
                "hooks_count": len(self._count_hooks(config)),
                "install_result": install_result,
                "run_result": run_result,
            }

        except Exception as e:
            return {"status": "error", "message": f"pre-commitéªŒè¯å¤±è´¥: {e}"}

    def _install_precommit_hooks(self) -> Dict[str, Any]:
        """å®‰è£…pre-commité’©å­"""
        try:
            result = subprocess.run(
                ["pre-commit", "install"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=60,
            )

            return {
                "success": result.returncode == 0,
                "output": result.stdout[-200:] if result.stdout else "",
                "error": result.stderr[-200:] if result.stderr else "",
            }

        except subprocess.TimeoutExpired:
            return {"success": False, "error": "pre-commitå®‰è£…è¶…æ—¶"}
        except FileNotFoundError:
            return {"success": False, "error": "pre-commitå·¥å…·æœªå®‰è£…"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _run_precommit_hooks(self) -> Dict[str, Any]:
        """è¿è¡Œpre-commité’©å­"""
        try:
            # åªè¿è¡Œå¿«é€Ÿæ£€æŸ¥ï¼Œé¿å…é•¿æ—¶é—´è¿è¡Œ
            result = subprocess.run(
                ["pre-commit", "run", "--all-files", "trailing-whitespace"],
                capture_output=True,
                text=True,
                cwd=self.project_root,
                timeout=30,
            )

            return {
                "success": True,  # pre-commitå¯èƒ½ä¼šæ‰¾åˆ°é—®é¢˜ï¼Œä½†è¿™æ˜¯æ­£å¸¸çš„
                "output": result.stdout[-200:] if result.stdout else "",
                "issues_found": result.returncode != 0,
            }

        except subprocess.TimeoutExpired:
            return {"success": False, "error": "pre-commitè¿è¡Œè¶…æ—¶"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _count_hooks(self, config: Dict) -> List[str]:
        """ç»Ÿè®¡é…ç½®ä¸­çš„é’©å­æ•°é‡"""
        hooks = []
        for repo in config.get("repos", []):
            for hook in repo.get("hooks", []):
                hooks.append(hook.get("id", "unknown"))
        return hooks


class CIWorkflowValidator:
    """CIå·¥ä½œæµéªŒè¯å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.workflows_dir = project_root / ".github" / "workflows"

    def validate_ci_workflows(self, workflow_files: List[str]) -> Dict[str, Any]:
        """éªŒè¯CIå·¥ä½œæµæ–‡ä»¶"""
        results = {
            "total_workflows": len(workflow_files),
            "valid_workflows": 0,
            "invalid_workflows": 0,
            "workflow_results": {},
        }

        for workflow_file in workflow_files:
            workflow_path = self.workflows_dir / workflow_file
            result = self._validate_single_workflow(workflow_path)
            results["workflow_results"][workflow_file] = result

            if result["status"] == "valid":
                results["valid_workflows"] += 1
            else:
                results["invalid_workflows"] += 1

        results["success_rate"] = (
            results["valid_workflows"] / max(results["total_workflows"], 1)
        ) * 100

        return results

    def _validate_single_workflow(self, workflow_path: Path) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªå·¥ä½œæµæ–‡ä»¶"""
        if not workflow_path.exists():
            return {
                "status": "not_found",
                "message": f"å·¥ä½œæµæ–‡ä»¶ä¸å­˜åœ¨: {workflow_path}",
            }

        try:
            with open(workflow_path, "r", encoding="utf-8") as f:
                import yaml

                workflow = yaml.safe_load(f)

            # æ£€æŸ¥åŸºæœ¬ç»“æ„
            if not isinstance(workflow, dict):
                return {"status": "invalid", "message": "å·¥ä½œæµæ–‡ä»¶æ ¼å¼é”™è¯¯"}

            required_fields = ["name", "on", "jobs"]
            missing_fields = [field for field in required_fields if field not in workflow]
            if missing_fields:
                return {
                    "status": "invalid",
                    "message": f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {', '.join(missing_fields)}",
                }

            # æ£€æŸ¥ä½œä¸šç»“æ„
            jobs = workflow.get("jobs", {})
            if not jobs:
                return {"status": "invalid", "message": "å·¥ä½œæµæ²¡æœ‰å®šä¹‰ä½œä¸š"}

            job_count = len(jobs)
            step_count = sum(len(job.get("steps", [])) for job in jobs.values())

            # æ£€æŸ¥æ˜¯å¦åŒ…å«é˜²å¾¡éªŒè¯æ­¥éª¤
            has_defense_steps = False
            for job in jobs.values():
                for step in job.get("steps", []):
                    step_name = step.get("name", "").lower()
                    if any(
                        keyword in step_name for keyword in ["defense", "validation", "guardian"]
                    ):
                        has_defense_steps = True
                        break
                if has_defense_steps:
                    break

            return {
                "status": "valid",
                "message": "å·¥ä½œæµæ–‡ä»¶æœ‰æ•ˆ",
                "job_count": job_count,
                "step_count": step_count,
                "has_defense_steps": has_defense_steps,
            }

        except yaml.YAMLError as e:
            return {"status": "invalid", "message": f"YAMLè¯­æ³•é”™è¯¯: {e}"}
        except Exception as e:
            return {"status": "error", "message": f"å·¥ä½œæµéªŒè¯å¤±è´¥: {e}"}


class EffectivenessValidator:
    """é˜²å¾¡æ•ˆæœéªŒè¯å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root

    def validate_defense_effectiveness(
        self, original_issues: List[Dict], defenses: Dict[str, List[str]]
    ) -> Dict[str, Any]:
        """éªŒè¯é˜²å¾¡æœºåˆ¶çš„æœ‰æ•ˆæ€§"""
        results = {
            "original_issues_count": len(original_issues),
            "defense_types": len([v for v in defenses.values() if v]),
            "test_reproduction": {},
            "issue_coverage": {},
            "effectiveness_score": 0,
        }

        # å°è¯•é‡ç°åŸå§‹é—®é¢˜ä»¥éªŒè¯é˜²å¾¡æªæ–½
        results["test_reproduction"] = self._test_issue_reproduction(original_issues)

        # åˆ†æé—®é¢˜è¦†ç›–ç‡
        results["issue_coverage"] = self._analyze_issue_coverage(original_issues, defenses)

        # è®¡ç®—æ•ˆæœè¯„åˆ†
        results["effectiveness_score"] = self._calculate_effectiveness_score(results)

        return results

    def _test_issue_reproduction(self, issues: List[Dict]) -> Dict[str, Any]:
        """æµ‹è¯•æ˜¯å¦èƒ½é‡ç°åŸå§‹é—®é¢˜"""
        reproduction_results = {
            "reproducible_issues": 0,
            "non_reproducible_issues": 0,
            "issue_details": {},
        }

        # åˆ›å»ºä¸´æ—¶çš„"å"ä»£ç æ¥æµ‹è¯•é˜²å¾¡æœºåˆ¶
        test_cases = self._create_test_cases_for_issues(issues)

        for issue_type, test_case in test_cases.items():
            result = self._run_defense_test(test_case)
            reproduction_results["issue_details"][issue_type] = result

            if result["detected"]:
                reproduction_results["reproducible_issues"] += 1
            else:
                reproduction_results["non_reproducible_issues"] += 1

        return reproduction_results

    def _create_test_cases_for_issues(self, issues: List[Dict]) -> Dict[str, Dict]:
        """ä¸ºä¸åŒç±»å‹çš„é—®é¢˜åˆ›å»ºæµ‹è¯•ç”¨ä¾‹"""
        test_cases = {}

        # æŒ‰é—®é¢˜ç±»å‹åˆ†ç»„
        issues_by_type = {}
        for issue in issues:
            issue_type = issue.get("category", "unknown")
            if issue_type not in issues_by_type:
                issues_by_type[issue_type] = []
            issues_by_type[issue_type].append(issue)

        # ä¸ºæ¯ç§é—®é¢˜ç±»å‹åˆ›å»ºæµ‹è¯•ç”¨ä¾‹
        for issue_type, issue_list in issues_by_type.items():
            if issue_type == "import_error":
                test_cases[issue_type] = self._create_import_error_test()
            elif issue_type == "type_error":
                test_cases[issue_type] = self._create_type_error_test()
            elif issue_type == "style_error":
                test_cases[issue_type] = self._create_style_error_test()
            elif issue_type == "security_issue":
                test_cases[issue_type] = self._create_security_issue_test()

        return test_cases

    def _create_import_error_test(self) -> Dict[str, Any]:
        """åˆ›å»ºå¯¼å…¥é”™è¯¯æµ‹è¯•ç”¨ä¾‹"""
        bad_code = """
# æ•…æ„åˆ›å»ºçš„é”™è¯¯ä»£ç æ¥æµ‹è¯•é˜²å¾¡æœºåˆ¶
import non_existent_module
from another_missing_module import something

def test_function():
    return non_existent_module.do_something()
"""

        return {
            "type": "import_error",
            "code": bad_code,
            "expected_tools": ["pytest", "mypy"],
            "description": "æµ‹è¯•å¯¼å…¥ä¸å­˜åœ¨çš„æ¨¡å—",
        }

    def _create_type_error_test(self) -> Dict[str, Any]:
        """åˆ›å»ºç±»å‹é”™è¯¯æµ‹è¯•ç”¨ä¾‹"""
        bad_code = """
# æ•…æ„åˆ›å»ºçš„ç±»å‹é”™è¯¯æ¥æµ‹è¯•é˜²å¾¡æœºåˆ¶
def add_numbers(a: int, b: int) -> int:
    return a + b

# ç±»å‹é”™è¯¯ï¼šä¼ é€’å­—ç¬¦ä¸²ç»™æœŸæœ›æ•´æ•°çš„å‡½æ•°
result = add_numbers("hello", "world")
"""

        return {
            "type": "type_error",
            "code": bad_code,
            "expected_tools": ["mypy"],
            "description": "æµ‹è¯•ç±»å‹ä¸åŒ¹é…é”™è¯¯",
        }

    def _create_style_error_test(self) -> Dict[str, Any]:
        """åˆ›å»ºä»£ç é£æ ¼é”™è¯¯æµ‹è¯•ç”¨ä¾‹"""
        bad_code = """
# æ•…æ„åˆ›å»ºçš„é£æ ¼é”™è¯¯æ¥æµ‹è¯•é˜²å¾¡æœºåˆ¶
import os,sys,json
import numpy

def badly_formatted_function(   x,y   ):
    if True:
            return x+y


    # å¤šä½™çš„ç©ºè¡Œå’Œç³Ÿç³•çš„æ ¼å¼
"""

        return {
            "type": "style_error",
            "code": bad_code,
            "expected_tools": ["ruff", "black"],
            "description": "æµ‹è¯•ä»£ç é£æ ¼é—®é¢˜",
        }

    def _create_security_issue_test(self) -> Dict[str, Any]:
        """åˆ›å»ºå®‰å…¨é—®é¢˜æµ‹è¯•ç”¨ä¾‹"""
        bad_code = """
# æ•…æ„åˆ›å»ºçš„å®‰å…¨é—®é¢˜æ¥æµ‹è¯•é˜²å¾¡æœºåˆ¶
import os
password = os.getenv("DEMO_HARDCODED_PASSWORD", "hardcoded_password_123")

def vulnerable_function():
    import subprocess
    # ä¸å®‰å…¨çš„å‘½ä»¤æ‰§è¡Œ
    subprocess.call("rm -rf /", shell=True)

    # ä¸å®‰å…¨çš„evalä½¿ç”¨
    user_input = "print('hello')"
    eval(user_input)
"""

        return {
            "type": "security_issue",
            "code": bad_code,
            "expected_tools": ["bandit"],
            "description": "æµ‹è¯•å®‰å…¨æ¼æ´æ£€æµ‹",
        }

    def _run_defense_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œé˜²å¾¡æµ‹è¯•"""
        result = {"detected": False, "detection_tools": [], "output": "", "error": ""}

        # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False) as f:
            f.write(test_case["code"])
            temp_file = Path(f.name)

        try:
            # å¯¹æ¯ä¸ªé¢„æœŸå·¥å…·è¿è¡Œæµ‹è¯•
            for tool in test_case.get("expected_tools", []):
                tool_result = self._test_tool_detection(tool, temp_file)
                if tool_result["detected"]:
                    result["detected"] = True
                    result["detection_tools"].append(tool)
                    result["output"] += f"{tool}: {tool_result['output']}\n"

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_file.unlink(missing_ok=True)

        return result

    def _test_tool_detection(self, tool: str, file_path: Path) -> Dict[str, Any]:
        """æµ‹è¯•ç‰¹å®šå·¥å…·æ˜¯å¦èƒ½æ£€æµ‹é—®é¢˜"""
        try:
            if tool == "ruff":
                result = subprocess.run(
                    ["ruff", "check", str(file_path)],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
            elif tool == "mypy":
                result = subprocess.run(
                    ["mypy", str(file_path)], capture_output=True, text=True, timeout=10
                )
            elif tool == "bandit":
                result = subprocess.run(
                    ["bandit", str(file_path)],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
            elif tool == "pytest":
                result = subprocess.run(
                    ["python", "-m", "py_compile", str(file_path)],
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
            else:
                return {"detected": False, "output": f"æœªçŸ¥å·¥å…·: {tool}"}

            # å¦‚æœå·¥å…·è¿”å›é0é€€å‡ºç ï¼Œè¯´æ˜æ£€æµ‹åˆ°äº†é—®é¢˜
            detected = result.returncode != 0
            output = result.stdout + result.stderr

            return {
                "detected": detected,
                "output": output[-100:] if output else "",  # åªä¿ç•™æœ€å100å­—ç¬¦
            }

        except Exception as e:
            return {"detected": False, "output": f"å·¥å…·è¿è¡Œé”™è¯¯: {e}"}

    def _analyze_issue_coverage(
        self, issues: List[Dict], defenses: Dict[str, List[str]]
    ) -> Dict[str, Any]:
        """åˆ†æé—®é¢˜è¦†ç›–ç‡"""
        issue_types = set(issue.get("category", "unknown") for issue in issues)

        coverage_analysis = {
            "total_issue_types": len(issue_types),
            "covered_types": 0,
            "uncovered_types": [],
            "coverage_details": {},
        }

        # æ£€æŸ¥æ¯ç§é—®é¢˜ç±»å‹æ˜¯å¦æœ‰å¯¹åº”çš„é˜²å¾¡æªæ–½
        for issue_type in issue_types:
            has_coverage = False
            coverage_methods = []

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åº”çš„æµ‹è¯•
            test_files = defenses.get("test_files", [])
            if any(issue_type in test_file for test_file in test_files):
                has_coverage = True
                coverage_methods.append("validation_tests")

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åº”çš„linté…ç½®
            if issue_type in ["style_error", "type_error", "security_issue"]:
                lint_configs = defenses.get("lint_configs", [])
                if lint_configs:
                    has_coverage = True
                    coverage_methods.append("lint_configs")

            # æ£€æŸ¥æ˜¯å¦æœ‰pre-commité’©å­
            if defenses.get("pre_commit_hooks"):
                has_coverage = True
                coverage_methods.append("pre_commit_hooks")

            coverage_analysis["coverage_details"][issue_type] = {
                "covered": has_coverage,
                "methods": coverage_methods,
            }

            if has_coverage:
                coverage_analysis["covered_types"] += 1
            else:
                coverage_analysis["uncovered_types"].append(issue_type)

        coverage_analysis["coverage_rate"] = (
            coverage_analysis["covered_types"] / max(coverage_analysis["total_issue_types"], 1)
        ) * 100

        return coverage_analysis

    def _calculate_effectiveness_score(self, results: Dict[str, Any]) -> float:
        """è®¡ç®—é˜²å¾¡æ•ˆæœè¯„åˆ†"""
        scores = []

        # æµ‹è¯•é‡ç°è¯„åˆ†ï¼ˆ30%ï¼‰
        repro = results.get("test_reproduction", {})
        total_repro = repro.get("reproducible_issues", 0) + repro.get("non_reproducible_issues", 0)
        if total_repro > 0:
            repro_score = (repro.get("reproducible_issues", 0) / total_repro) * 100
            scores.append(repro_score * 0.3)

        # é—®é¢˜è¦†ç›–ç‡è¯„åˆ†ï¼ˆ40%ï¼‰
        coverage = results.get("issue_coverage", {})
        coverage_rate = coverage.get("coverage_rate", 0)
        scores.append(coverage_rate * 0.4)

        # é˜²å¾¡ç±»å‹å¤šæ ·æ€§è¯„åˆ†ï¼ˆ20%ï¼‰
        defense_types = results.get("defense_types", 0)
        diversity_score = min(defense_types * 25, 100)  # æ¯ç§ç±»å‹25åˆ†ï¼Œæœ€é«˜100
        scores.append(diversity_score * 0.2)

        # åŸå§‹é—®é¢˜æ•°é‡è¯„åˆ†ï¼ˆ10%ï¼‰
        issue_count = results.get("original_issues_count", 0)
        issue_score = min(issue_count * 10, 100)  # æ¯ä¸ªé—®é¢˜10åˆ†ï¼Œæœ€é«˜100
        scores.append(issue_score * 0.1)

        return sum(scores)


class DefenseValidator:
    """é˜²å¾¡æœºåˆ¶éªŒè¯å™¨ä¸»æ§åˆ¶å™¨"""

    def __init__(self, project_root: Path = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()

        self.test_validator = TestValidator(self.project_root)
        self.lint_validator = LintValidator(self.project_root)
        self.precommit_validator = PreCommitValidator(self.project_root)
        self.workflow_validator = CIWorkflowValidator(self.project_root)
        self.effectiveness_validator = EffectivenessValidator(self.project_root)

    def validate_all_defenses(
        self, defenses: Dict[str, List[str]], original_issues: List[Dict] = None
    ) -> Dict[str, Any]:
        """éªŒè¯æ‰€æœ‰é˜²å¾¡æœºåˆ¶"""
        start_time = time.time()

        validation_results = {
            "timestamp": datetime.now().isoformat(),
            "project_root": str(self.project_root),
            "validation_summary": {},
            "detailed_results": {},
            "overall_score": 0,
            "validation_time": 0,
        }

        click.echo("ğŸ” å¼€å§‹éªŒè¯é˜²å¾¡æœºåˆ¶...")

        # 1. éªŒè¯æµ‹è¯•æ–‡ä»¶
        if defenses.get("test_files"):
            click.echo("  ğŸ“‹ éªŒè¯æµ‹è¯•æ–‡ä»¶...")
            test_results = self.test_validator.validate_test_files(defenses["test_files"])
            validation_results["detailed_results"]["tests"] = test_results

            # éªŒè¯æµ‹è¯•è¦†ç›–ç‡
            coverage_results = self.test_validator.validate_test_coverage(defenses["test_files"])
            validation_results["detailed_results"]["coverage"] = coverage_results

        # 2. éªŒè¯linté…ç½®
        if defenses.get("lint_configs"):
            click.echo("  ğŸ”§ éªŒè¯linté…ç½®...")
            lint_results = self.lint_validator.validate_lint_configs(defenses["lint_configs"])
            validation_results["detailed_results"]["lint_configs"] = lint_results

        # 3. éªŒè¯pre-commité’©å­
        if defenses.get("pre_commit_hooks"):
            click.echo("  ğŸª éªŒè¯pre-commité’©å­...")
            precommit_results = self.precommit_validator.validate_precommit_hooks(
                defenses["pre_commit_hooks"]
            )
            validation_results["detailed_results"]["pre_commit"] = precommit_results

        # 4. éªŒè¯CIå·¥ä½œæµ
        if defenses.get("ci_workflows"):
            click.echo("  âš™ï¸ éªŒè¯CIå·¥ä½œæµ...")
            workflow_results = self.workflow_validator.validate_ci_workflows(
                defenses["ci_workflows"]
            )
            validation_results["detailed_results"]["workflows"] = workflow_results

        # 5. éªŒè¯é˜²å¾¡æ•ˆæœ
        if original_issues:
            click.echo("  ğŸ¯ éªŒè¯é˜²å¾¡æ•ˆæœ...")
            effectiveness_results = self.effectiveness_validator.validate_defense_effectiveness(
                original_issues, defenses
            )
            validation_results["detailed_results"]["effectiveness"] = effectiveness_results

        # è®¡ç®—æ€»ä½“è¯„åˆ†
        validation_results["overall_score"] = self._calculate_overall_score(
            validation_results["detailed_results"]
        )
        validation_results["validation_time"] = time.time() - start_time

        # ç”ŸæˆéªŒè¯æ‘˜è¦
        validation_results["validation_summary"] = self._generate_validation_summary(
            validation_results
        )

        return validation_results

    def _calculate_overall_score(self, detailed_results: Dict[str, Any]) -> float:
        """è®¡ç®—æ€»ä½“éªŒè¯è¯„åˆ†"""
        scores = []
        weights = {
            "tests": 0.25,
            "lint_configs": 0.25,
            "pre_commit": 0.15,
            "workflows": 0.15,
            "effectiveness": 0.20,
        }

        for category, weight in weights.items():
            if category in detailed_results:
                result = detailed_results[category]

                if category == "tests":
                    score = result.get("success_rate", 0)
                elif category == "lint_configs":
                    score = result.get("success_rate", 0)
                elif category == "pre_commit":
                    score = 100 if result.get("status") == "valid" else 0
                elif category == "workflows":
                    score = result.get("success_rate", 0)
                elif category == "effectiveness":
                    score = result.get("effectiveness_score", 0)
                else:
                    score = 0

                scores.append(score * weight)

        return sum(scores)

    def _generate_validation_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆéªŒè¯æ‘˜è¦"""
        summary = {
            "overall_status": "passed" if results["overall_score"] >= 80 else "failed",
            "total_score": results["overall_score"],
            "validation_time": results["validation_time"],
            "component_status": {},
        }

        detailed = results.get("detailed_results", {})

        # æµ‹è¯•çŠ¶æ€
        if "tests" in detailed:
            test_result = detailed["tests"]
            summary["component_status"]["tests"] = {
                "status": ("passed" if test_result.get("success_rate", 0) >= 80 else "failed"),
                "passed": test_result.get("passed_tests", 0),
                "total": test_result.get("total_tests", 0),
                "success_rate": test_result.get("success_rate", 0),
            }

        # Linté…ç½®çŠ¶æ€
        if "lint_configs" in detailed:
            lint_result = detailed["lint_configs"]
            summary["component_status"]["lint_configs"] = {
                "status": ("passed" if lint_result.get("success_rate", 0) >= 80 else "failed"),
                "valid": lint_result.get("valid_configs", 0),
                "total": lint_result.get("total_configs", 0),
                "success_rate": lint_result.get("success_rate", 0),
            }

        # Pre-commitçŠ¶æ€
        if "pre_commit" in detailed:
            precommit_result = detailed["pre_commit"]
            summary["component_status"]["pre_commit"] = {
                "status": precommit_result.get("status", "unknown"),
                "message": precommit_result.get("message", ""),
            }

        # å·¥ä½œæµçŠ¶æ€
        if "workflows" in detailed:
            workflow_result = detailed["workflows"]
            summary["component_status"]["workflows"] = {
                "status": ("passed" if workflow_result.get("success_rate", 0) >= 80 else "failed"),
                "valid": workflow_result.get("valid_workflows", 0),
                "total": workflow_result.get("total_workflows", 0),
                "success_rate": workflow_result.get("success_rate", 0),
            }

        # æ•ˆæœéªŒè¯çŠ¶æ€
        if "effectiveness" in detailed:
            effectiveness_result = detailed["effectiveness"]
            summary["component_status"]["effectiveness"] = {
                "score": effectiveness_result.get("effectiveness_score", 0),
                "coverage_rate": effectiveness_result.get("issue_coverage", {}).get(
                    "coverage_rate", 0
                ),
                "reproducible_issues": effectiveness_result.get("test_reproduction", {}).get(
                    "reproducible_issues", 0
                ),
            }

        return summary


@click.command()
@click.option("--defenses-file", "-d", help="é˜²å¾¡æœºåˆ¶JSONæ–‡ä»¶è·¯å¾„")
@click.option("--issues-file", "-i", help="åŸå§‹é—®é¢˜JSONæ–‡ä»¶è·¯å¾„")
@click.option("--project-root", "-p", help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„")
@click.option("--output", "-o", help="éªŒè¯ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
@click.option("--tests-only", "-t", is_flag=True, help="ä»…éªŒè¯æµ‹è¯•æ–‡ä»¶")
@click.option("--configs-only", "-c", is_flag=True, help="ä»…éªŒè¯é…ç½®æ–‡ä»¶")
@click.option("--effectiveness-only", "-e", is_flag=True, help="ä»…éªŒè¯é˜²å¾¡æ•ˆæœ")
@click.option("--summary", "-s", is_flag=True, help="æ˜¾ç¤ºéªŒè¯æ‘˜è¦")
@click.option("--detailed", "-dt", is_flag=True, help="æ˜¾ç¤ºè¯¦ç»†ç»“æœ")
def main(
    defenses_file,
    issues_file,
    project_root,
    output,
    tests_only,
    configs_only,
    effectiveness_only,
    summary,
    detailed,
):
    """
    ğŸ” é˜²å¾¡æœºåˆ¶éªŒè¯ç³»ç»Ÿ

    éªŒè¯ç”Ÿæˆçš„é˜²å¾¡æœºåˆ¶æ˜¯å¦èƒ½æœ‰æ•ˆå·¥ä½œå¹¶æ•è·CIé—®é¢˜ã€‚

    Examples:
        defense_validator.py -d logs/defenses_generated.json -s
        defense_validator.py -d logs/defenses_generated.json -i logs/ci_issues.json -e
        defense_validator.py -t -c  # ä»…éªŒè¯æµ‹è¯•å’Œé…ç½®
    """

    project_path = Path(project_root) if project_root else Path.cwd()
    validator = DefenseValidator(project_path)

    click.echo("ğŸ” é˜²å¾¡æœºåˆ¶éªŒè¯ç³»ç»Ÿå¯åŠ¨")

    # è¯»å–é˜²å¾¡æœºåˆ¶æ–‡ä»¶
    if not defenses_file:
        defenses_file = project_path / "logs" / "defenses_generated.json"

    defenses_path = Path(defenses_file)
    if not defenses_path.exists():
        click.echo(f"âŒ é˜²å¾¡æœºåˆ¶æ–‡ä»¶ä¸å­˜åœ¨: {defenses_file}")
        return

    try:
        with open(defenses_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            defenses = data.get("defenses", {})
    except Exception as e:
        click.echo(f"âŒ è¯»å–é˜²å¾¡æœºåˆ¶æ–‡ä»¶å¤±è´¥: {e}")
        return

    # è¯»å–åŸå§‹é—®é¢˜æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    original_issues = []
    if issues_file:
        issues_path = Path(issues_file)
        if issues_path.exists():
            try:
                with open(issues_path, "r", encoding="utf-8") as f:
                    original_issues = json.load(f)
            except Exception as e:
                click.echo(f"âš ï¸ è¯»å–é—®é¢˜æ–‡ä»¶å¤±è´¥: {e}")

    click.echo(f"ğŸ“‹ éªŒè¯ {sum(len(files) for files in defenses.values())} ä¸ªé˜²å¾¡æœºåˆ¶æ–‡ä»¶")

    # æ ¹æ®é€‰é¡¹æ‰§è¡Œç‰¹å®šéªŒè¯
    if tests_only:
        test_results = validator.test_validator.validate_test_files(defenses.get("test_files", []))
        click.echo(f"ğŸ“Š æµ‹è¯•éªŒè¯ç»“æœ: {test_results.get('success_rate', 0):.1f}% é€šè¿‡ç‡")
    elif configs_only:
        lint_results = validator.lint_validator.validate_lint_configs(
            defenses.get("lint_configs", [])
        )
        click.echo(f"ğŸ“Š é…ç½®éªŒè¯ç»“æœ: {lint_results.get('success_rate', 0):.1f}% æœ‰æ•ˆç‡")
    elif effectiveness_only:
        if original_issues:
            effectiveness_results = (
                validator.effectiveness_validator.validate_defense_effectiveness(
                    original_issues, defenses
                )
            )
            click.echo(
                f"ğŸ“Š é˜²å¾¡æ•ˆæœè¯„åˆ†: {effectiveness_results.get('effectiveness_score', 0):.1f}"
            )
        else:
            click.echo("âŒ éœ€è¦åŸå§‹é—®é¢˜æ–‡ä»¶æ¥éªŒè¯é˜²å¾¡æ•ˆæœ")
    else:
        # æ‰§è¡Œå®Œæ•´éªŒè¯
        validation_results = validator.validate_all_defenses(defenses, original_issues)

        if summary:
            summary_data = validation_results["validation_summary"]
            click.echo("\nğŸ“Š éªŒè¯æ‘˜è¦:")
            click.echo(f"  æ€»ä½“çŠ¶æ€: {summary_data['overall_status']}")
            click.echo(f"  æ€»ä½“è¯„åˆ†: {summary_data['total_score']:.1f}")
            click.echo(f"  éªŒè¯ç”¨æ—¶: {summary_data['validation_time']:.2f}ç§’")

            for component, status in summary_data["component_status"].items():
                if isinstance(status, dict) and "status" in status:
                    click.echo(f"  {component}: {status['status']}")

        if detailed:
            click.echo("\nğŸ“‹ è¯¦ç»†ç»“æœ:")
            for category, result in validation_results["detailed_results"].items():
                click.echo(f"  {category}:")
                if "success_rate" in result:
                    click.echo(f"    æˆåŠŸç‡: {result['success_rate']:.1f}%")
                if "message" in result:
                    click.echo(f"    ä¿¡æ¯: {result['message']}")

        # ä¿å­˜éªŒè¯ç»“æœ
        if output:
            output_path = Path(output)
        else:
            output_path = project_path / "logs" / "validation_results.json"

        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(validation_results, f, indent=2, ensure_ascii=False)
            click.echo(f"ğŸ’¾ éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        except Exception as e:
            click.echo(f"âš ï¸ ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")

        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        overall_status = validation_results["validation_summary"]["overall_status"]
        if overall_status == "passed":
            click.echo("âœ… é˜²å¾¡æœºåˆ¶éªŒè¯é€šè¿‡ï¼Œå¯ä»¥æœ‰æ•ˆé˜²æŠ¤CIé—®é¢˜")
        else:
            click.echo("âŒ é˜²å¾¡æœºåˆ¶éªŒè¯å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥å®Œå–„")


if __name__ == "__main__":
    main()
