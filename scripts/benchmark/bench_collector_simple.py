#!/usr/bin/env python3
"""P1-7 é‡‡é›†å™¨å‹æµ‹è„šæœ¬ - ç®€åŒ–ç‰ˆ
P1-7 Collector Load Testing Script - Simplified Version.

ä¸“æ³¨äºRateLimiteræ€§èƒ½æµ‹è¯•ï¼Œé¿å…å¤æ‚çš„collectorä¾èµ–ã€‚
Focus on RateLimiter performance testing, avoiding complex collector dependencies.

Author: Claude Code
Version: 1.0.0
"""

import asyncio
import time
import json
import sys
import statistics
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from dataclasses import dataclass

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/app')

from src.collectors.rate_limiter import RateLimiter


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ."""
    test_name: str
    concurrent_requests: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    total_time: float
    min_response_time: float
    max_response_time: float
    avg_response_time: float
    p50_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    error_rate: float
    rate_limit_hits: int


@dataclass
class RequestMetric:
    """å•ä¸ªè¯·æ±‚æŒ‡æ ‡."""
    request_id: int
    start_time: float
    end_time: float
    response_time: float
    success: bool
    rate_limited: bool = False
    error_message: Optional[str] = None


class SimpleCollectorBenchmarker:
    """ç®€åŒ–é‡‡é›†å™¨åŸºå‡†æµ‹è¯•å™¨."""

    def __init__(self):
        """åˆå§‹åŒ–åŸºå‡†æµ‹è¯•å™¨."""
        self.rate_limiter = None
        self.results: list[RequestMetric] = []

    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ."""
        print("ğŸ”§ åˆå§‹åŒ–ç®€åŒ–é‡‡é›†å™¨å‹æµ‹ç¯å¢ƒ...")

        try:
            # åˆå§‹åŒ–RateLimiter
            rate_limit_config = {
                "test_domain": {
                    "rate": 10.0,  # 10 QPSé™åˆ¶
                    "burst": 20,  # çªå‘é™åˆ¶
                    "max_wait_time": 60.0  # æœ€å¤§ç­‰å¾…60ç§’
                },
                "default": {
                    "rate": 5.0,  # é»˜è®¤5 QPS
                    "burst": 10,  # çªå‘é™åˆ¶
                    "max_wait_time": 60.0
                }
            }
            self.rate_limiter = RateLimiter(rate_limit_config)

            print("âœ… RateLimiteråˆå§‹åŒ–å®Œæˆ")
            print("   ğŸ“Š é…ç½®: test_domain 10 QPS, default 5 QPS")

        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def simulate_request(self, request_id: int, domain: str = "test_domain") -> RequestMetric:
        """æ¨¡æ‹Ÿå•ä¸ªè¯·æ±‚."""
        start_time = time.time()
        success = False
        rate_limited = False
        error_message = None

        try:
            # ä½¿ç”¨RateLimiterè¿›è¡Œé™æµæ§åˆ¶
            async with self.rate_limiter.acquire(domain):
                # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚å’Œæ•°æ®è·å–
                await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿ50msç½‘ç»œå»¶è¿Ÿ
                await asyncio.sleep(0.02)  # æ¨¡æ‹Ÿ20msæ•°æ®å¤„ç†

                success = True
                # print(f"   âœ… Request {request_id}: æˆåŠŸ (domain: {domain})")

        except Exception as e:
            if "rate limit" in str(e).lower() or "timeout" in str(e).lower():
                rate_limited = True
                error_message = f"Rate limited: {e}"
            else:
                error_message = f"Request failed: {e}"
            # print(f"   âŒ Request {request_id}: å¤±è´¥ - {error_message}")

        end_time = time.time()
        response_time = end_time - start_time

        return RequestMetric(
            request_id=request_id,
            start_time=start_time,
            end_time=end_time,
            response_time=response_time,
            success=success,
            rate_limited=rate_limited,
            error_message=error_message
        )

    async def run_rate_limiter_test(self):
        """ä¸“é—¨æµ‹è¯•RateLimiteræ•ˆæœ."""
        print("\nğŸ¯ RateLimiterä¸“é¡¹æµ‹è¯•")
        print("-" * 30)

        domains = ["test_domain", "default"]

        for domain in domains:
            print(f"\n   ğŸ“Š æµ‹è¯•åŸŸå: {domain}")
            print(f"      ç†è®ºQPS: {self.rate_limiter.config[domain].rate}")

            # æµ‹è¯•è¿ç»­è¯·æ±‚
            request_times = []

            for i in range(5):
                start = time.time()
                async with self.rate_limiter.acquire(domain):
                    await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                end = time.time()
                request_time = end - start
                request_times.append(request_time)
                print(f"      è¯·æ±‚ {i+1}: é—´éš” {request_time*1000:.1f}ms")

            avg_interval = statistics.mean(request_times) if request_times else 0
            expected_interval = 1000 / self.rate_limiter.config[domain].rate

            print(f"      ç†è®ºé—´éš”: {expected_interval:.0f}ms")
            print(f"      å®é™…å¹³å‡é—´éš”: {avg_interval*1000:.1f}ms")
            print(f"      é™æµæ•ˆæœ: {'æœ‰æ•ˆ' if avg_interval >= expected_interval * 0.8 else 'æ— æ•ˆ'}")

    async def run_concurrent_test(self, concurrent_count: int = 50, test_name: str = "") -> BenchmarkResult:
        """è¿è¡Œå¹¶å‘æµ‹è¯•."""
        if test_name:
            print(f"\nğŸš€ {test_name}: {concurrent_count} ä¸ªå¹¶å‘è¯·æ±‚")
        else:
            print(f"\nğŸš€ å¹¶å‘æµ‹è¯•: {concurrent_count} ä¸ªå¹¶å‘è¯·æ±‚")
        print("-" * 50)

        # æ¸…ç©ºä¹‹å‰çš„ç»“æœ
        self.results.clear()

        # è®°å½•å¼€å§‹æ—¶é—´
        overall_start = time.time()

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for i in range(concurrent_count):
            domain = "test_domain" if i % 2 == 0 else "default"  # äº¤æ›¿ä½¿ç”¨ä¸åŒåŸŸå
            task = asyncio.create_task(
                self.simulate_request(i + 1, domain),
                name=f"request_{i+1}"
            )
            tasks.append(task)

        # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        print(f"   â³ æ‰§è¡Œ {len(tasks)} ä¸ªå¹¶å‘è¯·æ±‚...")
        completed_tasks = await asyncio.gather(*tasks, return_exceptions=True)

        # è®°å½•ç»“æŸæ—¶é—´
        overall_end = time.time()

        # å¤„ç†ç»“æœ
        for i, task_result in enumerate(completed_tasks):
            if isinstance(task_result, RequestMetric):
                self.results.append(task_result)
            else:
                # å¤„ç†å¼‚å¸¸
                self.results.append(RequestMetric(
                    request_id=i + 1,
                    start_time=overall_start,
                    end_time=time.time(),
                    response_time=time.time() - overall_start,
                    success=False,
                    error_message=f"Task exception: {str(task_result)}"
                ))

        # è®¡ç®—åŸºå‡†ç»“æœ
        total_time = overall_end - overall_start
        successful_requests = len([r for r in self.results if r.success])
        failed_requests = len(self.results) - successful_requests
        rate_limit_hits = len([r for r in self.results if r.rate_limited])

        response_times = [r.response_time for r in self.results]
        min_response_time = min(response_times) if response_times else 0
        max_response_time = max(response_times) if response_times else 0
        avg_response_time = statistics.mean(response_times) if response_times else 0
        p50_response_time = statistics.median(response_times) if response_times else 0

        if len(response_times) >= 20:
            sorted_times = sorted(response_times)
            p95_index = int(len(sorted_times) * 0.95)
            p99_index = int(len(sorted_times) * 0.99)
            p95_response_time = sorted_times[p95_index]
            p99_response_time = sorted_times[p99_index]
        else:
            p95_response_time = max_response_time
            p99_response_time = max_response_time

        requests_per_second = len(self.results) / total_time if total_time > 0 else 0
        error_rate = (failed_requests / len(self.results)) * 100 if self.results else 0

        return BenchmarkResult(
            test_name=test_name or f"Concurrent_{concurrent_count}",
            concurrent_requests=concurrent_count,
            total_requests=len(self.results),
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            total_time=total_time,
            min_response_time=min_response_time,
            max_response_time=max_response_time,
            avg_response_time=avg_response_time,
            p50_response_time=p50_response_time,
            p95_response_time=p95_response_time,
            p99_response_time=p99_response_time,
            requests_per_second=requests_per_second,
            error_rate=error_rate,
            rate_limit_hits=rate_limit_hits
        )

    async def generate_report(self, results: list[BenchmarkResult]) -> str:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š."""
        print("\nğŸ“‹ ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("-" * 50)

        report_lines = [
            "# P1-7 é‡‡é›†å™¨åŸºå‡†æµ‹è¯•æŠ¥å‘Š",
            "# P1-7 Collector Benchmark Report",
            "",
            f"**æµ‹è¯•æ—¶é—´**: {datetime.now().isoformat()}",
            "**æµ‹è¯•ç‰ˆæœ¬**: P1-7 v1.0.0",
            "",
            "## ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦",
            "",
            "| æµ‹è¯•åœºæ™¯ | å¹¶å‘æ•° | æ€»è¯·æ±‚æ•° | æˆåŠŸæ•° | å¤±è´¥æ•° | RPS | å¹³å‡å“åº”æ—¶é—´ | P95å“åº”æ—¶é—´ | é”™è¯¯ç‡ | é™æµå‘½ä¸­ |",
            "|----------|--------|----------|--------|--------|-----|-------------|-------------|--------|----------|",
        ]

        for result in results:
            report_lines.append(
                f"| {result.test_name} | {result.concurrent_requests} | "
                f"{result.total_requests} | {result.successful_requests} | "
                f"{result.failed_requests} | {result.requests_per_second:.2f} | "
                f"{result.avg_response_time*1000:.1f}ms | "
                f"{result.p95_response_time*1000:.1f}ms | "
                f"{result.error_rate:.2f}% | {result.rate_limit_hits} |"
            )

        report_lines.extend([
            "",
            "## ğŸ¯ æ€§èƒ½æŒ‡æ ‡åˆ†æ",
            "",
            f"- **å¹³å‡RPS**: {statistics.mean([r.requests_per_second for r in results]):.2f}",
            f"- **å¹³å‡å“åº”æ—¶é—´**: {statistics.mean([r.avg_response_time*1000 for r in results]):.1f}ms",
            f"- **å¹³å‡P95å“åº”æ—¶é—´**: {statistics.mean([r.p95_response_time*1000 for r in results]):.1f}ms",
            f"- **å¹³å‡æˆåŠŸç‡**: {statistics.mean([(r.successful_requests/r.total_requests)*100 for r in results]):.2f}%",
            f"- **å¹³å‡é™æµå‘½ä¸­**: {statistics.mean([r.rate_limit_hits for r in results]):.1f}",
            "",
            "## ğŸ” æ€§èƒ½åˆ†æ",
            ""
        ])

        # æ€§èƒ½åˆ†æ
        avg_rps = statistics.mean([r.requests_per_second for r in results])
        avg_response_time = statistics.mean([r.avg_response_time*1000 for r in results])
        avg_rate_limit_hits = statistics.mean([r.rate_limit_hits for r in results])

        if avg_rps >= 8 and avg_response_time <= 200:
            performance_level = "ä¼˜ç§€"
        elif avg_rps >= 5 and avg_response_time <= 500:
            performance_level = "è‰¯å¥½"
        else:
            performance_level = "éœ€è¦ä¼˜åŒ–"

        report_lines.extend([
            f"- **æ•´ä½“æ€§èƒ½è¯„çº§**: {performance_level}",
            f"- **RateLimiteræ•ˆæœ**: {'æœ‰æ•ˆ' if avg_rate_limit_hits > 0 else 'æœªè§¦å‘'}",
            f"- **å¹¶å‘å¤„ç†èƒ½åŠ›**: {max([r.concurrent_requests for r in results])} å¹¶å‘",
            "",
            "## ğŸ“ˆ ä¼˜åŒ–å»ºè®®",
            ""
        ])

        if avg_rate_limit_hits > len(results) * 2:  # å¦‚æœå¹³å‡é™æµå‘½ä¸­æ•°è¾ƒé«˜
            report_lines.extend([
                "1. **è°ƒæ•´é™æµç­–ç•¥**: å½“å‰é™æµè¾ƒä¸ºä¸¥æ ¼ï¼Œå¯é€‚å½“æé«˜QPSé™åˆ¶",
                "2. **å¢åŠ å¹¶å‘å®¹é‡**: ç³»ç»Ÿå¯ä»¥æ‰¿å—æ›´é«˜çš„å¹¶å‘è´Ÿè½½",
                "3. **ä¼˜åŒ–ç¼“å­˜ç­–ç•¥**: å‡å°‘å¯¹é™æµå™¨çš„ä¾èµ–"
            ])
        else:
            report_lines.extend([
                "1. **å¹¶å‘æ•°ä¼˜åŒ–**: å½“å‰ç³»ç»Ÿå¯ä»¥å¤„ç†æ›´é«˜çš„å¹¶å‘æ•°",
                "2. **æ€§èƒ½ç›‘æ§**: æŒç»­ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡",
                "3. **èµ„æºåˆ©ç”¨**: è¯„ä¼°ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"
            ])

        report_lines.extend([
            "",
            f"**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().isoformat()}",
        ])

        return "\n".join(report_lines)

    async def run_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•."""
        print("ğŸš€ å¼€å§‹P1-7é‡‡é›†å™¨åŸºå‡†æµ‹è¯• (ç®€åŒ–ç‰ˆ)")
        print("=" * 60)

        try:
            # è®¾ç½®ç¯å¢ƒ
            await self.setup()

            # RateLimiterä¸“é¡¹æµ‹è¯•
            await self.run_rate_limiter_test()

            # å¹¶å‘æµ‹è¯• - ä¸åŒçš„å¹¶å‘çº§åˆ«
            test_scenarios = [
                (10, "å°å¹¶å‘æµ‹è¯•"),
                (25, "ä¸­ç­‰å¹¶å‘æµ‹è¯•"),
                (50, "é«˜å¹¶å‘æµ‹è¯•"),
                (100, "æé«˜å¹¶å‘æµ‹è¯•")
            ]
            all_results = []

            for concurrent_count, test_name in test_scenarios:
                result = await self.run_concurrent_test(concurrent_count, test_name)
                all_results.append(result)

                # æ‰“å°å³æ—¶ç»“æœ
                print("\nğŸ“Š å³æ—¶ç»“æœ:")
                print(f"   âœ… æ€»è¯·æ±‚æ•°: {result.total_requests}")
                print(f"   âœ… æˆåŠŸè¯·æ±‚: {result.successful_requests}")
                print(f"   âŒ å¤±è´¥è¯·æ±‚: {result.failed_requests}")
                print(f"   ğŸ“ˆ RPS: {result.requests_per_second:.2f}")
                print(f"   â±ï¸  å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time*1000:.1f}ms")
                print(f"   â±ï¸  P95å“åº”æ—¶é—´: {result.p95_response_time*1000:.1f}ms")
                print(f"   âŒ é”™è¯¯ç‡: {result.error_rate:.2f}%")
                print(f"   ğŸš¦ é™æµå‘½ä¸­: {result.rate_limit_hits}")

                # æµ‹è¯•é—´éš”
                if concurrent_count != test_scenarios[-1][0]:
                    print("\nâ³ ç­‰å¾… 3 ç§’åè¿›è¡Œä¸‹ä¸€ä¸ªæµ‹è¯•...")
                    await asyncio.sleep(3)

            # ç”ŸæˆæŠ¥å‘Š
            report = await self.generate_report(all_results)

            # ä¿å­˜æŠ¥å‘Š
            report_path = "/app/reports/benchmark_collector_baseline.md"
            try:
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"\nâœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
            except Exception as e:
                print(f"\nâš ï¸ æŠ¥å‘Šä¿å­˜å¤±è´¥: {e}")

            return report, all_results

        except Exception as e:
            print(f"\nâŒ åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, []


async def main():
    """ä¸»å‡½æ•°."""
    benchmarker = SimpleCollectorBenchmarker()
    report, results = await benchmarker.run_benchmark()

    if report and results:
        print("\n" + "=" * 60)
        print("ğŸ¯ P1-7é‡‡é›†å™¨åŸºå‡†æµ‹è¯•å®Œæˆ")
        print("=" * 60)

        # æ€»ç»“
        avg_rps = statistics.mean([r.requests_per_second for r in results])
        avg_success_rate = statistics.mean([(r.successful_requests/r.total_requests)*100 for r in results])
        avg_p95 = statistics.mean([r.p95_response_time*1000 for r in results])
        avg_rate_limit_hits = statistics.mean([r.rate_limit_hits for r in results])

        print(f"ğŸ“Š å¹³å‡RPS: {avg_rps:.2f}")
        print(f"ğŸ“Š å¹³å‡æˆåŠŸç‡: {avg_success_rate:.2f}%")
        print(f"ğŸ“Š å¹³å‡P95å“åº”æ—¶é—´: {avg_p95:.1f}ms")
        print(f"ğŸ“Š å¹³å‡é™æµå‘½ä¸­: {avg_rate_limit_hits:.1f}")

        # æ€§èƒ½è¯„ä¼°
        if avg_rps >= 8 and avg_success_rate >= 90 and avg_p95 <= 500:
            print("ğŸ† æ€§èƒ½è¯„çº§: ä¼˜ç§€")
        elif avg_rps >= 5 and avg_success_rate >= 80 and avg_p95 <= 1000:
            print("ğŸ‘ æ€§èƒ½è¯„çº§: è‰¯å¥½")
        else:
            print("âš ï¸ æ€§èƒ½è¯„çº§: éœ€è¦ä¼˜åŒ–")

        print("\nğŸš€ P1-7é‡‡é›†å™¨å‹æµ‹æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼")

        return True
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
