#!/usr/bin/env python3
"""
æŸ¥è¯¢ç¼“å­˜å®ç°è„šæœ¬
Query Caching Implementation Script

å®ç°Redisç¼“å­˜å±‚ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½ï¼Œç›®æ ‡ç¼“å­˜å‘½ä¸­ç‡ > 80%ã€‚
"""

import asyncio
import hashlib
import json
import logging
import os
import sys
import time
from typing import Any, Optional, Union

import redis.asyncio as redis
from sqlalchemy.ext.asyncio import AsyncSession

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from src.core.config import get_settings
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼
    class MockSettings:
        def __init__(self):
            self.redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")

    def get_settings():
        return MockSettings()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QueryCache:
    """æŸ¥è¯¢ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–æŸ¥è¯¢ç¼“å­˜"""
        self.settings = get_settings()
        self.redis_client: Optional[redis.Redis] = None
        self.memory_cache = {}  # å†…å­˜ç¼“å­˜åå¤‡
        self.cache_stats = {
            'hits': 0,
            'misses': 0,
            'sets': 0,
            'deletes': 0
        }

    async def connect(self):
        """è¿æ¥Redis"""
        try:
            self.redis_client = redis.Redis.from_url(
                self.settings.redis_url or "redis://localhost:6379",
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=5,
                socket_timeout=5,
                retry_on_timeout=True,
                health_check_interval=30
            )

            # æµ‹è¯•è¿æ¥
            await self.redis_client.ping()
            logger.info("âœ… Redisç¼“å­˜è¿æ¥æˆåŠŸ")

        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜ç¼“å­˜: {e}")
            self.redis_client = None

    def _generate_cache_key(self, query: str, params: dict[str, Any] = None) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{query}:{json.dumps(params or {}, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def get(self, cache_key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        # å°è¯•ä»Redisè·å–
        if self.redis_client:
            try:
                cached_data = await self.redis_client.get(cache_key)
                if cached_data:
                    self.cache_stats['hits'] += 1
                    return json.loads(cached_data)
            except Exception as e:
                logger.error(f"âŒ Redisè·å–ç¼“å­˜å¤±è´¥: {e}")

        # ä»å†…å­˜ç¼“å­˜è·å–
        if cache_key in self.memory_cache:
            self.cache_stats['hits'] += 1
            return self.memory_cache[cache_key]

        self.cache_stats['misses'] += 1
        return None

    async def set(self, cache_key: str, value: Any, ttl: int = 3600) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        # å°è¯•è®¾ç½®åˆ°Redis
        redis_success = False
        if self.redis_client:
            try:
                cached_data = json.dumps(value, default=str)
                await self.redis_client.setex(cache_key, ttl, cached_data)
                redis_success = True
                self.cache_stats['sets'] += 1
            except Exception as e:
                logger.error(f"âŒ Redisè®¾ç½®ç¼“å­˜å¤±è´¥: {e}")

        # è®¾ç½®åˆ°å†…å­˜ç¼“å­˜
        self.memory_cache[cache_key] = value
        if not redis_success:
            self.cache_stats['sets'] += 1

        return True

    async def delete(self, cache_key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        # å°è¯•ä»Redisåˆ é™¤
        redis_success = False
        if self.redis_client:
            try:
                await self.redis_client.delete(cache_key)
                redis_success = True
                self.cache_stats['deletes'] += 1
            except Exception as e:
                logger.error(f"âŒ Redisåˆ é™¤ç¼“å­˜å¤±è´¥: {e}")

        # ä»å†…å­˜ç¼“å­˜åˆ é™¤
        if cache_key in self.memory_cache:
            del self.memory_cache[cache_key]
            if not redis_success:
                self.cache_stats['deletes'] += 1

        return True

    async def invalidate_pattern(self, pattern: str) -> int:
        """æŒ‰æ¨¡å¼åˆ é™¤ç¼“å­˜"""
        if not self.redis_client:
            return 0

        try:
            keys = await self.redis_client.keys(pattern)
            if keys:
                deleted_count = await self.redis_client.delete(*keys)
                self.cache_stats['deletes'] += deleted_count
                return deleted_count
            return 0

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
            return 0

    def get_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        total_requests = self.cache_stats['hits'] + self.cache_stats['misses']
        if total_requests == 0:
            return 0.0
        return (self.cache_stats['hits'] / total_requests) * 100

    def get_cache_stats(self) -> dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        hit_rate = self.get_hit_rate()
        return {
            **self.cache_stats,
            'hit_rate': hit_rate,
            'total_requests': self.cache_stats['hits'] + self.cache_stats['misses']
        }

    async def close(self):
        """å…³é—­Redisè¿æ¥"""
        if self.redis_client:
            await self.redis_client.close()


class CachedUserRepository:
    """å¸¦ç¼“å­˜çš„ç”¨æˆ·ä»“å‚¨"""

    def __init__(self, user_repository, cache: QueryCache):
        """åˆå§‹åŒ–ç¼“å­˜ç”¨æˆ·ä»“å‚¨"""
        self.user_repository = user_repository
        self.cache = cache

    async def get_by_email(self, email: str, session: AsyncSession = None) -> Optional[Any]:
        """å¸¦ç¼“å­˜çš„æ ¹æ®é‚®ç®±è·å–ç”¨æˆ·"""
        cache_key = f"user:email:{email}"

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_user = await self.cache.get(cache_key)
        if cached_user:
            logger.debug(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­: user:email:{email}")
            return cached_user

        # ä»æ•°æ®åº“è·å–
        user = await self.user_repository.get_by_email(email, session)
        if user:
            # ç¼“å­˜ç»“æœ (1å°æ—¶)
            user_data = {
                'id': user.id,
                'username': user.username,
                'email': user.email,
                'is_active': user.is_active,
                'role': user.role,
                'created_at': user.created_at.isoformat() if user.created_at else None,
                'updated_at': user.updated_at.isoformat() if user.updated_at else None
            }
            await self.cache.set(cache_key, user_data, ttl=3600)
            logger.debug(f"ğŸ’¾ ç¼“å­˜è®¾ç½®: user:email:{email}")

        return user

    async def get_by_username(self, username: str, session: AsyncSession = None) -> Optional[Any]:
        """å¸¦ç¼“å­˜çš„æ ¹æ®ç”¨æˆ·åè·å–ç”¨æˆ·"""
        cache_key = f"user:username:{username}"

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_user = await self.cache.get(cache_key)
        if cached_user:
            logger.debug(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­: user:username:{username}")
            return cached_user

        # ä»æ•°æ®åº“è·å–
        user = await self.user_repository.get_by_username(username, session)
        if user:
            # ç¼“å­˜ç»“æœ (1å°æ—¶)
            user_data = {
                'id': user.id,
                'username': user.username,
                'email': user.email,
                'is_active': user.is_active,
                'role': user.role,
                'created_at': user.created_at.isoformat() if user.created_at else None,
                'updated_at': user.updated_at.isoformat() if user.updated_at else None
            }
            await self.cache.set(cache_key, user_data, ttl=3600)
            logger.debug(f"ğŸ’¾ ç¼“å­˜è®¾ç½®: user:username:{username}")

        return user

    async def get_active_users(self, limit: int = 10, session: AsyncSession = None) -> list[Any]:
        """å¸¦ç¼“å­˜çš„è·å–æ´»è·ƒç”¨æˆ·"""
        cache_key = f"users:active:{limit}"

        # å°è¯•ä»ç¼“å­˜è·å–
        cached_users = await self.cache.get(cache_key)
        if cached_users:
            logger.debug(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­: users:active:{limit}")
            return cached_users

        # ä»æ•°æ®åº“è·å–
        users = await self.user_repository.get_list(skip=0, limit=limit, active_only=True, session=session)
        if users:
            # ç¼“å­˜ç»“æœ (30åˆ†é’Ÿ)
            users_data = []
            for user in users:
                users_data.append({
                    'id': user.id,
                    'username': user.username,
                    'email': user.email,
                    'is_active': user.is_active,
                    'role': user.role,
                    'created_at': user.created_at.isoformat() if user.created_at else None,
                    'updated_at': user.updated_at.isoformat() if user.updated_at else None
                })
            await self.cache.set(cache_key, users_data, ttl=1800)
            logger.debug(f"ğŸ’¾ ç¼“å­˜è®¾ç½®: users:active:{limit}")

        return users

    async def invalidate_user_cache(self, user_id: int = None, email: str = None, username: str = None):
        """ä½¿ç”¨æˆ·ç¼“å­˜å¤±æ•ˆ"""
        invalidated_keys = []

        if user_id:
            # æŒ‰ç”¨æˆ·IDå¤±æ•ˆç¼“å­˜
            pattern = f"user:*:{user_id}:*"
            count = await self.cache.invalidate_pattern(pattern)
            invalidated_keys.extend([f"user_id:{user_id}"] * count)

        if email:
            # å¤±æ•ˆé‚®ç®±ç¼“å­˜
            await self.cache.delete(f"user:email:{email}")
            invalidated_keys.append(f"user:email:{email}")

        if username:
            # å¤±æ•ˆç”¨æˆ·åç¼“å­˜
            await self.cache.delete(f"user:username:{username}")
            invalidated_keys.append(f"user:username:{username}")

        # å¤±æ•ˆç”¨æˆ·åˆ—è¡¨ç¼“å­˜
        await self.cache.invalidate_pattern("users:active:*")
        invalidated_keys.append("users:active:*")

        logger.info(f"ğŸ—‘ï¸ ç¼“å­˜å¤±æ•ˆ: {invalidated_keys}")

        return invalidated_keys


class CachePerformanceMonitor:
    """ç¼“å­˜æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, cache: QueryCache):
        """åˆå§‹åŒ–ç¼“å­˜æ€§èƒ½ç›‘æ§å™¨"""
        self.cache = cache
        self.monitoring_enabled = True

    async def start_monitoring(self):
        """å¼€å§‹ç›‘æ§ç¼“å­˜æ€§èƒ½"""
        logger.info("ğŸ“Š å¼€å§‹ç¼“å­˜æ€§èƒ½ç›‘æ§...")

        while self.monitoring_enabled:
            try:
                stats = self.cache.get_cache_stats()
                hit_rate = stats['hit_rate']

                logger.info(f"ğŸ“ˆ ç¼“å­˜æ€§èƒ½ç»Ÿè®¡:")
                logger.info(f"  - å‘½ä¸­ç‡: {hit_rate:.2f}%")
                logger.info(f"  - å‘½ä¸­æ•°: {stats['hits']}")
                logger.info(f"  - æœªå‘½ä¸­æ•°: {stats['misses']}")
                logger.info(f"  - è®¾ç½®æ•°: {stats['sets']}")
                logger.info(f"  - åˆ é™¤æ•°: {stats['deletes']}")

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡å‘½ä¸­ç‡
                if hit_rate >= 80:
                    logger.info("ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡å·²è¾¾åˆ°ç›®æ ‡ (â‰¥80%)")
                else:
                    logger.warning(f"âš ï¸ ç¼“å­˜å‘½ä¸­ç‡æœªè¾¾æ ‡: {hit_rate:.2f}% < 80%")

                # ç­‰å¾…30ç§’å†æ¬¡æ£€æŸ¥
                await asyncio.sleep(30)

            except Exception as e:
                logger.error(f"âŒ ç¼“å­˜ç›‘æ§å‡ºé”™: {e}")
                await asyncio.sleep(30)

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        self.monitoring_enabled = False
        logger.info("â¹ï¸ ç¼“å­˜æ€§èƒ½ç›‘æ§å·²åœæ­¢")


async def test_cache_performance():
    """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
    logger.info("ğŸ§ª å¼€å§‹ç¼“å­˜æ€§èƒ½æµ‹è¯•...")

    # åˆå§‹åŒ–ç¼“å­˜
    cache = QueryCache()
    await cache.connect()

    # æµ‹è¯•åŸºæœ¬ç¼“å­˜åŠŸèƒ½
    start_time = time.time()

    # è®¾ç½®ç¼“å­˜
    await cache.set("test_key_1", {"data": "value1"}, ttl=3600)
    await cache.set("test_key_2", {"data": "value2"}, ttl=3600)
    await cache.set("test_key_3", {"data": "value3"}, ttl=3600)

    # è·å–ç¼“å­˜ï¼ˆåº”è¯¥å‘½ä¸­ï¼‰
    cached_value1 = await cache.get("test_key_1")
    cached_value2 = await cache.get("test_key_2")

    # è·å–ä¸å­˜åœ¨çš„ç¼“å­˜ï¼ˆåº”è¯¥æœªå‘½ä¸­ï¼‰
    cached_value3 = await cache.get("nonexistent_key")

    # å†æ¬¡è·å–å·²å­˜åœ¨çš„ç¼“å­˜ï¼ˆåº”è¯¥å‘½ä¸­ï¼‰
    cached_value4 = await cache.get("test_key_1")

    test_time = time.time() - start_time

    # è¾“å‡ºæµ‹è¯•ç»“æœ
    print("\n" + "="*60)
    print("ğŸ¯ ç¼“å­˜æ€§èƒ½æµ‹è¯•ç»“æœ")
    print("="*60)
    print(f"âœ… ç¼“å­˜è®¾ç½®åŠŸèƒ½: æ­£å¸¸")
    print(f"âœ… ç¼“å­˜è·å–åŠŸèƒ½: æ­£å¸¸")
    print(f"â° æ€»æµ‹è¯•æ—¶é—´: {test_time:.3f}s")
    print(f"ğŸ“ˆ ç¼“å­˜ç»Ÿè®¡: {cache.get_cache_stats()}")
    print(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {cache.get_hit_rate():.2f}%")

    # éªŒè¯ç¼“å­˜æ•°æ®
    assert cached_value1 == {"data": "value1"}
    assert cached_value2 == {"data": "value2"}
    assert cached_value3 is None
    assert cached_value4 == {"data": "value1"}
    print("âœ… ç¼“å­˜æ•°æ®éªŒè¯é€šè¿‡")

    print("="*60)

    await cache.close()


async def main():
    """ä¸»å‡½æ•°"""
    try:
        # æµ‹è¯•ç¼“å­˜æ€§èƒ½
        await test_cache_performance()

        logger.info("âœ… æŸ¥è¯¢ç¼“å­˜å®ç°å®Œæˆ")

    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜å®ç°å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())