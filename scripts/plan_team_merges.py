#!/usr/bin/env python3
"""
é¦–å¸­æ•°æ®æ²»ç†ä¸“å®¶ä¸“ç”¨ - çƒé˜Ÿå®ä½“åˆå¹¶è®¡åˆ’ç”Ÿæˆå™¨
è¯†åˆ«é‡å¤çƒé˜Ÿå¹¶ç”Ÿæˆæ ‡å‡†åŒ–åˆå¹¶è®¡åˆ’
"""

import subprocess
import json
import logging
from datetime import datetime
from difflib import SequenceMatcher
from typing import List, Dict, Tuple

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class TeamMergePlanner:
    """çƒé˜Ÿåˆå¹¶è®¡åˆ’ç”Ÿæˆå™¨"""

    def __init__(self):
        self.teams = []
        self.merge_plan = []
        self.similarity_threshold = 0.85

    def load_teams_from_db(self) -> list[dict]:
        """ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰çƒé˜Ÿ"""
        try:
            cmd = [
                "docker-compose",
                "exec",
                "db",
                "psql",
                "-U",
                "postgres",
                "-d",
                "football_prediction",
                "-tAc",
                "SELECT id, name FROM teams ORDER BY id;",
            ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0:
                teams = []
                for line in result.stdout.strip().split("\n"):
                    if line.strip():
                        parts = line.split("|")
                        if len(parts) >= 2:
                            teams.append(
                                {"id": int(parts[0].strip()), "name": parts[1].strip()}
                            )

                self.teams = teams
                logger.info(f"ğŸ“Š åŠ è½½äº† {len(teams)} ä¸ªçƒé˜Ÿ")
                return teams
            else:
                logger.error(f"âŒ æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {result.stderr}")
                return []

        except Exception as e:
            logger.error(f"âŒ åŠ è½½çƒé˜Ÿæ•°æ®å¼‚å¸¸: {e}")
            return []

    def normalize_team_name(self, name: str) -> str:
        """æ ‡å‡†åŒ–çƒé˜Ÿåç§°ç”¨äºæ¯”è¾ƒ"""
        if not name:
            return ""

        # ç§»é™¤å›½å®¶ä»£ç å‰ç¼€/åç¼€ (å¦‚ "es Barcelona", "Barcelona es")
        parts = name.split()
        if len(parts) >= 2:
            # ç§»é™¤2å­—æ¯å›½å®¶ä»£ç 
            if len(parts[0]) == 2 and parts[0].islower():
                normalized = " ".join(parts[1:])
                logger.debug(f"ğŸ”§ æ ‡å‡†åŒ–: '{name}' -> '{normalized}'")
                return normalized
            elif len(parts[-1]) == 2 and parts[-1].islower():
                normalized = " ".join(parts[:-1])
                logger.debug(f"ğŸ”§ æ ‡å‡†åŒ–: '{name}' -> '{normalized}'")
                return normalized

        # ç§»é™¤å¸¸è§çš„åç¼€
        suffixes = ["FC", "CF", "SC", "AC", "CD", "UD", "SD"]
        normalized = name
        for suffix in suffixes:
            if normalized.endswith(f" {suffix}"):
                normalized = normalized[: -len(f" {suffix}")]
                break

        return normalized.strip()

    def calculate_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªçƒé˜Ÿåçš„ç›¸ä¼¼åº¦"""
        # å…ˆæ ‡å‡†åŒ–åç§°
        norm1 = self.normalize_team_name(name1.lower())
        norm2 = self.normalize_team_name(name2.lower())

        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, norm1, norm2).ratio()
        return similarity

    def find_duplicate_teams(self) -> list[tuple]:
        """æ‰¾å‡ºé‡å¤çš„çƒé˜Ÿ"""
        duplicates = []
        processed_ids = set()

        logger.info(f"ğŸ” å¼€å§‹åˆ†æ {len(self.teams)} ä¸ªçƒé˜Ÿçš„ç›¸ä¼¼åº¦...")

        for i, team1 in enumerate(self.teams):
            if team1["id"] in processed_ids:
                continue

            similar_teams = [team1]

            for j, team2 in enumerate(self.teams[i + 1 :], i + 1):
                if team2["id"] in processed_ids:
                    continue

                similarity = self.calculate_similarity(team1["name"], team2["name"])

                if similarity >= self.similarity_threshold:
                    similar_teams.append(team2)
                    processed_ids.add(team2["id"])
                    logger.info(
                        f"ğŸ¯ å‘ç°ç›¸ä¼¼çƒé˜Ÿ: '{team1['name']}' vs '{team2['name']}' (ç›¸ä¼¼åº¦: {similarity:.3f})"
                    )

            if len(similar_teams) > 1:
                # é€‰æ‹©masterçƒé˜Ÿï¼ˆåå­—æœ€çŸ­æœ€å¹²å‡€çš„ï¼‰
                master = min(similar_teams, key=lambda x: len(x["name"]))

                for team in similar_teams:
                    if team["id"] != master["id"]:
                        duplicates.append((master, team))
                        processed_ids.add(team["id"])

                logger.info(
                    f"ğŸ‘‘ é€‰æ‹©Masterçƒé˜Ÿ: '{master['name']}' (ID: {master['id']})"
                )

        return duplicates

    def generate_merge_plan(self, duplicates: list[tuple]) -> dict:
        """ç”Ÿæˆåˆå¹¶è®¡åˆ’"""
        merge_plan = {
            "generated_at": datetime.now().isoformat(),
            "similarity_threshold": self.similarity_threshold,
            "total_merges": len(duplicates),
            "merges": [],
        }

        for master, duplicate in duplicates:
            merge_plan["merges"].append(
                {
                    "master": {"id": master["id"], "name": master["name"]},
                    "duplicate": {"id": duplicate["id"], "name": duplicate["name"]},
                    "similarity": self.calculate_similarity(
                        master["name"], duplicate["name"]
                    ),
                }
            )

        return merge_plan

    def save_merge_plan(self, merge_plan: dict, filename: str = "merge_plan.json"):
        """ä¿å­˜åˆå¹¶è®¡åˆ’åˆ°æ–‡ä»¶"""
        try:
            with open(filename, "w", encoding="utf-8") as f:
                json.dump(merge_plan, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ’¾ åˆå¹¶è®¡åˆ’å·²ä¿å­˜åˆ° {filename}")
            return True

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜åˆå¹¶è®¡åˆ’å¤±è´¥: {e}")
            return False

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„åˆå¹¶è®¡åˆ’ç”Ÿæˆæµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨çƒé˜Ÿå®ä½“åˆå¹¶è®¡åˆ’ç”Ÿæˆå™¨")

        # åŠ è½½çƒé˜Ÿæ•°æ®
        teams = self.load_teams_from_db()
        if not teams:
            return False

        # æ‰¾å‡ºé‡å¤çƒé˜Ÿ
        duplicates = self.find_duplicate_teams()
        logger.info(f"ğŸ” å‘ç° {len(duplicates)} ç»„é‡å¤çƒé˜Ÿ")

        # ç”Ÿæˆåˆå¹¶è®¡åˆ’
        merge_plan = self.generate_merge_plan(duplicates)

        # ä¿å­˜è®¡åˆ’
        success = self.save_merge_plan(merge_plan)

        if success:
            logger.info("=" * 60)
            logger.info("ğŸ‰ çƒé˜Ÿåˆå¹¶è®¡åˆ’ç”Ÿæˆå®Œæˆï¼")
            logger.info("=" * 60)
            logger.info(f"ğŸ“Š æ€»çƒé˜Ÿæ•°: {len(teams)}")
            logger.info(f"ğŸ”„ åˆå¹¶å¯¹æ•°: {len(duplicates)}")
            logger.info("ğŸ’¾ è®¡åˆ’æ–‡ä»¶: merge_plan.json")
            logger.info("â±ï¸  ä¸‹ä¸€æ­¥: python scripts/execute_team_merges.py")

        return success


def main():
    """ä¸»å‡½æ•°"""
    try:
        planner = TeamMergePlanner()
        success = planner.run()

        if success:
            logger.info("âœ… åˆå¹¶è®¡åˆ’ç”ŸæˆæˆåŠŸ")
            return 0
        else:
            logger.error("âŒ åˆå¹¶è®¡åˆ’ç”Ÿæˆå¤±è´¥")
            return 1

    except Exception as e:
        logger.error(f"ğŸ’¥ ç¨‹åºå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
