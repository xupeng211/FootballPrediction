#!/usr/bin/env python3
"""
L2æ·±åº¦æ•°æ®é‡‡é›†å™¨ - é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡æ•°æ®
Chief Data Governance Engineer: L2æ•°æ®ç®¡é“
Purpose: é‡‡é›†é˜µå®¹ã€è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼Œè¡¥å……L1çš„xGæ•°æ®
"""

import asyncio
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.collectors.fbref_collector import FBrefCollector
from scripts.enhanced_database_saver import EnhancedDatabaseSaver

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class L2DetailsCollector:
    """L2è¯¦ç»†æ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        self.collector = FBrefCollector()
        self.saver = EnhancedDatabaseSaver()

    async def collect_match_details(self, match_report_url: str) -> Dict[str, any]:
        """
        é‡‡é›†å•åœºæ¯”èµ›çš„è¯¦ç»†æ•°æ®

        Args:
            match_report_url: FBrefæ¯”èµ›æŠ¥å‘ŠURL

        Returns:
            åŒ…å«é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡çš„å­—å…¸
        """
        logger.info(f"ğŸ” é‡‡é›†æ¯”èµ›è¯¦æƒ…: {match_report_url}")

        try:
            # è·å–æ¯”èµ›é¡µé¢HTML
            html_content = await self.collector.fetch_html(match_report_url)
            if not html_content:
                logger.warning(f"âš ï¸ æ— æ³•è·å–æ¯”èµ›é¡µé¢: {match_report_url}")
                return {}

            # è§£æè¯¦ç»†ä¿¡æ¯
            details = self._parse_match_details(html_content)
            logger.info(f"âœ… æå–åˆ°è¯¦æƒ…: {list(details.keys())}")
            return details

        except Exception as e:
            logger.error(f"âŒ é‡‡é›†æ¯”èµ›è¯¦æƒ…å¤±è´¥: {e}")
            return {}

    def _parse_match_details(self, html_content: str) -> Dict[str, any]:
        """
        è§£ææ¯”èµ›è¯¦æƒ…HTML

        Args:
            html_content: æ¯”èµ›é¡µé¢HTML

        Returns:
            åŒ…å«é˜µå®¹å’Œç»Ÿè®¡æ•°æ®çš„å­—å…¸
        """
        from bs4 import BeautifulSoup
        import pandas as pd
        from io import StringIO

        soup = BeautifulSoup(html_content, 'html.parser')
        details = {}

        try:
            # 1. è§£ææ‰€æœ‰è¡¨æ ¼
            tables = pd.read_html(StringIO(html_content))
            logger.info(f"ğŸ“Š å‘ç° {len(tables)} ä¸ªè¡¨æ ¼")

            # 2. è¯†åˆ«é˜µå®¹è¡¨æ ¼
            lineups = self._extract_lineups(soup, tables)
            if lineups:
                details['lineups'] = lineups
                logger.info(f"ğŸ‘¥ æå–åˆ°é˜µå®¹æ•°æ®")

            # 3. æå–è¯¦ç»†ç»Ÿè®¡æ•°æ®
            stats = self._extract_detailed_stats(tables)
            if stats:
                details['stats'] = stats
                logger.info(f"ğŸ“ˆ æå–åˆ°è¯¦ç»†ç»Ÿè®¡æ•°æ®")

            # 4. æå–æ¯”èµ›äº‹ä»¶
            events = self._extract_match_events(soup)
            if events:
                details['events'] = events
                logger.info(f"ğŸ“… æå–åˆ°æ¯”èµ›äº‹ä»¶")

        except Exception as e:
            logger.error(f"âŒ è§£ææ¯”èµ›è¯¦æƒ…å¤±è´¥: {e}")

        return details

    def _extract_lineups(self, soup, tables: List) -> Optional[Dict]:
        """æå–é˜µå®¹æ•°æ®"""
        try:
            # æŸ¥æ‰¾é˜µå®¹ç›¸å…³çš„è¡¨æ ¼
            for i, table in enumerate(tables):
                if table.empty:
                    continue

                # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦åŒ…å«é˜µå®¹ä¿¡æ¯
                columns_str = [str(col).lower() for col in table.columns]
                if any(keyword in ' '.join(columns_str) for keyword in
                      ['player', 'starter', 'substitute', 'minute', 'pos']):
                    logger.info(f"ğŸ‘¥ å‘ç°é˜µå®¹è¡¨æ ¼ (ç´¢å¼• {i}): {table.shape}")

                    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                    lineup_data = {
                        'home_lineup': [],
                        'away_lineup': []
                    }

                    # å¤„ç†é˜µå®¹æ•°æ®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                    for _, row in table.iterrows():
                        player_info = {}
                        for col in table.columns:
                            if pd.notna(row.get(col)):
                                player_info[str(col)] = str(row.get(col))

                        # æ ¹æ®æ•°æ®å†…å®¹åˆ¤æ–­æ˜¯ä¸»é˜Ÿè¿˜æ˜¯å®¢é˜Ÿ
                        if player_info:
                            lineup_data['home_lineup'].append(player_info)

                    return lineup_data

        except Exception as e:
            logger.error(f"âŒ æå–é˜µå®¹å¤±è´¥: {e}")

        return None

    def _extract_detailed_stats(self, tables: List) -> Optional[Dict]:
        """æå–è¯¦ç»†ç»Ÿè®¡æ•°æ®"""
        try:
            detailed_stats = {}

            # æŸ¥æ‰¾ç»Ÿè®¡è¡¨æ ¼
            for i, table in enumerate(tables):
                if table.empty:
                    continue

                columns_str = [str(col).lower() for col in table.columns]

                # è¯†åˆ«ä¸åŒç±»å‹çš„ç»Ÿè®¡è¡¨æ ¼
                if any(keyword in ' '.join(columns_str) for keyword in
                      ['possession', 'touches', 'passes', 'pressures', 'aerial']):

                    stat_type = self._identify_stat_type(columns_str)
                    if stat_type:
                        detailed_stats[stat_type] = self._convert_table_to_dict(table)
                        logger.info(f"ğŸ“ˆ å‘ç°{stat_type}ç»Ÿè®¡è¡¨æ ¼ (ç´¢å¼• {i})")

            return detailed_stats if detailed_stats else None

        except Exception as e:
            logger.error(f"âŒ æå–è¯¦ç»†ç»Ÿè®¡å¤±è´¥: {e}")

        return None

    def _identify_stat_type(self, columns: List[str]) -> Optional[str]:
        """è¯†åˆ«ç»Ÿè®¡è¡¨æ ¼ç±»å‹"""
        columns_text = ' '.join(columns).lower()

        if 'possession' in columns_text:
            return 'possession'
        elif 'touches' in columns_text:
            return 'touches'
        elif 'pass' in columns_text:
            return 'passes'
        elif 'pressure' in columns_text:
            return 'pressures'
        elif 'aerial' in columns_text:
            return 'aerial_duels'
        elif 'shot' in columns_text:
            return 'shooting'

        return None

    def _convert_table_to_dict(self, table) -> Dict:
        """å°†DataFrameè½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        try:
            # ç®€åŒ–è½¬æ¢ï¼šå–å‰å‡ è¡Œæ•°æ®
            result = {}
            for col in table.columns:
                if not table[col].empty:
                    # å–ç¬¬ä¸€ä¸ªéç©ºå€¼ä½œä¸ºä»£è¡¨
                    value = table[col].dropna().iloc[0] if not table[col].dropna().empty else None
                    if value is not None:
                        result[str(col)] = str(value)
            return result
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢è¡¨æ ¼å¤±è´¥: {e}")
            return {}

    def _extract_match_events(self, soup) -> Optional[List]:
        """æå–æ¯”èµ›äº‹ä»¶"""
        try:
            events = []

            # æŸ¥æ‰¾äº‹ä»¶ç›¸å…³çš„HTMLå…ƒç´ 
            event_elements = soup.find_all(['div', 'span'],
                                          class_=lambda x: x and ('event' in x.lower() or
                                                                   'goal' in x.lower() or
                                                                   'card' in x.lower() or
                                                                   'sub' in x.lower()))

            for element in event_elements[:10]:  # é™åˆ¶æ•°é‡
                event_text = element.get_text(strip=True)
                if event_text:
                    events.append({
                        'text': event_text,
                        'type': self._classify_event(event_text)
                    })

            return events if events else None

        except Exception as e:
            logger.error(f"âŒ æå–æ¯”èµ›äº‹ä»¶å¤±è´¥: {e}")

        return None

    def _classify_event(self, event_text: str) -> str:
        """åˆ†ç±»äº‹ä»¶ç±»å‹"""
        text_lower = event_text.lower()

        if any(keyword in text_lower for keyword in ['goal', 'âš½', 'scored']):
            return 'goal'
        elif any(keyword in text_lower for keyword in ['yellow', 'ğŸŸ¨']):
            return 'yellow_card'
        elif any(keyword in text_lower for keyword in ['red', 'ğŸŸ¥']):
            return 'red_card'
        elif any(keyword in text_lower for keyword in ['sub', 'substitute', 'â†’']):
            return 'substitution'
        else:
            return 'other'

    async def update_match_with_details(self, match_id: int, match_report_url: str) -> bool:
        """
        æ›´æ–°æ¯”èµ›è®°å½•çš„è¯¦ç»†ä¿¡æ¯

        Args:
            match_id: æ¯”èµ›è®°å½•ID
            match_report_url: æ¯”èµ›æŠ¥å‘ŠURL

        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸ”„ æ›´æ–°æ¯”èµ› {match_id} çš„è¯¦ç»†ä¿¡æ¯")

        try:
            # é‡‡é›†è¯¦ç»†ä¿¡æ¯
            details = await self.collect_match_details(match_report_url)

            if not details:
                logger.warning(f"âš ï¸ æ²¡æœ‰è·å–åˆ°è¯¦ç»†ä¿¡æ¯")
                return False

            # æ›´æ–°æ•°æ®åº“
            import psycopg2
            conn = psycopg2.connect(
                host='localhost',
                port=5432,
                user='postgres',
                password='postgres-dev-password',
                database='football_prediction'
            )

            with conn.cursor() as cur:
                # æ„å»ºæ›´æ–°è¯­å¥
                update_parts = []
                params = {}

                if 'lineups' in details:
                    update_parts.append("lineups = :lineups")
                    params['lineups'] = json.dumps(details['lineups'])

                if 'stats' in details:
                    # åˆå¹¶åˆ°ç°æœ‰statså­—æ®µ
                    cur.execute("SELECT stats FROM matches WHERE id = :match_id",
                              {'match_id': match_id})
                    existing_stats = cur.fetchone()[0] or '{}'

                    try:
                        existing_stats_dict = json.loads(existing_stats) if isinstance(existing_stats, str) else existing_stats
                        existing_stats_dict.update(details['stats'])
                        update_parts.append("stats = :stats")
                        params['stats'] = json.dumps(existing_stats_dict)
                    except:
                        update_parts.append("stats = :stats")
                        params['stats'] = json.dumps(details['stats'])

                if 'events' in details:
                    update_parts.append("events = :events")
                    params['events'] = json.dumps(details['events'])

                if update_parts:
                    update_parts.append("updated_at = CURRENT_TIMESTAMP")

                    sql = f"""
                        UPDATE matches
                        SET {', '.join(update_parts)}
                        WHERE id = :match_id
                    """

                    params['match_id'] = match_id
                    cur.execute(sql, params)
                    conn.commit()

                    logger.info(f"âœ… æˆåŠŸæ›´æ–°æ¯”èµ› {match_id}")
                    return True
                else:
                    logger.warning(f"âš ï¸ æ²¡æœ‰å¯æ›´æ–°çš„å­—æ®µ")
                    return False

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¯”èµ›è¯¦æƒ…å¤±è´¥: {e}")
            return False
        finally:
            if 'conn' in locals():
                conn.close()


async def main():
    """ä¸»å‡½æ•° - L2æ·±åº¦é‡‡é›†å¯åŠ¨ - æŒç»­è¿è¡Œç‰ˆæœ¬"""
    logger.info("ğŸš€ L2æ·±åº¦æ•°æ®é‡‡é›†å™¨å¯åŠ¨ (æŒç»­è¿è¡Œæ¨¡å¼)")
    logger.info("ğŸ¯ ç›®æ ‡: æŒç»­é‡‡é›†é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡æ•°æ®")
    logger.info("â±ï¸  å·¥ä½œæ¨¡å¼: æ¯10æ¡è®°å½•ä¼‘çœ 30ç§’ï¼Œé¿å…ä¸L1ç«äº‰èµ„æº")

    collector = L2DetailsCollector()

    import psycopg2
    conn = psycopg2.connect(
        host='localhost',
        port=5432,
        user='postgres',
        password='postgres-dev-password',
        database='football_prediction'
    )

    processed_count = 0
    batch_count = 0
    total_success = 0

    try:
        while True:  # æŒç»­è¿è¡Œå¾ªç¯
            logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {batch_count + 1} è½®L2é‡‡é›†")

            with conn.cursor() as cur:
                # æŸ¥è¯¢data_completeness = 'partial'ä¸”æœ‰match_report_urlçš„è®°å½•
                cur.execute("""
                    SELECT id, home_team_id, away_team_id, match_metadata
                    FROM matches
                    WHERE data_source = 'fbref'
                    AND data_completeness = 'partial'
                    AND match_metadata::text LIKE '%match_report%'
                    AND (stats IS NULL OR stats = '{}')
                    ORDER BY created_at ASC
                    LIMIT 20
                """)

                records = cur.fetchall()

                if not records:
                    logger.info("ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®°å½•ï¼Œç­‰å¾…60ç§’...")
                    await asyncio.sleep(60)
                    continue

                logger.info(f"ğŸ“Š æœ¬è½®æ‰¾åˆ° {len(records)} æ¡å¾…å¤„ç†è®°å½•")
                batch_success = 0

                for i, (record_id, home_id, away_id, metadata_json) in enumerate(records, 1):
                    try:
                        metadata = json.loads(metadata_json) if metadata_json else {}
                        match_report_url = metadata.get('match_report_url')

                        if match_report_url:
                            logger.info(f"ğŸ”„ [{i}/{len(records)}] å¤„ç†æ¯”èµ› {record_id}: {home_id} vs {away_id}")
                            success = await collector.update_match_with_details(record_id, match_report_url)

                            if success:
                                batch_success += 1
                                total_success += 1

                                # æ›´æ–°data_completenessçŠ¶æ€ä¸ºcomplete
                                cur.execute("""
                                    UPDATE matches
                                    SET data_completeness = 'complete'
                                    WHERE id = :record_id
                                """, {'record_id': record_id})
                                conn.commit()

                                logger.info(f"âœ… æ¯”èµ› {record_id} å·²å‡çº§ä¸ºå®Œæ•´æ•°æ®")

                            processed_count += 1

                            # æ¯å¤„ç†10æ¡è®°å½•ä¼‘çœ 30ç§’
                            if processed_count % 10 == 0:
                                logger.info(f"â¸ï¸ å·²å¤„ç† {processed_count} æ¡è®°å½•ï¼Œä¼‘çœ 30ç§’...")
                                await asyncio.sleep(30)
                            else:
                                # è®°å½•é—´æ­£å¸¸å»¶è¿Ÿ
                                await asyncio.sleep(3)
                        else:
                            logger.warning(f"âš ï¸ æ¯”èµ› {record_id} æ²¡æœ‰match_report_url")

                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ¯”èµ› {record_id} å¤±è´¥: {e}")
                        continue

                batch_count += 1
                logger.info(f"ğŸ‰ ç¬¬ {batch_count} è½®å®Œæˆ: {batch_success}/{len(records)} æ¡è®°å½•æˆåŠŸå‡çº§")
                logger.info(f"ğŸ“Š ç´¯è®¡ç»Ÿè®¡: {total_success} æ¡è®°å½•å·²å‡çº§ä¸ºå®Œæ•´æ•°æ®")

    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­L2é‡‡é›†å™¨")
    except Exception as e:
        logger.error(f"âŒ L2é‡‡é›†å™¨å¼‚å¸¸: {e}")
    finally:
        conn.close()
        logger.info(f"ğŸ”Œ L2é‡‡é›†å™¨å·²åœæ­¢ï¼Œæ€»è®¡å¤„ç† {total_success} æ¡è®°å½•")


if __name__ == "__main__":
    asyncio.run(main())