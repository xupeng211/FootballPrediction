#!/usr/bin/env python3
"""
L2æ·±åº¦æ•°æ®é‡‡é›†å™¨ - é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡æ•°æ®
Chief Data Governance Engineer: L2æ•°æ®ç®¡é“
Purpose: é‡‡é›†é˜µå®¹ã€è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼Œè¡¥å……L1çš„xGæ•°æ®
"""

import asyncio
import json
import logging
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

# æ·»åŠ å¿…è¦çš„ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import pandas as pd
import psycopg2

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.collectors.fbref_collector import FBrefCollector
from scripts.enhanced_database_saver import EnhancedDatabaseSaver

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class L2DetailsCollector:
    """L2è¯¦ç»†æ•°æ®é‡‡é›†å™¨"""

    def __init__(self):
        self.collector = FBrefCollector()
        self.saver = EnhancedDatabaseSaver()

    async def collect_match_details(self, match_report_url: str) -> Dict[str, any]:
        """
        é‡‡é›†å•åœºæ¯”èµ›çš„è¯¦ç»†æ•°æ®

        Args:
            match_report_url: FBrefæ¯”èµ›æŠ¥å‘ŠURL

        Returns:
            åŒ…å«é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡çš„å­—å…¸
        """
        logger.info(f"ğŸ” é‡‡é›†æ¯”èµ›è¯¦æƒ…: {match_report_url}")

        try:
            # è·å–æ¯”èµ›é¡µé¢HTML
            html_content = await self.collector.fetch_html(match_report_url)
            if not html_content:
                logger.warning(f"âš ï¸ æ— æ³•è·å–æ¯”èµ›é¡µé¢: {match_report_url}")
                return {}

            # è§£æè¯¦ç»†ä¿¡æ¯
            details = self._parse_match_details(html_content)
            logger.info(f"âœ… æå–åˆ°è¯¦æƒ…: {list(details.keys())}")
            return details

        except Exception as e:
            logger.error(f"âŒ é‡‡é›†æ¯”èµ›è¯¦æƒ…å¤±è´¥: {e}")
            return {}

    def _parse_match_details(self, html_content: str) -> Dict[str, any]:
        """
        è§£ææ¯”èµ›è¯¦æƒ…HTML

        Args:
            html_content: æ¯”èµ›é¡µé¢HTML

        Returns:
            åŒ…å«é˜µå®¹å’Œç»Ÿè®¡æ•°æ®çš„å­—å…¸
        """
        from bs4 import BeautifulSoup
        import pandas as pd
        from io import StringIO

        soup = BeautifulSoup(html_content, 'html.parser')
        details = {}

        try:
            # 1. è§£ææ‰€æœ‰è¡¨æ ¼
            tables = pd.read_html(StringIO(html_content))
            logger.info(f"ğŸ“Š å‘ç° {len(tables)} ä¸ªè¡¨æ ¼")

            # 2. è¯†åˆ«é˜µå®¹è¡¨æ ¼
            lineups = self._extract_lineups(soup, tables)
            if lineups:
                details['lineups'] = lineups
                logger.info(f"ğŸ‘¥ æå–åˆ°é˜µå®¹æ•°æ®")

            # 3. æå–è¯¦ç»†ç»Ÿè®¡æ•°æ®
            stats = self._extract_detailed_stats(tables)
            if stats:
                details['stats'] = stats
                logger.info(f"ğŸ“ˆ æå–åˆ°è¯¦ç»†ç»Ÿè®¡æ•°æ®")

            # 4. æå–æ¯”èµ›äº‹ä»¶
            events = self._extract_match_events(soup)
            if events:
                details['events'] = events
                logger.info(f"ğŸ“… æå–åˆ°æ¯”èµ›äº‹ä»¶")

        except Exception as e:
            logger.error(f"âŒ è§£ææ¯”èµ›è¯¦æƒ…å¤±è´¥: {e}")

        return details

    def _extract_lineups(self, soup, tables: List) -> Optional[Dict]:
        """æå–é˜µå®¹æ•°æ®ï¼ˆæ ¹æ®è°ƒè¯•ç»“æœä¼˜åŒ–ï¼‰"""
        try:
            # æ–¹æ³•1: æ ¹æ®è°ƒè¯•ç»“æœï¼Œä¼˜å…ˆæŸ¥æ‰¾é˜µå®¹divå…ƒç´ 
            lineup_divs = soup.find_all('div', {'class': 'lineup'})

            if lineup_divs:
                logger.info(f"âœ… æ‰¾åˆ° {len(lineup_divs)} ä¸ªé˜µå®¹divå…ƒç´ ")
                lineup_data = {'home_lineup': [], 'away_lineup': []}

                for div in lineup_divs:
                    team_key = 'home_lineup' if div.get('id') == 'a' else 'away_lineup' if div.get('id') == 'b' else None

                    if team_key:
                        # æŸ¥æ‰¾é˜µå®¹è¡¨æ ¼
                        table = div.find('table')
                        if table:
                            players = self._extract_players_from_soup_table(table)
                            if players:
                                lineup_data[team_key] = players
                                logger.info(f"ğŸ“‹ {team_key}æå–åˆ° {len(players)} åçƒå‘˜")

                # å¦‚æœæˆåŠŸæå–åˆ°é˜µå®¹æ•°æ®ï¼Œç›´æ¥è¿”å›
                if lineup_data['home_lineup'] or lineup_data['away_lineup']:
                    return lineup_data

            # æ–¹æ³•2: å¦‚æœdivæ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨pandasè¡¨æ ¼æ–¹å¼ï¼ˆå…¼å®¹åŸé€»è¾‘ï¼‰
            logger.info("ğŸ”„ divæ–¹æ³•å¤±è´¥ï¼Œå°è¯•pandasè¡¨æ ¼æ–¹å¼...")
            for i, table in enumerate(tables):
                if table.empty:
                    continue

                # æ£€æŸ¥è¡¨æ ¼æ˜¯å¦åŒ…å«é˜µå®¹ä¿¡æ¯
                columns_str = [str(col).lower() for col in table.columns]
                if any(keyword in ' '.join(columns_str) for keyword in
                      ['player', 'starter', 'substitute', 'minute', 'pos', 'number']):
                    logger.info(f"ğŸ‘¥ å‘ç°é˜µå®¹è¡¨æ ¼ (ç´¢å¼• {i}): {table.shape}")

                    lineup_data = {'home_lineup': [], 'away_lineup': []}

                    # å°è¯•æ™ºèƒ½åˆ¤æ–­ä¸»å®¢é˜Ÿ
                    for _, row in table.iterrows():
                        player_info = {}
                        has_valid_data = False

                        for col in table.columns:
                            if pd.notna(row.get(col)):
                                value = str(row.get(col)).strip()
                                if value:
                                    player_info[str(col)] = value
                                    has_valid_data = True

                        # éªŒè¯æ˜¯å¦æ˜¯æœ‰æ•ˆçš„çƒå‘˜ä¿¡æ¯
                        if has_valid_data and self._is_valid_player_info(player_info):
                            # æ™ºèƒ½åˆ†é…åˆ°ä¸»é˜Ÿæˆ–å®¢é˜Ÿï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œéƒ½æ”¾åˆ°ä¸»é˜Ÿï¼‰
                            lineup_data['home_lineup'].append(player_info)

                    if lineup_data['home_lineup']:
                        logger.info(f"ğŸ“‹ pandasæ–¹å¼æå–åˆ° {len(lineup_data['home_lineup'])} åçƒå‘˜")
                        return lineup_data

            # æ–¹æ³•3: æœ€åå°è¯•ï¼Œç›´æ¥ä»HTMLä¸­æŸ¥æ‰¾çƒå‘˜ä¿¡æ¯
            logger.info("ğŸ”„ å°è¯•ç›´æ¥ä»HTMLæŸ¥æ‰¾çƒå‘˜ä¿¡æ¯...")
            return self._extract_lineups_from_html(soup)

        except Exception as e:
            logger.error(f"âŒ æå–é˜µå®¹å¤±è´¥: {e}")

        return None

    def _extract_detailed_stats(self, tables: List) -> Optional[Dict]:
        """æå–è¯¦ç»†ç»Ÿè®¡æ•°æ®"""
        try:
            detailed_stats = {}

            # æŸ¥æ‰¾ç»Ÿè®¡è¡¨æ ¼
            for i, table in enumerate(tables):
                if table.empty:
                    continue

                columns_str = [str(col).lower() for col in table.columns]

                # è¯†åˆ«ä¸åŒç±»å‹çš„ç»Ÿè®¡è¡¨æ ¼
                if any(keyword in ' '.join(columns_str) for keyword in
                      ['possession', 'touches', 'passes', 'pressures', 'aerial']):

                    stat_type = self._identify_stat_type(columns_str)
                    if stat_type:
                        detailed_stats[stat_type] = self._convert_table_to_dict(table)
                        logger.info(f"ğŸ“ˆ å‘ç°{stat_type}ç»Ÿè®¡è¡¨æ ¼ (ç´¢å¼• {i})")

            return detailed_stats if detailed_stats else None

        except Exception as e:
            logger.error(f"âŒ æå–è¯¦ç»†ç»Ÿè®¡å¤±è´¥: {e}")

        return None

    def _identify_stat_type(self, columns: List[str]) -> Optional[str]:
        """è¯†åˆ«ç»Ÿè®¡è¡¨æ ¼ç±»å‹"""
        columns_text = ' '.join(columns).lower()

        if 'possession' in columns_text:
            return 'possession'
        elif 'touches' in columns_text:
            return 'touches'
        elif 'pass' in columns_text:
            return 'passes'
        elif 'pressure' in columns_text:
            return 'pressures'
        elif 'aerial' in columns_text:
            return 'aerial_duels'
        elif 'shot' in columns_text:
            return 'shooting'

        return None

    def _convert_table_to_dict(self, table) -> Dict:
        """å°†DataFrameè½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        try:
            # ç®€åŒ–è½¬æ¢ï¼šå–å‰å‡ è¡Œæ•°æ®
            result = {}
            for col in table.columns:
                if not table[col].empty:
                    # å–ç¬¬ä¸€ä¸ªéç©ºå€¼ä½œä¸ºä»£è¡¨
                    value = table[col].dropna().iloc[0] if not table[col].dropna().empty else None
                    if value is not None:
                        result[str(col)] = str(value)
            return result
        except Exception as e:
            logger.error(f"âŒ è½¬æ¢è¡¨æ ¼å¤±è´¥: {e}")
            return {}

    def _extract_match_events(self, soup) -> Optional[List]:
        """æå–æ¯”èµ›äº‹ä»¶"""
        try:
            events = []

            # æŸ¥æ‰¾äº‹ä»¶ç›¸å…³çš„HTMLå…ƒç´ 
            event_elements = soup.find_all(['div', 'span'],
                                          class_=lambda x: x and ('event' in x.lower() or
                                                                   'goal' in x.lower() or
                                                                   'card' in x.lower() or
                                                                   'sub' in x.lower()))

            for element in event_elements[:10]:  # é™åˆ¶æ•°é‡
                event_text = element.get_text(strip=True)
                if event_text:
                    events.append({
                        'text': event_text,
                        'type': self._classify_event(event_text)
                    })

            return events if events else None

        except Exception as e:
            logger.error(f"âŒ æå–æ¯”èµ›äº‹ä»¶å¤±è´¥: {e}")

        return None

    def _classify_event(self, event_text: str) -> str:
        """åˆ†ç±»äº‹ä»¶ç±»å‹"""
        text_lower = event_text.lower()

        if any(keyword in text_lower for keyword in ['goal', 'âš½', 'scored']):
            return 'goal'
        elif any(keyword in text_lower for keyword in ['yellow', 'ğŸŸ¨']):
            return 'yellow_card'
        elif any(keyword in text_lower for keyword in ['red', 'ğŸŸ¥']):
            return 'red_card'
        elif any(keyword in text_lower for keyword in ['sub', 'substitute', 'â†’']):
            return 'substitution'
        else:
            return 'other'

    async def update_match_with_details(self, match_id: int, match_report_url: str) -> bool:
        """
        æ›´æ–°æ¯”èµ›è®°å½•çš„è¯¦ç»†ä¿¡æ¯

        Args:
            match_id: æ¯”èµ›è®°å½•ID
            match_report_url: æ¯”èµ›æŠ¥å‘ŠURL

        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        logger.info(f"ğŸ”„ æ›´æ–°æ¯”èµ› {match_id} çš„è¯¦ç»†ä¿¡æ¯")

        try:
            # é‡‡é›†è¯¦ç»†ä¿¡æ¯
            details = await self.collect_match_details(match_report_url)

            if not details:
                logger.warning(f"âš ï¸ æ²¡æœ‰è·å–åˆ°è¯¦ç»†ä¿¡æ¯")
                return False

            # æ›´æ–°æ•°æ®åº“
            import psycopg2
            conn = psycopg2.connect(
                host='db',
                port=5432,
                user='postgres',
                password='postgres-dev-password',
                database='football_prediction'
            )

            with conn.cursor() as cur:
                # æ„å»ºæ›´æ–°è¯­å¥
                update_parts = []
                param_values = []

                if 'lineups' in details:
                    update_parts.append("lineups = %s")
                    param_values.append(json.dumps(details['lineups']))

                if 'stats' in details:
                    # åˆå¹¶åˆ°ç°æœ‰statså­—æ®µ
                    cur.execute("SELECT stats FROM matches WHERE id = %s",
                              (match_id,))
                    existing_stats = cur.fetchone()[0] or '{}'

                    try:
                        existing_stats_dict = json.loads(existing_stats) if isinstance(existing_stats, str) else existing_stats
                        existing_stats_dict.update(details['stats'])
                        update_parts.append("stats = %s")
                        param_values.append(json.dumps(existing_stats_dict))
                    except:
                        update_parts.append("stats = %s")
                        param_values.append(json.dumps(details['stats']))

                if 'events' in details:
                    update_parts.append("events = %s")
                    param_values.append(json.dumps(details['events']))

                if update_parts:
                    update_parts.append("updated_at = CURRENT_TIMESTAMP")
                    param_values.append(match_id)  # æœ€åæ·»åŠ match_id

                    sql = f"""
                        UPDATE matches
                        SET {', '.join(update_parts)}
                        WHERE id = %s
                    """

                    cur.execute(sql, param_values)
                    conn.commit()

                    logger.info(f"âœ… æˆåŠŸæ›´æ–°æ¯”èµ› {match_id}")
                    return True
                else:
                    logger.warning(f"âš ï¸ æ²¡æœ‰å¯æ›´æ–°çš„å­—æ®µ")
                    return False

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¯”èµ›è¯¦æƒ…å¤±è´¥: {e}")
            return False
        finally:
            if 'conn' in locals():
                conn.close()

    def _extract_players_from_soup_table(self, table) -> List[Dict]:
        """ä»BeautifulSoupè¡¨æ ¼ä¸­æå–çƒå‘˜ä¿¡æ¯"""
        players = []

        try:
            rows = table.find_all('tr')

            for row in rows:
                # è·³è¿‡è¡¨å¤´
                if row.find('th'):
                    # æå–é˜µå‹ä¿¡æ¯
                    th = row.find('th')
                    if th and th.get('colspan'):
                        header_text = th.get_text().strip()
                        formation_match = re.search(r'\((\d-\d-\d(?:-\d)?)\)', header_text)
                        if formation_match:
                            logger.info(f"ğŸ“‹ é˜µå‹: {formation_match.group(1)}")
                    continue

                cells = row.find_all('td')
                if len(cells) < 2:
                    continue

                # æå–çƒå‘˜ä¿¡æ¯
                player_info = {}

                # çƒè¡£å·ç 
                if cells[0]:
                    number_text = cells[0].get_text().strip()
                    if number_text.isdigit():
                        player_info['number'] = number_text

                # çƒå‘˜å§“å
                if len(cells) >= 2:
                    name_cell = cells[1]
                    player_link = name_cell.find('a', href=True)
                    if player_link:
                        player_info['name'] = player_link.get_text().strip()
                        player_info['url'] = f"https://fbref.com{player_link['href']}"
                    else:
                        player_info['name'] = name_cell.get_text().strip()

                # ä½ç½®ä¿¡æ¯
                if len(cells) >= 3:
                    position_text = cells[2].get_text().strip()
                    if position_text and position_text not in ['', '-', 'Sub']:
                        player_info['position'] = position_text

                # éªŒè¯çƒå‘˜ä¿¡æ¯æœ‰æ•ˆæ€§
                if self._is_valid_player_info(player_info):
                    players.append(player_info)

        except Exception as e:
            logger.warning(f"âš ï¸ è¡¨æ ¼çƒå‘˜æå–å¤±è´¥: {e}")

        return players

    def _is_valid_player_info(self, player_info: Dict) -> bool:
        """éªŒè¯çƒå‘˜ä¿¡æ¯çš„æœ‰æ•ˆæ€§"""
        if not player_info:
            return False

        # å¿…é¡»æœ‰çƒå‘˜å§“å
        name = player_info.get('name', '')
        if not name or len(name) < 2 or name in ['Player', 'Name', '']:
            return False

        # å§“åä¸èƒ½æ˜¯çº¯æ•°å­—æˆ–å¸¸è§çš„è¡¨å¤´æ–‡å­—
        if name.isdigit() or name.lower() in ['player', 'starter', 'substitute']:
            return False

        return True

    def _extract_lineups_from_html(self, soup) -> Optional[Dict]:
        """ç›´æ¥ä»HTMLä¸­æå–é˜µå®¹ä¿¡æ¯ï¼ˆæœ€åå°è¯•ï¼‰"""
        try:
            lineup_data = {'home_lineup': [], 'away_lineup': []}

            # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«çƒå‘˜é“¾æ¥çš„å…ƒç´ 
            player_links = soup.find_all('a', href=lambda x: x and '/players/' in x)

            if player_links:
                players = []
                for link in player_links:
                    name = link.get_text().strip()
                    if name and len(name) > 2:
                        players.append({
                            'name': name,
                            'url': f"https://fbref.com{link['href']}"
                        })

                if players:
                    logger.info(f"ğŸ“‹ HTMLæ–¹å¼æå–åˆ° {len(players)} åçƒå‘˜")
                    # ç®€åŒ–å¤„ç†ï¼šå°†å‰ä¸€åŠçƒå‘˜å½’ä¸ºä¸»é˜Ÿï¼Œåä¸€åŠä¸ºå®¢é˜Ÿ
                    mid = len(players) // 2
                    lineup_data['home_lineup'] = players[:mid]
                    lineup_data['away_lineup'] = players[mid:]
                    return lineup_data

        except Exception as e:
            logger.warning(f"âš ï¸ HTMLé˜µå®¹æå–å¤±è´¥: {e}")

        return None


async def main():
    """ä¸»å‡½æ•° - L2æ·±åº¦é‡‡é›†å¯åŠ¨ - æŒç»­è¿è¡Œç‰ˆæœ¬"""
    logger.info("ğŸš€ L2æ·±åº¦æ•°æ®é‡‡é›†å™¨å¯åŠ¨ (æŒç»­è¿è¡Œæ¨¡å¼)")
    logger.info("ğŸ¯ ç›®æ ‡: æŒç»­é‡‡é›†é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡æ•°æ®")
    logger.info("â±ï¸  å·¥ä½œæ¨¡å¼: æ¯10æ¡è®°å½•ä¼‘çœ 30ç§’ï¼Œé¿å…ä¸L1ç«äº‰èµ„æº")

    collector = L2DetailsCollector()

    import psycopg2
    conn = psycopg2.connect(
        host='db',
        port=5432,
        user='postgres',
        password='postgres-dev-password',
        database='football_prediction'
    )

    processed_count = 0
    batch_count = 0
    total_success = 0

    try:
        while True:  # æŒç»­è¿è¡Œå¾ªç¯
            logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {batch_count + 1} è½®L2é‡‡é›†")

            with conn.cursor() as cur:
                # æŸ¥è¯¢data_completeness = 'partial'ä¸”æœ‰match_report_urlçš„è®°å½•
                cur.execute("""
                    SELECT id, home_team_id, away_team_id, match_metadata, match_date
                    FROM matches
                    WHERE data_completeness = 'partial'
                    AND match_metadata->>'match_report_url' IS NOT NULL
                    AND home_score IS NOT NULL
                    AND away_score IS NOT NULL
                    AND match_date > NOW() - INTERVAL '3 years'
                    ORDER BY match_date DESC
                    LIMIT 20
                """)

                records = cur.fetchall()

                if not records:
                    logger.info("ğŸ“‹ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„è®°å½•ï¼Œç­‰å¾…60ç§’...")
                    await asyncio.sleep(60)
                    continue

                logger.info(f"ğŸ“Š æœ¬è½®æ‰¾åˆ° {len(records)} æ¡å¾…å¤„ç†è®°å½•")
                batch_success = 0

                for i, (record_id, home_id, away_id, metadata_json, match_date) in enumerate(records, 1):
                    try:
                        # ä¿®å¤JSONè§£æé”™è¯¯ï¼šæ·»åŠ ç±»å‹æ£€æŸ¥
                        if isinstance(metadata_json, str):
                            metadata = json.loads(metadata_json)
                        elif isinstance(metadata_json, dict):
                            metadata = metadata_json
                        else:
                            metadata = {}
                        match_report_url = metadata.get('match_report_url')

                        if match_report_url:
                            logger.info(f"ğŸ”„ [{i}/{len(records)}] å¤„ç†æ¯”èµ› {record_id}: {home_id} vs {away_id} (æ—¥æœŸ: {match_date})")
                            success = await collector.update_match_with_details(record_id, match_report_url)

                            if success:
                                batch_success += 1
                                total_success += 1

                                # æ›´æ–°data_completenessçŠ¶æ€ä¸ºcomplete
                                cur.execute("""
                                    UPDATE matches
                                    SET data_completeness = 'complete'
                                    WHERE id = :record_id
                                """, {'record_id': record_id})
                                conn.commit()

                                logger.info(f"âœ… æ¯”èµ› {record_id} å·²å‡çº§ä¸ºå®Œæ•´æ•°æ®")

                            processed_count += 1

                            # æ¯å¤„ç†10æ¡è®°å½•ä¼‘çœ 30ç§’
                            if processed_count % 10 == 0:
                                logger.info(f"â¸ï¸ å·²å¤„ç† {processed_count} æ¡è®°å½•ï¼Œä¼‘çœ 30ç§’...")
                                await asyncio.sleep(30)
                            else:
                                # è®°å½•é—´æ­£å¸¸å»¶è¿Ÿ
                                await asyncio.sleep(3)
                        else:
                            logger.warning(f"âš ï¸ æ¯”èµ› {record_id} (æ—¥æœŸ: {match_date}) æ²¡æœ‰match_report_url")

                    except Exception as e:
                        logger.error(f"âŒ å¤„ç†æ¯”èµ› {record_id} å¤±è´¥: {e}")
                        continue

                batch_count += 1
                logger.info(f"ğŸ‰ ç¬¬ {batch_count} è½®å®Œæˆ: {batch_success}/{len(records)} æ¡è®°å½•æˆåŠŸå‡çº§")
                logger.info(f"ğŸ“Š ç´¯è®¡ç»Ÿè®¡: {total_success} æ¡è®°å½•å·²å‡çº§ä¸ºå®Œæ•´æ•°æ®")

    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­L2é‡‡é›†å™¨")
    except Exception as e:
        logger.error(f"âŒ L2é‡‡é›†å™¨å¼‚å¸¸: {e}")
    finally:
        conn.close()
        logger.info(f"ğŸ”Œ L2é‡‡é›†å™¨å·²åœæ­¢ï¼Œæ€»è®¡å¤„ç† {total_success} æ¡è®°å½•")


if __name__ == "__main__":
    asyncio.run(main())