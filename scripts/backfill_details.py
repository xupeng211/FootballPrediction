#!/usr/bin/env python3
"""
FBrefè¯¦æƒ…é¡µæ•°æ®å›å¡«è„šæœ¬
é«˜çº§æ•°æ®æŒ–æ˜å·¥ç¨‹å¸ˆ: æ·±åº¦æ•°æ®è¡¥å…¨ä¸“å®¶

Purpose: Phase 2 - åŸºäºå·²æœ‰çš„match_report_urlè·å–é˜µå®¹å’Œè¯¦ç»†ç»Ÿè®¡æ•°æ®
å°†æ•°æ®å®Œæ•´æ€§ä»'partial'æå‡åˆ°'complete'
"""

import asyncio
import logging
import sys
import json
import time
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import pandas as pd

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# å¯¼å…¥æ•°æ®åº“è¿æ¥å’Œè¯¦æƒ…é¡µé‡‡é›†å™¨
try:
    from sqlalchemy import create_engine, text
    from sqlalchemy.orm import sessionmaker
    from src.data.collectors.fbref_details_collector import FBrefDetailsCollector

    DB_AVAILABLE = True
except ImportError as e:
    logging.warning(f"æ•°æ®åº“æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    DB_AVAILABLE = False

logging.basicConfig(
    level=logging.INFO,
    format="ğŸ” %(asctime)s [%(levelname)8s] %(name)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)


class FBrefDetailsBackfiller:
    """
    FBrefè¯¦æƒ…é¡µæ•°æ®å›å¡«ç®¡ç†å™¨

    åŠŸèƒ½ï¼š
    1. æŸ¥è¯¢éƒ¨åˆ†å®Œæ•´çš„æ¯”èµ›è®°å½•
    2. æ‰¹é‡è·å–è¯¦æƒ…é¡µæ•°æ®
    3. æ›´æ–°æ•°æ®åº“è®°å½•
    4. æ•°æ®å®Œæ•´æ€§ç›‘æ§
    """

    def __init__(self):
        if not DB_AVAILABLE:
            raise ImportError("æ•°æ®åº“ç»„ä»¶ä¸å¯ç”¨ï¼Œæ— æ³•åˆå§‹åŒ–å›å¡«å™¨")

        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        self.engine = self._create_database_connection()
        if not self.engine:
            raise ConnectionError("æ•°æ®åº“è¿æ¥å¤±è´¥")

        # åˆå§‹åŒ–è¯¦æƒ…é¡µé‡‡é›†å™¨
        self.details_collector = FBrefDetailsCollector()

        # å›å¡«é…ç½®
        self.batch_size = 50  # æ¯æ‰¹å¤„ç†50åœºæ¯”èµ›
        self.request_delay = (3, 6)  # è¯·æ±‚é—´éš”3-6ç§’
        self.error_delay = (15, 30)  # é”™è¯¯é‡è¯•é—´éš”15-30ç§’

    def _create_database_connection(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥"""
        try:
            # ä½¿ç”¨ç›¸åŒçš„æ•°æ®åº“è¿æ¥é…ç½®
            database_urls = [
                "postgresql://postgres:postgres-dev-password@db:5432/football_prediction",
                "postgresql://postgres:postgres-dev-password@localhost:5432/football_prediction",
            ]

            engine = None
            for db_url in database_urls:
                try:
                    engine = create_engine(db_url, connect_args={"connect_timeout": 10})
                    with engine.connect() as conn:
                        conn.execute(text("SELECT 1"))
                    logger.info(
                        f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ: {db_url.split('@')[1].split('/')[0]}"
                    )
                    break
                except Exception:
                    if engine:
                        engine.dispose()
                        engine = None
                    continue

            return engine

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            return None

    def get_partial_matches(self, limit: int = 50) -> List[Dict[str, Any]]:
        """è·å–éœ€è¦è¯¦æƒ…é¡µè¡¥å…¨çš„æ¯”èµ›è®°å½•"""
        logger.info(f"ğŸ” æŸ¥è¯¢éƒ¨åˆ†å®Œæ•´çš„æ¯”èµ›è®°å½• (é™åˆ¶: {limit})")

        try:
            with self.engine.connect() as conn:
                query = text(
                    """
                    SELECT
                        m.id,
                        m.match_date,
                        ht.name as home_team,
                        at.name as away_team,
                        m.data_source,
                        m.data_completeness,
                        m.match_metadata,
                        m.stats,
                        -- ä»statså­—æ®µæå–match_report_url
                        (m.stats->>'match_report_url') as match_report_url,
                        m.created_at
                    FROM matches m
                    LEFT JOIN teams ht ON m.home_team_id = ht.id
                    LEFT JOIN teams at ON m.away_team_id = at.id
                    WHERE m.data_source = 'fbref'
                    AND m.data_completeness = 'partial'
                    AND (m.stats->>'match_report_url') IS NOT NULL
                    AND (m.stats->>'match_report_url') != ''
                    ORDER BY m.match_date DESC, m.created_at ASC
                    LIMIT :limit
                """
                )

                result = conn.execute(query, {"limit": limit})
                rows = result.fetchall()

                matches = []
                for row in rows:
                    match = {
                        "id": row[0],
                        "match_date": row[1],
                        "home_team": row[2],
                        "away_team": row[3],
                        "data_source": row[4],
                        "data_completeness": row[5],
                        "match_metadata": row[6],
                        "stats": row[7],
                        "match_report_url": row[8],
                        "created_at": row[9],
                    }
                    matches.append(match)

                logger.info(f"ğŸ“Š æ‰¾åˆ° {len(matches)} åœºéœ€è¦è¡¥å…¨çš„æ¯”èµ›")
                return matches

        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢éƒ¨åˆ†å®Œæ•´æ¯”èµ›å¤±è´¥: {e}")
            return []

    async def process_match_details(
        self, match: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """å¤„ç†å•åœºæ¯”èµ›çš„è¯¦æƒ…æ•°æ®"""
        match_id = match["id"]
        match_report_url = match["match_report_url"]
        home_team = match["home_team"]
        away_team = match["away_team"]

        logger.info(f"ğŸ”„ å¤„ç†æ¯”èµ›è¯¦æƒ…: {home_team} vs {away_team} (ID: {match_id})")

        try:
            # è·å–è¯¦æƒ…é¡µæ•°æ®
            details = await self.details_collector.fetch_match_details(match_report_url)

            if not details:
                logger.warning(f"âš ï¸ æ— æ³•è·å–æ¯”èµ› {match_id} çš„è¯¦æƒ…æ•°æ®")
                return None

            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            lineups = details.get("lineups", {})
            detailed_stats = details.get("detailed_stats", {})

            # è‡³å°‘è¦æœ‰é˜µå®¹æˆ–ç»Ÿè®¡æ•°æ®
            if (
                not lineups.get("home")
                and not lineups.get("away")
                and not detailed_stats
            ):
                logger.warning(f"âš ï¸ æ¯”èµ› {match_id} è¯¦æƒ…æ•°æ®ä¸ºç©º")
                return None

            # æ„å»ºæ›´æ–°æ•°æ®
            update_data = {
                "lineups": lineups,
                "detailed_stats": detailed_stats,
                "extra_info": details.get("extra_info", {}),
                "details_extracted_at": details.get("extracted_at"),
                "details_source_url": details.get("source_url"),
            }

            # æ‰“å°å…³é”®ä¿¡æ¯ï¼ˆéªŒè¯ç”¨ï¼‰
            home_lineup = lineups.get("home", [])
            away_lineup = lineups.get("away", [])

            if home_lineup:
                logger.info(f"ğŸ”´ ä¸»é˜Ÿé˜µå®¹: {len(home_lineup)} åçƒå‘˜")
                # æ‰“å°å‰é”‹çƒå‘˜ï¼ˆé€šå¸¸ä½ç½®é å‰çš„æ˜¯å‰é”‹ï¼‰
                forwards = [
                    p["name"]
                    for p in home_lineup[:3]
                    if "forward" in p.get("position", "").lower()
                ]
                if forwards:
                    logger.info(f"âš¡ ä¸»é˜Ÿå‰é”‹: {', '.join(forwards)}")

            if away_lineup:
                logger.info(f"ğŸ”µ å®¢é˜Ÿé˜µå®¹: {len(away_lineup)} åçƒå‘˜")

            # æ‰“å°å…³é”®ç»Ÿè®¡æ•°æ®
            if "possession_pct" in detailed_stats:
                logger.info(f"ğŸ“ˆ ä¸»é˜Ÿæ§çƒç‡: {detailed_stats['possession_pct']}")

            if "shots_on_target" in detailed_stats:
                logger.info(f"ğŸ¯ å°„æ­£æ¬¡æ•°: {detailed_stats['shots_on_target']}")

            logger.info(f"âœ… æ¯”èµ› {match_id} è¯¦æƒ…æ•°æ®æå–æˆåŠŸ")
            return update_data

        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ¯”èµ› {match_id} è¯¦æƒ…å¤±è´¥: {e}")
            return None

    def update_match_in_database(
        self, match_id: int, details_data: Dict[str, Any]
    ) -> bool:
        """æ›´æ–°æ•°æ®åº“ä¸­çš„æ¯”èµ›è®°å½•"""
        try:
            with self.engine.connect() as conn:
                # è·å–ç°æœ‰çš„statsæ•°æ®
                current_stats_query = text("SELECT stats FROM matches WHERE id = :id")
                current_result = conn.execute(current_stats_query, {"id": match_id})
                current_stats_row = current_result.fetchone()

                if not current_stats_row:
                    logger.error(f"âŒ æ‰¾ä¸åˆ°æ¯”èµ› {match_id}")
                    return False

                # è§£æç°æœ‰çš„statsæ•°æ®
                current_stats = {}
                if current_stats_row[0]:
                    try:
                        current_stats = json.loads(current_stats_row[0])
                    except (json.JSONDecodeError, TypeError):
                        current_stats = {}

                # æ›´æ–°statsæ•°æ®
                current_stats.update(
                    {
                        "lineups": details_data["lineups"],
                        "detailed_stats": details_data["detailed_stats"],
                        "extra_info": details_data["extra_info"],
                        "details_extracted_at": details_data["details_extracted_at"],
                        "details_source_url": details_data["details_source_url"],
                    }
                )

                # æ›´æ–°æ•°æ®åº“è®°å½•
                update_query = text(
                    """
                    UPDATE matches
                    SET lineups = :lineups,
                        stats = :stats,
                        data_completeness = :completeness,
                        updated_at = NOW()
                    WHERE id = :id
                """
                )

                conn.execute(
                    update_query,
                    {
                        "id": match_id,
                        "lineups": json.dumps(details_data["lineups"]),
                        "stats": json.dumps(current_stats),
                        "completeness": "complete",
                    },
                )

                conn.commit()
                logger.info(f"âœ… æ¯”èµ› {match_id} æ•°æ®åº“æ›´æ–°æˆåŠŸ")
                return True

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°æ¯”èµ› {match_id} æ•°æ®åº“å¤±è´¥: {e}")
            return False

    async def process_batch(self, matches: List[Dict[str, Any]]) -> Tuple[int, int]:
        """å¤„ç†ä¸€æ‰¹æ¯”èµ›"""
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ‰¹æ¬¡: {len(matches)} åœºæ¯”èµ›")

        processed_count = 0
        success_count = 0

        for match in matches:
            try:
                # å¤„ç†æ¯”èµ›è¯¦æƒ…
                details_data = await self.process_match_details(match)

                if details_data:
                    # æ›´æ–°æ•°æ®åº“
                    if self.update_match_in_database(match["id"], details_data):
                        success_count += 1

                processed_count += 1

                # è¯·æ±‚é—´éš”
                delay = random.uniform(*self.request_delay)
                await asyncio.sleep(delay)

            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ¯”èµ› {match.get('id', 'unknown')} å¤±è´¥: {e}")

                # é”™è¯¯å»¶è¿Ÿ
                error_delay = random.uniform(*self.error_delay)
                await asyncio.sleep(error_delay)

        logger.info(f"ğŸ“Š æ‰¹æ¬¡å¤„ç†å®Œæˆ: {success_count}/{processed_count} æˆåŠŸ")
        return processed_count, success_count

    async def run_backfill(self, max_iterations: int = 10) -> bool:
        """è¿è¡Œè¯¦æƒ…é¡µå›å¡«"""
        logger.info("ğŸš€ å¼€å§‹FBrefè¯¦æƒ…é¡µæ•°æ®å›å¡« (Phase 2)")
        logger.info("=" * 80)

        total_processed = 0
        total_success = 0

        try:
            for iteration in range(max_iterations):
                logger.info(f"ğŸ”„ å›å¡«è¿­ä»£ {iteration + 1}/{max_iterations}")

                # è·å–å¾…å¤„ç†çš„æ¯”èµ›
                matches = self.get_partial_matches(self.batch_size)

                if not matches:
                    logger.info("ğŸ‰ æ‰€æœ‰æ¯”èµ›è¯¦æƒ…å·²è¡¥å…¨ï¼Œå›å¡«å®Œæˆ!")
                    break

                # å¤„ç†è¿™æ‰¹æ¯”èµ›
                processed, success = await self.process_batch(matches)

                total_processed += processed
                total_success += success

                logger.info(
                    f"ğŸ“ˆ è¿›åº¦ç»Ÿè®¡: æ€»å¤„ç† {total_processed}, æˆåŠŸ {total_success}"
                )

                # æ‰¹æ¬¡é—´å»¶è¿Ÿ
                if len(matches) == self.batch_size:  # å¯èƒ½è¿˜æœ‰æ›´å¤šæ•°æ®
                    batch_delay = random.uniform(10, 20)
                    logger.info(f"â³ æ‰¹æ¬¡é—´å»¶è¿Ÿ {batch_delay:.1f}s...")
                    await asyncio.sleep(batch_delay)

        except Exception as e:
            logger.error(f"âŒ å›å¡«è¿‡ç¨‹å¼‚å¸¸: {e}")
            import traceback

            traceback.print_exc()

        finally:
            # å…³é—­è¯¦æƒ…é¡µé‡‡é›†å™¨
            await self.details_collector.close()

        logger.info("=" * 80)
        logger.info("ğŸ‰ FBrefè¯¦æƒ…é¡µæ•°æ®å›å¡«å®Œæˆ!")
        logger.info(
            f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ€»å¤„ç† {total_processed} åœºæ¯”èµ›, æˆåŠŸ {total_success} åœº"
        )
        logger.info(
            f"âœ… æˆåŠŸç‡: {total_success/total_processed*100:.1f}%"
            if total_processed > 0
            else "âœ… æˆåŠŸç‡: N/A"
        )

        return total_success > 0


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ­ FBrefè¯¦æƒ…é¡µæ•°æ®å›å¡«ç³»ç»Ÿ")
    logger.info(f"ğŸ• å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        # åˆå§‹åŒ–å›å¡«å™¨
        backfiller = FBrefDetailsBackfiller()
        logger.info("âœ… è¯¦æƒ…é¡µå›å¡«å™¨åˆå§‹åŒ–æˆåŠŸ")

        # è¿è¡Œå›å¡«
        success = await backfiller.run_backfill(max_iterations=20)

        if success:
            logger.info("ğŸ¯ è¯¦æƒ…é¡µå›å¡«ä»»åŠ¡æˆåŠŸ!")
            sys.exit(0)
        else:
            logger.error("ğŸ’¥ è¯¦æƒ…é¡µå›å¡«ä»»åŠ¡å¤±è´¥!")
            sys.exit(1)

    except Exception as e:
        logger.error(f"ğŸ’¥ ç³»ç»Ÿå¼‚å¸¸: {e}")
        import traceback

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
