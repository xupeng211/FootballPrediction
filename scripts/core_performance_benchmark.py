#!/usr/bin/env python3
"""
Phase G Week 4 æ ¸å¿ƒæ¨¡å—æ€§èƒ½åŸºå‡†æµ‹è¯•
Core Performance Benchmark

ä¸“æ³¨äºæµ‹è¯•æ ¸å¿ƒæ¨¡å—æ€§èƒ½ï¼Œå»ºç«‹æ€§èƒ½åŸºçº¿ã€‚
"""

import asyncio
import json
import statistics
import time
import sys
import os
from typing import Dict, List, Any
from datetime import datetime

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

try:
    from src.utils.string_utils import StringUtils
    from src.utils.dict_utils import DictUtils
    from src.services.manager import service_manager
    from src.observers.manager import get_observer_manager
    from src.core.service_lifecycle import get_lifecycle_manager
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("âš ï¸  æŸäº›æ¨¡å—å¯èƒ½ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç›¸å…³æµ‹è¯•")


class CorePerformanceBenchmark:
    """æ ¸å¿ƒæ¨¡å—æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·"""

    def __init__(self):
        self.results: Dict[str, List[float]] = {}
        self.start_time = None

    def run_benchmark(self, name: str, func, iterations: int = 1000) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print(f"ğŸ”§ è¿è¡ŒåŸºå‡†æµ‹è¯•: {name}")

        times = []

        # é¢„çƒ­
        for _ in range(10):
            try:
                func()
            except Exception:
                pass

        # æ­£å¼æµ‹è¯•
        successful_runs = 0
        for _ in range(iterations):
            start = time.perf_counter()
            try:
                result = func()
                end = time.perf_counter()
                times.append(end - start)
                successful_runs += 1

                # ç®€å•éªŒè¯ç»“æœä¸ä¸ºç©º
                if result is None:
                    print(f"âš ï¸  è­¦å‘Š: å‡½æ•°è¿”å›Noneç»“æœ")
            except Exception as e:
                print(f"âš ï¸  æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
                continue

        if successful_runs == 0:
            print(f"âŒ æ‰€æœ‰æµ‹è¯•è¿è¡Œéƒ½å¤±è´¥äº†")
            return {
                'name': name,
                'iterations': iterations,
                'successful_runs': 0,
                'error': 'All runs failed'
            }

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = statistics.mean(times)
        median_time = statistics.median(times)
        min_time = min(times)
        max_time = max(times)
        std_dev = statistics.stdev(times) if len(times) > 1 else 0

        result_stats = {
            'name': name,
            'iterations': iterations,
            'successful_runs': successful_runs,
            'avg_time_ms': avg_time * 1000,
            'median_time_ms': median_time * 1000,
            'min_time_ms': min_time * 1000,
            'max_time_ms': max_time * 1000,
            'std_dev_ms': std_dev * 1000,
            'total_time_s': sum(times),
            'ops_per_second': successful_runs / sum(times) if sum(times) > 0 else 0,
            'success_rate': (successful_runs / iterations) * 100
        }

        self.results[name] = times

        return result_stats

    def test_string_utils_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å­—ç¬¦ä¸²å·¥å…·æ€§èƒ½"""

        def snake_case_conversion():
            return StringUtils.to_snake_case('TestPerformanceStringConversion')

        return self.run_benchmark('string_utils.snake_case', snake_case_conversion, 10000)

    def test_dict_utils_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å­—å…¸å·¥å…·æ€§èƒ½"""
        test_dict = {
            'key1': 'value1', 'key2': 'value2', 'key3': 'value3',
            'performance_test': 'test_value', 'filter_me': 'filtered',
            'keep_this': 'kept', 'another_key': 'another_value',
            'extra_key1': 'extra1', 'extra_key2': 'extra2'
        }

        def dict_filtering():
            return DictUtils.filter_keys(test_dict, ['key1', 'performance_test', 'keep_this'])

        return self.run_benchmark('dict_utils.filter_keys', dict_filtering, 10000)

    def test_service_manager_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æœåŠ¡ç®¡ç†å™¨æ€§èƒ½"""

        def service_loading():
            return len(service_manager.get_all_services())

        return self.run_benchmark('service_manager.get_all_services', service_loading, 1000)

    def test_observer_manager_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•è§‚å¯Ÿè€…ç®¡ç†å™¨æ€§èƒ½"""
        observer_manager = get_observer_manager()

        def metrics_collection():
            return observer_manager.get_all_metrics()

        return self.run_benchmark('observer_manager.get_metrics', metrics_collection, 1000)

    def test_lifecycle_manager_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç”Ÿå‘½å‘¨æœŸç®¡ç†å™¨æ€§èƒ½"""

        def lifecycle_operations():
            lifecycle_manager = get_lifecycle_manager()
            return len(lifecycle_manager.get_all_services())

        return self.run_benchmark('lifecycle_manager.get_services', lifecycle_operations, 1000)

    async def test_async_observer_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•å¼‚æ­¥è§‚å¯Ÿè€…æ€§èƒ½"""
        observer_manager = get_observer_manager()

        async def async_prediction_record():
            await observer_manager.record_prediction(
                strategy_name="test_strategy",
                response_time_ms=50.0,
                success=True,
                confidence=0.85
            )

        times = []
        iterations = 1000
        successful_runs = 0

        # é¢„çƒ­
        for _ in range(10):
            try:
                await async_prediction_record()
            except Exception:
                pass

        # æ­£å¼æµ‹è¯•
        for _ in range(iterations):
            start = time.perf_counter()
            try:
                await async_prediction_record()
                end = time.perf_counter()
                times.append(end - start)
                successful_runs += 1
            except Exception as e:
                print(f"âš ï¸  å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")
                continue

        if successful_runs == 0:
            return {
                'name': 'async_observer.record_prediction',
                'iterations': iterations,
                'successful_runs': 0,
                'error': 'All async runs failed'
            }

        avg_time = statistics.mean(times)
        result_stats = {
            'name': 'async_observer.record_prediction',
            'iterations': iterations,
            'successful_runs': successful_runs,
            'avg_time_ms': avg_time * 1000,
            'ops_per_second': successful_runs / sum(times) if sum(times) > 0 else 0,
            'success_rate': (successful_runs / iterations) * 100
        }

        return result_stats

    def test_complex_string_operations(self) -> Dict[str, Any]:
        """æµ‹è¯•å¤æ‚å­—ç¬¦ä¸²æ“ä½œæ€§èƒ½"""

        def complex_string_processing():
            test_strings = [
                'TestPerformanceStringConversion',
                'AnotherTestCaseWithMultipleWords',
                'XMLHttpRequestToAPIServiceEndpoint',
                'ComplexBusinessLogicValidationRule',
                'DatabaseConnectionPoolManager'
            ]
            results = []
            for s in test_strings:
                results.append(StringUtils.to_snake_case(s))
            return results

        return self.run_benchmark('string_utils.complex_processing', complex_string_processing, 1000)

    def test_complex_dict_operations(self) -> Dict[str, Any]:
        """æµ‹è¯•å¤æ‚å­—å…¸æ“ä½œæ€§èƒ½"""
        test_dict = {f'key_{i}': f'value_{i}' for i in range(100)}
        test_dict.update({
            'special_key_1': 'special_value_1',
            'special_key_2': 'special_value_2',
            'performance_test_key': 'performance_test_value'
        })

        def complex_dict_filtering():
            filter_keys = [f'key_{i}' for i in range(10, 20)]
            filter_keys.extend(['special_key_1', 'performance_test_key'])
            return DictUtils.filter_keys(test_dict, filter_keys)

        return self.run_benchmark('dict_utils.complex_filtering', complex_dict_filtering, 1000)

    def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ Phase G Week 4 æ ¸å¿ƒæ¨¡å—æ€§èƒ½åŸºå‡†æµ‹è¯•å¼€å§‹...")
        print("=" * 60)

        start_time = time.time()
        self.start_time = start_time

        all_results = {}

        # åŒæ­¥æµ‹è¯•
        tests = [
            self.test_string_utils_performance,
            self.test_complex_string_operations,
            self.test_dict_utils_performance,
            self.test_complex_dict_operations,
            self.test_service_manager_performance,
            self.test_observer_manager_performance,
            self.test_lifecycle_manager_performance,
        ]

        for test_func in tests:
            try:
                result = test_func()
                all_results[result['name']] = result
                if result.get('error'):
                    print(f"âŒ {result['name']}: {result['error']}")
                else:
                    print(f"âœ… {result['name']}: {result['avg_time_ms']:.3f}ms avg, {result['ops_per_second']:.0f} ops/s ({result['success_rate']:.1f}% success)")
            except Exception as e:
                print(f"âŒ {test_func.__name__} å¤±è´¥: {e}")

        # å¼‚æ­¥æµ‹è¯•
        try:
            async_result = asyncio.run(self.test_async_observer_performance())
            all_results[async_result['name']] = async_result
            if async_result.get('error'):
                print(f"âŒ {async_result['name']}: {async_result['error']}")
            else:
                print(f"âœ… {async_result['name']}: {async_result['avg_time_ms']:.3f}ms avg, {async_result['ops_per_second']:.0f} ops/s ({async_result['success_rate']:.1f}% success)")
        except Exception as e:
            print(f"âŒ å¼‚æ­¥æµ‹è¯•å¤±è´¥: {e}")

        total_time = time.time() - start_time

        print("=" * 60)
        print(f"ğŸ“Š æ ¸å¿ƒæ¨¡å—æ€§èƒ½åŸºå‡†æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
        successful_tests = len([r for r in all_results.values() if not r.get('error')])
        print(f"ğŸ“ˆ å…±å®Œæˆ {successful_tests}/{len(all_results)} ä¸ªåŸºå‡†æµ‹è¯•")

        return {
            'timestamp': datetime.now().isoformat(),
            'total_benchmark_time_s': total_time,
            'benchmarks': all_results,
            'summary': self._generate_summary(all_results),
            'successful_tests': successful_tests,
            'total_tests': len(all_results)
        }

    def _generate_summary(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        successful_results = [r for r in results.values() if not r.get('error')]

        if not successful_results:
            return {}

        avg_times = [r.get('avg_time_ms', 0) for r in successful_results]
        ops_per_second = [r.get('ops_per_second', 0) for r in successful_results]
        success_rates = [r.get('success_rate', 0) for r in successful_results]

        return {
            'total_benchmarks': len(results),
            'successful_benchmarks': len(successful_results),
            'avg_response_time_ms': statistics.mean(avg_times),
            'max_response_time_ms': max(avg_times),
            'min_response_time_ms': min(avg_times),
            'avg_ops_per_second': statistics.mean(ops_per_second),
            'max_ops_per_second': max(ops_per_second),
            'min_ops_per_second': min(ops_per_second),
            'avg_success_rate': statistics.mean(success_rates)
        }

    def save_results(self, results: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜åŸºå‡†æµ‹è¯•ç»“æœ"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"core_performance_benchmark_{timestamp}.json"

        filepath = os.path.join(os.path.dirname(__file__), '..', filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f"ğŸ’¾ åŸºå‡†æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filepath}")
        return filepath

    def display_results(self, results: Dict[str, Any]):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœ"""
        print("\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•è¯¦ç»†ç»“æœ:")
        print("=" * 80)

        for name, result in results.items():
            if result.get('error'):
                print(f"âŒ {name}: {result['error']}")
                continue

            print(f"\nğŸ”§ {name}")
            print(f"   è¿­ä»£æ¬¡æ•°: {result.get('iterations', 0)}")
            print(f"   æˆåŠŸæ¬¡æ•°: {result.get('successful_runs', 0)}")
            print(f"   æˆåŠŸç‡: {result.get('success_rate', 0):.1f}%")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {result.get('avg_time_ms', 0):.3f}ms")
            print(f"   æœ€å°å“åº”æ—¶é—´: {result.get('min_time_ms', 0):.3f}ms")
            print(f"   æœ€å¤§å“åº”æ—¶é—´: {result.get('max_time_ms', 0):.3f}ms")
            print(f"   ååé‡: {result.get('ops_per_second', 0):.0f} ops/s")

        # æ˜¾ç¤ºæ‘˜è¦
        summary = results.get('summary', {})
        if summary:
            print(f"\nğŸ“ˆ æ€§èƒ½æ‘˜è¦:")
            print(f"   æˆåŠŸæµ‹è¯•: {summary.get('successful_benchmarks', 0)}/{summary.get('total_benchmarks', 0)}")
            print(f"   å¹³å‡å“åº”æ—¶é—´: {summary.get('avg_response_time_ms', 0):.3f}ms")
            print(f"   æœ€å¤§å“åº”æ—¶é—´: {summary.get('max_response_time_ms', 0):.3f}ms")
            print(f"   å¹³å‡ååé‡: {summary.get('avg_ops_per_second', 0):.0f} ops/s")
            print(f"   æœ€å¤§ååé‡: {summary.get('max_ops_per_second', 0):.0f} ops/s")
            print(f"   å¹³å‡æˆåŠŸç‡: {summary.get('avg_success_rate', 0):.1f}%")

        print("\nğŸ¯ æ€§èƒ½åˆ†æ:")
        if summary.get('avg_response_time_ms', 0) < 1:
            print("   âœ… å“åº”æ—¶é—´: ä¼˜ç§€ (< 1ms)")
        elif summary.get('avg_response_time_ms', 0) < 5:
            print("   âš ï¸  å“åº”æ—¶é—´: è‰¯å¥½ (1-5ms)")
        else:
            print("   âŒ å“åº”æ—¶é—´: éœ€è¦ä¼˜åŒ– (> 5ms)")

        if summary.get('avg_ops_per_second', 0) > 10000:
            print("   âœ… ååé‡: ä¼˜ç§€ (> 10,000 ops/s)")
        elif summary.get('avg_ops_per_second', 0) > 5000:
            print("   âœ… ååé‡: è‰¯å¥½ (5,000-10,000 ops/s)")
        else:
            print("   âš ï¸  ååé‡: å¯æ¥å— (< 5,000 ops/s)")

        if summary.get('avg_success_rate', 0) > 99:
            print("   âœ… æˆåŠŸç‡: ä¼˜ç§€ (> 99%)")
        elif summary.get('avg_success_rate', 0) > 95:
            print("   âœ… æˆåŠŸç‡: è‰¯å¥½ (95-99%)")
        else:
            print("   âŒ æˆåŠŸç‡: éœ€è¦æ”¹è¿› (< 95%)")


def main():
    """ä¸»å‡½æ•°"""
    benchmark = CorePerformanceBenchmark()

    # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    results = benchmark.run_all_benchmarks()

    # æ˜¾ç¤ºç»“æœ
    benchmark.display_results(results)

    # ä¿å­˜ç»“æœ
    filepath = benchmark.save_results(results)

    return results


if __name__ == "__main__":
    main()