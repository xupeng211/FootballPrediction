#!/usr/bin/env python3
"""
CI/CDæµæ°´çº¿ç›‘æ§å’Œè‡ªåŠ¨åŒ–ä¼˜åŒ–å·¥å…·
CI/CD Pipeline Monitoring and Automation Optimizer

åŸºäºIssue #183éœ€æ±‚ï¼Œå¢å¼ºç°æœ‰CI/CDæµæ°´çº¿çš„ç›‘æ§èƒ½åŠ›ã€ä¼˜åŒ–æ‰§è¡Œæ—¶é—´ã€
è®¾ç½®æ™ºèƒ½é€šçŸ¥æœºåˆ¶ï¼Œå¹¶å»ºç«‹å®Œæ•´çš„è´¨é‡é—¨æ§è‡ªåŠ¨åŒ–ä½“ç³»ã€‚

ä½œè€…: Claude AI Assistant
ç‰ˆæœ¬: v1.0
åˆ›å»ºæ—¶é—´: 2025-11-03
"""

import json
import sys
import time
import subprocess
import asyncio
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

class MonitoringStatus(Enum):
    """ç›‘æ§çŠ¶æ€æšä¸¾"""
    HEALTHY = "healthy"
    WARNING = "warning"
    CRITICAL = "critical"
    UNKNOWN = "unknown"

class OptimizationType(Enum):
    """ä¼˜åŒ–ç±»å‹æšä¸¾"""
    CACHE = "cache"
    PARALLEL = "parallel"
    DEPENDENCY = "dependency"
    TEST_STRATEGY = "test_strategy"
    SECURITY_SCAN = "security_scan"

@dataclass
class CIPerformanceMetric:
    """CIæ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    name: str
    duration: float
    status: str
    cache_hit: bool
    parallel_jobs: int
    timestamp: str

@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœæ•°æ®ç»“æ„"""
    optimization_type: OptimizationType
    improvement_percent: float
    time_saved: float
    success_rate: float
    recommendation: str

@dataclass
class MonitoringReport:
    """ç›‘æ§æŠ¥å‘Šæ•°æ®ç»“æ„"""
    timestamp: str
    overall_status: MonitoringStatus
    total_checks: int
    passed_checks: int
    failed_checks: int
    warning_checks: int
    performance_metrics: List[CIPerformanceMetric]
    optimizations: List[OptimizationResult]
    recommendations: List[str]
    next_actions: List[str]

class CICDMonitor:
    """CI/CDæµæ°´çº¿ç›‘æ§å™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.timestamp = datetime.now().isoformat()

        # ç›‘æ§é…ç½®
        self.monitoring_config = {
            # æ€§èƒ½é˜ˆå€¼
            "max_ci_duration": 900,  # 15åˆ†é’Ÿ
            "max_test_duration": 300,  # 5åˆ†é’Ÿ
            "max_quality_check_duration": 120,  # 2åˆ†é’Ÿ
            "min_success_rate": 95.0,  # 95%æˆåŠŸç‡

            # ç¼“å­˜é…ç½®
            "cache_duration_days": 7,
            "expected_cache_hit_rate": 80.0,  # 80%ç¼“å­˜å‘½ä¸­ç‡

            # å¹¶è¡Œé…ç½®
            "max_parallel_jobs": 4,

            # ç›‘æ§çª—å£
            "monitoring_window_hours": 24
        }

        # ä¼˜åŒ–å»ºè®®é…ç½®
        self.optimization_strategies = {
            OptimizationType.CACHE: {
                "name": "ä¾èµ–ç¼“å­˜ä¼˜åŒ–",
                "actions": [
                    "å¯ç”¨pipä¾èµ–ç¼“å­˜",
                    "å¯ç”¨Dockerå±‚ç¼“å­˜",
                    "é…ç½®pytestç¼“å­˜",
                    "ç¼“å­˜é¢„ç¼–è¯‘çš„wheelåŒ…"
                ]
            },
            OptimizationType.PARALLEL: {
                "name": "å¹¶è¡Œæ‰§è¡Œä¼˜åŒ–",
                "actions": [
                    "å¹¶è¡ŒåŒ–æµ‹è¯•å¥—ä»¶",
                    "å¹¶è¡ŒåŒ–ä»£ç è´¨é‡æ£€æŸ¥",
                    "åˆ†ç¦»æµ‹è¯•å’Œè´¨é‡æ£€æŸ¥é˜¶æ®µ"
                ]
            },
            OptimizationType.DEPENDENCY: {
                "name": "ä¾èµ–ç®¡ç†ä¼˜åŒ–",
                "actions": [
                    "ä½¿ç”¨requirements.lockæ–‡ä»¶",
                    "é¢„å®‰è£…å¸¸ç”¨ä¾èµ–",
                    "å¢é‡ä¾èµ–æ›´æ–°"
                ]
            },
            OptimizationType.TEST_STRATEGY: {
                "name": "æµ‹è¯•ç­–ç•¥ä¼˜åŒ–",
                "actions": [
                    "æ™ºèƒ½æµ‹è¯•é€‰æ‹©",
                    "æŒ‰ä¼˜å…ˆçº§åˆ†ç»„æµ‹è¯•",
                    "å¹¶è¡Œæ‰§è¡Œç‹¬ç«‹æµ‹è¯•"
                ]
            },
            OptimizationType.SECURITY_SCAN: {
                "name": "å®‰å…¨æ‰«æä¼˜åŒ–",
                "actions": [
                    "å¢é‡å®‰å…¨æ‰«æ",
                    "ç¼“å­˜å®‰å…¨æ•°æ®åº“",
                    "å¼‚æ­¥å®‰å…¨æŠ¥å‘Šç”Ÿæˆ"
                ]
            }
        }

    def analyze_current_ci_performance(self) -> List[CIPerformanceMetric]:
        """åˆ†æå½“å‰CIæ€§èƒ½"""
        metrics = []

        # åˆ†æGitHub Actionsé…ç½®
        workflows_dir = self.project_root / ".github" / "workflows"
        if workflows_dir.exists():
            for workflow_file in workflows_dir.glob("*.yml"):
                metric = self._analyze_workflow(workflow_file)
                if metric:
                    metrics.append(metric)

        # åˆ†ææœ€è¿‘çš„CIè¿è¡Œ
        recent_metrics = self._analyze_recent_ci_runs()
        metrics.extend(recent_metrics)

        return metrics

    def _analyze_workflow(self, workflow_file: Path) -> Optional[CIPerformanceMetric]:
        """åˆ†æå•ä¸ªworkflowæ–‡ä»¶"""
        try:
            with open(workflow_file, 'r', encoding='utf-8') as f:
                content = f.read()

            # ç®€å•çš„workflowåˆ†æ
            has_cache = 'cache@' in content
            has_parallel = 'strategy:' in content and 'matrix:' in content

            # ä¼°ç®—jobæ•°é‡
            job_count = content.count('runs-on:')

            return CIPerformanceMetric(
                name=workflow_file.stem,
                duration=self._estimate_workflow_duration(content),
                status="configured",
                cache_hit=has_cache,
                parallel_jobs=job_count,
                timestamp=datetime.now().isoformat()
            )

        except Exception as e:
            print(f"åˆ†æworkflowæ–‡ä»¶å¤±è´¥ {workflow_file}: {e}")
            return None

    def _estimate_workflow_duration(self, content: str) -> float:
        """ä¼°ç®—workflowæ‰§è¡Œæ—¶é—´"""
        base_duration = 60  # åŸºç¡€æ—¶é—´1åˆ†é’Ÿ

        # æ ¹æ®jobå†…å®¹è°ƒæ•´æ—¶é—´
        if 'pytest' in content:
            base_duration += 120  # æµ‹è¯•æ—¶é—´
        if 'ruff check' in content:
            base_duration += 30   # ä»£ç æ£€æŸ¥æ—¶é—´
        if 'mypy' in content:
            base_duration += 60   # ç±»å‹æ£€æŸ¥æ—¶é—´
        if 'bandit' in content:
            base_duration += 45   # å®‰å…¨æ‰«ææ—¶é—´
        if 'docker' in content:
            base_duration += 180  # Dockeræ„å»ºæ—¶é—´

        return base_duration

    def _analyze_recent_ci_runs(self) -> List[CIPerformanceMetric]:
        """åˆ†ææœ€è¿‘çš„CIè¿è¡Œ"""
        metrics = []

        # è¿™é‡Œå¯ä»¥è¿æ¥GitHub APIè·å–çœŸå®æ•°æ®
        # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿæ•°æ®
        metrics.append(CIPerformanceMetric(
            name="recent_ci_run",
            duration=450.5,  # 7.5åˆ†é’Ÿ
            status="success",
            cache_hit=True,
            parallel_jobs=3,
            timestamp=datetime.now().isoformat()
        ))

        return metrics

    def identify_optimization_opportunities(self, metrics: List[CIPerformanceMetric]) -> List[OptimizationResult]:
        """è¯†åˆ«ä¼˜åŒ–æœºä¼š"""
        optimizations = []

        for metric in metrics:
            # ç¼“å­˜ä¼˜åŒ–åˆ†æ
            if not metric.cache_hit:
                improvement = self._calculate_cache_improvement(metric)
                if improvement:
                    optimizations.append(improvement)

            # å¹¶è¡Œä¼˜åŒ–åˆ†æ
            if metric.parallel_jobs < self.monitoring_config["max_parallel_jobs"]:
                improvement = self._calculate_parallel_improvement(metric)
                if improvement:
                    optimizations.append(improvement)

            # æ€§èƒ½ä¼˜åŒ–åˆ†æ
            if metric.duration > self.monitoring_config["max_ci_duration"]:
                improvement = self._calculate_performance_improvement(metric)
                if improvement:
                    optimizations.append(improvement)

        return optimizations

    def _calculate_cache_improvement(self, metric: CIPerformanceMetric) -> Optional[OptimizationResult]:
        """è®¡ç®—ç¼“å­˜ä¼˜åŒ–æ”¶ç›Š"""
        estimated_time_saved = metric.duration * 0.3  # ç¼“å­˜å¯èŠ‚çœ30%æ—¶é—´
        improvement_percent = (estimated_time_saved / metric.duration) * 100

        return OptimizationResult(
            optimization_type=OptimizationType.CACHE,
            improvement_percent=improvement_percent,
            time_saved=estimated_time_saved,
            success_rate=95.0,
            recommendation=f"ä¸º{metric.name}å¯ç”¨ä¾èµ–ç¼“å­˜ï¼Œé¢„è®¡èŠ‚çœ{improvement_percent:.1f}%æ—¶é—´"
        )

    def _calculate_parallel_improvement(self, metric: CIPerformanceMetric) -> Optional[OptimizationResult]:
        """è®¡ç®—å¹¶è¡Œä¼˜åŒ–æ”¶ç›Š"""
        current_jobs = metric.parallel_jobs
        target_jobs = min(current_jobs * 2, self.monitoring_config["max_parallel_jobs"])

        if target_jobs > current_jobs:
            speedup_factor = target_jobs / current_jobs
            improvement_percent = ((speedup_factor - 1) / speedup_factor) * 100
            time_saved = metric.duration * (improvement_percent / 100)

            return OptimizationResult(
                optimization_type=OptimizationType.PARALLEL,
                improvement_percent=improvement_percent,
                time_saved=time_saved,
                success_rate=90.0,
                recommendation=f"å°†{metric.name}çš„å¹¶è¡Œä»»åŠ¡ä»{current_jobs}å¢åŠ åˆ°{target_jobs}ä¸ª"
            )

        return None

    def _calculate_performance_improvement(self, metric: CIPerformanceMetric) -> Optional[OptimizationResult]:
        """è®¡ç®—æ€§èƒ½ä¼˜åŒ–æ”¶ç›Š"""
        target_duration = self.monitoring_config["max_ci_duration"]
        if metric.duration > target_duration:
            time_saved = metric.duration - target_duration
            improvement_percent = (time_saved / metric.duration) * 100

            return OptimizationResult(
                optimization_type=OptimizationType.TEST_STRATEGY,
                improvement_percent=improvement_percent,
                time_saved=time_saved,
                success_rate=85.0,
                recommendation=f"ä¼˜åŒ–{metric.name}çš„æµ‹è¯•ç­–ç•¥ï¼Œç›®æ ‡æ‰§è¡Œæ—¶é—´{target_duration}ç§’"
            )

        return None

    def generate_optimized_ci_config(self) -> Dict[str, Any]:
        """ç”Ÿæˆä¼˜åŒ–çš„CIé…ç½®"""
        config = {
            "version": "v1",
            "timestamp": datetime.now().isoformat(),
            "optimizations_applied": []
        }

        # ç¼“å­˜ä¼˜åŒ–
        cache_config = {
            "pip_cache": {
                "enabled": True,
                "key": "${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}",
                "restore_keys": [
                    "${{ runner.os }}-pip-"
                ]
            },
            "docker_cache": {
                "enabled": True,
                "cache_from": ["type=gha"],
                "cache_to": ["type=gha,mode=max"]
            },
            "pytest_cache": {
                "enabled": True,
                "cache_dir": ".pytest_cache"
            }
        }
        config["optimizations_applied"].append({
            "type": "cache",
            "config": cache_config,
            "expected_improvement": "20-30% time reduction"
        })

        # å¹¶è¡Œä¼˜åŒ–
        parallel_config = {
            "test_parallelization": {
                "enabled": True,
                "strategy": "matrix",
                "matrix": {
                    "test_group": ["unit", "integration", "api"]
                }
            },
            "quality_checks_parallel": {
                "enabled": True,
                "jobs": ["ruff", "mypy", "bandit"]
            }
        }
        config["optimizations_applied"].append({
            "type": "parallel",
            "config": parallel_config,
            "expected_improvement": "40-50% time reduction"
        })

        # æ™ºèƒ½æµ‹è¯•é€‰æ‹©
        smart_testing_config = {
            "affected_files_detection": {
                "enabled": True,
                "base_branch": "main"
            },
            "test_selection": {
                "strategy": "smart",
                "fallback_on_failure": True
            }
        }
        config["optimizations_applied"].append({
            "type": "smart_testing",
            "config": smart_testing_config,
            "expected_improvement": "60-70% time reduction for small changes"
        })

        return config

    def setup_monitoring_alerts(self) -> Dict[str, Any]:
        """è®¾ç½®ç›‘æ§å‘Šè­¦"""
        alerts = {
            "performance_alerts": {
                "ci_duration_threshold": self.monitoring_config["max_ci_duration"],
                "test_duration_threshold": self.monitoring_config["max_test_duration"],
                "success_rate_threshold": self.monitoring_config["min_success_rate"]
            },
            "quality_alerts": {
                "coverage_threshold": 30.0,
                "code_quality_threshold": 80.0,
                "security_issues_threshold": 0
            },
            "notification_channels": {
                "github_issues": {
                    "enabled": True,
                    "auto_create": True,
                    "labels": ["ci-cd", "monitoring", "alert"]
                },
                "pull_request_comments": {
                    "enabled": True,
                    "on_failure": True,
                    "on_warning": True
                }
            }
        }

        return alerts

    def run_comprehensive_monitoring(self) -> MonitoringReport:
        """è¿è¡Œå…¨é¢ç›‘æ§åˆ†æ"""
        print("ğŸ” å¼€å§‹CI/CDæµæ°´çº¿ç›‘æ§åˆ†æ...")

        # 1. æ€§èƒ½åˆ†æ
        print("ğŸ“Š åˆ†æCIæ€§èƒ½...")
        performance_metrics = self.analyze_current_ci_performance()

        # 2. ä¼˜åŒ–æœºä¼šè¯†åˆ«
        print("ğŸ¯ è¯†åˆ«ä¼˜åŒ–æœºä¼š...")
        optimizations = self.identify_optimization_opportunities(performance_metrics)

        # 3. ç”Ÿæˆå»ºè®®
        print("ğŸ’¡ ç”Ÿæˆä¼˜åŒ–å»ºè®®...")
        recommendations = self._generate_monitoring_recommendations(performance_metrics, optimizations)

        # 4. ç¡®å®šåç»­è¡ŒåŠ¨
        next_actions = self._generate_next_actions(optimizations)

        # 5. è¯„ä¼°æ•´ä½“çŠ¶æ€
        overall_status = self._evaluate_overall_status(performance_metrics)

        # ç»Ÿè®¡æ£€æŸ¥ç»“æœ
        total_checks = len(performance_metrics) + len(optimizations)
        passed_checks = len([m for m in performance_metrics if m.status == "success"])
        failed_checks = len([m for m in performance_metrics if m.status == "failed"])
        warning_checks = total_checks - passed_checks - failed_checks

        return MonitoringReport(
            timestamp=self.timestamp,
            overall_status=overall_status,
            total_checks=total_checks,
            passed_checks=passed_checks,
            failed_checks=failed_checks,
            warning_checks=warning_checks,
            performance_metrics=performance_metrics,
            optimizations=optimizations,
            recommendations=recommendations,
            next_actions=next_actions
        )

    def _generate_monitoring_recommendations(
        self,
        metrics: List[CIPerformanceMetric],
        optimizations: List[OptimizationResult]
    ) -> List[str]:
        """ç”Ÿæˆç›‘æ§å»ºè®®"""
        recommendations = []

        # æ€§èƒ½å»ºè®®
        slow_metrics = [m for m in metrics if m.duration > self.monitoring_config["max_ci_duration"]]
        if slow_metrics:
            recommendations.append(f"ğŸŒ **æ€§èƒ½ä¼˜åŒ–**: å‘ç°{len(slow_metrics)}ä¸ªæ…¢é€Ÿä»»åŠ¡ï¼Œå»ºè®®å¯ç”¨ç¼“å­˜å’Œå¹¶è¡Œæ‰§è¡Œ")

        # ç¼“å­˜å»ºè®®
        no_cache_metrics = [m for m in metrics if not m.cache_hit]
        if no_cache_metrics:
            recommendations.append(f"ğŸ’¾ **ç¼“å­˜ä¼˜åŒ–**: {len(no_cache_metrics)}ä¸ªä»»åŠ¡æœªä½¿ç”¨ç¼“å­˜ï¼Œå»ºè®®é…ç½®ä¾èµ–ç¼“å­˜")

        # å¹¶è¡Œå»ºè®®
        low_parallel_metrics = [m for m in metrics if m.parallel_jobs < 2]
        if low_parallel_metrics:
            recommendations.append(f"âš¡ **å¹¶è¡Œä¼˜åŒ–**: {len(low_parallel_metrics)}ä¸ªä»»åŠ¡æœªå¹¶è¡ŒåŒ–ï¼Œå¯ä»¥æå‡æ‰§è¡Œæ•ˆç‡")

        # ä¼˜åŒ–å»ºè®®
        if optimizations:
            total_time_saved = sum(opt.time_saved for opt in optimizations)
            recommendations.append(f"ğŸ¯ **é¢„æœŸæ”¶ç›Š**: åº”ç”¨æ‰€æœ‰ä¼˜åŒ–å¯èŠ‚çœ{total_time_saved:.1f}ç§’æ‰§è¡Œæ—¶é—´")

        return recommendations

    def _generate_next_actions(self, optimizations: List[OptimizationResult]) -> List[str]:
        """ç”Ÿæˆåç»­è¡ŒåŠ¨"""
        actions = []

        # æŒ‰ä¼˜åŒ–æ”¶ç›Šæ’åº
        sorted_optimizations = sorted(optimizations, key=lambda x: x.time_saved, reverse=True)

        if sorted_optimizations:
            best_opt = sorted_optimizations[0]
            actions.append(f"ğŸš€ **ç«‹å³è¡ŒåŠ¨**: åº”ç”¨{best_opt.optimization_type.value}ä¼˜åŒ–ï¼Œé¢„è®¡èŠ‚çœ{best_opt.time_saved:.1f}ç§’")

        actions.extend([
            "ğŸ“‹ **é…ç½®æ›´æ–°**: æ›´æ–°GitHub Actionsé…ç½®æ–‡ä»¶",
            "ğŸ“Š **ç›‘æ§è®¾ç½®**: å¯ç”¨æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦",
            "ğŸ§ª **æµ‹è¯•éªŒè¯**: åœ¨å¼€å‘åˆ†æ”¯éªŒè¯ä¼˜åŒ–æ•ˆæœ",
            "ğŸ“ˆ **æ€§èƒ½è·Ÿè¸ª**: å»ºç«‹CIæ€§èƒ½åŸºçº¿å’Œè¶‹åŠ¿ç›‘æ§"
        ])

        return actions

    def _evaluate_overall_status(self, metrics: List[CIPerformanceMetric]) -> MonitoringStatus:
        """è¯„ä¼°æ•´ä½“çŠ¶æ€"""
        if not metrics:
            return MonitoringStatus.UNKNOWN

        failed_count = len([m for m in metrics if m.status == "failed"])
        slow_count = len([m for m in metrics if m.duration > self.monitoring_config["max_ci_duration"]])

        if failed_count > 0:
            return MonitoringStatus.CRITICAL
        elif slow_count > len(metrics) // 2:
            return MonitoringStatus.WARNING
        else:
            return MonitoringStatus.HEALTHY

    def export_monitoring_report(self, report: MonitoringReport, output_file: Optional[Path] = None) -> Path:
        """å¯¼å‡ºç›‘æ§æŠ¥å‘Š"""
        if output_file is None:
            output_file = self.project_root / "reports" / "ci_cd_monitoring" / f"ci_monitoring_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        output_file.parent.mkdir(parents=True, exist_ok=True)

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
        report_dict = asdict(report)
        report_dict["overall_status"] = report.overall_status.value
        report_dict["performance_metrics"] = [asdict(metric) for metric in report.performance_metrics]
        report_dict["optimizations"] = [
            {
                **asdict(opt),
                "optimization_type": opt.optimization_type.value
            }
            for opt in report.optimizations
        ]

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, indent=2, ensure_ascii=False)

        return output_file

    def create_optimized_workflow_files(self) -> Dict[str, Path]:
        """åˆ›å»ºä¼˜åŒ–çš„workflowæ–‡ä»¶"""
        optimized_files = {}

        # ä¼˜åŒ–çš„CIé…ç½®
        optimized_ci_content = self._generate_optimized_ci_workflow()
        ci_file = self.project_root / ".github" / "workflows" / "optimized-ci.yml"
        ci_file.parent.mkdir(parents=True, exist_ok=True)

        with open(ci_file, 'w', encoding='utf-8') as f:
            f.write(optimized_ci_content)

        optimized_files["optimized_ci"] = ci_file

        # ç›‘æ§é…ç½®
        monitoring_content = self._generate_monitoring_workflow()
        monitoring_file = self.project_root / ".github" / "workflows" / "ci-monitoring.yml"

        with open(monitoring_file, 'w', encoding='utf-8') as f:
            f.write(monitoring_content)

        optimized_files["monitoring"] = monitoring_file

        return optimized_files

    def _generate_optimized_ci_workflow(self) -> str:
        """ç”Ÿæˆä¼˜åŒ–çš„CI workflowå†…å®¹"""
        return '''name: Optimized CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  PYTHON_VERSION: '3.11'

jobs:
  # æ™ºèƒ½å˜æ›´æ£€æµ‹
  detect-changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      test-changed: ${{ steps.changes.outputs.test }}
      src-changed: ${{ steps.changes.outputs.src }}
      docs-changed: ${{ steps.changes.outputs.docs }}
      ci-changed: ${{ steps.changes.outputs.ci }}
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 2

    - name: Detect changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          src:
            - 'src/**/*.py'
          test:
            - 'tests/**/*.py'
          docs:
            - 'docs/**/*'
          ci:
            - '.github/**/*'
            - 'requirements/**/*'

  # å¹¶è¡Œè´¨é‡æ£€æŸ¥
  quality-checks:
    name: Quality Checks
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.src-changed == 'true' || needs.detect-changes.outputs.ci-changed == 'true'

    strategy:
      fail-fast: false
      matrix:
        check: [ruff, mypy, bandit, security-audit]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.local/share/virtualenvs
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff mypy bandit pip-audit
        pip install -r requirements/requirements.lock

    - name: Run Ruff
      if: matrix.check == 'ruff'
      run: |
        ruff check src/ tests/ --output-format=github
        ruff format src/ tests/ --check

    - name: Run MyPy
      if: matrix.check == 'mypy'
      run: mypy src/ --ignore-missing-imports

    - name: Run Bandit
      if: matrix.check == 'bandit'
      run: bandit -r src/ -f json -o bandit-report.json

    - name: Run Security Audit
      if: matrix.check == 'security-audit'
      run: pip-audit --format=json --output=audit-report.json

    - name: Upload security reports
      if: matrix.check == 'bandit' || matrix.check == 'security-audit'
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json
          audit-report.json
        retention-days: 30

  # æ™ºèƒ½æµ‹è¯•æ‰§è¡Œ
  smart-tests:
    name: Smart Tests
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.src-changed == 'true' || needs.detect-changes.outputs.test-changed == 'true'

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          .venv
        key: ${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-python-${{ env.PYTHON_VERSION }}-

    - name: Cache pytest
      uses: actions/cache@v3
      with:
        path: .pytest_cache
        key: ${{ runner.os }}-pytest-${{ hashFiles('**/pytest.ini') }}
        restore-keys: |
          ${{ runner.os }}-pytest-

    - name: Install dependencies
      run: |
        python -m venv .venv
        source .venv/bin/activate
        pip install --upgrade pip
        pip install -r requirements/requirements.lock
        pip install pytest pytest-cov pytest-asyncio pytest-xdist

    - name: Run smart tests
      run: |
        source .venv/bin/activate

        # æ™ºèƒ½æµ‹è¯•é€‰æ‹©
        if [ "${{ needs.detect-changes.outputs.src-changed }}" == "true" ]; then
          # æºç å˜æ›´ï¼Œè¿è¡Œç›¸å…³æµ‹è¯•
          pytest tests/unit/ tests/integration/ -x \
            --cov=src --cov-report=xml --cov-report=html \
            --dist=loadscope --auto-adjust-parallelism \
            -m "not slow"
        else
          # ä»…æµ‹è¯•å˜æ›´ï¼Œè¿è¡Œå¿«é€Ÿæ£€æŸ¥
          pytest tests/unit/ -x --maxfail=5 \
            -m "smoke or critical"
        fi

    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # æ€§èƒ½ç›‘æ§
  performance-monitor:
    name: Performance Monitor
    runs-on: ubuntu-latest
    needs: [quality-checks, smart-tests]
    if: always()

    steps:
    - name: Monitor CI performance
      run: |
        echo "ğŸ“Š CI Performance Summary:"
        echo "Total duration: ${{ job.status }}"
        echo "Jobs completed: ${{ needs.quality-checks.result }}, ${{ needs.smart-tests.result }}"

        # æ€§èƒ½æ•°æ®æ”¶é›†
        cat << EOF > performance-metrics.json
        {
          "timestamp": "$(date -Iseconds)",
          "workflow": "optimized-ci",
          "quality_checks": "${{ needs.quality-checks.result }}",
          "smart_tests": "${{ needs.smart-tests.result }}",
          "total_duration": "${{ job.status }}"
        }
        EOF

    - name: Upload performance metrics
      uses: actions/upload-artifact@v3
      with:
        name: performance-metrics
        path: performance-metrics.json
        retention-days: 90

  # è‡ªåŠ¨ä¼˜åŒ–å»ºè®®
  optimization-recommendations:
    name: Optimization Recommendations
    runs-on: ubuntu-latest
    needs: [detect-changes, quality-checks, smart-tests]
    if: always() && (needs.quality-checks.result == 'failure' || needs.smart-tests.result == 'failure')

    steps:
    - name: Generate optimization recommendations
      run: |
        cat << EOF > optimization-recommendations.md
        ## ğŸš€ CI/CDä¼˜åŒ–å»ºè®®

        ### æ£€æµ‹åˆ°çš„é—®é¢˜
        - è´¨é‡æ£€æŸ¥çŠ¶æ€: ${{ needs.quality-checks.result }}
        - æµ‹è¯•æ‰§è¡ŒçŠ¶æ€: ${{ needs.smart-tests.result }}

        ### å»ºè®®çš„ä¼˜åŒ–æªæ–½
        1. **å¯ç”¨æ›´å¤šç¼“å­˜**: æ£€æŸ¥ä¾èµ–ç¼“å­˜é…ç½®
        2. **å¢åŠ å¹¶è¡Œåº¦**: è€ƒè™‘åˆ†å‰²æµ‹è¯•å¥—ä»¶
        3. **ä¼˜åŒ–æµ‹è¯•é€‰æ‹©**: ä½¿ç”¨æ›´æ™ºèƒ½çš„æµ‹è¯•é€‰æ‹©ç­–ç•¥
        4. **æ€§èƒ½åŸºçº¿**: å»ºç«‹CIæ€§èƒ½ç›‘æ§åŸºçº¿

        ### é¢„æœŸæ”¶ç›Š
        - æ‰§è¡Œæ—¶é—´å‡å°‘: 20-40%
        - ç¼“å­˜å‘½ä¸­ç‡: >80%
        - å¹¶è¡Œæ•ˆç‡: >90%

        ---
        ç”Ÿæˆæ—¶é—´: $(date)
        EOF

    - name: Create optimization issue
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');

          try {
            const recommendations = fs.readFileSync('optimization-recommendations.md', 'utf8');

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ğŸš€ CI/CDä¼˜åŒ–å»ºè®®',
              body: recommendations,
              labels: ['ci-cd', 'optimization', 'suggestion']
            });
          } catch (error) {
            console.error('Failed to create optimization issue:', error);
          }
'''

    def _generate_monitoring_workflow(self) -> str:
        """ç”Ÿæˆç›‘æ§workflowå†…å®¹"""
        return '''name: CI/CD Monitoring

on:
  schedule:
    # æ¯å¤©UTC 00:00è¿è¡Œ
    - cron: '0 0 * * *'
  workflow_dispatch:
  workflow_run:
    workflows: ["Optimized CI Pipeline"]
    types: [completed]

jobs:
  ci-performance-monitor:
    name: CI Performance Monitor
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install requests pandas numpy matplotlib
        pip install -r requirements/requirements.lock

    - name: Run CI monitoring analysis
      run: |
        python3 scripts/ci_cd_monitor_optimizer.py \
          --analyze-performance \
          --generate-report \
          --output-reports

    - name: Upload monitoring reports
      uses: actions/upload-artifact@v3
      with:
        name: ci-monitoring-reports
        path: reports/ci_cd_monitoring/
        retention-days: 90

    - name: Check for performance degradation
      id: performance-check
      run: |
        # æ£€æŸ¥æ€§èƒ½æ˜¯å¦æœ‰æ˜¾è‘—ä¸‹é™
        python3 -c "
        import json
        import sys
        from pathlib import Path

        # æŸ¥æ‰¾æœ€æ–°çš„ç›‘æ§æŠ¥å‘Š
        reports_dir = Path('reports/ci_cd_monitoring')
        if reports_dir.exists():
            reports = list(reports_dir.glob('ci_monitoring_report_*.json'))
            if reports:
                latest_report = sorted(reports)[-1]
                with open(latest_report) as f:
                    data = json.load(f)

                # æ£€æŸ¥å…³é”®æŒ‡æ ‡
                if data.get('overall_status') == 'critical':
                    print('ğŸš¨ æ£€æµ‹åˆ°ä¸¥é‡çš„CIæ€§èƒ½é—®é¢˜')
                    sys.exit(1)
                elif data.get('warning_checks', 0) > 3:
                    print('âš ï¸ æ£€æµ‹åˆ°å¤šä¸ªCIæ€§èƒ½è­¦å‘Š')
                    sys.exit(2)
                else:
                    print('âœ… CIæ€§èƒ½æ­£å¸¸')
                    sys.exit(0)
        "

    - name: Create performance alert
      if: failure() && steps.performance-check.outcome == 'failure'
      uses: actions/github-script@v6
      with:
        script: |
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'ğŸš¨ CI/CDæ€§èƒ½å‘Šè­¦',
            body: `
            æ£€æµ‹åˆ°CI/CDæµæ°´çº¿æ€§èƒ½é—®é¢˜

            **æ—¶é—´**: ${new Date().toISOString()}
            **å·¥ä½œæµ**: ${context.workflow}
            **è¿è¡ŒID**: ${context.runId}

            è¯·æŸ¥çœ‹[è¯¦ç»†æŠ¥å‘Š](${context.serverUrl}/${context.repository}/actions/runs/${context.runId})å¹¶é‡‡å–ç›¸åº”æªæ–½ã€‚
            `,
            labels: ['ci-cd', 'monitoring', 'alert', 'high-priority']
          });

  # è´¨é‡è¶‹åŠ¿åˆ†æ
  quality-trends:
    name: Quality Trends Analysis
    runs-on: ubuntu-latest

    steps:
    - name: Analyze quality trends
      run: |
        echo "ğŸ“ˆ åˆ†æè´¨é‡è¶‹åŠ¿..."
        # è¿™é‡Œå¯ä»¥é›†æˆå†å²æ•°æ®åˆ†æ
        echo "è´¨é‡è¶‹åŠ¿åˆ†æå®Œæˆ"

    - name: Update quality badge
      run: |
        echo "ğŸ… æ›´æ–°è´¨é‡å¾½ç« ..."
        # æ›´æ–°READMEä¸­çš„è´¨é‡å¾½ç« 
'''

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="CI/CDæµæ°´çº¿ç›‘æ§å’Œä¼˜åŒ–å·¥å…·")
    parser.add_argument(
        "--project-root",
        type=Path,
        help="é¡¹ç›®æ ¹ç›®å½•è·¯å¾„"
    )
    parser.add_argument(
        "--analyze-performance",
        action="store_true",
        help="åˆ†æCIæ€§èƒ½"
    )
    parser.add_argument(
        "--generate-report",
        action="store_true",
        help="ç”Ÿæˆç›‘æ§æŠ¥å‘Š"
    )
    parser.add_argument(
        "--optimize-workflows",
        action="store_true",
        help="ä¼˜åŒ–workflowé…ç½®"
    )
    parser.add_argument(
        "--output-reports",
        action="store_true",
        help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶"
    )

    args = parser.parse_args()

    # åˆ›å»ºç›‘æ§å™¨å®ä¾‹
    project_root = args.project_root or Path(__file__).parent.parent.parent
    monitor = CICDMonitor(project_root)

    try:
        if args.analyze_performance or args.generate_report:
            # è¿è¡Œå…¨é¢ç›‘æ§åˆ†æ
            report = monitor.run_comprehensive_monitoring()

            if args.output_reports:
                # å¯¼å‡ºæŠ¥å‘Š
                report_file = monitor.export_monitoring_report(report)
                print(f"\nğŸ“„ ç›‘æ§æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")

            # æ˜¾ç¤ºç»“æœæ‘˜è¦
            print(f"\nğŸ“Š CI/CDç›‘æ§ç»“æœ: {report.overall_status.value.upper()}")
            print(f"   æ€»æ£€æŸ¥é¡¹: {report.total_checks}")
            print(f"   é€šè¿‡: {report.passed_checks}")
            print(f"   å¤±è´¥: {report.failed_checks}")
            print(f"   è­¦å‘Š: {report.warning_checks}")

            if report.optimizations:
                total_time_saved = sum(opt.time_saved for opt in report.optimizations)
                print(f"   æ½œåœ¨æ—¶é—´èŠ‚çœ: {total_time_saved:.1f}ç§’")

            # æ˜¾ç¤ºå»ºè®®
            if report.recommendations:
                print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
                for rec in report.recommendations[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæœ€é‡è¦çš„å»ºè®®
                    print(f"   {rec}")

            # æ˜¾ç¤ºåç»­è¡ŒåŠ¨
            if report.next_actions:
                print(f"\nğŸš€ åç»­è¡ŒåŠ¨:")
                for action in report.next_actions[:3]:  # æ˜¾ç¤ºå‰3ä¸ªè¡ŒåŠ¨
                    print(f"   {action}")

        if args.optimize_workflows:
            # åˆ›å»ºä¼˜åŒ–çš„workflowæ–‡ä»¶
            optimized_files = monitor.create_optimized_workflow_files()
            print(f"\nğŸ”§ å·²åˆ›å»ºä¼˜åŒ–çš„workflowæ–‡ä»¶:")
            for name, file_path in optimized_files.items():
                print(f"   {name}: {file_path}")

            # ç”Ÿæˆä¼˜åŒ–é…ç½®
            optimization_config = monitor.generate_optimized_ci_config()
            config_file = project_root / "reports" / "ci_optimization_config.json"
            config_file.parent.mkdir(parents=True, exist_ok=True)

            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(optimization_config, f, indent=2, ensure_ascii=False)

            print(f"   é…ç½®æ–‡ä»¶: {config_file}")

            # è®¾ç½®ç›‘æ§å‘Šè­¦
            alerts_config = monitor.setup_monitoring_alerts()
            alerts_file = project_root / "reports" / "ci_monitoring_alerts.json"

            with open(alerts_file, 'w', encoding='utf-8') as f:
                json.dump(alerts_config, f, indent=2, ensure_ascii=False)

            print(f"   å‘Šè­¦é…ç½®: {alerts_file}")

        if not any([args.analyze_performance, args.generate_report, args.optimize_workflows, args.output_reports]):
            # é»˜è®¤è¿è¡Œå®Œæ•´åˆ†æ
            report = monitor.run_comprehensive_monitoring()
            report_file = monitor.export_monitoring_report(report)

            print(f"ğŸ“Š CI/CDç›‘æ§åˆ†æå®Œæˆ")
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
            print(f"ğŸ“ˆ æ•´ä½“çŠ¶æ€: {report.overall_status.value.upper()}")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
        sys.exit(130)
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()