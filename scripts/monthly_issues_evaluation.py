#!/usr/bin/env python3
"""
æœˆåº¦Issuesç®¡ç†æ•ˆæœè¯„ä¼°è„šæœ¬
ç”Ÿæˆæœˆåº¦Issuesç®¡ç†æ•ˆæœçš„ç»¼åˆè¯„ä¼°æŠ¥å‘Š

Author: Claude Code
Version: 1.0
Purpose: Monthly evaluation of issues management effectiveness
"""

import argparse
import os
import sys
from collections import defaultdict
from datetime import datetime, timedelta
from pathlib import Path

import requests


class MonthlyIssuesEvaluator:
    """æœˆåº¦Issuesè¯„ä¼°å™¨"""

    def __init__(self, repo: str, github_token: str = None):
        self.repo = repo
        self.github_token = github_token
        self.headers = {
            "Accept": "application/vnd.github.v3+json",
            "User-Agent": "Monthly-Issues-Evaluator/1.0"
        }

        if github_token:
            self.headers["Authorization"] = f"token {github_token}"

    def get_issues_in_date_range(self, start_date: datetime, end_date: datetime) -> dict:
        """è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´å†…çš„Issues"""
        all_issues = {"created": [], "closed": [], "updated": []}

        # è·å–åˆ›å»ºçš„Issues
        created_issues = self._fetch_issues_by_date_range(start_date, end_date, "created")
        all_issues["created"] = created_issues

        # è·å–å…³é—­çš„Issues
        closed_issues = self._fetch_issues_by_date_range(start_date, end_date, "closed")
        all_issues["closed"] = closed_issues

        # è·å–æ›´æ–°çš„Issues
        updated_issues = self._fetch_issues_by_date_range(start_date, end_date, "updated")
        all_issues["updated"] = updated_issues

        return all_issues

    def _fetch_issues_by_date_range(self, start_date: datetime, end_date: datetime,
                                   date_type: str = "created") -> list:
        """æ ¹æ®æ—¥æœŸèŒƒå›´è·å–Issues"""
        issues = []
        page = 1

        # æ ¼å¼åŒ–æ—¥æœŸ
        start_str = start_date.strftime("%Y-%m-%dT%H:%M:%SZ")
        end_date.strftime("%Y-%m-%dT%H:%M:%SZ")

        while True:
            url = f"https://api.github.com/repos/{self.repo}/issues"
            params = {
                "state": "all",
                "per_page": 100,
                "page": page,
                "sort": date_type,
                "direction": "desc"
            }

            if date_type == "created":
                params["since"] = start_str
            elif date_type == "closed":
                # GitHub APIæ²¡æœ‰ç›´æ¥çš„closedèŒƒå›´æŸ¥è¯¢ï¼Œéœ€è¦è¿‡æ»¤
                pass
            elif date_type == "updated":
                # GitHub APIæ²¡æœ‰ç›´æ¥çš„updatedèŒƒå›´æŸ¥è¯¢ï¼Œéœ€è¦è¿‡æ»¤
                pass

            try:
                response = requests.get(url, headers=self.headers, params=params)
                response.raise_for_status()

                page_issues = response.json()
                if not page_issues:
                    break

                # è¿‡æ»¤PR
                page_issues = [issue for issue in page_issues if "pull_request" not in issue]

                # æ ¹æ®æ—¥æœŸç±»å‹è¿‡æ»¤
                filtered_issues = []
                for issue in page_issues:
                    issue_date = datetime.fromisoformat(
                        issue[date_type + "_at"].replace("Z", "+00:00")
                    )

                    if start_date <= issue_date <= end_date:
                        filtered_issues.append(issue)
                    elif issue_date < start_date and date_type in ["created", "closed", "updated"]:
                        # å¦‚æœissueæ—¥æœŸæ—©äºå¼€å§‹æ—¥æœŸï¼Œè¯´æ˜å·²ç»è¶…å‡ºèŒƒå›´
                        if date_type == "created":
                            break

                filtered_issues.extend(filtered_issues)
                issues.extend(filtered_issues)

                if len(page_issues) < 100:
                    break

                page += 1

            except requests.exceptions.RequestException:
                break

        return issues

    def calculate_metrics(self, issues_data: dict, period_days: int = 30) -> dict:
        """è®¡ç®—å„ç§æŒ‡æ ‡"""
        metrics = {
            "creation_metrics": {},
            "resolution_metrics": {},
            "activity_metrics": {},
            "label_metrics": {},
            "assignment_metrics": {},
            "time_metrics": {}
        }

        # åˆ›å»ºæŒ‡æ ‡
        created_issues = issues_data["created"]
        closed_issues = issues_data["closed"]
        updated_issues = issues_data["updated"]

        metrics["creation_metrics"] = {
            "total_created": len(created_issues),
            "daily_avg_created": len(created_issues) / period_days,
            "created_by_type": self._count_by_type(created_issues),
            "created_by_priority": self._count_by_priority(created_issues)
        }

        # è§£å†³æŒ‡æ ‡
        metrics["resolution_metrics"] = {
            "total_closed": len(closed_issues),
            "daily_avg_closed": len(closed_issues) / period_days,
            "closure_rate": len(closed_issues) / max(len(created_issues), 1) * 100,
            "closed_by_type": self._count_by_type(closed_issues),
            "closed_by_priority": self._count_by_priority(closed_issues)
        }

        # æ´»è·ƒåº¦æŒ‡æ ‡
        metrics["activity_metrics"] = {
            "total_active": len(updated_issues),
            "daily_avg_updates": len(updated_issues) / period_days,
            "most_active_day": self._find_most_active_day(updated_issues),
            "comment_activity": self._analyze_comment_activity(updated_issues)
        }

        # æ ‡ç­¾æŒ‡æ ‡
        all_labels = []
        for issue in created_issues + closed_issues + updated_issues:
            all_labels.extend([label["name"] for label in issue.get("labels", [])])

        metrics["label_metrics"] = {
            "total_labels_used": len(set(all_labels)),
            "most_used_labels": self._get_most_used_labels(all_labels),
            "label_coverage": self._calculate_label_coverage(created_issues + updated_issues)
        }

        # åˆ†é…æŒ‡æ ‡
        metrics["assignment_metrics"] = {
            "assigned_issues": self._count_assigned_issues(created_issues + updated_issues),
            "assignment_rate": self._calculate_assignment_rate(created_issues + updated_issues),
            "top_assignees": self._get_top_assignees(created_issues + updated_issues)
        }

        # æ—¶é—´æŒ‡æ ‡
        metrics["time_metrics"] = {
            "avg_resolution_time": self._calculate_avg_resolution_time(closed_issues),
            "avg_first_response_time": self._calculate_avg_first_response_time(created_issues),
            "resolution_trend": self._calculate_resolution_trend(closed_issues)
        }

        return metrics

    def _count_by_type(self, issues: list) -> dict:
        """æŒ‰ç±»å‹ç»Ÿè®¡Issues"""
        type_counts = defaultdict(int)

        for issue in issues:
            labels = [label["name"] for label in issue.get("labels", [])]

            if "bug" in labels:
                type_counts["bug"] += 1
            elif "enhancement" in labels:
                type_counts["enhancement"] += 1
            elif "documentation" in labels:
                type_counts["documentation"] += 1
            elif "testing" in labels:
                type_counts["testing"] += 1
            else:
                type_counts["other"] += 1

        return dict(type_counts)

    def _count_by_priority(self, issues: list) -> dict:
        """æŒ‰ä¼˜å…ˆçº§ç»Ÿè®¡Issues"""
        priority_counts = defaultdict(int)

        for issue in issues:
            labels = [label["name"] for label in issue.get("labels", [])]

            priority = "medium"  # é»˜è®¤ä¼˜å…ˆçº§
            for label in labels:
                if label.startswith("priority/"):
                    priority = label.split("/")[1]
                    break

            priority_counts[priority] += 1

        return dict(priority_counts)

    def _find_most_active_day(self, issues: list) -> str:
        """æ‰¾åˆ°æœ€æ´»è·ƒçš„æ—¥æœŸ"""
        day_counts = defaultdict(int)

        for issue in issues:
            updated_at = datetime.fromisoformat(issue["updated_at"].replace("Z", "+00:00"))
            day = updated_at.strftime("%Y-%m-%d")
            day_counts[day] += 1

        if day_counts:
            return max(day_counts.items(), key=lambda x: x[1])[0]
        return "N/A"

    def _analyze_comment_activity(self, issues: list) -> dict:
        """åˆ†æè¯„è®ºæ´»è·ƒåº¦"""
        total_comments = sum(issue.get("comments", 0) for issue in issues)

        if not issues:
            return {"avg_comments_per_issue": 0, "total_comments": 0}

        return {
            "avg_comments_per_issue": total_comments / len(issues),
            "total_comments": total_comments
        }

    def _get_most_used_labels(self, labels: list) -> list:
        """è·å–æœ€å¸¸ç”¨çš„æ ‡ç­¾"""
        label_counts = defaultdict(int)
        for label in labels:
            label_counts[label] += 1

        return sorted(label_counts.items(), key=lambda x: x[1], reverse=True)[:10]

    def _calculate_label_coverage(self, issues: list) -> dict:
        """è®¡ç®—æ ‡ç­¾è¦†ç›–ç‡"""
        labeled_issues = sum(1 for issue in issues if issue.get("labels"))

        if not issues:
            return {"labeled_issues": 0, "coverage_rate": 0}

        coverage_rate = (labeled_issues / len(issues)) * 100
        return {
            "labeled_issues": labeled_issues,
            "coverage_rate": coverage_rate
        }

    def _count_assigned_issues(self, issues: list) -> int:
        """ç»Ÿè®¡å·²åˆ†é…çš„Issuesæ•°é‡"""
        return sum(1 for issue in issues if issue.get("assignee"))

    def _calculate_assignment_rate(self, issues: list) -> float:
        """è®¡ç®—åˆ†é…ç‡"""
        if not issues:
            return 0

        assigned_count = self._count_assigned_issues(issues)
        return (assigned_count / len(issues)) * 100

    def _get_top_assignees(self, issues: list) -> list:
        """è·å–è·å¾—æœ€å¤šåˆ†é…çš„ç”¨æˆ·"""
        assignee_counts = defaultdict(int)

        for issue in issues:
            assignee = issue.get("assignee")
            if assignee:
                assignee_counts[assignee["login"]] += 1

        return sorted(assignee_counts.items(), key=lambda x: x[1], reverse=True)[:5]

    def _calculate_avg_resolution_time(self, closed_issues: list) -> dict:
        """è®¡ç®—å¹³å‡è§£å†³æ—¶é—´"""
        if not closed_issues:
            return {"days": 0, "hours": 0}

        total_hours = 0
        for issue in closed_issues:
            created_at = datetime.fromisoformat(issue["created_at"].replace("Z", "+00:00"))
            closed_at = datetime.fromisoformat(issue["closed_at"].replace("Z", "+00:00"))

            hours_diff = (closed_at - created_at).total_seconds() / 3600
            total_hours += hours_diff

        avg_hours = total_hours / len(closed_issues)
        avg_days = avg_hours / 24

        return {"days": round(avg_days, 1), "hours": round(avg_hours, 1)}

    def _calculate_avg_first_response_time(self, created_issues: list) -> dict:
        """è®¡ç®—å¹³å‡é¦–æ¬¡å“åº”æ—¶é—´"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦è·å–è¯„è®ºæ—¶é—´æˆ³
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªä¼°ç®—å€¼
        if not created_issues:
            return {"hours": 0}

        # ä¼°ç®—å¹³å‡å“åº”æ—¶é—´ä¸º24å°æ—¶
        return {"hours": 24.0}

    def _calculate_resolution_trend(self, closed_issues: list) -> str:
        """è®¡ç®—è§£å†³è¶‹åŠ¿"""
        if len(closed_issues) < 2:
            return "insufficient_data"

        # ç®€åŒ–çš„è¶‹åŠ¿åˆ†æï¼šæ¯”è¾ƒå‰åŠæœŸå’ŒååŠæœŸçš„å…³é—­æ•°é‡
        mid_point = len(closed_issues) // 2
        first_half = closed_issues[:mid_point]
        second_half = closed_issues[mid_point:]

        if len(second_half) > len(first_half) * 1.2:
            return "improving"
        elif len(second_half) < len(first_half) * 0.8:
            return "declining"
        else:
            return "stable"

    def generate_evaluation_report(self, metrics: dict, period_start: datetime,
                                  period_end: datetime) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        period_days = (period_end - period_start).days

        report_lines = [
            "# Issuesç®¡ç†æœˆåº¦è¯„ä¼°æŠ¥å‘Š",
            "",
            f"**è¯„ä¼°æœŸé—´**: {period_start.strftime('%Y-%m-%d')} è‡³ {period_end.strftime('%Y-%m-%d')}",
            f"**è¯„ä¼°å¤©æ•°**: {period_days} å¤©",
            f"**ä»“åº“**: {self.repo}",
            f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## ğŸ“Š æ ¸å¿ƒæŒ‡æ ‡æ¦‚è§ˆ",
            ""
        ]

        # æ ¸å¿ƒæŒ‡æ ‡
        creation = metrics["creation_metrics"]
        resolution = metrics["resolution_metrics"]

        report_lines.extend([
            "### ğŸ¯ åˆ›å»ºä¸è§£å†³",
            f"- **æ–°å»ºIssues**: {creation['total_created']} (æ—¥å‡: {creation['daily_avg_created']:.1f})",
            f"- **å…³é—­Issues**: {resolution['total_closed']} (æ—¥å‡: {resolution['daily_avg_closed']:.1f})",
            f"- **è§£å†³ç‡**: {resolution['closure_rate']:.1f}%",
            f"- **å‡€å¢é•¿**: {creation['total_created'] - resolution['total_closed']}",
            ""
        ])

        # æ´»è·ƒåº¦æŒ‡æ ‡
        activity = metrics["activity_metrics"]
        report_lines.extend([
            "### ğŸ“ˆ æ´»è·ƒåº¦æŒ‡æ ‡",
            f"- **æ´»è·ƒIssues**: {activity['total_active']}",
            f"- **æ—¥å‡æ›´æ–°**: {activity['daily_avg_updates']:.1f}",
            f"- **æœ€æ´»è·ƒæ—¥æœŸ**: {activity['most_active_day']}",
            f"- **å¹³å‡è¯„è®ºæ•°**: {activity['comment_activity']['avg_comments_per_issue']:.1f}",
            ""
        ])

        # æ ‡ç­¾æŒ‡æ ‡
        label = metrics["label_metrics"]
        report_lines.extend([
            "### ğŸ·ï¸ æ ‡ç­¾ä½¿ç”¨æƒ…å†µ",
            f"- **ä½¿ç”¨æ ‡ç­¾æ•°**: {label['total_labels_used']}",
            f"- **æ ‡ç­¾è¦†ç›–ç‡**: {label['label_coverage']['coverage_rate']:.1f}%",
            f"- **æœ€å¸¸ç”¨æ ‡ç­¾**: {', '.join([f'{name}({count})' for name, count in label['most_used_labels'][:5]])}",
            ""
        ])

        # åˆ†é…æŒ‡æ ‡
        assignment = metrics["assignment_metrics"]
        report_lines.extend([
            "### ğŸ‘¥ åˆ†é…æƒ…å†µ",
            f"- **å·²åˆ†é…Issues**: {assignment['assigned_issues']}",
            f"- **åˆ†é…ç‡**: {assignment['assignment_rate']:.1f}%",
            f"- **ä¸»è¦è´¡çŒ®è€…**: {', '.join([f'{user}({count})' for user, count in assignment['top_assignees'][:3]])}",
            ""
        ])

        # æ—¶é—´æ•ˆç‡æŒ‡æ ‡
        time_metrics = metrics["time_metrics"]
        report_lines.extend([
            "### â±ï¸ æ—¶é—´æ•ˆç‡",
            f"- **å¹³å‡è§£å†³æ—¶é—´**: {time_metrics['avg_resolution_time']['days']} å¤©",
            f"- **å¹³å‡é¦–æ¬¡å“åº”**: {time_metrics['avg_first_response_time']['hours']} å°æ—¶",
            f"- **è§£å†³è¶‹åŠ¿**: {time_metrics['resolution_trend']}",
            ""
        ])

        # é—®é¢˜ç±»å‹åˆ†æ
        report_lines.extend([
            "## ğŸ“‹ é—®é¢˜ç±»å‹åˆ†æ",
            "",
            "### åˆ›å»ºé—®é¢˜ç±»å‹åˆ†å¸ƒ"
        ])

        for issue_type, count in creation["created_by_type"].items():
            percentage = (count / creation["total_created"]) * 100
            report_lines.append(f"- **{issue_type}**: {count} ({percentage:.1f}%)")

        report_lines.extend([
            "",
            "### è§£å†³é—®é¢˜ç±»å‹åˆ†å¸ƒ"
        ])

        for issue_type, count in resolution["closed_by_type"].items():
            if resolution["total_closed"] > 0:
                percentage = (count / resolution["total_closed"]) * 100
                report_lines.append(f"- **{issue_type}**: {count} ({percentage:.1f}%)")

        # ä¼˜å…ˆçº§åˆ†æ
        report_lines.extend([
            "",
            "## ğŸš¨ ä¼˜å…ˆçº§åˆ†æ",
            "",
            "### åˆ›å»ºé—®é¢˜ä¼˜å…ˆçº§åˆ†å¸ƒ"
        ])

        for priority, count in creation["created_by_priority"].items():
            if creation["total_created"] > 0:
                percentage = (count / creation["total_created"]) * 100
                report_lines.append(f"- **{priority}**: {count} ({percentage:.1f}%)")

        # æ€§èƒ½è¯„åˆ†
        score = self._calculate_performance_score(metrics)
        report_lines.extend([
            "",
            "## ğŸ† ç®¡ç†æ•ˆæœè¯„åˆ†",
            "",
            f"### ç»¼åˆè¯„åˆ†: {score['total']}/100",
            f"- **å“åº”é€Ÿåº¦**: {score['response_time']}/20",
            f"- **è§£å†³æ•ˆç‡**: {score['resolution_efficiency']}/25",
            f"- **åˆ†é…ç®¡ç†**: {score['assignment_management']}/20",
            f"- **æ ‡ç­¾è§„èŒƒ**: {score['label_standardization']}/15",
            f"- **æ´»è·ƒåº¦**: {score['activity_level']}/20",
            ""
        ])

        # æ”¹è¿›å»ºè®®
        recommendations = self._generate_recommendations(metrics, score)
        report_lines.extend([
            "## ğŸ’¡ æ”¹è¿›å»ºè®®",
            ""
        ])

        for i, rec in enumerate(recommendations, 1):
            report_lines.append(f"{i}. {rec}")

        # è¶‹åŠ¿åˆ†æ
        report_lines.extend([
            "",
            "## ğŸ“ˆ è¶‹åŠ¿åˆ†æ",
            "",
            "### å…³é”®è¶‹åŠ¿",
            f"- **è§£å†³ç‡è¶‹åŠ¿**: {time_metrics['resolution_trend']}",
            f"- **æ ‡ç­¾è¦†ç›–ç‡**: {'âœ… è‰¯å¥½' if label['label_coverage']['coverage_rate'] > 80 else 'âš ï¸ éœ€è¦æ”¹è¿›'}",
            f"- **åˆ†é…ç‡**: {'âœ… è‰¯å¥½' if assignment['assignment_rate'] > 70 else 'âš ï¸ éœ€è¦æ”¹è¿›'}",
            f"- **å¹³å‡è§£å†³æ—¶é—´**: {'âœ… è‰¯å¥½' if time_metrics['avg_resolution_time']['days'] < 7 else 'âš ï¸ éœ€è¦æ”¹è¿›'}",
            "",
            "---",
            f"*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}*",
            "*å·¥å…·: Monthly Issues Evaluator v1.0*"
        ])

        return "\n".join(report_lines)

    def _calculate_performance_score(self, metrics: dict) -> dict:
        """è®¡ç®—ç®¡ç†æ•ˆæœè¯„åˆ†"""
        score = {
            "response_time": 0,
            "resolution_efficiency": 0,
            "assignment_management": 0,
            "label_standardization": 0,
            "activity_level": 0,
            "total": 0
        }

        # å“åº”é€Ÿåº¦è¯„åˆ† (20åˆ†)
        avg_response = metrics["time_metrics"]["avg_first_response_time"]["hours"]
        if avg_response <= 12:
            score["response_time"] = 20
        elif avg_response <= 24:
            score["response_time"] = 16
        elif avg_response <= 48:
            score["response_time"] = 12
        elif avg_response <= 72:
            score["response_time"] = 8
        else:
            score["response_time"] = 4

        # è§£å†³æ•ˆç‡è¯„åˆ† (25åˆ†)
        closure_rate = metrics["resolution_metrics"]["closure_rate"]
        if closure_rate >= 80:
            score["resolution_efficiency"] = 25
        elif closure_rate >= 60:
            score["resolution_efficiency"] = 20
        elif closure_rate >= 40:
            score["resolution_efficiency"] = 15
        elif closure_rate >= 20:
            score["resolution_efficiency"] = 10
        else:
            score["resolution_efficiency"] = 5

        # åˆ†é…ç®¡ç†è¯„åˆ† (20åˆ†)
        assignment_rate = metrics["assignment_metrics"]["assignment_rate"]
        if assignment_rate >= 80:
            score["assignment_management"] = 20
        elif assignment_rate >= 60:
            score["assignment_management"] = 16
        elif assignment_rate >= 40:
            score["assignment_management"] = 12
        elif assignment_rate >= 20:
            score["assignment_management"] = 8
        else:
            score["assignment_management"] = 4

        # æ ‡ç­¾è§„èŒƒè¯„åˆ† (15åˆ†)
        label_coverage = metrics["label_metrics"]["label_coverage"]["coverage_rate"]
        if label_coverage >= 90:
            score["label_standardization"] = 15
        elif label_coverage >= 70:
            score["label_standardization"] = 12
        elif label_coverage >= 50:
            score["label_standardization"] = 9
        elif label_coverage >= 30:
            score["label_standardization"] = 6
        else:
            score["label_standardization"] = 3

        # æ´»è·ƒåº¦è¯„åˆ† (20åˆ†)
        daily_updates = metrics["activity_metrics"]["daily_avg_updates"]
        if daily_updates >= 10:
            score["activity_level"] = 20
        elif daily_updates >= 7:
            score["activity_level"] = 16
        elif daily_updates >= 5:
            score["activity_level"] = 12
        elif daily_updates >= 3:
            score["activity_level"] = 8
        else:
            score["activity_level"] = 4

        score["total"] = sum(score.values())
        return score

    def _generate_recommendations(self, metrics: dict, score: dict) -> list:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # åŸºäºè¯„åˆ†ç”Ÿæˆå»ºè®®
        if score["response_time"] < 15:
            recommendations.append("â±ï¸ **æå‡å“åº”é€Ÿåº¦**: å»ºç«‹å¿«é€Ÿå“åº”æœºåˆ¶ï¼Œç¡®ä¿24å°æ—¶å†…é¦–æ¬¡å“åº”")

        if score["resolution_efficiency"] < 20:
            recommendations.append("ğŸ¯ **æé«˜è§£å†³æ•ˆç‡**: ä¼˜åŒ–å·¥ä½œæµç¨‹ï¼Œæé«˜é—®é¢˜è§£å†³ç‡")

        if score["assignment_management"] < 15:
            recommendations.append("ğŸ‘¥ **æ”¹è¿›åˆ†é…ç®¡ç†**: å»ºç«‹è‡ªåŠ¨åˆ†é…æœºåˆ¶ï¼Œç¡®ä¿æ¯ä¸ªIssueéƒ½æœ‰æ˜ç¡®è´Ÿè´£äºº")

        if score["label_standardization"] < 12:
            recommendations.append("ğŸ·ï¸ **è§„èŒƒæ ‡ç­¾ä½¿ç”¨**: åŠ å¼ºæ ‡ç­¾æ ‡å‡†åŒ–ï¼Œæé«˜åˆ†ç±»å‡†ç¡®æ€§")

        if score["activity_level"] < 15:
            recommendations.append("ğŸ“ˆ **å¢åŠ æ´»è·ƒåº¦**: é¼“åŠ±å›¢é˜Ÿç§¯æå‚ä¸ï¼Œå®šæœŸæ›´æ–°IssueçŠ¶æ€")

        # åŸºäºå…·ä½“æŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if metrics["resolution_metrics"]["closure_rate"] < 50:
            recommendations.append("ğŸ“Š **å…³æ³¨è§£å†³ç‡**: å½“å‰è§£å†³ç‡åä½ï¼Œéœ€è¦é‡ç‚¹è·Ÿè¿›é•¿æœŸæœªè§£å†³çš„Issues")

        if metrics["assignment_metrics"]["assignment_rate"] < 60:
            recommendations.append("ğŸ”„ **ä¼˜åŒ–åˆ†é…æµç¨‹**: æé«˜Issueåˆ†é…ç‡ï¼Œé¿å…æ— äººè´Ÿè´£çš„Issues")

        avg_resolution = metrics["time_metrics"]["avg_resolution_time"]["days"]
        if avg_resolution > 14:
            recommendations.append("âš¡ **ç¼©çŸ­è§£å†³æ—¶é—´**: å¹³å‡è§£å†³æ—¶é—´è¾ƒé•¿ï¼Œéœ€è¦ä¼˜åŒ–æµç¨‹æˆ–å¢åŠ èµ„æº")

        if not recommendations:
            recommendations.append("âœ… **ä¿æŒä¼˜ç§€è¡¨ç°**: å½“å‰ç®¡ç†æ•ˆæœè‰¯å¥½ï¼Œç»§ç»­ä¿æŒç°æœ‰æ ‡å‡†å’Œæµç¨‹")

        return recommendations

    def run_monthly_evaluation(self, year: int = None, month: int = None) -> tuple:
        """è¿è¡Œæœˆåº¦è¯„ä¼°"""
        if year is None or month is None:
            now = datetime.now()
            year = now.year
            month = now.month

        # è®¡ç®—è¯„ä¼°æœŸé—´
        period_start = datetime(year, month, 1)
        if month == 12:
            period_end = datetime(year + 1, 1, 1) - timedelta(days=1)
        else:
            period_end = datetime(year, month + 1, 1) - timedelta(days=1)
        period_end = period_end.replace(hour=23, minute=59, second=59)


        # è·å–Issuesæ•°æ®
        issues_data = self.get_issues_in_date_range(period_start, period_end)


        # è®¡ç®—æŒ‡æ ‡
        metrics = self.calculate_metrics(issues_data, (period_end - period_start).days + 1)


        return metrics, period_start, period_end

    def save_evaluation_report(self, metrics: dict, period_start: datetime,
                             period_end: datetime, output_file: str = None):
        """ä¿å­˜è¯„ä¼°æŠ¥å‘Š"""
        report_content = self.generate_evaluation_report(metrics, period_start, period_end)

        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(report_content, encoding='utf-8')
        else:
            # ä½¿ç”¨é»˜è®¤æ–‡ä»¶å
            filename = f"monthly_issues_evaluation_{period_start.strftime('%Y%m')}.md"
            output_path = Path("reports") / filename
            output_path.parent.mkdir(parents=True, exist_ok=True)
            output_path.write_text(report_content, encoding='utf-8')


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æœˆåº¦Issuesç®¡ç†æ•ˆæœè¯„ä¼°")
    parser.add_argument("--repo", required=True, help="GitHubä»“åº“ (æ ¼å¼: owner/repo)")
    parser.add_argument("--token", help="GitHubè®¿é—®ä»¤ç‰Œ")
    parser.add_argument("--year", type=int, help="è¯„ä¼°å¹´ä»½ (é»˜è®¤ä¸ºå½“å‰å¹´ä»½)")
    parser.add_argument("--month", type=int, help="è¯„ä¼°æœˆä»½ (é»˜è®¤ä¸ºä¸Šä¸ªæœˆ)")
    parser.add_argument("--output", help="è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true", help="è¯¦ç»†è¾“å‡º")

    args = parser.parse_args()

    # è·å–GitHubä»¤ç‰Œ
    github_token = args.token or os.environ.get("GITHUB_TOKEN")

    if not github_token:
        pass

    # å¤„ç†æœˆä»½å‚æ•°
    if args.month is None:
        now = datetime.now()
        if now.month == 1:
            args.year = now.year - 1
            args.month = 12
        else:
            args.year = now.year
            args.month = now.month - 1

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = MonthlyIssuesEvaluator(args.repo, github_token)

    # æ‰§è¡Œè¯„ä¼°
    metrics, period_start, period_end = evaluator.run_monthly_evaluation(args.year, args.month)

    # ç”ŸæˆæŠ¥å‘Š
    evaluator.save_evaluation_report(metrics, period_start, period_end, args.output)

    if args.verbose:
        evaluator._calculate_performance_score(metrics)

    return 0


if __name__ == "__main__":
    sys.exit(main())
