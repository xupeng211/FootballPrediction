#!/usr/bin/env python3
"""
æ•°æ®é›†æ„å»ºè„šæœ¬ - Dataset Builder (ETL Pipeline)
Phase 2: æ„å»ºæœºå™¨å­¦ä¹ çš„é»„é‡‘æ•°æ®é›†

MLOpsæœ€ä½³å®è·µ:
1. Extract: ä»æ•°æ®åº“æå–å®Œæ•´æ¯”èµ›æ•°æ®
2. Transform: æ‰¹é‡ç‰¹å¾æå–å’Œå·¥ç¨‹
3. Load: ä¿å­˜ä¸ºç‰ˆæœ¬åŒ–çš„é™æ€æ•°æ®é›†

ä½œè€…: Data Engineer
åˆ›å»ºæ—¶é—´: 2025-12-10
ç‰ˆæœ¬: 1.0.0 - Golden Dataset v1
"""

import sys
import os
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
import pandas as pd

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.features.enhanced_feature_extractor import EnhancedFeatureExtractor, FeatureConfig
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy import text

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DatasetBuilder:
    """æ•°æ®é›†æ„å»ºå™¨ - ETLæµæ°´çº¿"""

    def __init__(self):
        self.feature_extractor = EnhancedFeatureExtractor(
            FeatureConfig(
                include_metadata=True,
                include_basic_stats=True,
                include_advanced_stats=True,
                include_context=True,
                include_derived_features=True
            )
        )

        # å¤„ç†ç»Ÿè®¡
        self.stats = {
            'total_matches': 0,
            'processed_matches': 0,
            'failed_matches': 0,
            'skipped_matches': 0,
            'start_time': None,
            'end_time': None,
            'error_details': []
        }

    async def extract(self) -> List[Dict[str, Any]]:
        """
        Extract: ä»æ•°æ®åº“æå–å®Œæ•´çš„æ¯”èµ›æ•°æ®

        Returns:
            æ¯”èµ›æ•°æ®åˆ—è¡¨
        """
        logger.info("ğŸ”„ Step 1: Extract - å¼€å§‹ä»æ•°æ®åº“æå–æ¯”èµ›æ•°æ®")

        # æ•°æ®åº“è¿æ¥é…ç½®
        database_url = os.getenv("ASYNC_DATABASE_URL", "postgresql+asyncpg://postgres:postgres@localhost:5432/football_prediction")

        engine = create_async_engine(database_url, echo=False)
        async_session = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

        try:
            async with async_session() as session:
                # æŸ¥è¯¢æ‰€æœ‰å®Œæ•´æ¯”èµ›
                query = """
                    SELECT COUNT(*) as total
                    FROM matches
                    WHERE status = 'FT'
                    AND stats_json IS NOT NULL
                    AND home_xg IS NOT NULL
                    AND away_xg IS NOT NULL
                    AND home_score IS NOT NULL
                    AND away_score IS NOT NULL
                """

                result = await session.execute(text(query))
                total_matches = result.scalar()
                self.stats['total_matches'] = total_matches

                logger.info(f"ğŸ“Š æ‰¾åˆ° {total_matches:,} åœºå®Œæ•´æ¯”èµ›")

                if total_matches == 0:
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ¯”èµ›æ•°æ®")
                    return []

                # åˆ†æ‰¹æå–æ•°æ®ï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰
                batch_size = 500
                all_matches = []

                for offset in range(0, total_matches, batch_size):
                    batch_query = """
                        SELECT *
                        FROM matches
                        WHERE status = 'FT'
                        AND stats_json IS NOT NULL
                        AND home_xg IS NOT NULL
                        AND away_xg IS NOT NULL
                        AND home_score IS NOT NULL
                        AND away_score IS NOT NULL
                        ORDER BY match_date DESC
                        LIMIT :batch_size OFFSET :offset
                    """

                    result = await session.execute(
                        text(batch_query),
                        {"batch_size": batch_size, "offset": offset}
                    )
                    batch_matches = result.fetchall()

                    # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                    batch_dicts = [dict(row._mapping) for row in batch_matches]
                    all_matches.extend(batch_dicts)

                    logger.info(f"ğŸ“¦ æå–æ‰¹æ¬¡ {offset//batch_size + 1}: {len(batch_dicts)} åœºæ¯”èµ›")

                logger.info(f"âœ… Extract å®Œæˆ: æ€»å…±æå– {len(all_matches):,} åœºæ¯”èµ›")
                return all_matches

        except Exception as e:
            logger.error(f"âŒ Extract å¤±è´¥: {e}")
            raise
        finally:
            await engine.dispose()

    async def transform(self, matches: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Transform: æ‰¹é‡ç‰¹å¾æå–

        Args:
            matches: åŸå§‹æ¯”èµ›æ•°æ®åˆ—è¡¨

        Returns:
            ç‰¹å¾æ•°æ®åˆ—è¡¨
        """
        logger.info("ğŸ”„ Step 2: Transform - å¼€å§‹æ‰¹é‡ç‰¹å¾æå–")

        features_list = []

        for i, match_data in enumerate(matches, 1):
            try:
                # ç‰¹å¾æå–
                features = self.feature_extractor.extract_features(match_data)

                if features:
                    # æ·»åŠ å¤„ç†å…ƒæ•°æ®
                    features['_processing_timestamp'] = datetime.now().isoformat()
                    features['_source_match_id'] = match_data.get('id')

                    features_list.append(features)
                    self.stats['processed_matches'] += 1

                    # è¿›åº¦æŠ¥å‘Š
                    if i % 100 == 0 or i == len(matches):
                        progress = (i / len(matches)) * 100
                        logger.info(f"âš¡ å¤„ç†è¿›åº¦: {i}/{len(matches)} ({progress:.1f}%)")
                else:
                    self.stats['failed_matches'] += 1
                    logger.warning(f"âš ï¸ ç‰¹å¾æå–å¤±è´¥: Match ID {match_data.get('id')}")

            except Exception as e:
                self.stats['failed_matches'] += 1
                error_detail = {
                    'match_id': match_data.get('id'),
                    'error': str(e),
                    'timestamp': datetime.now().isoformat()
                }
                self.stats['error_details'].append(error_detail)
                logger.warning(f"âš ï¸ å¤„ç†å¤±è´¥: Match ID {match_data.get('id')} - {e}")

        logger.info(f"âœ… Transform å®Œæˆ: æˆåŠŸ {self.stats['processed_matches']:,}, å¤±è´¥ {self.stats['failed_matches']:,}")
        return features_list

    def load(self, features_list: List[Dict[str, Any]], output_path: str) -> pd.DataFrame:
        """
        Load: ä¿å­˜ç‰¹å¾æ•°æ®ä¸ºCSVæ–‡ä»¶

        Args:
            features_list: ç‰¹å¾æ•°æ®åˆ—è¡¨
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            pandas DataFrame
        """
        logger.info("ğŸ”„ Step 3: Load - å¼€å§‹ä¿å­˜æ•°æ®é›†")

        try:
            # è½¬æ¢ä¸ºDataFrame
            df = pd.DataFrame(features_list)

            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜ä¸ºCSV
            df.to_csv(output_path, index=False, encoding='utf-8')

            # æ–‡ä»¶å¤§å°ä¿¡æ¯
            file_size = Path(output_path).stat().st_size / (1024 * 1024)  # MB

            logger.info(f"âœ… Load å®Œæˆ:")
            logger.info(f"   ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
            logger.info(f"   ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            logger.info(f"   ğŸ’¾ æ–‡ä»¶å¤§å°: {file_size:.2f} MB")
            logger.info(f"   ğŸ“‹ ç‰¹å¾æ•°é‡: {len(df.columns)}")

            return df

        except Exception as e:
            logger.error(f"âŒ Load å¤±è´¥: {e}")
            raise

    def generate_report(self, df: pd.DataFrame, output_path: str):
        """ç”Ÿæˆæ•°æ®é›†æŠ¥å‘Š"""
        logger.info("ğŸ“‹ ç”Ÿæˆæ•°æ®é›†è´¨é‡æŠ¥å‘Š")

        print("\n" + "="*80)
        print("ğŸ† FOOTBALL PREDICTION GOLDEN DATASET v1")
        print("="*80)

        # å¤„ç†ç»Ÿè®¡
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds()

        print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»æ¯”èµ›æ•°:     {self.stats['total_matches']:,}")
        print(f"   æˆåŠŸå¤„ç†:     {self.stats['processed_matches']:,}")
        print(f"   å¤„ç†å¤±è´¥:     {self.stats['failed_matches']:,}")
        print(f"   æˆåŠŸç‡:       {(self.stats['processed_matches']/self.stats['total_matches']*100):.1f}%")
        print(f"   å¤„ç†è€—æ—¶:     {duration:.1f} ç§’")
        print(f"   å¤„ç†é€Ÿåº¦:     {self.stats['processed_matches']/duration:.1f} åœº/ç§’")

        print(f"\nğŸ“‹ æ•°æ®é›†ä¿¡æ¯:")
        print(f"   è¾“å‡ºæ–‡ä»¶:     {output_path}")
        print(f"   æ•°æ®å½¢çŠ¶:     {df.shape}")
        print(f"   ç‰¹å¾æ•°é‡:     {len(df.columns)}")
        print(f"   å†…å­˜å ç”¨:     {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

        print(f"\nğŸ” æ•°æ®è´¨é‡:")

        # æŒ‰ç±»åˆ«ç»Ÿè®¡ç‰¹å¾
        feature_categories = {
            'å…ƒæ•°æ®ç‰¹å¾': ['match_id', 'year', 'month', 'day_of_week', 'day_of_year', 'is_weekend'],
            'ç›®æ ‡å˜é‡': ['home_score', 'away_score', 'result', 'result_numeric', 'has_winner', 'goal_difference', 'total_goals', 'over_2_5_goals'],
            'é«˜çº§ç»Ÿè®¡': ['home_xg', 'away_xg', 'xg_difference', 'total_xg', 'home_xg_vs_actual', 'away_xg_vs_actual', 'total_xg_vs_actual'],
            'åŸºç¡€ç»Ÿè®¡': [col for col in df.columns if any(keyword in col for keyword in ['possession', 'shots', 'corners', 'fouls', 'cards', 'passes', 'tackles'])],
            'ä¸Šä¸‹æ–‡ç‰¹å¾': [col for col in df.columns if any(keyword in col for keyword in ['referee', 'stadium', 'weather', 'odds'])],
            'è¡ç”Ÿç‰¹å¾': [col for col in df.columns if any(keyword in col for keyword in ['accuracy', 'overperformance', 'advantage', 'difference', 'ratio'])]
        }

        for category, features in feature_categories.items():
            existing_features = [f for f in features if f in df.columns]
            if existing_features:
                non_null_count = df[existing_features].notna().any(axis=1).sum()
                coverage = (non_null_count / len(df)) * 100
                print(f"   {category:12}: {len(existing_features):2} ä¸ªç‰¹å¾, è¦†ç›–ç‡: {coverage:5.1f}%")

        # æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
        print(f"\nğŸ“ˆ æ•°æ®å®Œæ•´æ€§:")
        null_counts = df.isnull().sum()
        important_features = ['home_xg', 'away_xg', 'home_score', 'away_score', 'result', 'home_possession', 'away_possession']

        for feature in important_features:
            if feature in df.columns:
                null_count = null_counts[feature]
                non_null_count = len(df) - null_count
                coverage = (non_null_count / len(df)) * 100
                print(f"   {feature:20}: {non_null_count:6,}/{len(df):6,} ({coverage:5.1f}%)")

        if self.stats['error_details']:
            print(f"\nâš ï¸ é”™è¯¯è¯¦æƒ… (å‰5ä¸ª):")
            for error in self.stats['error_details'][:5]:
                print(f"   Match {error['match_id']}: {error['error']}")

        print(f"\nğŸ¯ æ•°æ®é›†å°±ç»ªç”¨é€”:")
        print(f"   â€¢ æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ")
        print(f"   â€¢ ç‰¹å¾é‡è¦æ€§åˆ†æ")
        print(f"   â€¢ æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print(f"   â€¢ å®æ—¶é¢„æµ‹æœåŠ¡å¼€å‘")

        print("="*80)

    async def build_dataset(self, output_path: str = "data/processed/features_v1.csv"):
        """
        å®Œæ•´çš„ETLæµæ°´çº¿

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        logger.info("ğŸš€ å¼€å§‹æ„å»º Golden Dataset v1")
        self.stats['start_time'] = datetime.now()

        try:
            # Step 1: Extract
            matches = await self.extract()

            if not matches:
                logger.error("âŒ æ²¡æœ‰æ•°æ®å¯å¤„ç†")
                return None

            # Step 2: Transform
            features_list = await self.transform(matches)

            if not features_list:
                logger.error("âŒ æ²¡æœ‰æˆåŠŸæå–çš„ç‰¹å¾æ•°æ®")
                return None

            # Step 3: Load
            df = self.load(features_list, output_path)

            self.stats['end_time'] = datetime.now()

            # ç”ŸæˆæŠ¥å‘Š
            self.generate_report(df, output_path)

            logger.info("ğŸ‰ Golden Dataset v1 æ„å»ºå®Œæˆ!")
            return df

        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†æ„å»ºå¤±è´¥: {e}")
            raise


async def main():
    """ä¸»å‡½æ•°"""
    # æ„å»ºæ•°æ®é›†
    builder = DatasetBuilder()

    try:
        df = await builder.build_dataset()

        if df is not None:
            print(f"\nâœ… æ•°æ®é›†æ„å»ºæˆåŠŸ!")
            print(f"ğŸ“Š æ•°æ®é¢„è§ˆ:")
            print(df.head())
            print(f"\nğŸ“‹ æ•°æ®ä¿¡æ¯:")
            df.info()

    except KeyboardInterrupt:
        logger.info("âš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"âŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())