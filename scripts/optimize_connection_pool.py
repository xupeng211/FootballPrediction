#!/usr/bin/env python3
"""
æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–è„šæœ¬
Database Connection Pool Optimization Script

ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± é…ç½®ï¼Œæå‡æŸ¥è¯¢æ€§èƒ½å’Œå¹¶å‘å¤„ç†èƒ½åŠ›ã€‚
"""

import asyncio
import logging
import time
from typing import Any

from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

from src.core.config import get_settings

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ConnectionPoolOptimizer:
    """è¿æ¥æ± ä¼˜åŒ–å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–è¿æ¥æ± ä¼˜åŒ–å™¨"""
        self.settings = get_settings()
        self.engines = {}
        self.pool_configs = {}

    def generate_optimized_pool_configs(self) -> dict[str, dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–çš„è¿æ¥æ± é…ç½®"""
        logger.info("ğŸ”§ ç”Ÿæˆä¼˜åŒ–çš„è¿æ¥æ± é…ç½®...")

        configs = {
            'default': {
                'pool_size': 20,
                'max_overflow': 30,
                'pool_timeout': 30,
                'pool_recycle': 3600,
                'pool_pre_ping': True,
                'pool_reset_on_return': 'commit',
                'echo': False,
                'future': True,
                'description': 'é»˜è®¤è¿æ¥æ± é…ç½®'
            },
            'high_concurrency': {
                'pool_size': 50,
                'max_overflow': 100,
                'pool_timeout': 60,
                'pool_recycle': 1800,
                'pool_pre_ping': True,
                'pool_reset_on_return': 'commit',
                'echo': False,
                'future': True,
                'description': 'é«˜å¹¶å‘è¿æ¥æ± é…ç½®'
            },
            'low_memory': {
                'pool_size': 5,
                'max_overflow': 10,
                'pool_timeout': 30,
                'pool_recycle': 7200,
                'pool_pre_ping': True,
                'pool_reset_on_return': 'commit',
                'echo': False,
                'future': True,
                'description': 'ä½å†…å­˜è¿æ¥æ± é…ç½®'
            },
            'batch_processing': {
                'pool_size': 10,
                'max_overflow': 20,
                'pool_timeout': 120,
                'pool_recycle': 900,
                'pool_pre_ping': True,
                'pool_reset_on_return': 'commit',
                'echo': False,
                'future': True,
                'description': 'æ‰¹å¤„ç†è¿æ¥æ± é…ç½®'
            }
        }

        self.pool_configs = configs
        return configs

    def create_engine_with_config(self, config_name: str) -> Any:
        """æ ¹æ®é…ç½®åˆ›å»ºå¼•æ“"""
        if config_name not in self.pool_configs:
            raise ValueError(f"æœªçŸ¥çš„è¿æ¥æ± é…ç½®: {config_name}")

        config = self.pool_configs[config_name]

        engine = create_async_engine(
            self.settings.database_url,
            **{k: v for k, v in config.items() if k != 'description'}
        )

        self.engines[config_name] = engine
        logger.info(f"âœ… åˆ›å»ºå¼•æ“ {config_name}: {config['description']}")
        return engine

    async def test_connection_pool(self,
    config_name: str,
    concurrent_connections: int = 20) -> dict[str,
    Any]:
        """æµ‹è¯•è¿æ¥æ± æ€§èƒ½"""
        logger.info(f"ğŸ§ª æµ‹è¯•è¿æ¥æ± é…ç½®: {config_name}")

        if config_name not in self.engines:
            self.create_engine_with_config(config_name)

        engine = self.engines[config_name]
        results = {
            'config_name': config_name,
            'concurrent_connections': concurrent_connections,
            'total_time': 0,
            'successful_connections': 0,
            'failed_connections': 0,
            'average_response_time': 0,
            'max_response_time': 0,
            'min_response_time': float('inf'),
            'connection_errors': []
        }

        async def test_single_connection(conn_id: int) -> dict[str, Any]:
            """æµ‹è¯•å•ä¸ªè¿æ¥"""
            start_time = time.time()
            result = {'conn_id': conn_id, 'success': False, 'error': None, 'response_time': 0}

            try:
                async with AsyncSession(engine) as session:
                    # æ‰§è¡Œç®€å•æŸ¥è¯¢
                    await session.execute("SELECT 1")

                    response_time = time.time() - start_time
                    result.update({
                        'success': True,
                        'response_time': response_time
                    })

            except Exception as e:
                response_time = time.time() - start_time
                result.update({
                    'success': False,
                    'error': str(e),
                    'response_time': response_time
                })

            return result

        # å¹¶å‘æµ‹è¯•è¿æ¥
        start_time = time.time()
        tasks = [test_single_connection(i) for i in range(concurrent_connections)]
        test_results = await asyncio.gather(*tasks, return_exceptions=True)
        total_time = time.time() - start_time

        # ç»Ÿè®¡ç»“æœ
        successful_connections = 0
        failed_connections = 0
        response_times = []
        connection_errors = []

        for result in test_results:
            if isinstance(result, Exception):
                failed_connections += 1
                connection_errors.append(str(result))
            elif result['success']:
                successful_connections += 1
                response_times.append(result['response_time'])
            else:
                failed_connections += 1
                connection_errors.append(result['error'])

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if response_times:
            avg_response_time = sum(response_times) / len(response_times)
            max_response_time = max(response_times)
            min_response_time = min(response_times)
        else:
            avg_response_time = max_response_time = min_response_time = 0

        results.update({
            'total_time': total_time,
            'successful_connections': successful_connections,
            'failed_connections': failed_connections,
            'average_response_time': avg_response_time,
            'max_response_time': max_response_time,
            'min_response_time': min_response_time,
            'connection_errors': connection_errors[:5]  # åªä¿ç•™å‰5ä¸ªé”™è¯¯
        })

        logger.info(f"ğŸ“Š è¿æ¥æ±  {config_name} æµ‹è¯•ç»“æœ:")
        logger.info(f"  - æˆåŠŸè¿æ¥: {successful_connections}/{concurrent_connections}")
        logger.info(f"  - å¤±è´¥è¿æ¥: {failed_connections}")
        logger.info(f"  - å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}s")
        logger.info(f"  - æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.3f}s")

        return results

    async def compare_pool_configs(self) -> dict[str, Any]:
        """æ¯”è¾ƒä¸åŒè¿æ¥æ± é…ç½®çš„æ€§èƒ½"""
        logger.info("ğŸ“Š æ¯”è¾ƒä¸åŒè¿æ¥æ± é…ç½®çš„æ€§èƒ½...")

        configs_to_test = ['default', 'high_concurrency', 'low_memory', 'batch_processing']
        comparison_results = {}

        for config_name in configs_to_test:
            try:
                results = await self.test_connection_pool(config_name,
    concurrent_connections=20)
                comparison_results[config_name] = results

                # çŸ­æš‚ä¼‘æ¯ä»¥é¿å…è¿æ¥æ± å†²çª
                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•è¿æ¥æ± é…ç½® {config_name} å¤±è´¥: {e}")
                comparison_results[config_name] = {'error': str(e)}

        return comparison_results

    async def analyze_pool_usage(self) -> dict[str, Any]:
        """åˆ†æè¿æ¥æ± ä½¿ç”¨æƒ…å†µ"""
        logger.info("ğŸ” åˆ†æè¿æ¥æ± ä½¿ç”¨æƒ…å†µ...")

        pool_usage_data = {}

        for config_name, engine in self.engines.items():
            try:
                pool = engine.pool

                pool_info = {
                    'config_name': config_name,
                    'pool_size': pool.size(),
                    'checked_in': pool.checkedin(),
                    'checked_out': pool.checkedout(),
                    'overflow': pool.overflow(),
                    'invalid': pool.invalid(),
                    'total_connections': pool.checkedin() + pool.checkedout(),
                    'available_connections': pool.checkedin(),
                    'busy_connections': pool.checkedout(),
                    'usage_percentage': 0
                }

                total = pool_info['total_connections']
                if total > 0:
                    pool_info['usage_percentage'] = (pool_info['busy_connections'] / total) * 100

                pool_usage_data[config_name] = pool_info

                logger.info(f"ğŸ“Š è¿æ¥æ±  {config_name} ä½¿ç”¨æƒ…å†µ:")
                logger.info(f"  - æ€»è¿æ¥æ•°: {pool_info['total_connections']}")
                logger.info(f"  - å¯ç”¨è¿æ¥æ•°: {pool_info['available_connections']}")
                logger.info(f"  - å¿™ç¢Œè¿æ¥æ•°: {pool_info['busy_connections']}")
                logger.info(f"  - ä½¿ç”¨ç‡: {pool_info['usage_percentage']:.1f}%")

            except Exception as e:
                logger.error(f"âŒ åˆ†æè¿æ¥æ±  {config_name} å¤±è´¥: {e}")
                pool_usage_data[config_name] = {'error': str(e)}

        return pool_usage_data

    async def generate_optimization_recommendations(self,
    comparison_results: dict[str,
    Any]) -> list[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        logger.info("ğŸ’¡ ç”Ÿæˆè¿æ¥æ± ä¼˜åŒ–å»ºè®®...")

        recommendations = []

        # åˆ†ææµ‹è¯•ç»“æœ
        successful_configs = {
            name: results for name, results in comparison_results.items()
            if 'error' not in results and results['successful_connections'] > 0
        }

        if not successful_configs:
            return ["âŒ æ‰€æœ‰è¿æ¥æ± é…ç½®æµ‹è¯•éƒ½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®åº“è¿æ¥"]

        # æ‰¾å‡ºæœ€ä½³é…ç½®
        best_config = min(successful_configs.items(),
    key=lambda x: x[1]['average_response_time'])
        best_config_name, best_config_results = best_config

        recommendations.append(f"ğŸ¯ æ¨èä½¿ç”¨è¿æ¥æ± é…ç½®: {best_config_name}")
        recommendations.append(f"ğŸ“Š å¹³å‡å“åº”æ—¶é—´: {best_config_results['average_response_time']:.3f}s")
        recommendations.append(f"âœ… æˆåŠŸç‡: {(best_config_results['successful_connections']/best_config_results['concurrent_connections'])*100:.1f}%")

        # åˆ†æä¸åŒä½¿ç”¨åœºæ™¯çš„æ¨è
        if 'high_concurrency' in successful_configs:
            high_concurrency = successful_configs['high_concurrency']
            if high_concurrency['successful_connections'] >= high_concurrency['concurrent_connections'] * 0.9:
                recommendations.append("ğŸš€ å¯¹äºé«˜å¹¶å‘åœºæ™¯ï¼Œæ¨èä½¿ç”¨ high_concurrency é…ç½®")

        if 'low_memory' in successful_configs:
            low_memory = successful_configs['low_memory']
            if low_memory['successful_connections'] >= low_memory['concurrent_connections'] * 0.8:
                recommendations.append("ğŸ’¾ å¯¹äºå†…å­˜å—é™ç¯å¢ƒï¼Œæ¨èä½¿ç”¨ low_memory é…ç½®")

        # é€šç”¨ä¼˜åŒ–å»ºè®®
        recommendations.extend([
            "ğŸ”§ æ ¹æ®å®é™…è´Ÿè½½è°ƒæ•´ pool_size å’Œ max_overflow",
            "â° è®¾ç½®åˆé€‚çš„ pool_timeout ä»¥é¿å…è¶…æ—¶",
            "ğŸ”„ å®šæœŸå›æ”¶è¿æ¥ (pool_recycle) é¿å…è¿æ¥è¿‡æœŸ",
            "âœ… å¯ç”¨ pool_pre_ping ç¡®ä¿è¿æ¥æœ‰æ•ˆæ€§",
            "ğŸ“Š ç›‘æ§è¿æ¥æ± ä½¿ç”¨æƒ…å†µï¼Œé€‚æ—¶è°ƒæ•´é…ç½®"
        ])

        return recommendations

    async def run_optimization_analysis(self) -> dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„è¿æ¥æ± ä¼˜åŒ–åˆ†æ"""
        logger.info("ğŸš€ å¼€å§‹è¿æ¥æ± ä¼˜åŒ–åˆ†æ...")

        start_time = time.time()

        try:
            # ç”Ÿæˆé…ç½®
            configs = self.generate_optimized_pool_configs()

            # æ¯”è¾ƒä¸åŒé…ç½®çš„æ€§èƒ½
            comparison_results = await self.compare_pool_configs()

            # åˆ†æè¿æ¥æ± ä½¿ç”¨æƒ…å†µ
            pool_usage = await self.analyze_pool_usage()

            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            recommendations = await self.generate_optimization_recommendations(comparison_results)

            analysis_time = time.time() - start_time

            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = {
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'analysis_time': analysis_time,
                'pool_configs': configs,
                'performance_comparison': comparison_results,
                'pool_usage_analysis': pool_usage,
                'recommendations': recommendations,
                'summary': {
                    'total_configs_tested': len(comparison_results),
                    'successful_configs': len([r for r in comparison_results.values() if 'error' not in r]),


                    'best_config': max(
                        [(name,
    results) for name,
    results in comparison_results.items() if 'error' not in results],

                        key=lambda x: x[1]['successful_connections']
                    )[0] if comparison_results else None
                }
            }

            logger.info(f"âœ… è¿æ¥æ± ä¼˜åŒ–åˆ†æå®Œæˆï¼Œè€—æ—¶: {analysis_time:.2f}s")

            return report

        except Exception as e:
            logger.error(f"âŒ è¿æ¥æ± ä¼˜åŒ–åˆ†æå¤±è´¥: {e}")
            raise

    async def close(self):
        """å…³é—­æ‰€æœ‰å¼•æ“"""
        for config_name, engine in self.engines.items():
            try:
                await engine.dispose()
                logger.info(f"âœ… å¼•æ“ {config_name} å·²å…³é—­")
            except Exception as e:
                logger.error(f"âŒ å…³é—­å¼•æ“ {config_name} å¤±è´¥: {e}")


async def main():
    """ä¸»å‡½æ•°"""
    optimizer = ConnectionPoolOptimizer()

    try:
        # è¿è¡Œä¼˜åŒ–åˆ†æ
        report = await optimizer.run_optimization_analysis()

        # ä¿å­˜åˆ†ææŠ¥å‘Š
        import json
        with open('connection_pool_optimization_report.json',
    'w',
    encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        logger.info("ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ° connection_pool_optimization_report.json")

        # è¾“å‡ºæ‘˜è¦
        if report['summary']['best_config']:
            report['summary']['best_config'][0]

        # è¾“å‡ºä¼˜åŒ–å»ºè®®
        for _i, _rec in enumerate(report['recommendations'], 1):
            pass

    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–åˆ†æè¿‡ç¨‹å¤±è´¥: {e}")
        raise
    finally:
        await optimizer.close()


if __name__ == "__main__":
    asyncio.run(main())
