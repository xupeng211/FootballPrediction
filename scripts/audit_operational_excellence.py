#!/usr/bin/env python3
"""
MLOpsé¦–å¸­è¿ç»´å·¥ç¨‹å¸ˆ - è¿ç»´ä¼˜é›…åº¦å®¡è®¡è„šæœ¬
æ‰§è¡Œæœ€ç»ˆéªŒæ”¶ï¼Œè¯„ä¼°æ•°æ®å·¥å‚çš„"å¥å£®æ€§"å’Œ"ä¼˜é›…åº¦"

SRE Lead: éªŒè¯åŒè¿›ç¨‹åè°ƒç³»ç»Ÿçš„ç”Ÿäº§å°±ç»ªæ€§
Purpose: ç³»ç»Ÿå¥åº·æ£€æŸ¥ã€æ€§èƒ½è¯„ä¼°å’Œè´¨é‡ç›‘æ§
"""

import json
import logging
import time
import subprocess
import re
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

import psycopg2

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class OperationalExcellenceAuditor:
    """è¿ç»´ä¼˜é›…åº¦å®¡è®¡å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–å®¡è®¡å™¨"""
        self.report = {
            'audit_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'heartbeat_check': {},
            'throughput_check': {},
            'robustness_metrics': {},
            'quality_metrics': {},
            'final_score': {}
        }

    def execute_full_audit(self) -> Dict[str, any]:
        """æ‰§è¡Œå®Œæ•´å®¡è®¡"""
        logger.info("ğŸš€ MLOpsé¦–å¸­è¿ç»´å·¥ç¨‹å¸ˆ - å¼€å§‹è¿ç»´ä¼˜é›…åº¦å®¡è®¡")
        logger.info("=" * 60)

        try:
            # Step 1: å¿ƒè·³æ£€æŸ¥ (Health & Uptime)
            logger.info("ğŸ“‹ STEP 1: å¿ƒè·³æ£€æŸ¥ (Health & Uptime)")
            self.heartbeat_check = self._check_heartbeat()
            time.sleep(1)

            # Step 2: æ•°æ®ååé‡æ£€æŸ¥ (Throughput Check)
            logger.info("ğŸ“ˆ STEP 2: æ•°æ®ååé‡æ£€æŸ¥ (Throughput Check)")
            self.throughput_check = self._check_throughput()
            time.sleep(1)

            # Step 3: å¥å£®æ€§æŒ‡æ ‡ (Robustness Metrics)
            logger.info("ğŸ›¡ï¸ STEP 3: å¥å£®æ€§æŒ‡æ ‡ (Robustness Metrics)")
            self.robustness_metrics = self._check_robustness()
            time.sleep(1)

            # Step 4: æ•°æ®è´¨é‡åº¦é‡ (Quality Metrics)
            logger.info("ğŸ¯ STEP 4: æ•°æ®è´¨é‡åº¦é‡ (Quality Metrics)")
            self.quality_metrics = self._check_quality()

            # Step 5: ç”Ÿæˆæœ€ç»ˆè¯„åˆ†
            logger.info("ğŸ† STEP 5: ç”Ÿæˆæœ€ç»ˆè¯„åˆ†")
            self.final_score = self._calculate_final_score()

            logger.info("âœ… å®¡è®¡å®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
            return self.report

        except Exception as e:
            logger.error(f"âŒ å®¡è®¡å¤±è´¥: {e}")
            return self.report

    def _check_heartbeat(self) -> Dict[str, any]:
        """å¿ƒè·³æ£€æŸ¥"""
        results = {}

        try:
            # æ£€æŸ¥L1è¿›ç¨‹
            l1_pid = self._get_process_pid("launch_robust_coverage.py")
            results['l1_process'] = {
                'status': 'running' if l1_pid else 'stopped',
                'pid': l1_pid,
                'name': 'L1é‡‡é›†å™¨ (å…¨åŸŸåŸºç¡€æ•°æ®)'
            }

            # æ£€æŸ¥L2è¿›ç¨‹
            l2_pid = self._get_process_pid("backfill_details.py")
            results['l2_process'] = {
                'status': 'running' if l2_pid else 'stopped',
                'pid': l2_pid,
                'name': 'L2é‡‡é›†å™¨ (æ·±åº¦æ•°æ®è¡¥å…¨)'
            }

            # æ£€æŸ¥CrontabæœåŠ¡
            try:
                cron_result = subprocess.run(['systemctl', 'is-active', 'cron'],
                                           capture_output=True, text=True)
                results['cron_service'] = {
                    'status': 'active' if cron_result.returncode == 0 else 'inactive',
                    'output': cron_result.stdout.strip()
                }
            except:
                results['cron_service'] = {'status': 'unknown', 'output': 'æ— æ³•æ£€æŸ¥'}

            # è®¡ç®—ç³»ç»Ÿå¯åŠ¨æ—¶é—´
            boot_time = self._get_system_uptime()
            results['system_uptime'] = f"{boot_time:.1f} å°æ—¶"

            logger.info(f"âœ… å¿ƒè·³æ£€æŸ¥å®Œæˆ: L1={results['l1_process']['status']}, L2={results['l2_process']['status']}")
            return results

        except Exception as e:
            logger.error(f"âŒ å¿ƒè·³æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}

    def _get_process_pid(self, process_name: str) -> Optional[int]:
        """è·å–è¿›ç¨‹PID"""
        try:
            result = subprocess.run(['pgrep', '-f', process_name],
                                      capture_output=True, text=True)
            if result.returncode == 0 and result.stdout.strip():
                lines = result.stdout.strip().split('\n')
                if lines:
                    return int(lines[0].split()[0])
        except:
            pass
        return None

    def _get_system_uptime(self) -> float:
        """è·å–ç³»ç»Ÿè¿è¡Œæ—¶é—´ï¼ˆå°æ—¶ï¼‰"""
        try:
            with open('/proc/uptime', 'r') as f:
                uptime_seconds = float(f.read().split()[0])
                return uptime_seconds / 3600.0
        except:
            return 0.0

    def _check_throughput(self) -> Dict[str, any]:
        """æ•°æ®ååé‡æ£€æŸ¥"""
        results = {}

        try:
            conn = psycopg2.connect(
                host='localhost',
                port=5432,
                user='postgres',
                password='postgres-dev-password',
                database='football_prediction'
            )

            with conn.cursor() as cur:
                # L1é€Ÿç‡ï¼šè¿‡å»60åˆ†é’Ÿæ–°å¢çš„åŸºç¡€è®°å½•
                cur.execute("""
                    SELECT COUNT(*)
                    FROM matches
                    WHERE data_source = 'fbref'
                    AND created_at > NOW() - INTERVAL '60 minutes'
                """)
                l1_new_records = cur.fetchone()[0]

                # L2è½¬æ¢é€Ÿç‡ï¼šè¿‡å»60åˆ†é’Ÿå‡çº§çš„è®°å½•
                cur.execute("""
                    SELECT COUNT(*)
                    FROM matches
                    WHERE data_source = 'fbref'
                    AND data_completeness = 'complete'
                    AND updated_at > NOW() - INTERVAL '60 minutes'
                """)
                l2_upgraded_records = cur.fetchone()[0]

                # æ€»ä½“æ•°æ®é‡ç»Ÿè®¡
                cur.execute("SELECT COUNT(*) FROM matches WHERE data_source = 'fbref'")
                total_records = cur.fetchone()[0]

                # æ•°æ®å®Œæ•´æ€§ç»Ÿè®¡
                cur.execute("""
                    SELECT
                        COUNT(*) as total,
                        COUNT(CASE WHEN stats IS NOT NULL AND stats != '{}' THEN 1 END) as with_stats,
                        COUNT(CASE WHEN data_completeness = 'complete' THEN 1 END) as complete,
                        COUNT(CASE WHEN data_completeness = 'partial' THEN 1 END) as partial
                    FROM matches
                    WHERE data_source = 'fbref'
                """)
                completeness_stats = cur.fetchone()

                results['l1_throughput'] = {
                    'records_per_hour': l1_new_records,
                    'target': 100,
                    'score': min(100, (l1_new_records / 100) * 100)
                }

                results['l2_throughput'] = {
                    'upgraded_per_hour': l2_upgraded_records,
                    'target': 50,
                    'score': min(100, (l2_upgraded_records / 50) * 100)
                }

                results['data_summary'] = {
                    'total_records': total_records,
                    'with_stats': completeness_stats[1],
                    'complete_records': completeness_stats[2],
                    'partial_records': completeness_stats[3],
                    'completion_rate': (completeness_stats[2] / completeness_stats[0] * 100) if completeness_stats[0] > 0 else 0
                }

            conn.close()

            logger.info(f"âœ… ååé‡æ£€æŸ¥: L1={l1_new_records}/h, L2={l2_upgraded_records}/h")
            return results

        except Exception as e:
            logger.error(f"âŒ ååé‡æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}

    def _check_robustness(self) -> Dict[str, any]:
        """å¥å£®æ€§æŒ‡æ ‡æ£€æŸ¥"""
        results = {}

        try:
            # æ£€æŸ¥åçˆ¬è™«æ—¥å¿—
            l1_log_path = 'logs/skynet_live_run.log'
            if Path(l1_log_path).exists():
                anti_crawl_stats = self._analyze_anti_crawl_metrics(l1_log_path)
                results['anti_crawl_metrics'] = anti_crawl_stats
            else:
                results['anti_crawl_metrics'] = {'error': 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨'}

            # æ£€æŸ¥ç³»ç»Ÿèµ„æº
            cpu_usage = self._get_cpu_usage()
            memory_usage = self._get_memory_usage()

            results['system_resources'] = {
                'cpu_usage_percent': cpu_usage,
                'memory_usage_mb': memory_usage,
                'status': 'healthy' if cpu_usage < 80 and memory_usage < 2048 else 'warning'
            }

            # æ£€æŸ¥é”™è¯¯ç‡
            error_rate = self._calculate_error_rate()
            results['error_rate'] = {
                'error_rate_percent': error_rate,
                'status': 'good' if error_rate < 5 else 'needs_attention'
            }

            logger.info(f"âœ… å¥å£®æ€§æ£€æŸ¥: åçˆ¬è™«æœºåˆ¶æ­£å¸¸ï¼Œç³»ç»Ÿèµ„æºå……è¶³")
            return results

        except Exception as e:
            logger.error(f"âŒ å¥å£®æ€§æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}

    def _analyze_anti_crawl_metrics(self, log_path: str) -> Dict[str, any]:
        """åˆ†æåçˆ¬è™«æŒ‡æ ‡"""
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                log_content = f.read()

            # ç»Ÿè®¡403é”™è¯¯
            pattern_403 = r'403.*?Forbidden'
            matches_403 = len(re.findall(pattern_403, log_content, re.IGNORECASE))

            # ç»Ÿè®¡ç­‰å¾…å»¶è¿Ÿ
            pattern_wait = r'Waiting\s+\d+\s+seconds'
            matches_wait = len(re.findall(pattern_wait, log_content, re.IGNORECASE))

            # ç»Ÿè®¡è¯·æ±‚æ€»æ•°ï¼ˆä¼°ç®—ï¼‰
            pattern_request = r'HTTPçŠ¶æ€:\s*\d+'
            total_requests = len(re.findall(pattern_request, log_content, re.IGNORECASE))

            # è®¡ç®—æˆåŠŸç‡
            success_rate = ((total_requests - matches_403) / total_requests * 100) if total_requests > 0 else 100

            return {
                'total_requests': total_requests,
                '403_errors': matches_403,
                'wait_events': matches_wait,
                'success_rate': success_rate,
                '403_rate': (matches_403 / total_requests * 100) if total_requests > 0 else 0,
                'status': 'excellent' if matches_403 / total_requests < 0.05 else 'needs_optimization'
            }

        except Exception as e:
            return {'error': f'æ—¥å¿—åˆ†æå¤±è´¥: {e}'}

    def _get_cpu_usage(self) -> float:
        """è·å–CPUä½¿ç”¨ç‡"""
        try:
            result = subprocess.run(['top', '-bn1', '-p', 'pgrep', '-P', 'launch_robust_coverage|backfill_details'],
                                      capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                for line in lines:
                    if line.strip() and 'python' in line:
                        # æå–CPUä½¿ç”¨ç‡ï¼ˆé€šå¸¸æ˜¯ç¬¬9åˆ—ï¼‰
                        parts = line.split()
                        if len(parts) >= 9:
                            try:
                                return float(parts[8])
                            except ValueError:
                                continue
        except:
            pass
        return 0.0

    def _get_memory_usage(self) -> int:
        """è·å–å†…å­˜ä½¿ç”¨é‡ï¼ˆMBï¼‰"""
        try:
            result = subprocess.run(['ps', '--no-headers', '-o', 'rss,comm', '-C', 'python', '-p', 'pgrep', '-f', 'launch_robust_coverage|backfill_details'],
                                      capture_output=True, text=True)
            if result.returncode == 0:
                total_memory = 0
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        try:
                            memory_kb = int(line.split()[0])
                            total_memory += memory_kb
                        except (ValueError, IndexError):
                            continue
                return total_memory // 1024  # è½¬æ¢ä¸ºMB
        except:
            pass
        return 0

    def _calculate_error_rate(self) -> float:
        """è®¡ç®—é”™è¯¯ç‡"""
        try:
            with open('logs/skynet_live_run.log', 'r', encoding='utf-8') as f:
                log_content = f.read()

            error_keywords = ['ERROR', 'EXCEPTION', 'FAILED']
            error_count = sum(1 for keyword in error_keywords if keyword.lower() in log_content.lower())

            total_lines = len(log_content.split('\n'))
            return (error_count / total_lines * 100) if total_lines > 0 else 0

        except:
            return 0.0

    def _check_quality(self) -> Dict[str, any]:
        """æ•°æ®è´¨é‡åº¦é‡"""
        results = {}

        try:
            conn = psycopg2.connect(
                host='localhost',
                port=5432,
                user='postgres',
                password='postgres-dev-password',
                database='football_prediction'
            )

            with conn.cursor() as cur:
                # éšæœºæŠ½å–5æ¡completeè®°å½•è¿›è¡Œè´¨é‡æ£€æŸ¥
                cur.execute("""
                    SELECT
                        id, stats, match_date, home_score, away_score
                    FROM matches
                    WHERE data_source = 'fbref'
                    AND data_completeness = 'complete'
                    ORDER BY updated_at DESC
                    LIMIT 5
                """)

                complete_records = cur.fetchall()

                quality_scores = []
                missing_fields = []

                for record_id, stats_json, match_date, home_score, away_score in complete_records:
                    record_score = 100
                    record_missing = []

                    if stats_json and stats_json != '{}':
                        try:
                            stats_data = json.loads(stats_json) if isinstance(stats_json, str) else stats_json

                            # æ£€æŸ¥å…³é”®å­—æ®µ
                            if 'xg_home' not in stats_data:
                                record_missing.append('xg_home')
                                record_score -= 25
                            if 'possession_home' not in stats_data:
                                record_missing.append('possession_home')
                                record_score -= 25

                            # æ£€æŸ¥åŸºç¡€æ•°æ®å®Œæ•´æ€§
                            if home_score is None or away_score is None:
                                record_missing.append('basic_score')
                                record_score -= 25

                            if match_date is None:
                                record_missing.append('match_date')
                                record_score -= 25

                        except (json.JSONDecodeError, TypeError):
                            record_score = 0
                            record_missing = ['JSONè§£æå¤±è´¥']
                    else:
                        record_score = 0
                        record_missing = ['Statså­—æ®µä¸ºç©º']

                    quality_scores.append(record_score)
                    missing_fields.extend(record_missing)

                # è®¡ç®—å¹³å‡è´¨é‡åˆ†æ•°
                avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0

                results['sample_quality'] = {
                    'sample_size': len(complete_records),
                    'scores': quality_scores,
                    'average_score': avg_quality,
                    'missing_fields_summary': list(set(missing_fields))
                }

                # æ•´ä½“æ•°æ®è´¨é‡è¯„ä¼°
                cur.execute("""
                    SELECT
                        COUNT(*) as total_complete,
                        COUNT(CASE WHEN stats::text LIKE '%xg_home%' THEN 1 END) as with_xg,
                        COUNT(CASE WHEN stats::text LIKE '%possession_home%' THEN 1 END) as with_possession
                    FROM matches
                    WHERE data_source = 'fbref'
                    AND data_completeness = 'complete'
                """)

                quality_stats = cur.fetchone()

                results['overall_quality'] = {
                    'total_complete_records': quality_stats[0],
                    'with_xg_percentage': (quality_stats[1] / quality_stats[0] * 100) if quality_stats[0] > 0 else 0,
                    'with_possession_percentage': (quality_stats[2] / quality_stats[0] * 100) if quality_stats[0] > 0 else 0,
                    'data_quality_grade': self._get_quality_grade(avg_quality)
                }

            conn.close()

            logger.info(f"âœ… è´¨é‡æ£€æŸ¥å®Œæˆ: å¹³å‡åˆ†æ•°={avg_quality:.1f}")
            return results

        except Exception as e:
            logger.error(f"âŒ è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return {'error': str(e)}

    def _get_quality_grade(self, score: float) -> str:
        """è·å–è´¨é‡ç­‰çº§"""
        if score >= 90:
            return 'A+ (ä¼˜ç§€)'
        elif score >= 80:
            return 'A (è‰¯å¥½)'
        elif score >= 70:
            return 'B (åˆæ ¼)'
        elif score >= 60:
            return 'C (éœ€è¦æ”¹è¿›)'
        else:
            return 'D (ä¸åˆæ ¼)'

    def _calculate_final_score(self) -> Dict[str, any]:
        """è®¡ç®—æœ€ç»ˆè¯„åˆ†"""
        scores = []

        # å¿ƒè·³è¯„åˆ† (40%)
        heartbeat_score = 0
        if self.heartbeat_check.get('l1_process', {}).get('status') == 'running':
            heartbeat_score += 20
        if self.heartbeat_check.get('l2_process', {}).get('status') == 'running':
            heartbeat_score += 20
        scores.append(heartbeat_score)

        # ååé‡è¯„åˆ† (30%)
        throughput_score = 0
        if self.throughput_check.get('l1_throughput', {}).get('score', 0) >= 50:
            throughput_score += 15
        if self.throughput_check.get('l2_throughput', {}).get('score', 0) >= 50:
            throughput_score += 15
        scores.append(throughput_score)

        # å¥å£®æ€§è¯„åˆ† (20%)
        robustness_score = 0
        if self.robustness_metrics.get('system_resources', {}).get('status') == 'healthy':
            robustness_score += 10
        if self.robustness_metrics.get('anti_crawl_metrics', {}).get('status') == 'excellent':
            robustness_score += 10
        scores.append(robustness_score)

        # è´¨é‡è¯„åˆ† (10%)
        quality_score = 0
        if self.quality_metrics.get('overall_quality', {}).get('data_quality_grade') in ['A+', 'A']:
            quality_score = 10
        elif self.quality_metrics.get('overall_quality', {}).get('data_quality_grade') == 'B':
            quality_score = 7
        scores.append(quality_score)

        total_score = sum(scores)
        grade = self._get_overall_grade(total_score)

        return {
            'total_score': total_score,
            'max_score': 100,
            'grade': grade,
            'component_scores': {
                'heartbeat': f'{heartbeat_score}/40',
                'throughput': f'{throughput_score}/30',
                'robustness': f'{robustness_score}/20',
                'quality': f'{quality_score}/10'
            }
        }

    def _get_overall_grade(self, score: float) -> str:
        """è·å–æ€»ä½“ç­‰çº§"""
        if score >= 90:
            return 'A+ (å“è¶Š)'
        elif score >= 80:
            return 'A (ä¼˜ç§€)'
        elif score >= 70:
            return 'B (è‰¯å¥½)'
        elif score >= 60:
            return 'C (åˆæ ¼)'
        else:
            return 'D (éœ€è¦æ”¹è¿›)'

    def generate_report(self) -> str:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report = []

        report.append("# MLOpsé¦–å¸­è¿ç»´å·¥ç¨‹å¸ˆ - è¿ç»´ä¼˜é›…åº¦å®¡è®¡æŠ¥å‘Š")
        report.append(f"")
        report.append(f"**å®¡è®¡æ—¶é—´**: {self.report['audit_time']}")
        report.append(f"")
        report.append("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦")
        report.append("")
        report.append(f"- **æ€»åˆ†**: {self.final_score.get('total_score', 0)}/100")
        report.append(f"- **ç­‰çº§**: {self.final_score.get('grade', 'Unknown')}")
        report.append(f"- **çŠ¶æ€**: {'âœ… ä¼˜é›…' if self.final_score.get('total_score', 0) >= 80 else 'âš ï¸ éœ€è¦ä¼˜åŒ–'}")
        report.append("")

        # è¯¦ç»†ç»“æœ
        report.append("## ğŸ” è¯¦ç»†å®¡è®¡ç»“æœ")
        report.append("")

        # å¿ƒè·³æ£€æŸ¥
        report.append("### 1. å¿ƒè·³æ£€æŸ¥ (Health & Uptime)")
        report.append("")
        heartbeat = self.report['heartbeat_check']
        if 'error' not in heartbeat:
            report.append(f"- **L1é‡‡é›†å™¨**: {heartbeat.get('l1_process', {}).get('status', 'Unknown')} (PID: {heartbeat.get('l1_process', {}).get('pid', 'N/A')})")
            report.append(f"- **L2é‡‡é›†å™¨**: {heartbeat.get('l2_process', {}).get('status', 'Unknown')} (PID: {heartbeat.get('l2_process', {}).get('pid', 'N/A')})")
            report.append(f"- **CronæœåŠ¡**: {heartbeat.get('cron_service', {}).get('status', 'Unknown')}")
            report.append(f"- **ç³»ç»Ÿè¿è¡Œæ—¶é—´**: {heartbeat.get('system_uptime', 'N/A')}")
        report.append("")

        # ååé‡æ£€æŸ¥
        report.append("### 2. æ•°æ®ååé‡æ£€æŸ¥ (Throughput Check)")
        report.append("")
        throughput = self.report['throughput_check']
        if 'error' not in throughput:
            l1 = throughput.get('l1_throughput', {})
            l2 = throughput.get('l2_throughput', {})
            summary = throughput.get('data_summary', {})

            report.append(f"- **L1é‡‡é›†é€Ÿç‡**: {l1.get('records_per_hour', 0)} æ¡/å°æ—¶ (ç›®æ ‡: {l1.get('target', 0)}, è¯„åˆ†: {l1.get('score', 0)})")
            report.append(f"- **L2è½¬æ¢é€Ÿç‡**: {l2.get('upgraded_per_hour', 0)} æ¡/å°æ—¶ (ç›®æ ‡: {l2.get('target', 0)}, è¯„åˆ†: {l2.get('score', 0)})")
            report.append(f"- **æ•°æ®æ€»é‡**: {summary.get('total_records', 0):,} æ¡è®°å½•")
            report.append(f"- **å®Œæ•´ç‡**: {summary.get('completion_rate', 0):.1f}%")
        report.append("")

        # å¥å£®æ€§æŒ‡æ ‡
        report.append("### 3. å¥å£®æ€§æŒ‡æ ‡ (Robustness Metrics)")
        report.append("")
        robustness = self.report['robustness_metrics']
        if 'error' not in robustness:
            anti_crawl = robustness.get('anti_crawl_metrics', {})
            resources = robustness.get('system_resources', {})

            report.append(f"- **403é­é‡ç‡**: {anti_crawl.get('403_rate', 0):.2f}%")
            report.append(f"- **æˆåŠŸç‡**: {anti_crawl.get('success_rate', 0):.1f}%")
            report.append(f"- **ç­‰å¾…äº‹ä»¶**: {anti_crawl.get('wait_events', 0)} æ¬¡")
            report.append(f"- **CPUä½¿ç”¨ç‡**: {resources.get('cpu_usage_percent', 0):.1f}%")
            report.append(f"- **å†…å­˜ä½¿ç”¨**: {resources.get('memory_usage_mb', 0):,} MB")
            report.append(f"- **ç³»ç»ŸçŠ¶æ€**: {resources.get('status', 'Unknown')}")
        report.append("")

        # è´¨é‡åº¦é‡
        report.append("### 4. æ•°æ®è´¨é‡åº¦é‡ (Quality Metrics)")
        report.append("")
        quality = self.report['quality_metrics']
        if 'error' not in quality:
            overall = quality.get('overall_quality', {})
            sample = quality.get('sample_quality', {})

            report.append(f"- **æŠ½æ ·è´¨é‡**: å¹³å‡åˆ†æ•° {sample.get('average_score', 0):.1f}")
            report.append(f"- **xGè¦†ç›–ç‡**: {overall.get('with_xg_percentage', 0):.1f}%")
            report.append(f"- **æ§çƒç‡è¦†ç›–ç‡**: {overall.get('with_possession_percentage', 0):.1f}%")
            report.append(f"- **æ•°æ®è´¨é‡ç­‰çº§**: {overall.get('data_quality_grade', 'Unknown')}")
        report.append("")

        # é£é™©ç‚¹åˆ†æ
        report.append("## âš ï¸ é£é™©ç‚¹åˆ†æ")
        report.append("")
        risks = []

        if self.final_score.get('total_score', 0) < 80:
            risks.append("ğŸ”´ æ€»åˆ†ä½äº80åˆ†ï¼Œéœ€è¦ä¼˜åŒ–")

        if self.robustness_metrics.get('anti_crawl_metrics', {}).get('403_rate', 0) > 5:
            risks.append("ğŸŸ¡ 403é”™è¯¯ç‡åé«˜ï¼Œå»ºè®®å¢åŠ å»¶è¿Ÿæ—¶é—´")

        if self.throughput_check.get('l1_throughput', {}).get('score', 0) < 50:
            risks.append("ğŸŸ¡ L1é‡‡é›†é€Ÿç‡åä½ï¼Œéœ€è¦ä¼˜åŒ–")

        if self.quality_metrics.get('overall_quality', {}).get('data_quality_grade') in ['C', 'D']:
            risks.append("ğŸŸ¡ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›")

        if risks:
            for risk in risks:
                report.append(f"- {risk}")
        else:
            report.append("- âœ… æœªå‘ç°é‡å¤§é£é™©ç‚¹ï¼Œç³»ç»Ÿè¿è¡Œä¼˜é›…")

        report.append("")

        return '\n'.join(report)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ MLOpsé¦–å¸­è¿ç»´å·¥ç¨‹å¸ˆ - å¯åŠ¨è¿ç»´ä¼˜é›…åº¦å®¡è®¡")
    print("=" * 60)

    auditor = OperationalExcellenceAuditor()

    try:
        # æ‰§è¡Œå®¡è®¡
        report_data = auditor.execute_full_audit()

        # ç”ŸæˆæŠ¥å‘Š
        report = auditor.generate_report()

        # è¾“å‡ºæŠ¥å‘Š
        print(report)

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = f"reports/operational_excellence_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        # ç¡®ä¿æŠ¥å‘Šç›®å½•å­˜åœ¨
        Path("reports").mkdir(exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        print("ğŸ‰ è¿ç»´ä¼˜é›…åº¦å®¡è®¡å®Œæˆï¼")

        return 0

    except Exception as e:
        print(f"âŒ å®¡è®¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())