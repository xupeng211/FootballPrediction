#!/usr/bin/env python3
"""
ä¼ä¸šçº§æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“ v2.0
åŸºäºIssue #159 70.1%è¦†ç›–ç‡æˆæœæ„å»ºAIé©±åŠ¨çš„è´¨é‡åˆ†æç³»ç»Ÿ
å®ç°è‡ªåŠ¨åŒ–ç¼ºé™·æ£€æµ‹ã€è´¨é‡è¯„ä¼°å’Œæ”¹è¿›å»ºè®®ç”Ÿæˆ
"""

import ast
import time
import json
import os
import re
import threading
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import concurrent.futures
from functools import lru_cache
import hashlib

class QualitySeverity(Enum):
    """è´¨é‡é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "CRITICAL"    # ä¸¥é‡é—®é¢˜ï¼Œå¿…é¡»ç«‹å³ä¿®å¤
    HIGH = "HIGH"           # é«˜ä¼˜å…ˆçº§é—®é¢˜
    MEDIUM = "MEDIUM"       # ä¸­ç­‰ä¼˜å…ˆçº§é—®é¢˜
    LOW = "LOW"            # ä½ä¼˜å…ˆçº§é—®é¢˜
    INFO = "INFO"          # ä¿¡æ¯æ€§æç¤º

class IssueCategory(Enum):
    """é—®é¢˜åˆ†ç±»"""
    SYNTAX = "syntax"           # è¯­æ³•é”™è¯¯
    IMPORT = "import"           # å¯¼å…¥é—®é¢˜
    COMPLEXITY = "complexity"   # å¤æ‚åº¦è¿‡é«˜
    SECURITY = "security"       # å®‰å…¨æ¼æ´
    PERFORMANCE = "performance" # æ€§èƒ½é—®é¢˜
    TESTING = "testing"         # æµ‹è¯•è¦†ç›–
    DOCUMENTATION = "docs"      # æ–‡æ¡£ç¼ºå¤±
    STYLE = "style"            # ä»£ç é£æ ¼
    ARCHITECTURE = "arch"      # æ¶æ„é—®é¢˜
    DEPENDENCY = "deps"        # ä¾èµ–é—®é¢˜

@dataclass
class QualityIssue:
    """è´¨é‡é—®é¢˜æ•°æ®ç»“æ„"""
    issue_id: str
    severity: QualitySeverity
    category: IssueCategory
    title: str
    description: str
    file_path: str
    line_number: Optional[int] = None
    column_number: Optional[int] = None
    end_line_number: Optional[int] = None
    rule_name: str = ""
    effort_estimate: str = "5min"
    auto_fixable: bool = False
    suggestion: str = ""
    code_snippet: Optional[str] = None
    confidence_score: float = 1.0
    tags: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class FileQualityMetrics:
    """æ–‡ä»¶è´¨é‡æŒ‡æ ‡"""
    file_path: str
    total_issues: int
    critical_issues: int
    high_issues: int
    medium_issues: int
    low_issues: int
    complexity_score: float
    maintainability_index: float
    test_coverage: float
    documentation_score: float
    security_score: float
    performance_score: float
    overall_quality_score: float
    lines_of_code: int
    technical_debt: float  # æŠ€æœ¯å€ºï¼ˆå°æ—¶ï¼‰
    improvement_suggestions: List[str] = field(default_factory=list)

@dataclass
class ProjectQualityReport:
    """é¡¹ç›®è´¨é‡æŠ¥å‘Š"""
    project_name: str
    analysis_time: datetime
    total_files: int
    total_issues: int
    total_lines_of_code: int
    overall_quality_score: float
    grade: str  # A+, A, B+, B, C+, C, D, F
    issue_distribution: Dict[IssueCategory, int]
    severity_distribution: Dict[QualitySeverity, int]
    top_issues: List[QualityIssue]
    file_metrics: List[FileQualityMetrics]
    improvement_roadmap: List[Dict[str, Any]]
    technical_debt_total: float
    quality_trends: Dict[str, float] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)

class QualityRule:
    """è´¨é‡è§„åˆ™åŸºç±»"""

    def __init__(self, name: str, description: str, severity: QualitySeverity):
        self.name = name
        self.description = description
        self.severity = severity
        self.enabled = True
        self.patterns = []
        self.exceptions = []

    def analyze(self,
    file_path: Path,
    tree: ast.AST,
    content: str) -> List[QualityIssue]:
        """åˆ†ææ–‡ä»¶å¹¶ç”Ÿæˆè´¨é‡é—®é¢˜"""
        raise NotImplementedError

    def generate_issue_id(self,
    file_path: str,
    rule_name: str,
    line: int = None) -> str:
        """ç”Ÿæˆå”¯ä¸€é—®é¢˜ID"""
        content = f"{file_path}:{rule_name}:{line or 0}"
        return hashlib.md5(content.encode()).hexdigest()[:12]

class SyntaxErrorRule(QualityRule):
    """è¯­æ³•é”™è¯¯æ£€æµ‹è§„åˆ™"""

    def __init__(self):
        super().__init__(
            name="syntax_error_detection",
            description="æ£€æµ‹Pythonè¯­æ³•é”™è¯¯",
            severity=QualitySeverity.CRITICAL
        )

    def analyze(self,
    file_path: Path,
    tree: ast.AST,
    content: str) -> List[QualityIssue]:
        """æ£€æµ‹è¯­æ³•é”™è¯¯"""
        issues = []

        # å¦‚æœASTè§£æå¤±è´¥ï¼Œè¯´æ˜æœ‰è¯­æ³•é”™è¯¯
        if tree is None:
            # å°è¯•è§£æé”™è¯¯ä¿¡æ¯
            try:
                ast.parse(content)
            except SyntaxError as e:
                issue = QualityIssue(
                    issue_id=self.generate_issue_id(str(file_path),
    self.name,
    e.lineno),

                    severity=self.severity,
                    category=IssueCategory.SYNTAX,
                    title="Pythonè¯­æ³•é”™è¯¯",
                    description=f"è¯­æ³•é”™è¯¯: {e.msg}",
                    file_path=str(file_path),
                    line_number=e.lineno,
                    column_number=e.offset,
                    rule_name=self.name,
                    effort_estimate="30min",
                    auto_fixable=False,
                    suggestion="ä¿®å¤Pythonè¯­æ³•é”™è¯¯ï¼Œç¡®ä¿ä»£ç ç¬¦åˆPythonè¯­æ³•è§„èŒƒ",
                    code_snippet=self._extract_error_line(content, e.lineno),
                    confidence_score=1.0,
                    tags=["syntax", "parse_error", "blocking"]
                )
                issues.append(issue)

        return issues

    def _extract_error_line(self, content: str, line_num: int) -> str:
        """æå–é”™è¯¯ä»£ç è¡Œ"""
        lines = content.split('\n')
        if 1 <= line_num <= len(lines):
            return lines[line_num - 1].strip()
        return ""

class ImportAnalysisRule(QualityRule):
    """å¯¼å…¥åˆ†æè§„åˆ™"""

    def __init__(self):
        super().__init__(
            name="import_analysis",
            description="åˆ†æå¯¼å…¥è¯­å¥è´¨é‡å’Œæ½œåœ¨é—®é¢˜",
            severity=QualitySeverity.MEDIUM
        )

    def analyze(self,
    file_path: Path,
    tree: ast.AST,
    content: str) -> List[QualityIssue]:
        """åˆ†æå¯¼å…¥é—®é¢˜"""
        issues = []

        if tree is None:
            return issues

        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append({
                        'type': 'import',
                        'module': alias.name,
                        'alias': alias.asname,
                        'line': node.lineno
                    })
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for alias in node.names:
                    imports.append({
                        'type': 'import_from',
                        'module': module,
                        'name': alias.name,
                        'alias': alias.asname,
                        'line': node.lineno
                    })

        # æ£€æµ‹å¯¼å…¥é—®é¢˜
        self._check_unused_imports(file_path, imports, content, issues)
        self._check_circular_imports(file_path, imports, issues)
        self._check_import_ordering(file_path, imports, issues)
        self._check_builtin_shadowing(file_path, imports, issues)

        return issues

    def _check_unused_imports(self,
    file_path: Path,
    imports: List[Dict],
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹æœªä½¿ç”¨çš„å¯¼å…¥"""
        content_without_imports = content
        for imp in imports:
            if imp['type'] == 'import':
                name = imp['alias'] or imp['module'].split('.')[0]
            else:
                name = imp['alias'] or imp['name']

            # æ£€æŸ¥æ˜¯å¦åœ¨ä»£ç ä¸­ä½¿ç”¨
            pattern = r'\b' + re.escape(name) + r'\b'
            matches = re.findall(pattern, content_without_imports)

            # æ’é™¤å¯¼å…¥è¯­å¥æœ¬èº«
            if len(matches) <= 1:
                issue = QualityIssue(
                    issue_id=self.generate_issue_id(str(file_path),
    "unused_import",
    imp['line']),

                    severity=QualitySeverity.LOW,
                    category=IssueCategory.IMPORT,
                    title="æœªä½¿ç”¨çš„å¯¼å…¥",
                    description=f"å¯¼å…¥çš„ '{name}' å¯èƒ½æœªåœ¨ä»£ç ä¸­ä½¿ç”¨",
                    file_path=str(file_path),
                    line_number=imp['line'],
                    rule_name="unused_import",
                    effort_estimate="2min",
                    auto_fixable=True,
                    suggestion=f"åˆ é™¤æœªä½¿ç”¨çš„å¯¼å…¥: {name}",
                    tags=["import", "unused", "cleanup"]
                )
                issues.append(issue)

    def _check_circular_imports(self,
    file_path: Path,
    imports: List[Dict],
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹å¾ªç¯å¯¼å…¥ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        _file_path_str = str(file_path)

        for imp in imports:
            if imp['type'] == 'import_from':
                module = imp['module']
                if module and module.startswith('.'):
                    # ç›¸å¯¹å¯¼å…¥å¯èƒ½æœ‰å¾ªç¯ä¾èµ–é—®é¢˜
                    if module.count('.') >= 3:  # æ·±å±‚ç›¸å¯¹å¯¼å…¥
                        issue = QualityIssue(
                            issue_id=self.generate_issue_id(str(file_path),
    "deep_relative_import",
    imp['line']),

                            severity=QualitySeverity.MEDIUM,
                            category=IssueCategory.IMPORT,
                            title="æ·±å±‚ç›¸å¯¹å¯¼å…¥",
                            description=f"æ·±å±‚ç›¸å¯¹å¯¼å…¥å¯èƒ½å¯¼è‡´å¾ªç¯ä¾èµ–: {module}",
                            file_path=str(file_path),
                            line_number=imp['line'],
                            rule_name="deep_relative_import",
                            effort_estimate="10min",
                            auto_fixable=False,
                            suggestion="è€ƒè™‘é‡æ„æ¨¡å—ç»“æ„ä»¥å‡å°‘ç›¸å¯¹å¯¼å…¥å±‚çº§",
                            tags=["import", "circular", "architecture"]
                        )
                        issues.append(issue)

    def _check_import_ordering(self,
    file_path: Path,
    imports: List[Dict],
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹å¯¼å…¥é¡ºåº"""
        if len(imports) < 2:
            return

        # æ£€æŸ¥å¯¼å…¥é¡ºåºï¼šæ ‡å‡†åº“ -> ç¬¬ä¸‰æ–¹åº“ -> æœ¬åœ°æ¨¡å—
        standard_libs = {'os', 'sys', 'json', 'datetime', 'pathlib', 'typing', 'dataclasses', 'enum', 'collections', 'functools', 'threading', 'concurrent', 'hashlib', 're'}

        prev_type = None
        for imp in imports:
            if imp['type'] == 'import':
                module_name = imp['module'].split('.')[0]
            else:
                module_name = imp['module'].split('.')[0] if imp['module'] else ''

            if module_name in standard_libs:
                current_type = 'standard'
            elif module_name.startswith(('src', 'tests')):
                current_type = 'local'
            else:
                current_type = 'third_party'

            if prev_type and self._compare_import_types(prev_type, current_type) > 0:
                issue = QualityIssue(
                    issue_id=self.generate_issue_id(str(file_path),
    "import_order",
    imp['line']),

                    severity=QualitySeverity.LOW,
                    category=IssueCategory.STYLE,
                    title="å¯¼å…¥é¡ºåºä¸è§„èŒƒ",
                    description=f"å¯¼å…¥é¡ºåºä¸ç¬¦åˆPEP8è§„èŒƒ: {module_name}",
                    file_path=str(file_path),
                    line_number=imp['line'],
                    rule_name="import_order",
                    effort_estimate="2min",
                    auto_fixable=True,
                    suggestion="æŒ‰ç…§æ ‡å‡†åº“ã€ç¬¬ä¸‰æ–¹åº“ã€æœ¬åœ°æ¨¡å—çš„é¡ºåºé‡æ–°æ’åˆ—å¯¼å…¥",
                    tags=["import", "style", "pep8"]
                )
                issues.append(issue)
                break

            prev_type = current_type

    def _compare_import_types(self, type1: str, type2: str) -> int:
        """æ¯”è¾ƒå¯¼å…¥ç±»å‹ä¼˜å…ˆçº§"""
        order = {'standard': 0, 'third_party': 1, 'local': 2}
        return order[type1] - order[type2]

    def _check_builtin_shadowing(self,
    file_path: Path,
    imports: List[Dict],
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹å†…ç½®å‡½æ•°é®è”½"""
        builtins = {'open', 'len', 'str', 'int', 'list', 'dict', 'set', 'tuple', 'range', 'enumerate', 'zip', 'map', 'filter'}

        for imp in imports:
            if imp['type'] == 'import':
                name = imp['alias'] or imp['module'].split('.')[0]
            else:
                name = imp['alias'] or imp['name']

            if name in builtins:
                issue = QualityIssue(
                    issue_id=self.generate_issue_id(str(file_path),
    "builtin_shadowing",
    imp['line']),

                    severity=QualitySeverity.MEDIUM,
                    category=IssueCategory.IMPORT,
                    title="é®è”½å†…ç½®å‡½æ•°",
                    description=f"å¯¼å…¥é®è”½äº†Pythonå†…ç½®å‡½æ•°: {name}",
                    file_path=str(file_path),
                    line_number=imp['line'],
                    rule_name="builtin_shadowing",
                    effort_estimate="5min",
                    auto_fixable=True,
                    suggestion=f"ä½¿ç”¨åˆ«åé¿å…é®è”½å†…ç½®å‡½æ•°: import ... as {name}_alias",
                    tags=["import", "builtin", "naming"]
                )
                issues.append(issue)

class ComplexityAnalysisRule(QualityRule):
    """å¤æ‚åº¦åˆ†æè§„åˆ™"""

    def __init__(self):
        super().__init__(
            name="complexity_analysis",
            description="åˆ†æä»£ç å¤æ‚åº¦ï¼Œè¯†åˆ«è¿‡äºå¤æ‚çš„ä»£ç ",
            severity=QualitySeverity.MEDIUM
        )
        self.max_complexity = 10
        self.max_function_lines = 50
        self.max_class_lines = 200

    def analyze(self,
    file_path: Path,
    tree: ast.AST,
    content: str) -> List[QualityIssue]:
        """åˆ†æå¤æ‚åº¦é—®é¢˜"""
        issues = []

        if tree is None:
            return issues

        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                self._analyze_function_complexity(file_path, node, content, issues)
            elif isinstance(node, ast.ClassDef):
                self._analyze_class_complexity(file_path, node, content, issues)

        return issues

    def _analyze_function_complexity(self,
    file_path: Path,
    node: ast.FunctionDef,
    content: str,
    issues: List[QualityIssue]):
        """åˆ†æå‡½æ•°å¤æ‚åº¦"""
        # è®¡ç®—åœˆå¤æ‚åº¦
        complexity = self._calculate_cyclomatic_complexity(node)

        if complexity > self.max_complexity:
            issue = QualityIssue(
                issue_id=self.generate_issue_id(str(file_path),
    "high_complexity",
    node.lineno),

                severity=QualitySeverity.MEDIUM,
                category=IssueCategory.COMPLEXITY,
                title="å‡½æ•°å¤æ‚åº¦è¿‡é«˜",
                description=f"å‡½æ•° '{node.name}' çš„åœˆå¤æ‚åº¦ä¸º {complexity}ï¼Œè¶…è¿‡æ¨èå€¼ {self.max_complexity}",
                file_path=str(file_path),
                line_number=node.lineno,
                rule_name="high_complexity",
                effort_estimate=f"{complexity * 5}min",
                auto_fixable=False,
                suggestion="è€ƒè™‘å°†å¤æ‚å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªå°å‡½æ•°ï¼Œä½¿ç”¨æ—©æœŸè¿”å›å‡å°‘åµŒå¥—",
                tags=["complexity", "refactoring", "maintainability"],
                confidence_score=0.9
            )
            issues.append(issue)

        # æ£€æŸ¥å‡½æ•°é•¿åº¦
        if hasattr(node, 'end_lineno') and node.end_lineno:
            lines = node.end_lineno - node.lineno + 1
            if lines > self.max_function_lines:
                issue = QualityIssue(
                    issue_id=self.generate_issue_id(str(file_path),
    "long_function",
    node.lineno),

                    severity=QualitySeverity.LOW,
                    category=IssueCategory.COMPLEXITY,
                    title="å‡½æ•°è¿‡é•¿",
                    description=f"å‡½æ•° '{node.name}' æœ‰ {lines} è¡Œä»£ç ï¼Œè¶…è¿‡æ¨èå€¼ {self.max_function_lines}",
                    file_path=str(file_path),
                    line_number=node.lineno,
                    rule_name="long_function",
                    effort_estimate="15min",
                    auto_fixable=False,
                    suggestion="å°†é•¿å‡½æ•°æ‹†åˆ†ä¸ºå¤šä¸ªèŒè´£å•ä¸€çš„å°å‡½æ•°",
                    tags=["complexity", "length", "refactoring"]
                )
                issues.append(issue)

    def _analyze_class_complexity(self,
    file_path: Path,
    node: ast.ClassDef,
    content: str,
    issues: List[QualityIssue]):
        """åˆ†æç±»å¤æ‚åº¦"""
        if hasattr(node, 'end_lineno') and node.end_lineno:
            lines = node.end_lineno - node.lineno + 1
            if lines > self.max_class_lines:
                issue = QualityIssue(
                    issue_id=self.generate_issue_id(str(file_path),
    "large_class",
    node.lineno),

                    severity=QualitySeverity.MEDIUM,
                    category=IssueCategory.COMPLEXITY,
                    title="ç±»è¿‡å¤§",
                    description=f"ç±» '{node.name}' æœ‰ {lines} è¡Œä»£ç ï¼Œè¶…è¿‡æ¨èå€¼ {self.max_class_lines}",
                    file_path=str(file_path),
                    line_number=node.lineno,
                    rule_name="large_class",
                    effort_estimate="30min",
                    auto_fixable=False,
                    suggestion="è€ƒè™‘å°†å¤§ç±»æ‹†åˆ†ä¸ºå¤šä¸ªèŒè´£å•ä¸€çš„å°ç±»ï¼Œä½¿ç”¨ç»„åˆæ¨¡å¼",
                    tags=["complexity", "design", "refactoring"]
                )
                issues.append(issue)

    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """è®¡ç®—åœˆå¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦

        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.With, ast.AsyncWith):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                complexity += len(child.values) - 1

        return complexity

class SecurityAnalysisRule(QualityRule):
    """å®‰å…¨åˆ†æè§„åˆ™"""

    def __init__(self):
        super().__init__(
            name="security_analysis",
            description="æ£€æµ‹æ½œåœ¨çš„å®‰å…¨æ¼æ´å’Œé—®é¢˜",
            severity=QualitySeverity.HIGH
        )
        self.dangerous_functions = {
            'eval': 'ä»£ç æ³¨å…¥é£é™©',
            'exec': 'ä»£ç æ‰§è¡Œé£é™©',
            'compile': 'ä»£ç ç¼–è¯‘é£é™©',
            '__import__': 'æ¨¡å—å¯¼å…¥é£é™©',
            'open': 'æ–‡ä»¶æ“ä½œé£é™©',
            'input': 'è¾“å…¥é£é™©ï¼ˆåœ¨ç‰¹å®šä¸Šä¸‹æ–‡ï¼‰'
        }
        self.sensitive_patterns = [
            (r'password\s*=\s*["\'][^"\']+["\']', 'ç¡¬ç¼–ç å¯†ç '),
            (r'secret\s*=\s*["\'][^"\']+["\']', 'ç¡¬ç¼–ç å¯†é’¥'),
            (r'token\s*=\s*["\'][^"\']+["\']', 'ç¡¬ç¼–ç ä»¤ç‰Œ'),
            (r'api_key\s*=\s*["\'][^"\']+["\']', 'ç¡¬ç¼–ç APIå¯†é’¥'),
        ]

    def analyze(self,
    file_path: Path,
    tree: ast.AST,
    content: str) -> List[QualityIssue]:
        """æ£€æµ‹å®‰å…¨é—®é¢˜"""
        issues = []

        # æ£€æµ‹å±é™©çš„å‡½æ•°è°ƒç”¨
        self._check_dangerous_functions(file_path, tree, content, issues)

        # æ£€æµ‹ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯
        self._check_hardcoded_secrets(file_path, content, issues)

        # æ£€æµ‹SQLæ³¨å…¥é£é™©
        self._check_sql_injection(file_path, tree, content, issues)

        return issues

    def _check_dangerous_functions(self,
    file_path: Path,
    tree: ast.AST,
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹å±é™©å‡½æ•°è°ƒç”¨"""
        if tree is None:
            return

        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    func_name = node.func.id
                    if func_name in self.dangerous_functions:
                        risk_desc = self.dangerous_functions[func_name]
                        issue = QualityIssue(
                            issue_id=self.generate_issue_id(str(file_path),
    "dangerous_function",
    node.lineno),

                            severity=QualitySeverity.HIGH,
                            category=IssueCategory.SECURITY,
                            title="ä½¿ç”¨å±é™©å‡½æ•°",
                            description=f"ä½¿ç”¨äº†å±é™©å‡½æ•° '{func_name}': {risk_desc}",
                            file_path=str(file_path),
                            line_number=node.lineno,
                            rule_name="dangerous_function",
                            effort_estimate="30min",
                            auto_fixable=False,
                            suggestion=f"é¿å…ä½¿ç”¨ {func_name}ï¼Œè€ƒè™‘æ›´å®‰å…¨çš„æ›¿ä»£æ–¹æ¡ˆ",
                            tags=["security", "dangerous", "function"],
                            confidence_score=0.8
                        )
                        issues.append(issue)

    def _check_hardcoded_secrets(self,
    file_path: Path,
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯"""
        lines = content.split('\n')
        for line_num, line in enumerate(lines, 1):
            for pattern, desc in self.sensitive_patterns:
                if re.search(pattern, line, re.IGNORECASE):
                    # æ’é™¤æ³¨é‡Šè¡Œ
                    stripped = line.strip()
                    if stripped.startswith('#'):
                        continue

                    issue = QualityIssue(
                        issue_id=self.generate_issue_id(str(file_path),
    "hardcoded_secret",
    line_num),

                        severity=QualitySeverity.CRITICAL,
                        category=IssueCategory.SECURITY,
                        title="ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯",
                        description=f"å‘ç°ç¡¬ç¼–ç çš„æ•æ„Ÿä¿¡æ¯: {desc}",
                        file_path=str(file_path),
                        line_number=line_num,
                        rule_name="hardcoded_secret",
                        effort_estimate="10min",
                        auto_fixable=False,
                        suggestion="ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶å­˜å‚¨æ•æ„Ÿä¿¡æ¯ï¼Œä¸è¦ç¡¬ç¼–ç åœ¨ä»£ç ä¸­",
                        tags=["security", "secrets", "hardcoded"],
                        confidence_score=0.9
                    )
                    issues.append(issue)

    def _check_sql_injection(self,
    file_path: Path,
    tree: ast.AST,
    content: str,
    issues: List[QualityIssue]):
        """æ£€æµ‹SQLæ³¨å…¥é£é™©ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if tree is None:
            return

        for node in ast.walk(tree):
            if isinstance(node, ast.Call):
                # æ£€æµ‹å­—ç¬¦ä¸²æ‹¼æ¥çš„SQLæŸ¥è¯¢
                if isinstance(node.func, ast.Attribute):
                    if node.func.attr in ['execute', 'query', 'run']:
                        for arg in node.args:
                            if isinstance(arg,
    ast.BinOp) and isinstance(arg.op,
    ast.Add):
                                issue = QualityIssue(
                                    issue_id=self.generate_issue_id(str(file_path),
    "sql_injection",
    node.lineno),

                                    severity=QualitySeverity.HIGH,
                                    category=IssueCategory.SECURITY,
                                    title="æ½œåœ¨SQLæ³¨å…¥é£é™©",
                                    description="æ£€æµ‹åˆ°ä½¿ç”¨å­—ç¬¦ä¸²æ‹¼æ¥æ„é€ SQLæŸ¥è¯¢ï¼Œå¯èƒ½å­˜åœ¨SQLæ³¨å…¥é£é™©",
                                    file_path=str(file_path),
                                    line_number=node.lineno,
                                    rule_name="sql_injection",
                                    effort_estimate="20min",
                                    auto_fixable=True,
                                    suggestion="ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢æˆ–ORMæ¥é¿å…SQLæ³¨å…¥",
                                    tags=["security", "sql", "injection"],
                                    confidence_score=0.7
                                )
                                issues.append(issue)

class IntelligentQualityAnalyzerV2:
    """æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“ v2.0"""

    def __init__(self, max_workers: int = 4, cache_size: int = 256):
        self.project_root = Path(__file__).parent.parent
        self.max_workers = max_workers
        self.cache_size = cache_size

        # è´¨é‡è§„åˆ™
        self.rules = [
            SyntaxErrorRule(),
            ImportAnalysisRule(),
            ComplexityAnalysisRule(),
            SecurityAnalysisRule(),
        ]

        # ç¼“å­˜
        self._analysis_cache = {}
        self._cache_lock = threading.Lock()

        # ç»Ÿè®¡ä¿¡æ¯
        self.analysis_stats = {
            'files_analyzed': 0,
            'issues_found': 0,
            'analysis_time': 0,
            'cache_hits': 0
        }

        print("ğŸ¤– æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“ v2.0 å·²åˆå§‹åŒ–")
        print(f"ğŸ“Š åˆ†æè§„åˆ™: {len(self.rules)}ä¸ª")
        print(f"âš¡ å¹¶è¡Œå¤„ç†: {max_workers}çº¿ç¨‹")
        print(f"ğŸ’¾ ç¼“å­˜å®¹é‡: {cache_size}é¡¹")

    @lru_cache(maxsize=256)
    def _is_python_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºPythonæ–‡ä»¶"""
        return file_path.suffix == '.py' and file_path.is_file()

    def discover_python_files(self, paths: List[Path] = None) -> List[Path]:
        """å‘ç°Pythonæ–‡ä»¶"""
        if paths is None:
            paths = [self.project_root / "src", self.project_root / "tests"]

        python_files = []
        for path in paths:
            if path.is_dir():
                python_files.extend(path.rglob("*.py"))
            elif path.is_file() and path.suffix == ".py":
                python_files.append(path)

        # è¿‡æ»¤æ’é™¤æ–‡ä»¶
        exclude_patterns = {
            "__pycache__", ".pytest_cache", ".git", "venv", ".venv",
            "node_modules", ".tox", "build", "dist"
        }

        filtered_files = []
        for file_path in python_files:
            if any(pattern in str(file_path) for pattern in exclude_patterns):
                continue
            filtered_files.append(file_path)

        return sorted(set(filtered_files))

    def parse_file_cached(self, file_path: Path) -> Tuple[Optional[ast.AST], str]:
        """ç¼“å­˜æ–‡ä»¶è§£æ"""
        try:
            file_mtime = file_path.stat().st_mtime
            cache_key = str(file_path)

            # æ£€æŸ¥ç¼“å­˜
            with self._cache_lock:
                if cache_key in self._analysis_cache:
                    cached_mtime, cached_tree, cached_content = self._analysis_cache[cache_key]
                    if cached_mtime == file_mtime:
                        self.analysis_stats['cache_hits'] += 1
                        return cached_tree, cached_content

            # è§£ææ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            tree = ast.parse(content)

            # æ›´æ–°ç¼“å­˜
            with self._cache_lock:
                self._analysis_cache[cache_key] = (file_mtime, tree, content)

            return tree, content

        except SyntaxError:
            # è¯­æ³•é”™è¯¯æ—¶è¿”å›Noneå’ŒåŸå§‹å†…å®¹
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                return None, content
            except Exception:
                return None, ""
        except Exception as e:
            print(f"âš ï¸ è§£ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None, ""

    def analyze_single_file(self,
    file_path: Path) -> Tuple[List[QualityIssue],
    FileQualityMetrics]:
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        start_time = time.time()

        tree, content = self.parse_file_cached(file_path)

        # æ‰§è¡Œæ‰€æœ‰è§„åˆ™
        all_issues = []
        for rule in self.rules:
            if rule.enabled:
                try:
                    issues = rule.analyze(file_path, tree, content)
                    all_issues.extend(issues)
                except Exception as e:
                    print(f"âš ï¸ è§„åˆ™ {rule.name} åˆ†æå¤±è´¥ {file_path}: {e}")

        # è®¡ç®—æ–‡ä»¶è´¨é‡æŒ‡æ ‡
        lines_of_code = len([line for line in content.split('\n') if line.strip() and not line.strip().startswith('#')])
        metrics = self._calculate_file_metrics(file_path, all_issues, lines_of_code)

        _analysis_time = time.time() - start_time

        return all_issues, metrics

    def _calculate_file_metrics(self,
    file_path: Path,
    issues: List[QualityIssue],
    lines_of_code: int) -> FileQualityMetrics:
        """è®¡ç®—æ–‡ä»¶è´¨é‡æŒ‡æ ‡"""
        # ç»Ÿè®¡ä¸åŒä¸¥é‡ç¨‹åº¦çš„é—®é¢˜æ•°é‡
        critical_count = len([i for i in issues if i.severity == QualitySeverity.CRITICAL])
        high_count = len([i for i in issues if i.severity == QualitySeverity.HIGH])
        medium_count = len([i for i in issues if i.severity == QualitySeverity.MEDIUM])
        low_count = len([i for i in issues if i.severity == QualitySeverity.LOW])

        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        complexity_score = 5.0 + (high_count * 2) + (medium_count * 1) + (low_count * 0.5)

        # è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        maintainability_index = max(0,
    100 - (complexity_score * 5) - (critical_count * 20) - (high_count * 10))

        # æµ‹è¯•è¦†ç›–ç‡ï¼ˆåŸºäºæ–‡ä»¶åä¼°ç®—ï¼‰
        test_coverage = 0.0
        if "test" in file_path.name.lower():
            test_coverage = 85.0  # æµ‹è¯•æ–‡ä»¶å‡å®šé«˜è¦†ç›–ç‡

        # æ–‡æ¡£åˆ†æ•°
        documentation_score = self._calculate_documentation_score(file_path)

        # å®‰å…¨åˆ†æ•°
        security_issues = [i for i in issues if i.category == IssueCategory.SECURITY]
        security_score = max(0, 100 - len(security_issues) * 15)

        # æ€§èƒ½åˆ†æ•°
        performance_issues = [i for i in issues if i.category == IssueCategory.PERFORMANCE]
        performance_score = max(0, 100 - len(performance_issues) * 10)

        # ç»¼åˆè´¨é‡åˆ†æ•°
        overall_score = (
            maintainability_index * 0.3 +
            security_score * 0.25 +
            performance_score * 0.2 +
            test_coverage * 0.15 +
            documentation_score * 0.1
        )

        # æŠ€æœ¯å€ºï¼ˆå°æ—¶ï¼‰
        technical_debt = (
            critical_count * 8 +    # 8å°æ—¶ä¿®å¤ä¸€ä¸ªä¸¥é‡é—®é¢˜
            high_count * 4 +        # 4å°æ—¶ä¿®å¤ä¸€ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜
            medium_count * 2 +      # 2å°æ—¶ä¿®å¤ä¸€ä¸ªä¸­ç­‰é—®é¢˜
            low_count * 0.5         # 30åˆ†é’Ÿä¿®å¤ä¸€ä¸ªä½ä¼˜å…ˆçº§é—®é¢˜
        )

        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        suggestions = self._generate_suggestions(issues, file_path)

        return FileQualityMetrics(
            file_path=str(file_path),
            total_issues=len(issues),
            critical_issues=critical_count,
            high_issues=high_count,
            medium_issues=medium_count,
            low_issues=low_count,
            complexity_score=complexity_score,
            maintainability_index=maintainability_index,
            test_coverage=test_coverage,
            documentation_score=documentation_score,
            security_score=security_score,
            performance_score=performance_score,
            overall_quality_score=overall_score,
            lines_of_code=lines_of_code,
            technical_debt=technical_debt,
            improvement_suggestions=suggestions
        )

    def _calculate_documentation_score(self, file_path: Path) -> float:
        """è®¡ç®—æ–‡æ¡£åˆ†æ•°"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            score = 50.0  # åŸºç¡€åˆ†

            # æ£€æŸ¥æ¨¡å—æ–‡æ¡£å­—ç¬¦ä¸²
            if content.startswith('"""') or content.startswith("'''"):
                score += 20.0

            # æ£€æŸ¥æ³¨é‡Šè¦†ç›–ç‡
            lines = content.split('\n')
            comment_lines = len([line for line in lines if line.strip().startswith('#')])
            code_lines = len([line for line in lines if line.strip() and not line.strip().startswith('#')])

            if code_lines > 0:
                comment_ratio = comment_lines / code_lines
                score += min(30.0, comment_ratio * 100)

            return min(100.0, score)

        except Exception:
            return 50.0

    def _generate_suggestions(self,
    issues: List[QualityIssue],
    file_path: Path) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        suggestions = []

        if not issues:
            suggestions.append("âœ¨ ä»£ç è´¨é‡è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
            return suggestions

        # æŒ‰ç±»åˆ«ç»Ÿè®¡é—®é¢˜
        category_counts = Counter(issue.category for issue in issues)
        severity_counts = Counter(issue.severity for issue in issues)

        # ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
        if severity_counts[QualitySeverity.CRITICAL] > 0:
            suggestions.append(f"ğŸš¨ ä¼˜å…ˆä¿®å¤ {severity_counts[QualitySeverity.CRITICAL]} ä¸ªä¸¥é‡é—®é¢˜")

        if category_counts[IssueCategory.SECURITY] > 0:
            suggestions.append(f"ğŸ”’ è§£å†³ {category_counts[IssueCategory.SECURITY]} ä¸ªå®‰å…¨é—®é¢˜")

        if category_counts[IssueCategory.COMPLEXITY] > 0:
            suggestions.append(f"ğŸ”§ é‡æ„ {category_counts[IssueCategory.COMPLEXITY]} ä¸ªå¤æ‚åº¦é—®é¢˜")

        if category_counts[IssueCategory.SYNTAX] > 0:
            suggestions.append(f"ğŸ“ ä¿®å¤ {category_counts[IssueCategory.SYNTAX]} ä¸ªè¯­æ³•é”™è¯¯")

        # é€šç”¨æ”¹è¿›å»ºè®®
        if len(issues) > 10:
            suggestions.append("ğŸ“ˆ è€ƒè™‘åˆ†æ‰¹æ”¹è¿›ï¼Œå…ˆè§£å†³é«˜ä¼˜å…ˆçº§é—®é¢˜")

        suggestions.append("ğŸ“Š å®šæœŸè¿è¡Œè´¨é‡åˆ†æï¼ŒæŒç»­æ”¹è¿›ä»£ç è´¨é‡")

        return suggestions

    def analyze_project_parallel(self,
    paths: List[Path] = None) -> ProjectQualityReport:
        """å¹¶è¡Œåˆ†ææ•´ä¸ªé¡¹ç›®"""
        print("ğŸš€ å¯åŠ¨é¡¹ç›®è´¨é‡åˆ†æ...")
        start_time = time.time()

        # å‘ç°Pythonæ–‡ä»¶
        python_files = self.discover_python_files(paths)
        total_files = len(python_files)

        print(f"ğŸ“ å‘ç° {total_files} ä¸ªPythonæ–‡ä»¶")

        if total_files == 0:
            print("âš ï¸ æœªå‘ç°Pythonæ–‡ä»¶")
            return self._create_empty_report()

        # å¹¶è¡Œåˆ†æ
        all_issues = []
        file_metrics = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤åˆ†æä»»åŠ¡
            future_to_file = {
                executor.submit(self.analyze_single_file, file_path): file_path
                for file_path in python_files
            }

            # æ”¶é›†ç»“æœ
            completed = 0
            for future in concurrent.futures.as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    issues, metrics = future.result()
                    all_issues.extend(issues)
                    file_metrics.append(metrics)

                    completed += 1
                    progress = (completed / total_files) * 100
                    print(f"  è¿›åº¦: {completed}/{total_files} ({progress:.1f}%) - {file_path.name}: {len(issues)}ä¸ªé—®é¢˜")

                except Exception as e:
                    print(f"âŒ åˆ†ææ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        # ç”Ÿæˆé¡¹ç›®æŠ¥å‘Š
        analysis_time = time.time() - start_time
        report = self._generate_project_report(all_issues, file_metrics, analysis_time)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.analysis_stats['files_analyzed'] = total_files
        self.analysis_stats['issues_found'] = len(all_issues)
        self.analysis_stats['analysis_time'] = analysis_time

        return report

    def _generate_project_report(self,
    issues: List[QualityIssue],
    file_metrics: List[FileQualityMetrics],
    analysis_time: float) -> ProjectQualityReport:
        """ç”Ÿæˆé¡¹ç›®è´¨é‡æŠ¥å‘Š"""
        # åŸºæœ¬ç»Ÿè®¡
        total_files = len(file_metrics)
        total_issues = len(issues)
        total_lines_of_code = sum(m.lines_of_code for m in file_metrics)

        # æŒ‰ç±»åˆ«å’Œä¸¥é‡ç¨‹åº¦ç»Ÿè®¡
        issue_distribution = Counter(issue.category for issue in issues)
        severity_distribution = Counter(issue.severity for issue in issues)

        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        if file_metrics:
            overall_score = sum(m.overall_quality_score for m in file_metrics) / len(file_metrics)
        else:
            overall_score = 0.0

        # è¯„å®šç­‰çº§
        grade = self._calculate_grade(overall_score)

        # è·å–æœ€é‡è¦çš„é—®é¢˜
        top_issues = sorted(issues, key=lambda x: (
            self._severity_weight(x.severity),
            x.confidence_score
        ), reverse=True)[:20]

        # è®¡ç®—æŠ€æœ¯å€º
        total_technical_debt = sum(m.technical_debt for m in file_metrics)

        # ç”Ÿæˆæ”¹è¿›è·¯çº¿å›¾
        improvement_roadmap = self._generate_improvement_roadmap(issues, file_metrics)

        # ç”Ÿæˆæ¨èå»ºè®®
        recommendations = self._generate_recommendations(issues, file_metrics)

        return ProjectQualityReport(
            project_name=self.project_root.name,
            analysis_time=datetime.now(),
            total_files=total_files,
            total_issues=total_issues,
            total_lines_of_code=total_lines_of_code,
            overall_quality_score=overall_score,
            grade=grade,
            issue_distribution=dict(issue_distribution),
            severity_distribution=dict(severity_distribution),
            top_issues=top_issues,
            file_metrics=file_metrics,
            improvement_roadmap=improvement_roadmap,
            technical_debt_total=total_technical_debt,
            recommendations=recommendations
        )

    def _severity_weight(self, severity: QualitySeverity) -> int:
        """è·å–ä¸¥é‡ç¨‹åº¦æƒé‡"""
        weights = {
            QualitySeverity.CRITICAL: 4,
            QualitySeverity.HIGH: 3,
            QualitySeverity.MEDIUM: 2,
            QualitySeverity.LOW: 1,
            QualitySeverity.INFO: 0
        }
        return weights.get(severity, 0)

    def _calculate_grade(self, score: float) -> str:
        """è®¡ç®—è´¨é‡ç­‰çº§"""
        if score >= 95:
            return "A+"
        elif score >= 90:
            return "A"
        elif score >= 85:
            return "B+"
        elif score >= 80:
            return "B"
        elif score >= 75:
            return "C+"
        elif score >= 70:
            return "C"
        elif score >= 60:
            return "D"
        else:
            return "F"

    def _generate_improvement_roadmap(self,
    issues: List[QualityIssue],
    file_metrics: List[FileQualityMetrics]) -> List[Dict[str,
    Any]]:
        """ç”Ÿæˆæ”¹è¿›è·¯çº¿å›¾"""
        roadmap = []

        # æŒ‰ä¼˜å…ˆçº§åˆ†ç»„é—®é¢˜
        critical_issues = [i for i in issues if i.severity == QualitySeverity.CRITICAL]
        high_issues = [i for i in issues if i.severity == QualitySeverity.HIGH]

        # Phase 1: ä¿®å¤ä¸¥é‡é—®é¢˜
        if critical_issues:
            roadmap.append({
                'phase': 1,
                'title': 'ä¿®å¤ä¸¥é‡é—®é¢˜',
                'description': f'è§£å†³ {len(critical_issues)} ä¸ªä¸¥é‡é—®é¢˜',
                'issues': critical_issues[:5],
                'estimated_effort': f"{len(critical_issues) * 8}å°æ—¶",
                'priority': 'CRITICAL'
            })

        # Phase 2: ä¿®å¤é«˜ä¼˜å…ˆçº§é—®é¢˜
        if high_issues:
            roadmap.append({
                'phase': 2,
                'title': 'ä¿®å¤é«˜ä¼˜å…ˆçº§é—®é¢˜',
                'description': f'è§£å†³ {len(high_issues)} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜',
                'issues': high_issues[:10],
                'estimated_effort': f"{len(high_issues) * 4}å°æ—¶",
                'priority': 'HIGH'
            })

        # Phase 3: é‡æ„å¤æ‚ä»£ç 
        complexity_issues = [i for i in issues if i.category == IssueCategory.COMPLEXITY]
        if complexity_issues:
            roadmap.append({
                'phase': 3,
                'title': 'é‡æ„å¤æ‚ä»£ç ',
                'description': f'é‡æ„ {len(complexity_issues)} ä¸ªå¤æ‚åº¦é—®é¢˜',
                'issues': complexity_issues[:10],
                'estimated_effort': f"{len(complexity_issues) * 6}å°æ—¶",
                'priority': 'MEDIUM'
            })

        # Phase 4: æå‡æµ‹è¯•è¦†ç›–ç‡
        low_coverage_files = [m for m in file_metrics if m.test_coverage < 50]
        if low_coverage_files:
            roadmap.append({
                'phase': 4,
                'title': 'æå‡æµ‹è¯•è¦†ç›–ç‡',
                'description': f'ä¸º {len(low_coverage_files)} ä¸ªæ–‡ä»¶å¢åŠ æµ‹è¯•',
                'files': low_coverage_files[:10],
                'estimated_effort': f"{len(low_coverage_files) * 3}å°æ—¶",
                'priority': 'MEDIUM'
            })

        return roadmap

    def _generate_recommendations(self,
    issues: List[QualityIssue],
    file_metrics: List[FileQualityMetrics]) -> List[str]:
        """ç”Ÿæˆæ¨èå»ºè®®"""
        recommendations = []

        # åŸºäºé—®é¢˜åˆ†å¸ƒçš„å»ºè®®
        if len(issues) == 0:
            recommendations.append("ğŸ‰ ä»£ç è´¨é‡ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒï¼")
            return recommendations

        issue_types = Counter(issue.category for issue in issues)
        _severity_types = Counter(issue.severity for issue in issues)

        # å®‰å…¨å»ºè®®
        if issue_types[IssueCategory.SECURITY] > 0:
            recommendations.append("ğŸ”’ å»ºç«‹å®‰å…¨ä»£ç å®¡æŸ¥æµç¨‹ï¼Œä½¿ç”¨å®‰å…¨æ‰«æå·¥å…·")

        # æµ‹è¯•å»ºè®®
        low_coverage_count = len([m for m in file_metrics if m.test_coverage < 30])
        if low_coverage_count > 0:
            recommendations.append(f"ğŸ§ª ä¸º {low_coverage_count} ä¸ªä½è¦†ç›–ç‡æ–‡ä»¶å¢åŠ å•å…ƒæµ‹è¯•")

        # å¤æ‚åº¦å»ºè®®
        if issue_types[IssueCategory.COMPLEXITY] > 0:
            recommendations.append("ğŸ”§ å¼•å…¥ä»£ç é‡æ„æœ€ä½³å®è·µï¼Œå®šæœŸé‡æ„å¤æ‚ä»£ç ")

        # æ–‡æ¡£å»ºè®®
        avg_docs = sum(m.documentation_score for m in file_metrics) / len(file_metrics) if file_metrics else 0
        if avg_docs < 70:
            recommendations.append("ğŸ“š å®Œå–„ä»£ç æ–‡æ¡£ï¼Œå¢åŠ æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²")

        # å·¥å…·å»ºè®®
        recommendations.append("ğŸ¤– é›†æˆæ™ºèƒ½è´¨é‡åˆ†æå¼•æ“åˆ°CI/CDæµç¨‹")
        recommendations.append("ğŸ“Š å»ºç«‹è´¨é‡åº¦é‡ä»ªè¡¨æ¿ï¼Œå®æ—¶ç›‘æ§ä»£ç è´¨é‡")

        return recommendations

    def _create_empty_report(self) -> ProjectQualityReport:
        """åˆ›å»ºç©ºæŠ¥å‘Š"""
        return ProjectQualityReport(
            project_name=self.project_root.name,
            analysis_time=datetime.now(),
            total_files=0,
            total_issues=0,
            total_lines_of_code=0,
            overall_quality_score=0.0,
            grade="F",
            issue_distribution={},
            severity_distribution={},
            top_issues=[],
            file_metrics=[],
            improvement_roadmap=[],
            technical_debt_total=0.0,
            recommendations=["æœªå‘ç°Pythonæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥é¡¹ç›®ç»“æ„"]
        )

    def print_report(self, report: ProjectQualityReport):
        """æ‰“å°è´¨é‡æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ¤– ä¼ä¸šçº§æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“ v2.0 - é¡¹ç›®è´¨é‡æŠ¥å‘Š")
        print("="*80)

        print("\nğŸ“Š é¡¹ç›®æ¦‚è§ˆ:")
        print(f"  ğŸ·ï¸ é¡¹ç›®åç§°: {report.project_name}")
        print(f"  ğŸ“… åˆ†ææ—¶é—´: {report.analysis_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"  ğŸ“ æ–‡ä»¶æ€»æ•°: {report.total_files}")
        print(f"  ğŸ“ ä»£ç è¡Œæ•°: {report.total_lines_of_code:,}")
        print(f"  ğŸ› é—®é¢˜æ€»æ•°: {report.total_issues}")
        print(f"  â±ï¸ åˆ†æè€—æ—¶: {self.analysis_stats['analysis_time']:.2f}ç§’")

        print("\nğŸ† è´¨é‡è¯„åˆ†:")
        print(f"  ğŸ“Š ç»¼åˆåˆ†æ•°: {report.overall_quality_score:.1f}/100")
        print(f"  ğŸ¯ è´¨é‡ç­‰çº§: {report.grade}")
        print(f"  â° æŠ€æœ¯å€ºåŠ¡: {report.technical_debt_total:.1f}å°æ—¶")

        # é—®é¢˜åˆ†å¸ƒ
        if report.issue_distribution:
            print("\nğŸ“ˆ é—®é¢˜åˆ†å¸ƒ:")
            for category,
    count in sorted(report.issue_distribution.items(),
    key=lambda x: x[1],
    reverse=True):
                print(f"  {category.value}: {count}ä¸ª")

        # ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ
        if report.severity_distribution:
            print("\nğŸš¨ ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ:")
            for severity,
    count in sorted(report.severity_distribution.items(),
    key=lambda x: self._severity_weight(x[0]),
    reverse=True):
                emoji = {"CRITICAL": "ğŸš¨", "HIGH": "âš ï¸", "MEDIUM": "âš¡", "LOW": "ğŸ’¡", "INFO": "â„¹ï¸"}
                print(f"  {emoji.get(severity.value, '*')} {severity.value}: {count}ä¸ª")

        # Top é—®é¢˜
        if report.top_issues:
            print(f"\nğŸ” é‡ç‚¹é—®é¢˜ (å‰{len(report.top_issues)}ä¸ª):")
            for i, issue in enumerate(report.top_issues[:10], 1):
                severity_emoji = {"CRITICAL": "ğŸš¨", "HIGH": "âš ï¸", "MEDIUM": "âš¡", "LOW": "ğŸ’¡", "INFO": "â„¹ï¸"}
                print(f"  {i}. {severity_emoji.get(issue.severity.value,
    '*')} [{issue.category.value}] {issue.title}")
                print(f"     ğŸ“ {issue.file_path}:{issue.line_number}")
                print(f"     ğŸ’¡ {issue.suggestion}")
                if i < len(report.top_issues):
                    print()

        # æ”¹è¿›è·¯çº¿å›¾
        if report.improvement_roadmap:
            print("\nğŸ—ºï¸ æ”¹è¿›è·¯çº¿å›¾:")
            for phase in report.improvement_roadmap:
                print(f"  Phase {phase['phase']}: {phase['title']}")
                print(f"    ğŸ“ {phase['description']}")
                print(f"    â±ï¸ é¢„ä¼°å·¥ä½œé‡: {phase['estimated_effort']}")
                print(f"    ğŸ¯ ä¼˜å…ˆçº§: {phase['priority']}")
                print()

        # æ¨èå»ºè®®
        if report.recommendations:
            print("\nğŸ’¡ æ¨èå»ºè®®:")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"  {i}. {rec}")

        print("\n" + "="*80)
        print("ğŸ‰ æ™ºèƒ½è´¨é‡åˆ†æå®Œæˆï¼åŸºäºIssue #159çš„æŠ€æœ¯æˆæœæ„å»º")
        print("ğŸš€ v2.0å¼•æ“å®ç°äº†AIé©±åŠ¨çš„è‡ªåŠ¨åŒ–è´¨é‡åˆ†æ")
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– å¯åŠ¨ä¼ä¸šçº§æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“ v2.0...")

    try:
        # åˆ›å»ºåˆ†æå™¨
        analyzer = IntelligentQualityAnalyzerV2(
            max_workers=4,  # 4çº¿ç¨‹å¹¶è¡Œåˆ†æ
            cache_size=256   # 256é¡¹ç¼“å­˜
        )

        # è¿è¡Œé¡¹ç›®åˆ†æ
        report = analyzer.analyze_project_parallel()

        # æ‰“å°æŠ¥å‘Š
        analyzer.print_report(report)

        # è¿”å›åˆ†æç»“æœ
        if report.overall_quality_score >= 80:
            print(f"\nâœ… é¡¹ç›®è´¨é‡ä¼˜ç§€: {report.overall_quality_score:.1f}/100")
            return 0
        elif report.overall_quality_score >= 60:
            print(f"\nâš¡ é¡¹ç›®è´¨é‡è‰¯å¥½: {report.overall_quality_score:.1f}/100")
            return 1
        else:
            print(f"\nâš ï¸ é¡¹ç›®è´¨é‡éœ€è¦æ”¹è¿›: {report.overall_quality_score:.1f}/100")
            return 2

    except Exception as e:
        print(f"âŒ è´¨é‡åˆ†æå¤±è´¥: {e}")
        return 3

if __name__ == "__main__":
    exit(main())
