#!/usr/bin/env python3
"""
æ–‡æ¡£CI/CDæµæ°´çº¿è„šæœ¬
Documentation CI/CD Pipeline Script

è¿™ä¸ªè„šæœ¬æä¾›äº†å®Œæ•´çš„æ–‡æ¡£æ„å»ºã€éªŒè¯å’Œéƒ¨ç½²åŠŸèƒ½ï¼Œ
æ”¯æŒæœ¬åœ°å¼€å‘å’ŒCI/CDè‡ªåŠ¨åŒ–ã€‚

ä½œè€…: Claude AI Assistant
ç‰ˆæœ¬: v1.0.0
åˆ›å»ºæ—¶é—´: 2025-10-24
"""

import os
import sys
import json
import subprocess
import argparse
import shutil
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import asyncio
import aiohttp
import yaml

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


class DocsCIPipeline:
    """æ–‡æ¡£CI/CDæµæ°´çº¿ç®¡ç†å™¨"""

    def __init__(self, config_path: str = "mkdocs.yml"):
        """
        åˆå§‹åŒ–CI/CDæµæ°´çº¿

        Args:
            config_path: MkDocsé…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config_path = Path(config_path)
        self.project_root = Path.cwd()
        self.docs_dir = self.project_root / "docs"
        self.site_dir = self.project_root / "site"
        self.build_dir = self.project_root / ".build" / "docs"

        # ç¡®ä¿æ„å»ºç›®å½•å­˜åœ¨
        self.build_dir.mkdir(parents=True, exist_ok=True)

        # åŠ è½½é…ç½®
        self.config = self._load_config()

        # åˆå§‹åŒ–çŠ¶æ€
        self.pipeline_state = {
            "start_time": datetime.now().isoformat(),
            "stages": {},
            "errors": [],
            "warnings": [],
            "success": False,
        }

    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½MkDocsé…ç½®"""
        try:
            with open(self.config_path, "r", encoding="utf-8") as f:
                config = yaml.safe_load(f)
            logger.info(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"âŒ åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise

    async def run_full_pipeline(
        self,
        build: bool = True,
        deploy: bool = False,
        quality_check: bool = True,
        deploy_target: str = "github-pages",
    ) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„CI/CDæµæ°´çº¿

        Args:
            build: æ˜¯å¦æ„å»ºæ–‡æ¡£
            deploy: æ˜¯å¦éƒ¨ç½²æ–‡æ¡£
            quality_check: æ˜¯å¦è¿›è¡Œè´¨é‡æ£€æŸ¥
            deploy_target: éƒ¨ç½²ç›®æ ‡ (github-pages, s3, custom)

        Returns:
            æµæ°´çº¿æ‰§è¡Œç»“æœ
        """
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œæ–‡æ¡£CI/CDæµæ°´çº¿")

        try:
            # é˜¶æ®µ1: ç¯å¢ƒæ£€æŸ¥
            await self._stage_environment_check()

            # é˜¶æ®µ2: ä¾èµ–æ£€æŸ¥
            await self._stage_dependency_check()

            # é˜¶æ®µ3: æ–‡æ¡£éªŒè¯
            await self._stage_docs_validation()

            # é˜¶æ®µ4: æ„å»ºæ–‡æ¡£
            if build:
                await self._stage_build()

            # é˜¶æ®µ5: è´¨é‡æ£€æŸ¥
            if quality_check:
                await self._stage_quality_check()

            # é˜¶æ®µ6: éƒ¨ç½²æ–‡æ¡£
            if deploy:
                await self._stage_deploy(deploy_target)

            # å®Œæˆæµæ°´çº¿
            self.pipeline_state["success"] = True
            self.pipeline_state["end_time"] = datetime.now().isoformat()
            logger.info("ğŸ‰ æ–‡æ¡£CI/CDæµæ°´çº¿æ‰§è¡ŒæˆåŠŸ")

        except Exception as e:
            self.pipeline_state["errors"].append(str(e))
            self.pipeline_state["success"] = False
            self.pipeline_state["end_time"] = datetime.now().isoformat()
            logger.error(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")

        return self.pipeline_state

    async def _stage_environment_check(self) -> None:
        """é˜¶æ®µ1: ç¯å¢ƒæ£€æŸ¥"""
        stage_name = "environment_check"
        logger.info("ğŸ” é˜¶æ®µ1: ç¯å¢ƒæ£€æŸ¥")

        stage_result = {"start_time": datetime.now().isoformat(), "checks": {}}

        try:
            # æ£€æŸ¥Pythonç‰ˆæœ¬
            python_version = sys.version_info
            stage_result["checks"]["python_version"] = {
                "version": f"{python_version.major}.{python_version.minor}.{python_version.micro}",
                "valid": python_version >= (3, 8),
            }

            # æ£€æŸ¥å¿…éœ€çš„ç›®å½•
            required_dirs = ["docs", ".build"]
            for dir_name in required_dirs:
                dir_path = self.project_root / dir_name
                stage_result["checks"][f"directory_{dir_name}"] = {
                    "exists": dir_path.exists(),
                    "path": str(dir_path),
                }

            # æ£€æŸ¥å¿…éœ€çš„æ–‡ä»¶
            required_files = ["mkdocs.yml", "docs/INDEX.md"]
            for file_name in required_files:
                file_path = self.project_root / file_name
                stage_result["checks"][f"file_{file_name.replace('/', '_')}"] = {
                    "exists": file_path.exists(),
                    "path": str(file_path),
                }

            # æ£€æŸ¥GitçŠ¶æ€
            try:
                git_result = subprocess.run(
                    ["git", "status", "--porcelain"],
                    capture_output=True,
                    text=True,
                    cwd=self.project_root,
                )
                stage_result["checks"]["git_status"] = {
                    "clean": len(git_result.stdout.strip()) == 0,
                    "status": git_result.stdout.strip()[:100] if git_result.stdout else "",
                }
            except Exception:
                stage_result["checks"]["git_status"] = {"error": "Git not available"}

            stage_result["success"] = True
            stage_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            stage_result["success"] = False
            stage_result["error"] = str(e)
            stage_result["end_time"] = datetime.now().isoformat()
            raise

        self.pipeline_state["stages"][stage_name] = stage_result
        logger.info("âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ")

    async def _stage_dependency_check(self) -> None:
        """é˜¶æ®µ2: ä¾èµ–æ£€æŸ¥"""
        stage_name = "dependency_check"
        logger.info("ğŸ“¦ é˜¶æ®µ2: ä¾èµ–æ£€æŸ¥")

        stage_result = {"start_time": datetime.now().isoformat(), "dependencies": {}}

        try:
            # æ£€æŸ¥å¿…éœ€çš„PythonåŒ…
            required_packages = [
                "mkdocs",
                "mkdocs_material",
                "pymdown_extensions",
                "markdown_extensions",
            ]

            for package in required_packages:
                try:
                    result = subprocess.run(
                        [sys.executable, "-c", f"import {package}"], capture_output=True, text=True
                    )
                    stage_result["dependencies"][package] = {
                        "installed": result.returncode == 0,
                        "version": (
                            self._get_package_version(package) if result.returncode == 0 else None
                        ),
                    }
                except Exception as e:
                    stage_result["dependencies"][package] = {"installed": False, "error": str(e)}

            # æ£€æŸ¥å¯é€‰ä¾èµ–
            optional_packages = ["mkdocs-minify-plugin", "mkdocs-git-revision-date-plugin"]
            for package in optional_packages:
                try:
                    result = subprocess.run(
                        [sys.executable, "-c", f"import {package}"], capture_output=True, text=True
                    )
                    stage_result["dependencies"][f"optional_{package}"] = {
                        "installed": result.returncode == 0,
                        "version": (
                            self._get_package_version(package) if result.returncode == 0 else None
                        ),
                    }
                except Exception:
                    stage_result["dependencies"][f"optional_{package}"] = {"installed": False}

            stage_result["success"] = True
            stage_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            stage_result["success"] = False
            stage_result["error"] = str(e)
            stage_result["end_time"] = datetime.now().isoformat()
            raise

        self.pipeline_state["stages"][stage_name] = stage_result
        logger.info("âœ… ä¾èµ–æ£€æŸ¥å®Œæˆ")

    def _get_package_version(self, package_name: str) -> Optional[str]:
        """è·å–åŒ…ç‰ˆæœ¬"""
        try:
            result = subprocess.run(
                [sys.executable, "-c", f"import {package_name}; print({package_name}.__version__)"],
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception:
            pass
        return None

    async def _stage_docs_validation(self) -> None:
        """é˜¶æ®µ3: æ–‡æ¡£éªŒè¯"""
        stage_name = "docs_validation"
        logger.info("ğŸ“‹ é˜¶æ®µ3: æ–‡æ¡£éªŒè¯")

        stage_result = {"start_time": datetime.now().isoformat(), "validation": {}}

        try:
            # æ£€æŸ¥æ–‡æ¡£ç›®å½•ç»“æ„
            stage_result["validation"]["structure"] = self._validate_docs_structure()

            # æ£€æŸ¥æ–‡æ¡£å†…å®¹è´¨é‡
            stage_result["validation"]["content"] = await self._validate_docs_content()

            # æ£€æŸ¥æ–‡æ¡£é“¾æ¥
            stage_result["validation"]["links"] = await self._validate_docs_links()

            # æ£€æŸ¥æ–‡æ¡£é…ç½®
            stage_result["validation"]["config"] = self._validate_docs_config()

            stage_result["success"] = True
            stage_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            stage_result["success"] = False
            stage_result["error"] = str(e)
            stage_result["end_time"] = datetime.now().isoformat()
            raise

        self.pipeline_state["stages"][stage_name] = stage_result
        logger.info("âœ… æ–‡æ¡£éªŒè¯å®Œæˆ")

    def _validate_docs_structure(self) -> Dict[str, Any]:
        """éªŒè¯æ–‡æ¡£ç»“æ„"""
        result = {
            "total_files": 0,
            "total_directories": 0,
            "index_exists": False,
            "readme_exists": False,
            "issues": [],
        }

        try:
            # ç»Ÿè®¡æ–‡ä»¶å’Œç›®å½•
            for root, dirs, files in os.walk(self.docs_dir):
                result["total_directories"] += len(dirs)
                result["total_files"] += len([f for f in files if f.endswith(".md")])

            # æ£€æŸ¥å…³é”®æ–‡ä»¶
            index_file = self.docs_dir / "INDEX.md"
            result["index_exists"] = index_file.exists()

            readme_file = self.docs_dir / "README.md"
            result["readme_exists"] = readme_file.exists()

            # æ£€æŸ¥å¿…éœ€çš„å­ç›®å½•
            required_subdirs = ["architecture", "reference", "how-to", "testing"]
            for subdir in required_subdirs:
                subdir_path = self.docs_dir / subdir
                if not subdir_path.exists():
                    result["issues"].append(f"ç¼ºå°‘å¿…éœ€ç›®å½•: {subdir}")

        except Exception as e:
            result["issues"].append(f"ç»“æ„éªŒè¯é”™è¯¯: {e}")

        return result

    async def _validate_docs_content(self) -> Dict[str, Any]:
        """éªŒè¯æ–‡æ¡£å†…å®¹"""
        result = {
            "total_words": 0,
            "total_lines": 0,
            "files_with_frontmatter": 0,
            "files_with_toc": 0,
            "issues": [],
        }

        try:
            # éå†æ‰€æœ‰Markdownæ–‡ä»¶
            md_files = list(self.docs_dir.rglob("*.md"))

            for md_file in md_files:
                try:
                    with open(md_file, "r", encoding="utf-8") as f:
                        content = f.read()

                    lines = content.split("\n")
                    result["total_lines"] += len(lines)
                    result["total_words"] += len(content.split())

                    # æ£€æŸ¥Front Matter
                    if content.startswith("---"):
                        result["files_with_frontmatter"] += 1

                    # æ£€æŸ¥ç›®å½•
                    if "## ğŸ“‘ ç›®å½•" in content or "## ç›®å½•" in content:
                        result["files_with_toc"] += 1

                    # æ£€æŸ¥å†…å®¹é•¿åº¦
                    if len(content.strip()) < 100:
                        result["issues"].append(
                            f"æ–‡ä»¶å†…å®¹è¿‡å°‘: {md_file.relative_to(self.docs_dir)}"
                        )

                except Exception as e:
                    result["issues"].append(f"è¯»å–æ–‡ä»¶å¤±è´¥ {md_file}: {e}")

        except Exception as e:
            result["issues"].append(f"å†…å®¹éªŒè¯é”™è¯¯: {e}")

        return result

    async def _validate_docs_links(self) -> Dict[str, Any]:
        """éªŒè¯æ–‡æ¡£é“¾æ¥"""
        result = {
            "total_links": 0,
            "internal_links": 0,
            "external_links": 0,
            "broken_links": [],
            "issues": [],
        }

        try:
            md_files = list(self.docs_dir.rglob("*.md"))

            for md_file in md_files:
                try:
                    with open(md_file, "r", encoding="utf-8") as f:
                        content = f.read()

                    # ç®€å•çš„é“¾æ¥æ£€æŸ¥ï¼ˆå¯ä»¥æ‰©å±•ï¼‰
                    import re

                    # æŸ¥æ‰¾Markdowné“¾æ¥
                    link_pattern = r"\[([^\]]+)\]\(([^)]+)\)"
                    matches = re.findall(link_pattern, content)

                    for text, url in matches:
                        result["total_links"] += 1

                        if url.startswith("http"):
                            result["external_links"] += 1
                        elif url.startswith("#") or url.startswith("./") or url.startswith("../"):
                            result["internal_links"] += 1
                        else:
                            result["issues"].append(f"ä¸æ˜ç¡®çš„é“¾æ¥æ ¼å¼: {url} in {md_file}")

                except Exception as e:
                    result["issues"].append(f"é“¾æ¥éªŒè¯å¤±è´¥ {md_file}: {e}")

        except Exception as e:
            result["issues"].append(f"é“¾æ¥éªŒè¯é”™è¯¯: {e}")

        return result

    def _validate_docs_config(self) -> Dict[str, Any]:
        """éªŒè¯æ–‡æ¡£é…ç½®"""
        result = {
            "config_valid": False,
            "theme_configured": False,
            "plugins_configured": False,
            "nav_configured": False,
            "issues": [],
        }

        try:
            # æ£€æŸ¥é…ç½®åŸºæœ¬ç»“æ„
            if "theme" in self.config:
                result["theme_configured"] = True
                if self.config["theme"].get("name") == "material":
                    result["theme_name"] = "material"
                else:
                    result["issues"].append("å»ºè®®ä½¿ç”¨materialä¸»é¢˜")
            else:
                result["issues"].append("ç¼ºå°‘ä¸»é¢˜é…ç½®")

            # æ£€æŸ¥æ’ä»¶é…ç½®
            if "plugins" in self.config:
                result["plugins_configured"] = True
            else:
                result["issues"].append("ç¼ºå°‘æ’ä»¶é…ç½®")

            # æ£€æŸ¥å¯¼èˆªé…ç½®
            if "nav" in self.config:
                result["nav_configured"] = True
            else:
                result["issues"].append("ç¼ºå°‘å¯¼èˆªé…ç½®")

            result["config_valid"] = len(result["issues"]) == 0

        except Exception as e:
            result["issues"].append(f"é…ç½®éªŒè¯é”™è¯¯: {e}")

        return result

    async def _stage_build(self) -> None:
        """é˜¶æ®µ4: æ„å»ºæ–‡æ¡£"""
        stage_name = "build"
        logger.info("ğŸ”¨ é˜¶æ®µ4: æ„å»ºæ–‡æ¡£")

        stage_result = {"start_time": datetime.now().isoformat(), "build": {}}

        try:
            # æ¸…ç†ä¹‹å‰çš„æ„å»º
            if self.site_dir.exists():
                shutil.rmtree(self.site_dir)
                logger.info("ğŸ§¹ æ¸…ç†ä¹‹å‰çš„æ„å»ºæ–‡ä»¶")

            # æ‰§è¡ŒMkDocsæ„å»º
            cmd = ["mkdocs", "build", "--verbose"]
            result = subprocess.run(cmd, cwd=self.project_root, capture_output=True, text=True)

            if result.returncode == 0:
                # ç»Ÿè®¡æ„å»ºç»“æœ
                html_files = list(self.site_dir.rglob("*.html"))
                css_files = list(self.site_dir.rglob("*.css"))
                js_files = list(self.site_dir.rglob("*.js"))

                stage_result["build"] = {
                    "success": True,
                    "html_pages": len(html_files),
                    "css_files": len(css_files),
                    "js_files": len(js_files),
                    "site_size": self._get_directory_size(self.site_dir),
                    "build_output": result.stdout[-500:] if result.stdout else "",
                    "build_time": datetime.now().isoformat(),
                }

                logger.info(f"âœ… æ„å»ºæˆåŠŸ: {len(html_files)} HTMLé¡µé¢")
            else:
                stage_result["build"] = {
                    "success": False,
                    "error": result.stderr,
                    "return_code": result.returncode,
                }
                raise Exception(f"MkDocsæ„å»ºå¤±è´¥: {result.stderr}")

            stage_result["success"] = True
            stage_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            stage_result["success"] = False
            stage_result["error"] = str(e)
            stage_result["end_time"] = datetime.now().isoformat()
            raise

        self.pipeline_state["stages"][stage_name] = stage_result
        logger.info("âœ… æ–‡æ¡£æ„å»ºå®Œæˆ")

    def _get_directory_size(self, directory: Path) -> str:
        """è·å–ç›®å½•å¤§å°"""
        try:
            result = subprocess.run(["du", "-sh", str(directory)], capture_output=True, text=True)
            if result.returncode == 0:
                return result.stdout.split()[0]
        except Exception:
            pass
        return "æœªçŸ¥"

    async def _stage_quality_check(self) -> None:
        """é˜¶æ®µ5: è´¨é‡æ£€æŸ¥"""
        stage_name = "quality_check"
        logger.info("ğŸ“ˆ é˜¶æ®µ5: è´¨é‡æ£€æŸ¥")

        stage_result = {"start_time": datetime.now().isoformat(), "quality": {}}

        try:
            # æ£€æŸ¥é¡µé¢å¤§å°
            stage_result["quality"]["page_sizes"] = self._check_page_sizes()

            # æ£€æŸ¥å›¾ç‰‡ä¼˜åŒ–
            stage_result["quality"]["images"] = self._check_images()

            # æ£€æŸ¥SEOä¼˜åŒ–
            stage_result["quality"]["seo"] = self._check_seo()

            # æ£€æŸ¥æ€§èƒ½
            stage_result["quality"]["performance"] = self._check_performance()

            stage_result["success"] = True
            stage_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            stage_result["success"] = False
            stage_result["error"] = str(e)
            stage_result["end_time"] = datetime.now().isoformat()
            raise

        self.pipeline_state["stages"][stage_name] = stage_result
        logger.info("âœ… è´¨é‡æ£€æŸ¥å®Œæˆ")

    def _check_page_sizes(self) -> Dict[str, Any]:
        """æ£€æŸ¥é¡µé¢å¤§å°"""
        result = {"total_pages": 0, "large_pages": [], "average_size": 0, "max_size": 0}

        try:
            html_files = list(self.site_dir.rglob("*.html"))
            sizes = []

            for html_file in html_files:
                size = html_file.stat().st_size
                sizes.append(size)

                if size > 50000:  # å¤§äº50KB
                    result["large_pages"].append(
                        {"file": str(html_file.relative_to(self.site_dir)), "size": size}
                    )

            result["total_pages"] = len(html_files)
            if sizes:
                result["average_size"] = sum(sizes) / len(sizes)
                result["max_size"] = max(sizes)

        except Exception as e:
            logger.error(f"é¡µé¢å¤§å°æ£€æŸ¥å¤±è´¥: {e}")

        return result

    def _check_images(self) -> Dict[str, Any]:
        """æ£€æŸ¥å›¾ç‰‡"""
        result = {"total_images": 0, "image_types": {}, "large_images": []}

        try:
            image_extensions = [".png", ".jpg", ".jpeg", ".gif", ".svg"]
            image_files = []

            for ext in image_extensions:
                files = list(self.site_dir.rglob(f"*{ext}"))
                image_files.extend(files)
                result["image_types"][ext.lstrip(".")] = len(files)

            result["total_images"] = len(image_files)

            # æ£€æŸ¥å¤§å›¾ç‰‡
            for img_file in image_files:
                size = img_file.stat().st_size
                if size > 1024 * 1024:  # å¤§äº1MB
                    result["large_images"].append(
                        {"file": str(img_file.relative_to(self.site_dir)), "size": size}
                    )

        except Exception as e:
            logger.error(f"å›¾ç‰‡æ£€æŸ¥å¤±è´¥: {e}")

        return result

    def _check_seo(self) -> Dict[str, Any]:
        """æ£€æŸ¥SEOä¼˜åŒ–"""
        result = {
            "pages_with_title": 0,
            "pages_with_description": 0,
            "pages_with_h1": 0,
            "issues": [],
        }

        try:
            html_files = list(self.site_dir.rglob("*.html"))

            for html_file in html_files[:10]:  # æ£€æŸ¥å‰10ä¸ªé¡µé¢ä½œä¸ºæ ·æœ¬
                try:
                    with open(html_file, "r", encoding="utf-8") as f:
                        content = f.read()

                    if "<title>" in content:
                        result["pages_with_title"] += 1

                    if 'name="description"' in content:
                        result["pages_with_description"] += 1

                    if "<h1" in content:
                        result["pages_with_h1"] += 1

                except Exception as e:
                    result["issues"].append(f"SEOæ£€æŸ¥å¤±è´¥ {html_file}: {e}")

        except Exception as e:
            logger.error(f"SEOæ£€æŸ¥å¤±è´¥: {e}")

        return result

    def _check_performance(self) -> Dict[str, Any]:
        """æ£€æŸ¥æ€§èƒ½"""
        result = {"css_files": 0, "js_files": 0, "external_resources": 0, "issues": []}

        try:
            result["css_files"] = len(list(self.site_dir.rglob("*.css")))
            result["js_files"] = len(list(self.site_dir.rglob("*.js")))

            # ç®€å•çš„å¤–éƒ¨èµ„æºæ£€æŸ¥
            index_file = self.site_dir / "index.html"
            if index_file.exists():
                with open(index_file, "r", encoding="utf-8") as f:
                    content = f.read()

                import re

                external_links = re.findall(r'https?://[^\s"\'<>]+', content)
                result["external_resources"] = len(external_links)

        except Exception as e:
            result["issues"].append(f"æ€§èƒ½æ£€æŸ¥å¤±è´¥: {e}")

        return result

    async def _stage_deploy(self, deploy_target: str) -> None:
        """é˜¶æ®µ6: éƒ¨ç½²æ–‡æ¡£"""
        stage_name = "deploy"
        logger.info(f"ğŸš€ é˜¶æ®µ6: éƒ¨ç½²åˆ° {deploy_target}")

        stage_result = {"start_time": datetime.now().isoformat(), "deploy": {}}

        try:
            if deploy_target == "github-pages":
                await self._deploy_to_github_pages(stage_result)
            elif deploy_target == "s3":
                await self._deploy_to_s3(stage_result)
            elif deploy_target == "local":
                await self._deploy_local(stage_result)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„éƒ¨ç½²ç›®æ ‡: {deploy_target}")

            stage_result["success"] = True
            stage_result["end_time"] = datetime.now().isoformat()

        except Exception as e:
            stage_result["success"] = False
            stage_result["error"] = str(e)
            stage_result["end_time"] = datetime.now().isoformat()
            raise

        self.pipeline_state["stages"][stage_name] = stage_result
        logger.info("âœ… æ–‡æ¡£éƒ¨ç½²å®Œæˆ")

    async def _deploy_to_github_pages(self, stage_result: Dict[str, Any]) -> None:
        """éƒ¨ç½²åˆ°GitHub Pages"""
        try:
            # æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒä¸­
            if os.getenv("GITHUB_ACTIONS"):
                logger.info("ğŸ“ æ£€æµ‹åˆ°GitHub Actionsç¯å¢ƒï¼Œä½¿ç”¨å†…ç½®éƒ¨ç½²")
                stage_result["deploy"] = {
                    "method": "github_actions",
                    "url": f"https://{os.getenv('GITHUB_REPOSITORY_OWNER')}.github.io/{os.getenv('GITHUB_REPOSITORY_NAME')}/",
                }
            else:
                # æœ¬åœ°éƒ¨ç½²åˆ°gh-pagesåˆ†æ”¯
                logger.info("ğŸ“ æœ¬åœ°éƒ¨ç½²åˆ°gh-pagesåˆ†æ”¯")

                # æ£€æŸ¥gh-pagesåˆ†æ”¯æ˜¯å¦å­˜åœ¨
                result = subprocess.run(["git", "branch", "-a"], capture_output=True, text=True)

                if "gh-pages" not in result.stdout:
                    # åˆ›å»ºgh-pagesåˆ†æ”¯
                    subprocess.run(["git", "checkout", "--orphan", "gh-pages"], check=True)
                    subprocess.run(["git", "rm", "-rf", "."], check=True)
                else:
                    subprocess.run(["git", "checkout", "gh-pages"], check=True)

                # å¤åˆ¶æ„å»ºæ–‡ä»¶
                if self.site_dir.exists():
                    subprocess.run(["cp", "-r", f"{self.site_dir}/.", "."], check=True)

                # æäº¤å¹¶æ¨é€
                subprocess.run(["git", "add", "."], check=True)
                subprocess.run(["git", "commit", "-m", "Deploy documentation"], check=True)
                subprocess.run(["git", "push", "origin", "gh-pages"], check=True)

                stage_result["deploy"] = {"method": "local_gh_pages", "branch": "gh-pages"}

        except Exception as e:
            raise Exception(f"GitHub Pageséƒ¨ç½²å¤±è´¥: {e}")

    async def _deploy_to_s3(self, stage_result: Dict[str, Any]) -> None:
        """éƒ¨ç½²åˆ°S3"""
        # è¿™é‡Œå¯ä»¥å®ç°S3éƒ¨ç½²é€»è¾‘
        logger.warning("S3éƒ¨ç½²åŠŸèƒ½å°šæœªå®ç°")
        stage_result["deploy"] = {"method": "s3", "status": "not_implemented"}

    async def _deploy_local(self, stage_result: Dict[str, Any]) -> None:
        """æœ¬åœ°éƒ¨ç½²"""
        try:
            # åˆ›å»ºæœ¬åœ°éƒ¨ç½²ç›®å½•
            deploy_dir = self.project_root / "deploy" / "docs"
            deploy_dir.mkdir(parents=True, exist_ok=True)

            # å¤åˆ¶æ–‡ä»¶
            if self.site_dir.exists():
                shutil.rmtree(deploy_dir)
                shutil.copytree(self.site_dir, deploy_dir)

            stage_result["deploy"] = {"method": "local", "path": str(deploy_dir)}

            logger.info(f"ğŸ“ æ–‡æ¡£å·²éƒ¨ç½²åˆ°æœ¬åœ°: {deploy_dir}")

        except Exception as e:
            raise Exception(f"æœ¬åœ°éƒ¨ç½²å¤±è´¥: {e}")

    def save_pipeline_report(self, output_path: Optional[str] = None) -> str:
        """
        ä¿å­˜æµæ°´çº¿æŠ¥å‘Š

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„

        Returns:
            æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = self.build_dir / f"pipeline_report_{timestamp}.json"

        try:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(self.pipeline_state, f, indent=2, ensure_ascii=False, default=str)

            logger.info(f"ğŸ“„ æµæ°´çº¿æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
            return str(output_path)

        except Exception as e:
            logger.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
            raise

    def print_pipeline_summary(self) -> None:
        """æ‰“å°æµæ°´çº¿æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ–‡æ¡£CI/CDæµæ°´çº¿æ‰§è¡Œæ‘˜è¦")
        print("=" * 60)

        print(f"å¼€å§‹æ—¶é—´: {self.pipeline_state.get('start_time')}")
        print(f"ç»“æŸæ—¶é—´: {self.pipeline_state.get('end_time')}")
        print(f"æ‰§è¡ŒçŠ¶æ€: {'âœ… æˆåŠŸ' if self.pipeline_state.get('success') else 'âŒ å¤±è´¥'}")

        if self.pipeline_state.get("errors"):
            print("\nâŒ é”™è¯¯ä¿¡æ¯:")
            for error in self.pipeline_state["errors"]:
                print(f"  - {error}")

        if self.pipeline_state.get("warnings"):
            print("\nâš ï¸ è­¦å‘Šä¿¡æ¯:")
            for warning in self.pipeline_state["warnings"]:
                print(f"  - {warning}")

        print("\nğŸ“‹ é˜¶æ®µæ‰§è¡Œæƒ…å†µ:")
        for stage_name, stage_result in self.pipeline_state.get("stages", {}).items():
            status = "âœ… æˆåŠŸ" if stage_result.get("success") else "âŒ å¤±è´¥"
            print(f"  - {stage_name}: {status}")

        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ–‡æ¡£CI/CDæµæ°´çº¿")
    parser.add_argument("--build", action="store_true", help="æ„å»ºæ–‡æ¡£")
    parser.add_argument("--deploy", action="store_true", help="éƒ¨ç½²æ–‡æ¡£")
    parser.add_argument("--quality-check", action="store_true", help="è´¨é‡æ£€æŸ¥")
    parser.add_argument(
        "--deploy-target",
        choices=["github-pages", "s3", "local"],
        default="github-pages",
        help="éƒ¨ç½²ç›®æ ‡",
    )
    parser.add_argument("--config", default="mkdocs.yml", help="MkDocsé…ç½®æ–‡ä»¶")
    parser.add_argument("--report-output", help="æŠ¥å‘Šè¾“å‡ºè·¯å¾„")
    parser.add_argument("--full-pipeline", action="store_true", help="æ‰§è¡Œå®Œæ•´æµæ°´çº¿")

    args = parser.parse_args()

    # åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pipeline = DocsCIPipeline(args.config)

    # æ‰§è¡Œæµæ°´çº¿
    if args.full_pipeline:
        result = await pipeline.run_full_pipeline(
            build=args.build or True,
            deploy=args.deploy,
            quality_check=args.quality_check,
            deploy_target=args.deploy_target,
        )
    else:
        # é»˜è®¤æ‰§è¡Œå®Œæ•´æµæ°´çº¿
        result = await pipeline.run_full_pipeline(
            build=True, deploy=args.deploy, quality_check=True, deploy_target=args.deploy_target
        )

    # ä¿å­˜æŠ¥å‘Š
    pipeline.save_pipeline_report(args.report_output)

    # æ‰“å°æ‘˜è¦
    pipeline.print_pipeline_summary()

    # è®¾ç½®é€€å‡ºç 
    sys.exit(0 if result.get("success") else 1)


if __name__ == "__main__":
    asyncio.run(main())
