#!/usr/bin/env python3
"""
ç”Ÿäº§ç¯å¢ƒå‹åŠ›æµ‹è¯•å’Œæ€§èƒ½éªŒè¯è„šæœ¬
Production Environment Stress Testing and Performance Validation Script

Phase G Week 5 Day 2 - å‹åŠ›æµ‹è¯•å’Œæ€§èƒ½éªŒè¯
"""

import asyncio
import aiohttp
import time
import json
import statistics
import argparse
import sys
import signal
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import logging
import random
import string

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('stress_test.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    base_url: str = "http://localhost:8000"
    concurrent_users: int = 1000
    duration: int = 300  # 5åˆ†é’Ÿ
    ramp_up_time: int = 30  # 30ç§’æ¸è¿›åŠ è½½
    requests_per_second: int = 100
    timeout: int = 10
    think_time: float = 0.1  # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    url: str
    status_code: int
    response_time: float
    timestamp: datetime
    success: bool
    error_message: Optional[str] = None

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡"""
    total_requests: int
    successful_requests: int
    failed_requests: int
    avg_response_time: float
    min_response_time: float
    max_response_time: float
    p50_response_time: float
    p90_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    error_rate: float

class StressTester:
    """å‹åŠ›æµ‹è¯•å™¨"""

    def __init__(self, config: TestConfig):
        self.config = config
        self.results: List[TestResult] = []
        self.start_time = None
        self.end_time = None
        self.stop_event = asyncio.Event()

        # æµ‹è¯•ç«¯ç‚¹
        self.test_endpoints = [
            "/health",
            "/api/v1/predictions/simple",
            "/api/v1/teams",
            "/api/v1/matches",
            "/api/v1/user/profile"
        ]

        # æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36"
        ]

    async def setup_session(self) -> aiohttp.ClientSession:
        """è®¾ç½®HTTPä¼šè¯"""
        timeout = aiohttp.ClientTimeout(total=self.config.timeout)
        connector = aiohttp.TCPConnector(
            limit=1000,  # è¿æ¥æ± å¤§å°
            limit_per_host=500,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )

        return aiohttp.ClientSession(
            timeout=timeout,
            connector=connector
        )

    def generate_test_data(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        return {
            "home_team": "Team " + ''.join(random.choices(string.ascii_uppercase, k=3)),
            "away_team": "Team " + ''.join(random.choices(string.ascii_uppercase, k=3)),
            "match_date": "2024-01-01",
            "competition": "Test League"
        }

    async def single_request(self, session: aiohttp.ClientSession, endpoint: str, user_id: int) -> TestResult:
        """æ‰§è¡Œå•ä¸ªè¯·æ±‚"""
        url = f"{self.config.base_url}{endpoint}"
        start_time = time.time()

        headers = {
            "User-Agent": random.choice(self.user_agents),
            "X-User-ID": str(user_id),
            "Content-Type": "application/json"
        }

        try:
            if endpoint == "/api/v1/predictions/simple":
                # POSTè¯·æ±‚ï¼Œå‘é€æµ‹è¯•æ•°æ®
                data = self.generate_test_data()
                async with session.post(url, json=data, headers=headers) as response:
                    content = await response.text()
                    response_time = time.time() - start_time
                    return TestResult(
                        url=url,
                        status_code=response.status,
                        response_time=response_time,
                        timestamp=datetime.now(),
                        success=response.status < 400,
                        error_message=None if response.status < 400 else f"HTTP {response.status}"
                    )
            else:
                # GETè¯·æ±‚
                async with session.get(url, headers=headers) as response:
                    content = await response.text()
                    response_time = time.time() - start_time
                    return TestResult(
                        url=url,
                        status_code=response.status,
                        response_time=response_time,
                        timestamp=datetime.now(),
                        success=response.status < 400,
                        error_message=None if response.status < 400 else f"HTTP {response.status}"
                    )

        except asyncio.TimeoutError:
            response_time = time.time() - start_time
            return TestResult(
                url=url,
                status_code=0,
                response_time=response_time,
                timestamp=datetime.now(),
                success=False,
                error_message="Timeout"
            )

        except Exception as e:
            response_time = time.time() - start_time
            return TestResult(
                url=url,
                status_code=0,
                response_time=response_time,
                timestamp=datetime.now(),
                success=False,
                error_message=str(e)
            )

    async def user_simulation(self, session: aiohttp.ClientSession, user_id: int, duration: int) -> None:
        """æ¨¡æ‹Ÿå•ä¸ªç”¨æˆ·è¡Œä¸º"""
        start_time = time.time()

        while time.time() - start_time < duration and not self.stop_event.is_set():
            # éšæœºé€‰æ‹©ç«¯ç‚¹
            endpoint = random.choice(self.test_endpoints)

            # æ‰§è¡Œè¯·æ±‚
            result = await self.single_request(session, endpoint, user_id)
            self.results.append(result)

            # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
            if self.config.think_time > 0:
                await asyncio.sleep(self.config.think_time)

    async def ramp_up_users(self, session: aiohttp.ClientSession, total_users: int, ramp_time: int) -> List[asyncio.Task]:
        """æ¸è¿›å¼å¢åŠ ç”¨æˆ·"""
        tasks = []
        users_per_second = total_users / ramp_time

        for i in range(total_users):
            delay = i / users_per_second
            task = asyncio.create_task(
                self.delayed_user_simulation(session, i + 1, delay)
            )
            tasks.append(task)

        return tasks

    async def delayed_user_simulation(self, session: aiohttp.ClientSession, user_id: int, delay: float) -> None:
        """å»¶è¿Ÿå¯åŠ¨ç”¨æˆ·æ¨¡æ‹Ÿ"""
        await asyncio.sleep(delay)
        await self.user_simulation(session, user_id, self.config.duration)

    async def run_stress_test(self) -> None:
        """è¿è¡Œå‹åŠ›æµ‹è¯•"""
        logger.info(f"ğŸš€ å¼€å§‹å‹åŠ›æµ‹è¯•...")
        logger.info(f"ğŸ“Š é…ç½®: {self.config.concurrent_users}å¹¶å‘ç”¨æˆ·, {self.config.duration}ç§’æŒç»­æ—¶é—´")
        logger.info(f"ğŸ¯ ç›®æ ‡: P95å“åº”æ—¶é—´ < 200ms, é”™è¯¯ç‡ < 0.1%")

        self.start_time = datetime.now()

        # åˆ›å»ºHTTPä¼šè¯
        async with await self.setup_session() as session:
            # æ¸è¿›å¼åŠ è½½ç”¨æˆ·
            tasks = await self.ramp_up_users(
                session,
                self.config.concurrent_users,
                self.config.ramp_up_time
            )

            logger.info(f"âœ… å·²å¯åŠ¨{len(tasks)}ä¸ªå¹¶å‘ç”¨æˆ·ä»»åŠ¡")

            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            try:
                await asyncio.gather(*tasks, return_exceptions=True)
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
            finally:
                self.stop_event.set()

        self.end_time = datetime.now()
        logger.info(f"âœ… å‹åŠ›æµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {(self.end_time - self.start_time).total_seconds():.2f}ç§’")

    def calculate_metrics(self) -> PerformanceMetrics:
        """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
        if not self.results:
            raise ValueError("æ²¡æœ‰æµ‹è¯•ç»“æœ")

        successful_results = [r for r in self.results if r.success]
        failed_results = [r for r in self.results if not r.success]

        response_times = [r.response_time for r in successful_results]

        if not response_times:
            raise ValueError("æ²¡æœ‰æˆåŠŸçš„è¯·æ±‚")

        total_duration = (self.end_time - self.start_time).total_seconds()

        return PerformanceMetrics(
            total_requests=len(self.results),
            successful_requests=len(successful_results),
            failed_requests=len(failed_results),
            avg_response_time=statistics.mean(response_times),
            min_response_time=min(response_times),
            max_response_time=max(response_times),
            p50_response_time=statistics.quantiles(response_times, n=100)[49],
            p90_response_time=statistics.quantiles(response_times, n=100)[89],
            p95_response_time=statistics.quantiles(response_times, n=100)[94],
            p99_response_time=statistics.quantiles(response_times, n=100)[98],
            requests_per_second=len(self.results) / total_duration,
            error_rate=len(failed_results) / len(self.results)
        )

    def evaluate_performance(self, metrics: PerformanceMetrics) -> Dict[str, Any]:
        """è¯„ä¼°æ€§èƒ½è¡¨ç°"""
        evaluation = {
            "overall_grade": "A+",
            "passed_all_tests": True,
            "issues": [],
            "recommendations": []
        }

        # P95å“åº”æ—¶é—´æ£€æŸ¥ (ç›®æ ‡: < 200ms)
        if metrics.p95_response_time < 100:
            evaluation["overall_grade"] = "A+"
            evaluation["recommendations"].append("âœ… P95å“åº”æ—¶é—´ä¼˜ç§€ (< 100ms)")
        elif metrics.p95_response_time < 200:
            evaluation["overall_grade"] = "A"
            evaluation["recommendations"].append("âœ… P95å“åº”æ—¶é—´è‰¯å¥½ (< 200ms)")
        elif metrics.p95_response_time < 500:
            evaluation["overall_grade"] = "B"
            evaluation["overall_grade"] = "B"
            evaluation["issues"].append("âš ï¸ P95å“åº”æ—¶é—´ä¸€èˆ¬ (> 200ms)")
            evaluation["recommendations"].append("è€ƒè™‘ä¼˜åŒ–APIå“åº”æ—¶é—´")
        else:
            evaluation["overall_grade"] = "C"
            evaluation["passed_all_tests"] = False
            evaluation["issues"].append("âŒ P95å“åº”æ—¶é—´è¿‡æ…¢ (> 500ms)")
            evaluation["recommendations"].append("å¿…é¡»ä¼˜åŒ–APIå“åº”æ—¶é—´")

        # é”™è¯¯ç‡æ£€æŸ¥ (ç›®æ ‡: < 0.1%)
        error_rate_percent = metrics.error_rate * 100
        if error_rate_percent < 0.01:
            evaluation["recommendations"].append("âœ… é”™è¯¯ç‡ä¼˜ç§€ (< 0.01%)")
        elif error_rate_percent < 0.1:
            evaluation["recommendations"].append("âœ… é”™è¯¯ç‡è‰¯å¥½ (< 0.1%)")
        elif error_rate_percent < 1.0:
            evaluation["overall_grade"] = "B" if evaluation["overall_grade"] == "A+" else evaluation["overall_grade"]
            evaluation["issues"].append(f"âš ï¸ é”™è¯¯ç‡åé«˜ ({error_rate_percent:.2f}%)")
            evaluation["recommendations"].append("æ£€æŸ¥ç³»ç»Ÿç¨³å®šæ€§")
        else:
            evaluation["overall_grade"] = "C"
            evaluation["passed_all_tests"] = False
            evaluation["issues"].append(f"âŒ é”™è¯¯ç‡è¿‡é«˜ ({error_rate_percent:.2f}%)")
            evaluation["recommendations"].append("å¿…é¡»ä¿®å¤ç³»ç»Ÿé”™è¯¯")

        # RPSæ£€æŸ¥
        if metrics.requests_per_second >= 100:
            evaluation["recommendations"].append(f"âœ… ååé‡ä¼˜ç§€ ({metrics.requests_per_second:.1f} RPS)")
        elif metrics.requests_per_second >= 50:
            evaluation["recommendations"].append(f"âœ… ååé‡è‰¯å¥½ ({metrics.requests_per_second:.1f} RPS)")
        else:
            evaluation["overall_grade"] = "B" if evaluation["overall_grade"] == "A+" else evaluation["overall_grade"]
            evaluation["issues"].append(f"âš ï¸ ååé‡è¾ƒä½ ({metrics.requests_per_second:.1f} RPS)")
            evaluation["recommendations"].append("è€ƒè™‘ä¼˜åŒ–ç³»ç»Ÿååé‡")

        return evaluation

    def generate_report(self, metrics: PerformanceMetrics, evaluation: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        return {
            "test_info": {
                "start_time": self.start_time.isoformat(),
                "end_time": self.end_time.isoformat(),
                "duration_seconds": (self.end_time - self.start_time).total_seconds(),
                "configuration": {
                    "concurrent_users": self.config.concurrent_users,
                    "duration": self.config.duration,
                    "ramp_up_time": self.config.ramp_up_time,
                    "base_url": self.config.base_url
                }
            },
            "performance_metrics": {
                "total_requests": metrics.total_requests,
                "successful_requests": metrics.successful_requests,
                "failed_requests": metrics.failed_requests,
                "success_rate_percent": (metrics.successful_requests / metrics.total_requests) * 100,
                "error_rate_percent": metrics.error_rate * 100,
                "requests_per_second": metrics.requests_per_second,
                "response_times": {
                    "avg_ms": metrics.avg_response_time * 1000,
                    "min_ms": metrics.min_response_time * 1000,
                    "max_ms": metrics.max_response_time * 1000,
                    "p50_ms": metrics.p50_response_time * 1000,
                    "p90_ms": metrics.p90_response_time * 1000,
                    "p95_ms": metrics.p95_response_time * 1000,
                    "p99_ms": metrics.p99_response_time * 1000
                }
            },
            "evaluation": evaluation,
            "endpoint_analysis": self.analyze_endpoints()
        }

    def analyze_endpoints(self) -> Dict[str, Any]:
        """åˆ†æå„ç«¯ç‚¹æ€§èƒ½"""
        endpoint_stats = {}

        for result in self.results:
            endpoint = result.url.split('/')[-1] or 'root'
            if endpoint not in endpoint_stats:
                endpoint_stats[endpoint] = {
                    "requests": 0,
                    "successful": 0,
                    "failed": 0,
                    "response_times": []
                }

            endpoint_stats[endpoint]["requests"] += 1
            if result.success:
                endpoint_stats[endpoint]["successful"] += 1
                endpoint_stats[endpoint]["response_times"].append(result.response_time)
            else:
                endpoint_stats[endpoint]["failed"] += 1

        # è®¡ç®—æ¯ä¸ªç«¯ç‚¹çš„ç»Ÿè®¡ä¿¡æ¯
        for endpoint, stats in endpoint_stats.items():
            if stats["response_times"]:
                stats["avg_response_time"] = statistics.mean(stats["response_times"])
                stats["p95_response_time"] = statistics.quantiles(stats["response_times"], n=100)[94]
                stats["success_rate"] = stats["successful"] / stats["requests"]
            else:
                stats["avg_response_time"] = 0
                stats["p95_response_time"] = 0
                stats["success_rate"] = 0

        return endpoint_stats

    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"stress_test_report_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"ğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

    def print_summary(self, metrics: PerformanceMetrics, evaluation: Dict[str, Any]) -> None:
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ å‹åŠ›æµ‹è¯•ç»“æœæ‘˜è¦")
        print("="*60)

        print(f"ğŸ“Š åŸºç¡€æŒ‡æ ‡:")
        print(f"   æ€»è¯·æ±‚æ•°: {metrics.total_requests:,}")
        print(f"   æˆåŠŸè¯·æ±‚: {metrics.successful_requests:,}")
        print(f"   å¤±è´¥è¯·æ±‚: {metrics.failed_requests:,}")
        print(f"   æˆåŠŸç‡: {(metrics.successful_requests/metrics.total_requests)*100:.2f}%")
        print(f"   ååé‡: {metrics.requests_per_second:.1f} RPS")

        print(f"\nâš¡ å“åº”æ—¶é—´:")
        print(f"   å¹³å‡: {metrics.avg_response_time*1000:.1f}ms")
        print(f"   P50: {metrics.p50_response_time*1000:.1f}ms")
        print(f"   P90: {metrics.p90_response_time*1000:.1f}ms")
        print(f"   P95: {metrics.p95_response_time*1000:.1f}ms")
        print(f"   P99: {metrics.p99_response_time*1000:.1f}ms")

        print(f"\nğŸ† æ€§èƒ½è¯„ä¼°:")
        print(f"   æ€»ä½“è¯„çº§: {evaluation['overall_grade']}")
        print(f"   æµ‹è¯•ç»“æœ: {'âœ… é€šè¿‡' if evaluation['passed_all_tests'] else 'âŒ å¤±è´¥'}")

        if evaluation['issues']:
            print(f"\nâš ï¸ å‘ç°é—®é¢˜:")
            for issue in evaluation['issues']:
                print(f"   - {issue}")

        if evaluation['recommendations']:
            print(f"\nğŸ’¡ å»ºè®®:")
            for rec in evaluation['recommendations']:
                print(f"   - {rec}")

        print("="*60)

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ç”Ÿäº§ç¯å¢ƒå‹åŠ›æµ‹è¯•å·¥å…·")
    parser.add_argument("--url", default="http://localhost:8000", help="æµ‹è¯•URL")
    parser.add_argument("--users", type=int, default=1000, help="å¹¶å‘ç”¨æˆ·æ•°")
    parser.add_argument("--duration", type=int, default=300, help="æµ‹è¯•æŒç»­æ—¶é—´(ç§’)")
    parser.add_argument("--ramp-up", type=int, default=30, help="æ¸è¿›åŠ è½½æ—¶é—´(ç§’)")
    parser.add_argument("--rps", type=int, default=100, help="æ¯ç§’è¯·æ±‚æ•°")
    parser.add_argument("--timeout", type=int, default=10, help="è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)")

    args = parser.parse_args()

    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        base_url=args.url,
        concurrent_users=args.users,
        duration=args.duration,
        ramp_up_time=args.ramp_up,
        requests_per_second=args.rps,
        timeout=args.timeout
    )

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = StressTester(config)

    # è®¾ç½®ä¿¡å·å¤„ç†
    def signal_handler(signum, frame):
        logger.info("ğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­...")
        tester.stop_event.set()

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    try:
        # è¿è¡Œå‹åŠ›æµ‹è¯•
        await tester.run_stress_test()

        # è®¡ç®—æŒ‡æ ‡
        metrics = tester.calculate_metrics()

        # è¯„ä¼°æ€§èƒ½
        evaluation = tester.evaluate_performance(metrics)

        # ç”ŸæˆæŠ¥å‘Š
        report = tester.generate_report(metrics, evaluation)

        # ä¿å­˜æŠ¥å‘Š
        report_file = tester.save_report(report)

        # æ‰“å°æ‘˜è¦
        tester.print_summary(metrics, evaluation)

        # è¿”å›ç»“æœ
        if evaluation['passed_all_tests']:
            logger.info("ğŸ‰ å‹åŠ›æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿæ€§èƒ½æ»¡è¶³è¦æ±‚ã€‚")
            return 0
        else:
            logger.error("âŒ å‹åŠ›æµ‹è¯•å¤±è´¥ï¼ç³»ç»Ÿæ€§èƒ½éœ€è¦ä¼˜åŒ–ã€‚")
            return 1

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        return 1

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)