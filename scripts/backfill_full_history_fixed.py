#!/usr/bin/env python3
"""
å…¨å†å²æ•°æ®å›å¡«è„šæœ¬ - å®‰å…¨åŠ å›ºç‰ˆ (å·²ä¿®å¤å¼‚æ­¥è°ƒç”¨é”™è¯¯)
Full Historical Data Backfill Script - Security Hardened Edition (Fixed Async Call Error)

# Strategy: Newest -> Oldest | Concurrency: 4 (Safe Mode)
ä½¿ç”¨ Super Greedy é‡‡é›†å™¨å¯¹è¿‡å» 5 å¹´ (2020-2025) çš„å…¨çƒæ ¸å¿ƒèµ›äº‹è¿›è¡Œåœ°æ¯¯å¼é‡‡é›†ã€‚
å…·å¤‡æ–­ç‚¹ç»­ä¼ ã€æ™ºèƒ½é£æ§ã€429é¿éšœç­‰ä¼ä¸šçº§å®‰å…¨ç‰¹æ€§ã€‚

Security Features:
- ğŸ›¡ï¸ æ™ºèƒ½é£æ§é™çº§ (4å¹¶å‘ + 1-3ç§’å»¶è¿Ÿ)
- ğŸ”„ å€’åºå›å¡«ç­–ç•¥ (ä¼˜å…ˆè¿‘æœŸé«˜ä»·å€¼æ•°æ®)
- ğŸš¨ æ™ºèƒ½429é¿éšœ (è‡ªåŠ¨å†·å´+é‡è¯•)
- â¯ï¸ æ–­ç‚¹ç»­ä¼ æœºåˆ¶ (æ”¯æŒéšæ—¶ä¸­æ–­/ç»§ç»­)
- ğŸ“Š å®æ—¶è¿›åº¦ç›‘æ§
- ğŸ”§ ç¡¬ç¼–ç è¡¥ä¸æœºåˆ¶

Author: DevOps & Security Engineer
Version: 2.1.1 Security Hardened Edition (Fixed)
Date: 2025-01-08

ğŸ”§ ä¿®å¤å†…å®¹:
- ä¿®å¤äº† initialize_database() çš„é”™è¯¯å¼‚æ­¥è°ƒç”¨
- ä¿®å¤äº† _apply_hardcoded_patches() çš„é”™è¯¯å¼‚æ­¥å£°æ˜
- ç¡®ä¿åŒæ­¥/å¼‚æ­¥æ–¹æ³•æ­£ç¡®åŒ¹é…
"""

import asyncio
import json
import logging
import sys
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from random import uniform

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("backfill_full_history.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥è¿›åº¦æ¡
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    logger.warning("âš ï¸ tqdmæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•è¿›åº¦æ˜¾ç¤º")

# å¯¼å…¥é¡¹ç›®æ¨¡å—
try:
    from collectors.fotmob_api_collector import FotMobAPICollector
    from database.async_manager import get_db_session, initialize_database
    from database.models.match import Match
    COLLECTOR_AVAILABLE = True
except ImportError as e:
    logger.error(f"âŒ æ— æ³•å¯¼å…¥é‡‡é›†å™¨æ¨¡å—: {e}")
    COLLECTOR_AVAILABLE = False

# é…ç½®å¸¸é‡
DATABASE_URL = os.getenv(
    "DATABASE_URL",
    "postgresql://postgres:postgres@localhost:5432/football_prediction"
)

# ğŸ—ï¸ ç¡¬ç¼–ç è¡¥ä¸ - é«˜ä»·å€¼è”èµ› ID
HARDCODED_PATCHES = {
    "Championship": 48,      # è‹±å†  - å…³é”®æ¬¡çº§è”èµ›
    "Liga Portugal": 61,     # è‘¡è¶… - è‘¡è„ç‰™é¡¶çº§è”èµ›
}

# æ—¶é—´æœºå™¨é…ç½® - å€’åºå›å¡«ç­–ç•¥ (ä¼˜å…ˆè¿‘æœŸé«˜ä»·å€¼æ•°æ®)
YEARS_TO_BACKFILL = [2025, 2024, 2023, 2022, 2021, 2020]  # æ–° -> æ—§
CONCURRENT_LIMIT = 4  # é£æ§é™çº§ï¼šå®‰å…¨å¹¶å‘æ•°
MIN_DELAY = 1.0  # é£æ§é™çº§ï¼šæœ€å°å»¶è¿Ÿ
MAX_DELAY = 3.0  # é£æ§é™çº§ï¼šæœ€å¤§å»¶è¿Ÿ
RATE_LIMIT_COOLDOWN = 60  # 429 è§¦å‘æ—¶çš„å†·å´æ—¶é—´(ç§’)

# æ´²é™…è”èµ›é…ç½® (ç”¨äºèµ›å­£æ ¼å¼åˆ¤æ–­)
EUROPEAN_COUNTRIES = {
    "England", "Spain", "Germany", "Italy", "France",
    "Netherlands", "Portugal", "Belgium", "Scotland",
    "Turkey", "Russia", "Ukraine", "Poland", "Czech Republic",
    "Austria", "Switzerland", "Denmark", "Norway", "Sweden"
}

AMERICAN_COUNTRIES = {
    "USA", "Brazil", "Argentina", "Mexico", "Chile", "Colombia",
    "Peru", "Uruguay", "Paraguay", "Ecuador", "Bolivia", "Venezuela"
}

ASIAN_COUNTRIES = {
    "Japan", "South Korea", "China", "Australia", "Saudi Arabia",
    "UAE", "Qatar", "Iran", "Iraq", "Jordan"
}

@dataclass
class BackfillStats:
    """å›å¡«ç»Ÿè®¡ä¿¡æ¯"""
    total_matches: int = 0
    processed_matches: int = 0
    skipped_matches: int = 0
    successful_matches: int = 0
    failed_matches: int = 0
    start_time: datetime = None
    errors_by_type: Dict[str, int] = None

    def __post_init__(self):
        if self.errors_by_type is None:
            self.errors_by_type = {}
        if self.start_time is None:
            self.start_time = datetime.now()

    @property
    def progress_percentage(self) -> float:
        """è®¡ç®—è¿›åº¦ç™¾åˆ†æ¯”"""
        if self.total_matches == 0:
            return 0.0
        return (self.processed_matches / self.total_matches) * 100

    @property
    def success_rate(self) -> float:
        """è®¡ç®—æˆåŠŸç‡"""
        if self.processed_matches == 0:
            return 0.0
        return (self.successful_matches / self.processed_matches) * 100

    @property
    def elapsed_time(self) -> timedelta:
        """è®¡ç®—å·²ç”¨æ—¶é—´"""
        return datetime.now() - self.start_time

    def log_progress(self):
        """è®°å½•è¿›åº¦æ—¥å¿—"""
        logger.info(
            f"ğŸ“Š è¿›åº¦: {self.processed_matches}/{self.total_matches} "
            f"({self.progress_percentage:.1f}%) | "
            f"âœ… æˆåŠŸ: {self.successful_matches} | "
            f"â­ï¸ è·³è¿‡: {self.skipped_matches} | "
            f"âŒ å¤±è´¥: {self.failed_matches} | "
            f"ğŸ“ˆ æˆåŠŸç‡: {self.success_rate:.1f}% | "
            f"â±ï¸ å·²ç”¨: {self.elapsed_time}"
        )

class SeasonFormatGenerator:
    """èµ›å­£æ ¼å¼ç”Ÿæˆå™¨ - æ™ºèƒ½å¤„ç†ä¸åŒè”èµ›çš„èµ›å­£æ ¼å¼"""

    @staticmethod
    def generate_season_string(year: int, league_info: Dict[str, Any]) -> List[str]:
        """
        ç”Ÿæˆèµ›å­£å­—ç¬¦ä¸²åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        country = league_info.get("country", "")
        league_type = league_info.get("type", "league")

        season_formats = []

        # æ¬§æ´²è”èµ›ä¸»è¦ä½¿ç”¨è·¨å¹´æ ¼å¼
        if country in EUROPEAN_COUNTRIES:
            if league_type == "league":
                season_formats.extend([
                    f"{year}/{year + 1}",  # 2023/2024
                    f"{year-1}/{year}",    # 2022/2023 (å¯¹äºå¹´åˆæ¯”èµ›)
                ])
            else:  # æ¯èµ›
                season_formats.extend([
                    str(year),            # 2023
                    f"{year}/{year + 1}",  # 2023/2024
                ])

        # ç¾æ´²è”èµ›ä¸»è¦ä½¿ç”¨å•å¹´æ ¼å¼
        elif country in AMERICAN_COUNTRIES:
            season_formats.extend([
                str(year),            # 2023
                f"{year}/{year + 1}",  # å¤‡é€‰è·¨å¹´æ ¼å¼
            ])

        # äºšæ´²è”èµ›æ··åˆä½¿ç”¨
        elif country in ASIAN_COUNTRIES:
            if country in ["Japan", "South Korea"]:  # è·¨å¹´èµ›å­£
                season_formats.extend([
                    f"{year}/{year + 1}",
                    str(year),
                ])
            else:  # å•å¹´èµ›å­£
                season_formats.extend([
                    str(year),
                    f"{year}/{year + 1}",
                ])

        # å›½é™…èµ›äº‹
        elif league_type == "cup" and "International" in country:
            season_formats.extend([
                str(year),
                f"{year}/{year + 1}",
            ])

        # é»˜è®¤æ ¼å¼
        else:
            season_formats.extend([
                f"{year}/{year + 1}",
                str(year),
            ])

        # å»é‡å¹¶è¿”å›
        return list(dict.fromkeys(season_formats))

class IndustrialBackfillEngine:
    """å·¥ä¸šçº§å›å¡«å¼•æ“"""

    def __init__(self):
        self.stats = BackfillStats()
        self.semaphore = asyncio.Semaphore(CONCURRENT_LIMIT)
        self.collector = None
        self.processed_match_ids: Set[str] = set()
        self.league_cache: Dict[int, Dict[str, Any]] = {}

    async def initialize(self):
        """åˆå§‹åŒ–å›å¡«å¼•æ“"""
        logger.info("ğŸš€ åˆå§‹åŒ–å·¥ä¸šçº§å›å¡«å¼•æ“...")

        if not COLLECTOR_AVAILABLE:
            raise RuntimeError("âŒ é‡‡é›†å™¨æ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•ç»§ç»­")

        # ğŸ”§ ä¿®å¤: initialize_database() æ˜¯åŒæ­¥å‡½æ•°ï¼Œä¸éœ€è¦ await
        initialize_database()

        # åˆå§‹åŒ–é‡‡é›†å™¨
        self.collector = FotMobAPICollector(
            max_concurrent=CONCURRENT_LIMIT,
            timeout=60,
            max_retries=3,
            base_delay=1.0,
            enable_proxy=False,  # å›å¡«æ—¶ç¦ç”¨ä»£ç†ä»¥æé«˜é€Ÿåº¦
            enable_jitter=True
        )

        await self.collector.initialize()

        # é¢„åŠ è½½å·²å¤„ç†è¿‡çš„æ¯”èµ›ID
        await self._preload_processed_matches()

        logger.info("âœ… å·¥ä¸šçº§å›å¡«å¼•æ“åˆå§‹åŒ–å®Œæˆ")

    async def _preload_processed_matches(self):
        """é¢„åŠ è½½å·²å¤„ç†çš„æ¯”èµ›ID"""
        logger.info("ğŸ“‹ é¢„åŠ è½½å·²å¤„ç†çš„æ¯”èµ›ID...")

        try:
            async with get_db_session() as session:
                result = await session.execute(
                    "SELECT fotmob_id FROM matches WHERE status = 'finished' AND fotmob_id IS NOT NULL"
                )
                matches = result.fetchall()
                self.processed_match_ids = {match[0] for match in matches}
                logger.info(f"âœ… å·²åŠ è½½ {len(self.processed_match_ids)} ä¸ªå·²å¤„ç†æ¯”èµ›ID")

        except Exception as e:
            logger.warning(f"âš ï¸ é¢„åŠ è½½æ¯”èµ›IDå¤±è´¥ï¼Œå°†è·³è¿‡æ–­ç‚¹ç»­ä¼ : {e}")
            self.processed_match_ids = set()

    async def load_league_config(self) -> List[Dict[str, Any]]:
        """åŠ è½½è”èµ›é…ç½®å¹¶åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸"""
        logger.info("ğŸ“‹ åŠ è½½è”èµ›é…ç½®...")

        config_path = project_root / "config" / "target_leagues.json"

        if not config_path.exists():
            logger.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return []

        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            leagues = config.get("leagues", [])
            logger.info(f"âœ… ä»é…ç½®æ–‡ä»¶åŠ è½½äº† {len(leagues)} ä¸ªè”èµ›")

            # ğŸ—ï¸ åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸
            # ğŸ”§ ä¿®å¤: _apply_hardcoded_patches() æ˜¯åŒæ­¥å‡½æ•°ï¼Œä¸éœ€è¦ await
            patched_leagues = self._apply_hardcoded_patches(leagues)
            logger.info(f"ğŸ”§ åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸å: {len(patched_leagues)} ä¸ªè”èµ›")

            return patched_leagues

        except Exception as e:
            logger.error(f"âŒ åŠ è½½è”èµ›é…ç½®å¤±è´¥: {e}")
            return []

    # ğŸ”§ ä¿®å¤: è¿™ä¸ªæ–¹æ³•æ²¡æœ‰å¼‚æ­¥æ“ä½œï¼Œæ”¹ä¸ºåŒæ­¥å‡½æ•°
    def _apply_hardcoded_patches(self, leagues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸"""
        logger.info("ğŸ”§ åº”ç”¨ç¡¬ç¼–ç è¡¥ä¸...")

        existing_names = {league.get("name") for league in leagues}
        existing_ids = {league.get("id") for league in leagues}

        for league_name, league_id in HARDCODED_PATCHES.items():
            if league_name not in existing_names and league_id not in existing_ids:
                # æ·»åŠ ç¡¬ç¼–ç çš„è”èµ›
                patch_league = {
                    "name": league_name,
                    "id": league_id,
                    "tier": 2,  # é»˜è®¤ä¸ºäºŒçº§è”èµ›
                    "country": "England" if league_name == "Championship" else "Portugal",
                    "type": "league",
                    "source": "hardcoded_patch"
                }
                leagues.append(patch_league)
                logger.info(f"ğŸ”§ æ·»åŠ ç¡¬ç¼–ç è¡¥ä¸è”èµ›: {league_name} (ID: {league_id})")
            else:
                logger.debug(f"â„¹ï¸ è”èµ›å·²å­˜åœ¨ï¼Œè·³è¿‡è¡¥ä¸: {league_name}")

        return leagues

    # å…¶ä½™æ–¹æ³•ä¿æŒä¸å˜...
    async def fetch_league_matches(self, league_id: int, season: str) -> List[str]:
        """è·å–è”èµ›æŒ‡å®šèµ›å­£çš„æ¯”èµ›IDåˆ—è¡¨"""
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨ FotMob API è·å–è”èµ›èµ›ç¨‹
            # ç”±äºAPIé™åˆ¶ï¼Œè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®

            # æ¨¡æ‹Ÿè·å–æ¯”èµ›ID
            logger.debug(f"ğŸ” è·å–è”èµ› {league_id} èµ›å­£ {season} çš„æ¯”èµ›åˆ—è¡¨...")

            # æ¨¡æ‹Ÿæ•°æ® - å®é™…å®ç°ä¸­éœ€è¦æ›¿æ¢ä¸ºçœŸå®çš„APIè°ƒç”¨
            match_ids = [f"{league_id}_{i}_{hash(season) % 10000}" for i in range(1, 50)]  # æ¨¡æ‹Ÿ49åœºæ¯”èµ›

            # éšæœºå»¶è¿Ÿæ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
            await asyncio.sleep(uniform(0.1, 0.3))

            logger.info(f"âœ… è”èµ› {league_id} èµ›å­£ {season}: æ‰¾åˆ° {len(match_ids)} åœºæ¯”èµ›")
            return match_ids

        except Exception as e:
            logger.error(f"âŒ è·å–è”èµ› {league_id} èµ›å­£ {season} æ¯”èµ›åˆ—è¡¨å¤±è´¥: {e}")
            return []

    async def process_match(self, match_id: str, league_info: Dict[str, Any]) -> bool:
        """å¤„ç†å•ä¸ªæ¯”èµ› - æ™ºèƒ½429é¿éšœç‰ˆ"""
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘
            try:
                # æ–­ç‚¹ç»­ä¼ æ£€æŸ¥
                if match_id in self.processed_match_ids:
                    logger.debug(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨æ¯”èµ›: {match_id}")
                    self.stats.skipped_matches += 1
                    return True

                # æ™ºèƒ½é‡‡é›†æ¯”èµ›æ•°æ® (å«429é¿éšœ)
                logger.debug(f"ğŸ”„ æ­£åœ¨é‡‡é›†: {match_id}")
                match_data = await self._collect_with_429_protection(match_id)

                if match_data:
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    await self._save_match_data(match_data, league_info)
                    self.stats.successful_matches += 1
                    logger.debug(f"âœ… æˆåŠŸå¤„ç†: {match_id}")
                    return True
                else:
                    self.stats.failed_matches += 1
                    self.stats.errors_by_type["collection_failed"] = self.stats.errors_by_type.get("collection_failed", 0) + 1
                    return False

            except Exception as e:
                self.stats.failed_matches += 1
                error_type = type(e).__name__
                self.stats.errors_by_type[error_type] = self.stats.errors_by_type.get(error_type, 0) + 1
                logger.error(f"âŒ å¤„ç†æ¯”èµ› {match_id} å¤±è´¥: {e}")
                return False

            finally:
                # å®‰å…¨æµé‡æ§åˆ¶
                await asyncio.sleep(uniform(MIN_DELAY, MAX_DELAY))

    async def _collect_with_429_protection(self, match_id: str):
        """æ™ºèƒ½429é¿éšœçš„æ•°æ®é‡‡é›†æ–¹æ³•"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # å°è¯•é‡‡é›†æ•°æ®
                return await self.collector.collect_match_details(match_id)

            except Exception as e:
                error_str = str(e).lower()

                # æ£€æŸ¥æ˜¯å¦ä¸º429é”™è¯¯
                if "429" in error_str or "too many requests" in error_str or "rate limit" in error_str:
                    logger.warning(f"âš ï¸ Rate Limit Hit! Cooling down for {RATE_LIMIT_COOLDOWN}s... (Attempt {attempt + 1}/{max_retries})")
                    self.stats.errors_by_type["rate_limit_429"] = self.stats.errors_by_type.get("rate_limit_429", 0) + 1

                    # å¼ºåˆ¶å†·å´
                    await asyncio.sleep(RATE_LIMIT_COOLDOWN)

                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç»§ç»­é‡è¯•
                    if attempt < max_retries - 1:
                        logger.info(f"ğŸ”„ Retrying after cooldown...")
                        continue
                    else:
                        logger.error(f"âŒ Max retries exceeded for {match_id} after 429 errors")
                        return None

                else:
                    # å…¶ä»–é”™è¯¯ç›´æ¥æŠ›å‡º
                    logger.debug(f"ğŸ”„ Non-429 error, will be handled by caller: {e}")
                    raise

        return None

    async def _save_match_data(self, match_data, league_info: Dict[str, Any]):
        """ä¿å­˜æ¯”èµ›æ•°æ®åˆ°æ•°æ®åº“"""
        try:
            async with get_db_session() as session:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = await session.execute(
                    f"SELECT id FROM matches WHERE fotmob_id = '{match_data.fotmob_id}'"
                )
                if existing.fetchone():
                    # æ›´æ–°ç°æœ‰è®°å½•
                    update_query = f"""
                    UPDATE matches SET
                        stats_json = :stats_json,
                        lineups_json = :lineups_json,
                        odds_snapshot_json = :odds_snapshot_json,
                        match_info = :match_info,
                        environment_json = :environment_json,
                        updated_at = NOW()
                    WHERE fotmob_id = :fotmob_id
                    """
                    await session.execute(update_query, {
                        "stats_json": json.dumps(match_data.stats_json) if match_data.stats_json else None,
                        "lineups_json": json.dumps(match_data.lineups_json) if match_data.lineups_json else None,
                        "odds_snapshot_json": json.dumps(match_data.odds_snapshot_json) if match_data.odds_snapshot_json else None,
                        "match_info": json.dumps(match_data.match_info) if match_data.match_info else None,
                        "environment_json": json.dumps(match_data.environment_json) if match_data.environment_json else None,
                        "fotmob_id": match_data.fotmob_id
                    })
                else:
                    # æ’å…¥æ–°è®°å½•
                    insert_query = f"""
                    INSERT INTO matches (
                        fotmob_id, home_score, away_score, status, match_time,
                        venue, attendance, referee,
                        stats_json, lineups_json, odds_snapshot_json,
                        match_info, environment_json, created_at, updated_at
                    ) VALUES (
                        :fotmob_id, :home_score, :away_score, :status, :match_time,
                        :venue, :attendance, :referee,
                        :stats_json, :lineups_json, :odds_snapshot_json,
                        :match_info, :environment_json, NOW(), NOW()
                    )
                    """
                    await session.execute(insert_query, {
                        "fotmob_id": match_data.fotmob_id,
                        "home_score": match_data.home_score,
                        "away_score": match_data.away_score,
                        "status": match_data.status,
                        "match_time": match_data.match_time,
                        "venue": match_data.venue,
                        "attendance": match_data.attendance,
                        "referee": match_data.referee,
                        "stats_json": json.dumps(match_data.stats_json) if match_data.stats_json else None,
                        "lineups_json": json.dumps(match_data.lineups_json) if match_data.lineups_json else None,
                        "odds_snapshot_json": json.dumps(match_data.odds_snapshot_json) if match_data.odds_snapshot_json else None,
                        "match_info": json.dumps(match_data.match_info) if match_data.match_info else None,
                        "environment_json": json.dumps(match_data.environment_json) if match_data.environment_json else None,
                    })

                await session.commit()
                self.processed_match_ids.add(match_data.fotmob_id)

        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æ¯”èµ›æ•°æ®å¤±è´¥ {match_data.fotmob_id}: {e}")
            raise

    async def run_backfill(self):
        """è¿è¡Œå®Œæ•´å›å¡«æµç¨‹"""
        logger.info("ğŸš€ å¯åŠ¨å…¨å†å²æ•°æ®å›å¡«...")

        try:
            # åŠ è½½è”èµ›é…ç½®
            leagues = await self.load_league_config()
            if not leagues:
                logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„è”èµ›é…ç½®ï¼Œé€€å‡º")
                return False

            # ç”Ÿæˆå›å¡«ä»»åŠ¡
            backfill_tasks = await self._generate_backfill_tasks(leagues)
            if not backfill_tasks:
                logger.error("âŒ æ²¡æœ‰ç”Ÿæˆå›å¡«ä»»åŠ¡ï¼Œé€€å‡º")
                return False

            self.stats.total_matches = len(backfill_tasks)
            logger.info(f"ğŸ“Š æ€»è®¡éœ€è¦å¤„ç† {self.stats.total_matches} åœºæ¯”èµ›")

            # æ‰§è¡Œå›å¡«ä»»åŠ¡
            await self._execute_backfill_tasks(backfill_tasks)

            # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
            await self._print_final_stats()

            return True

        except Exception as e:
            logger.error(f"âŒ å›å¡«æµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return False

    async def _generate_backfill_tasks(self, leagues: List[Dict[str, Any]]) -> List[Tuple[str, Dict[str, Any]]]:
        """ç”Ÿæˆå›å¡«ä»»åŠ¡åˆ—è¡¨"""
        logger.info("ğŸ“‹ ç”Ÿæˆå›å¡«ä»»åŠ¡...")

        tasks = []

        for league in leagues:
            league_id = league.get("id")
            league_name = league.get("name", "Unknown")

            logger.info(f"ğŸ” å¤„ç†è”èµ›: {league_name} (ID: {league_id})")

            for year in YEARS_TO_BACKFILL:
                # ç”Ÿæˆèµ›å­£æ ¼å¼
                season_formats = SeasonFormatGenerator.generate_season_string(year, league)

                for season in season_formats:
                    try:
                        # è·å–è¯¥èµ›å­£çš„æ¯”èµ›åˆ—è¡¨
                        match_ids = await self.fetch_league_matches(league_id, season)

                        for match_id in match_ids:
                            tasks.append((match_id, league))

                    except Exception as e:
                        logger.warning(f"âš ï¸ è·å–è”èµ› {league_name} èµ›å­£ {season} å¤±è´¥: {e}")
                        continue

        logger.info(f"âœ… ç”Ÿæˆå›å¡«ä»»åŠ¡: {len(tasks)} ä¸ª")
        return tasks

    async def _execute_backfill_tasks(self, tasks: List[Tuple[str, Dict[str, Any]]]):
        """æ‰§è¡Œå›å¡«ä»»åŠ¡"""
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå›å¡«ä»»åŠ¡...")

        # åˆ›å»ºè¿›åº¦æ¡
        if TQDM_AVAILABLE:
            pbar = tqdm(total=len(tasks), desc="å›å¡«è¿›åº¦", unit="æ¯”èµ›")

        # æ‰¹é‡å¤„ç†ä»»åŠ¡
        batch_size = 50
        processed_count = 0

        for i in range(0, len(tasks), batch_size):
            batch = tasks[i:i + batch_size]

            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            batch_tasks = [
                self.process_match(match_id, league_info)
                for match_id, league_info in batch
            ]

            # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
            results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            # ç»Ÿè®¡ç»“æœ
            for result in results:
                self.stats.processed_matches += 1
                processed_count += 1

                if isinstance(result, Exception):
                    logger.error(f"âŒ ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {result}")
                    self.stats.failed_matches += 1

                # æ›´æ–°è¿›åº¦æ¡
                if TQDM_AVAILABLE:
                    pbar.update(1)

            # æ¯50ä¸ªæ¯”èµ›æ‰“å°ä¸€æ¬¡è¿›åº¦
            if processed_count % 50 == 0:
                self.stats.log_progress()

        # å…³é—­è¿›åº¦æ¡
        if TQDM_AVAILABLE:
            pbar.close()

    async def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ å…¨å†å²æ•°æ®å›å¡«å®Œæˆ")
        logger.info("="*60)
        logger.info(f"ğŸ“Š æ€»æ¯”èµ›æ•°: {self.stats.total_matches}")
        logger.info(f"âœ… æˆåŠŸå¤„ç†: {self.stats.successful_matches}")
        logger.info(f"â­ï¸ è·³è¿‡å·²å­˜åœ¨: {self.stats.skipped_matches}")
        logger.info(f"âŒ å¤„ç†å¤±è´¥: {self.stats.failed_matches}")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {self.stats.success_rate:.1f}%")
        logger.info(f"â±ï¸ æ€»ç”¨æ—¶: {self.stats.elapsed_time}")

        if self.stats.errors_by_type:
            logger.info("\nğŸš¨ é”™è¯¯ç»Ÿè®¡:")
            for error_type, count in self.stats.errors_by_type.items():
                # ç‰¹åˆ«é«˜äº®429é”™è¯¯
                if "rate_limit_429" in error_type:
                    logger.info(f"  ğŸš¨ {error_type}: {count} (è§¦å‘äº†æ™ºèƒ½å†·å´)")
                else:
                    logger.info(f"  ğŸ“Š {error_type}: {count}")

            # é£æ§çŠ¶æ€æŠ¥å‘Š
            rate_429_count = self.stats.errors_by_type.get("rate_limit_429", 0)
            if rate_429_count > 0:
                total_429_cooldown = rate_429_count * RATE_LIMIT_COOLDOWN
                logger.info(f"\nğŸ›¡ï¸ é£æ§æŠ¥å‘Š:")
                logger.info(f"  429è§¦å‘æ¬¡æ•°: {rate_429_count}")
                logger.info(f"  æ€»å†·å´æ—¶é—´: {total_429_cooldown//60}åˆ†{total_429_cooldown%60}ç§’")
                logger.info(f"  å¹³å‡å¤„ç†é€Ÿåº¦: ~{self.stats.successful_matches / max(1, (self.stats.elapsed_time.total_seconds() - total_429_cooldown) / 3600):.0f}åœº/å°æ—¶ (ä¸å«å†·å´æ—¶é—´)")
            else:
                logger.info(f"\nğŸ›¡ï¸ é£æ§æŠ¥å‘Š: æœªè§¦å‘429é™åˆ¶ï¼Œå®‰å…¨è¿è¡Œ")

        logger.info("="*60)
        logger.info("ğŸ‰ å…¨å†å²æ•°æ®å›å¡«ä»»åŠ¡å®Œæˆ!")

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.collector:
            await self.collector.close()
        logger.info("ğŸ§¹ èµ„æºæ¸…ç†å®Œæˆ")

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸ›¡ï¸ å¯åŠ¨å®‰å…¨åŠ å›ºç‰ˆå…¨å†å²æ•°æ®å›å¡«è„šæœ¬ (å·²ä¿®å¤å¼‚æ­¥è°ƒç”¨é”™è¯¯)")
    logger.info(f"ğŸ“Š ç­–ç•¥é…ç½®: å€’åºå›å¡« ({YEARS_TO_BACKFILL[0]} -> {YEARS_TO_BACKFILL[-1]})")
    logger.info(f"ğŸ”’ é£æ§è®¾ç½®: {CONCURRENT_LIMIT}å¹¶å‘ + {MIN_DELAY}-{MAX_DELAY}ç§’å»¶è¿Ÿ")
    logger.info(f"ğŸš¨ 429é¿éšœ: {RATE_LIMIT_COOLDOWN}ç§’è‡ªåŠ¨å†·å´ + 3æ¬¡é‡è¯•")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    if not os.getenv("DATABASE_URL"):
        os.environ["DATABASE_URL"] = DATABASE_URL

    # åˆ›å»ºå›å¡«å¼•æ“
    engine = IndustrialBackfillEngine()

    try:
        # åˆå§‹åŒ–
        await engine.initialize()

        # æ‰§è¡Œå›å¡«
        success = await engine.run_backfill()

        if success:
            logger.info("âœ… å®‰å…¨åŠ å›ºç‰ˆå…¨å†å²æ•°æ®å›å¡«ä»»åŠ¡æˆåŠŸå®Œæˆ!")
            sys.exit(0)
        else:
            logger.error("âŒ å®‰å…¨åŠ å›ºç‰ˆå…¨å†å²æ•°æ®å›å¡«ä»»åŠ¡å¤±è´¥!")
            sys.exit(1)

    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦...")
        engine.stats.log_progress()
        sys.exit(130)

    except Exception as e:
        logger.error(f"âŒ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)

    finally:
        await engine.cleanup()

if __name__ == "__main__":
    # è¿è¡Œä¸»ç¨‹åº
    asyncio.run(main())