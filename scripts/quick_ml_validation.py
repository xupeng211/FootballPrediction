#!/usr/bin/env python3
"""
å¿«é€ŸMLéªŒè¯è„šæœ¬ - Issue #120æ ¸å¿ƒéªŒè¯
éªŒè¯XGBoost/LightGBMç¯å¢ƒé…ç½®ä¸æ€§èƒ½

åŸºäºSRSæˆåŠŸç»éªŒè¿›è¡Œå¿«é€ŸéªŒè¯ï¼š
- SRS XGBoostè®­ç»ƒï¼š68%å‡†ç¡®ç‡ï¼Œ1500ä¸ªSRSå…¼å®¹æ•°æ®ç‚¹
- 45ä¸ªç‰¹å¾å·¥ç¨‹ï¼Œ48ä¸ªåŸå§‹ç‰¹å¾
"""

import json
import logging
import time
from datetime import datetime
from typing import Any

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# æ£€æŸ¥MLåº“å¯ç”¨æ€§
try:
    import xgboost as xgb

    XGB_AVAILABLE = True
    logger.info("âœ… XGBoostå¯ç”¨")
except ImportError:
    XGB_AVAILABLE = False
    logger.warning("âŒ XGBoostä¸å¯ç”¨")

try:
    import lightgbm as lgb

    LGB_AVAILABLE = True
    logger.info("âœ… LightGBMå¯ç”¨")
except ImportError:
    LGB_AVAILABLE = False
    logger.warning("âŒ LightGBMä¸å¯ç”¨")

# SRSåŸºå‡†æ•°æ®
SRS_BASELINE = {
    "accuracy": 0.68,
    "data_points": 1500,
    "features": 45,
    "distribution": {"draw": 791, "home_win": 480, "away_win": 229},
}


def generate_test_data(n_samples: int = 1000) -> tuple[pd.DataFrame, pd.Series]:
    """ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼ŒåŸºäºSRSç»éªŒ"""
    logger.info(f"ç”Ÿæˆæµ‹è¯•æ•°æ®ï¼Œæ ·æœ¬æ•°: {n_samples}")

    np.random.seed(42)

    # åŸºäºSRSçš„ç‰¹å¾å·¥ç¨‹
    feature_names = [
        "home_team_strength",
        "away_team_strength",
        "home_form",
        "away_form",
        "head_to_head_home",
        "head_to_head_away",
        "home_goals_scored",
        "away_goals_scored",
        "home_goals_conceded",
        "away_goals_conceded",
        "home_win_rate",
        "away_win_rate",
        "home_draw_rate",
        "away_draw_rate",
        "home_loss_rate",
        "away_loss_rate",
        "home_clean_sheets",
        "away_clean_sheets",
        "home_failed_to_score",
        "away_failed_to_score",
        "avg_total_goals",
        "avg_home_goals",
        "avg_away_goals",
        "home_shots_on_target",
        "away_shots_on_target",
        "home_corners",
        "away_corners",
        "home_fouls",
        "away_fouls",
        "home_yellow_cards",
        "away_yellow_cards",
        "home_red_cards",
        "away_red_cards",
        "home_possession",
        "away_possession",
        "travel_distance",
        "rest_days",
        "weather_impact",
        "referee_tendency",
        "stadium_advantage",
        "season_phase",
        "motivation_factor",
        "injury_impact",
        "market_odds_home",
        "market_odds_draw",
        "market_odds_away",
        "betting_volume",
        "momentum_home",
        "momentum_away",
        "xg_home",
        "xg_away",
        "xga_home",
        "xga_away",
    ]

    X = np.random.randn(n_samples, len(feature_names))
    X_df = pd.DataFrame(X, columns=feature_names)

    # åŸºäºSRSåˆ†å¸ƒç”Ÿæˆç»“æœ
    probabilities = [0.53, 0.32, 0.15]  # draw, home_win, away_win
    results = np.random.choice(["draw",
    "home_win",
    "away_win"],
    size=n_samples,
    p=probabilities)
    y = pd.Series(results)

    logger.info(f"æ•°æ®åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    return X_df, y


def test_xgboost(
    X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series
) -> dict[str, Any]:
    """æµ‹è¯•XGBoostæ¨¡å‹"""
    if not XGB_AVAILABLE:
        return {"error": "XGBoostä¸å¯ç”¨"}

    logger.info("ğŸš€ æµ‹è¯•XGBoostæ¨¡å‹...")

    start_time = time.time()

    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    label_encoder = LabelEncoder()

    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    # XGBoostæ¨¡å‹
    model = xgb.XGBClassifier(
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42,
        eval_metric="mlogloss",
        use_label_encoder=False,
    )

    model.fit(X_train_scaled, y_train_encoded)
    training_time = time.time() - start_time

    # é¢„æµ‹
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test_encoded, y_pred)

    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(X_train.columns, model.feature_importances_, strict=False))
    top_features = sorted(feature_importance.items(),
    key=lambda x: x[1],
    reverse=True)[:10]

    result = {
        "model": "XGBoost",
        "accuracy": accuracy,
        "training_time": training_time,
        "feature_importance": top_features,
        "improvement_over_srs": (accuracy - SRS_BASELINE["accuracy"])
        / SRS_BASELINE["accuracy"]
        * 100,
    }

    logger.info(f"  XGBoostå‡†ç¡®ç‡: {accuracy:.4f} (è®­ç»ƒæ—¶é—´: {training_time:.2f}s)")
    logger.info(f"  ç›¸å¯¹SRSæ”¹è¿›: {result['improvement_over_srs']:+.2f}%")

    return result


def test_lightgbm(
    X_train: pd.DataFrame, X_test: pd.DataFrame, y_train: pd.Series, y_test: pd.Series
) -> dict[str, Any]:
    """æµ‹è¯•LightGBMæ¨¡å‹"""
    if not LGB_AVAILABLE:
        return {"error": "LightGBMä¸å¯ç”¨"}

    logger.info("ğŸš€ æµ‹è¯•LightGBMæ¨¡å‹...")

    start_time = time.time()

    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    label_encoder = LabelEncoder()

    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_test_encoded = label_encoder.transform(y_test)

    # LightGBMæ¨¡å‹
    model = lgb.LGBMClassifier(
        n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, verbose=-1
    )

    model.fit(X_train_scaled, y_train_encoded)
    training_time = time.time() - start_time

    # é¢„æµ‹
    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test_encoded, y_pred)

    # ç‰¹å¾é‡è¦æ€§
    feature_importance = dict(zip(X_train.columns, model.feature_importances_, strict=False))
    top_features = sorted(feature_importance.items(),
    key=lambda x: x[1],
    reverse=True)[:10]

    result = {
        "model": "LightGBM",
        "accuracy": accuracy,
        "training_time": training_time,
        "feature_importance": top_features,
        "improvement_over_srs": (accuracy - SRS_BASELINE["accuracy"])
        / SRS_BASELINE["accuracy"]
        * 100,
    }

    logger.info(f"  LightGBMå‡†ç¡®ç‡: {accuracy:.4f} (è®­ç»ƒæ—¶é—´: {training_time:.2f}s)")
    logger.info(f"  ç›¸å¯¹SRSæ”¹è¿›: {result['improvement_over_srs']:+.2f}%")

    return result


def main():
    """ä¸»éªŒè¯å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸš€ å¿«é€ŸMLéªŒè¯å¼€å§‹")
    logger.info("Issue #120: MLæ¨¡å‹è®­ç»ƒå’ŒçœŸå®æ•°æ®é›†æˆ")
    logger.info("=" * 60)

    # ç¯å¢ƒæ£€æŸ¥
    logger.info("ğŸ“Š ç¯å¢ƒçŠ¶æ€æ£€æŸ¥:")
    logger.info(f"  XGBoost: {'âœ… å¯ç”¨' if XGB_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")
    logger.info(f"  LightGBM: {'âœ… å¯ç”¨' if LGB_AVAILABLE else 'âŒ ä¸å¯ç”¨'}")

    if not XGB_AVAILABLE and not LGB_AVAILABLE:
        logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„MLåº“ï¼ŒéªŒè¯å¤±è´¥")
        return

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    logger.info("ğŸ“Š ç”Ÿæˆæµ‹è¯•æ•°æ®...")
    X, y = generate_test_data(n_samples=1000)

    # æ•°æ®åˆ†å‰²
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    logger.info(f"è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬, æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")

    # æµ‹è¯•æ¨¡å‹
    results = []

    if XGB_AVAILABLE:
        xgb_result = test_xgboost(X_train, X_test, y_train, y_test)
        results.append(xgb_result)

    if LGB_AVAILABLE:
        lgb_result = test_lightgbm(X_train, X_test, y_train, y_test)
        results.append(lgb_result)

    # ç”ŸæˆæŠ¥å‘Š
    logger.info("=" * 60)
    logger.info("ğŸ“‹ éªŒè¯ç»“æœæ±‡æ€»")
    logger.info("=" * 60)

    validation_report = {
        "timestamp": datetime.now().isoformat(),
        "environment": {"xgboost_available": XGB_AVAILABLE, "lightgbm_available": LGB_AVAILABLE},
        "data_info": {
            "total_samples": len(X),
            "training_samples": len(X_train),
            "test_samples": len(X_test),
            "features": len(X.columns),
            "distribution": y.value_counts().to_dict(),
        },
        "srs_baseline": SRS_BASELINE,
        "results": results,
        "summary": {},
    }

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    if results:
        best_result = max(results, key=lambda x: x.get("accuracy", 0))
        validation_report["summary"] = {
            "best_model": best_result["model"],
            "best_accuracy": best_result["accuracy"],
            "improvement_over_srs": best_result["improvement_over_srs"],
            "models_tested": len(results),
        }

        logger.info(f"ğŸ† æœ€ä½³æ¨¡å‹: {best_result['model']}")
        logger.info(f"ğŸ“Š æœ€ä½³å‡†ç¡®ç‡: {best_result['accuracy']:.4f}")
        logger.info(f"ğŸ“ˆ ç›¸å¯¹SRSæ”¹è¿›: {best_result['improvement_over_srs']:+.2f}%")

    # ä¿å­˜ç»“æœ
    try:
        with open("quick_ml_validation_results.json", "w", encoding="utf-8") as f:
            json.dump(validation_report, f, indent=2, ensure_ascii=False, default=str)
        logger.info("ğŸ“„ éªŒè¯ç»“æœå·²ä¿å­˜åˆ° quick_ml_validation_results.json")
    except Exception as e:
        logger.error(f"ä¿å­˜éªŒè¯ç»“æœå¤±è´¥: {e}")

    logger.info("=" * 60)
    logger.info("ğŸ‰ å¿«é€ŸMLéªŒè¯å®Œæˆ!")
    logger.info("=" * 60)

    return validation_report


if __name__ == "__main__":
    results = main()
