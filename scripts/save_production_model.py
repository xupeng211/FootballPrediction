#!/usr/bin/env python3
"""
ç”Ÿäº§æ¨¡å‹åºåˆ—åŒ–è„šæœ¬
ä¿å­˜Phase 2æœ€ç»ˆæ¨¡å‹å·¥ä»¶ä¸ºPhase 3æ¨ç†APIåšå‡†å¤‡

åŠŸèƒ½:
1. è®­ç»ƒæœ€ç»ˆåŸºçº¿æ¨¡å‹
2. ä¿å­˜XGBoostæ¨¡å‹ä¸ºJSONæ ¼å¼
3. ä¿å­˜LabelEncoderç”¨äºé¢„æµ‹è§£ç 
4. ä¿å­˜ç‰¹å¾åˆ—è¡¨ç¡®ä¿è¾“å…¥é¡ºåºä¸€è‡´

ä½œè€…: Chief Architect
åˆ›å»ºæ—¶é—´: 2025-12-10
ç‰ˆæœ¬: 1.0.0 - Production Model v1
"""

import json
import pickle
from datetime import datetime
import logging
from pathlib import Path

# å¯¼å…¥åŸºçº¿è®­ç»ƒå™¨
import sys

sys.path.append(str(Path(__file__).parent))
from train_baseline import BaselineTrainer

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ProductionModelSaver(BaselineTrainer):
    """ç”Ÿäº§æ¨¡å‹ä¿å­˜å™¨"""

    def __init__(self):
        super().__init__()
        self.model_version = "v1.0.0"
        self.model_name = "football_xgboost_baseline"

    def train_production_model(self, data_path: str) -> dict:
        """è®­ç»ƒç”Ÿäº§æ¨¡å‹"""
        logger.info(f"ğŸ­ å¼€å§‹è®­ç»ƒç”Ÿäº§æ¨¡å‹ {self.model_name} {self.model_version}")

        # åŠ è½½å’Œå‡†å¤‡æ•°æ®
        df = self.load_and_prepare_data(data_path)

        # é€‰æ‹©ç‰¹å¾
        X, y, feature_columns = self.select_features(df)

        # æ—¶åºåˆ†å‰²
        X_train, X_test, y_train_enc, y_test_enc, y_train_orig, y_test_orig = (
            self.split_data_chronological(X, y)
        )

        # ä¿å­˜ç‰¹å¾åˆ—è¡¨
        self.feature_columns = feature_columns

        # è®­ç»ƒæ¨¡å‹
        self.train_xgboost(X_train, y_train_enc)

        # è¯„ä¼°æ¨¡å‹
        results = self.evaluate_model(X_test, y_test_enc, y_test_orig)

        # ä¿å­˜æ¨¡å‹å·¥ä»¶
        self.save_model_artifacts(results)

        logger.info("ğŸ‰ ç”Ÿäº§æ¨¡å‹è®­ç»ƒå®Œæˆ!")
        return results

    def save_model_artifacts(self, results: dict):
        """ä¿å­˜æ‰€æœ‰æ¨¡å‹å·¥ä»¶"""
        logger.info("ğŸ’¾ ä¿å­˜ç”Ÿäº§æ¨¡å‹å·¥ä»¶...")

        # åˆ›å»ºmodelsç›®å½•
        models_dir = Path("models")
        models_dir.mkdir(exist_ok=True)

        # 1. ä¿å­˜XGBoostæ¨¡å‹ (JSONæ ¼å¼ - ç”Ÿäº§æ¨è)
        model_json_path = models_dir / f"{self.model_name}.json"
        self.model.save_model(str(model_json_path))
        logger.info(f"   âœ… XGBoostæ¨¡å‹å·²ä¿å­˜: {model_json_path}")

        # 2. ä¿å­˜XGBoostæ¨¡å‹ (pickleæ ¼å¼ - å¤‡ä»½)
        model_pkl_path = models_dir / f"{self.model_name}.pkl"
        with open(model_pkl_path, "wb") as f:
            pickle.dump(self.model, f)
        logger.info(f"   âœ… XGBoostæ¨¡å‹å¤‡ä»½å·²ä¿å­˜: {model_pkl_path}")

        # 3. ä¿å­˜LabelEncoder
        encoder_path = models_dir / f"{self.model_name}_label_encoder.pkl"
        with open(encoder_path, "wb") as f:
            pickle.dump(self.label_encoder, f)
        logger.info(f"   âœ… LabelEncoderå·²ä¿å­˜: {encoder_path}")

        # 4. ä¿å­˜ç‰¹å¾åˆ—è¡¨
        features_path = models_dir / f"{self.model_name}_features.json"
        features_data = {
            "feature_columns": self.feature_columns,
            "feature_count": len(self.feature_columns),
            "model_version": self.model_version,
        }
        with open(features_path, "w") as f:
            json.dump(features_data, f, indent=2)
        logger.info(f"   âœ… ç‰¹å¾åˆ—è¡¨å·²ä¿å­˜: {features_path}")

        # 5. ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
        metadata = {
            "model_name": self.model_name,
            "version": self.model_version,
            "training_date": datetime.now().isoformat(),
            "performance": {
                "accuracy": results["accuracy"],
                "log_loss": results["log_loss"],
                "classification_report": results["classification_report"],
            },
            "model_params": {
                "n_estimators": self.model.n_estimators,
                "max_depth": self.model.max_depth,
                "learning_rate": self.model.learning_rate,
                "objective": self.model.objective,
                "eval_metric": self.model.eval_metric,
            },
            "dataset_info": {
                "training_samples": len(self.feature_columns),
                "feature_count": len(self.feature_columns),
                "target_classes": list(self.label_encoder.classes_),
            },
            "data_leakage_safe": True,
            "feature_engineering": "rolling_features_time_series_safe",
        }

        metadata_path = models_dir / f"{self.model_name}_metadata.json"
        with open(metadata_path, "w") as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"   âœ… æ¨¡å‹å…ƒæ•°æ®å·²ä¿å­˜: {metadata_path}")

        # 6. ä¿å­˜éƒ¨ç½²é…ç½®
        deployment_config = {
            "model_files": {
                "model_json": str(model_json_path.name),
                "model_pkl": str(model_pkl_path.name),
                "label_encoder": str(encoder_path.name),
                "features": str(features_path.name),
                "metadata": str(metadata_path.name),
            },
            "input_schema": {
                "type": "DataFrame",
                "features_required": self.feature_columns,
                "feature_order": "must_match_features_json",
            },
            "output_schema": {
                "prediction": "encoded_class",
                "prediction_label": "H/D/A",
                "probabilities": "dict_class_to_probability",
            },
            "inference": {
                "preprocessing_steps": [
                    "ensure_feature_order",
                    "fill_missing_values_0",
                    "encode_prediction",
                ]
            },
        }

        config_path = models_dir / f"{self.model_name}_deployment_config.json"
        with open(config_path, "w") as f:
            json.dump(deployment_config, f, indent=2)
        logger.info(f"   âœ… éƒ¨ç½²é…ç½®å·²ä¿å­˜: {config_path}")

    def print_production_summary(self, results: dict):
        """æ‰“å°ç”Ÿäº§æ¨¡å‹æ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ† PRODUCTION MODEL SUMMARY")
        print("=" * 80)

        print("\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
        print(f"   åç§°: {self.model_name}")
        print(f"   ç‰ˆæœ¬: {self.model_version}")
        print(f"   è®­ç»ƒæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        print("\nğŸ“Š æ€§èƒ½æŒ‡æ ‡:")
        print(f"   ğŸ¯ å‡†ç¡®ç‡: {results['accuracy']:.4f}")
        print(f"   ğŸ“‰ Log Loss: {results['log_loss']:.4f}")

        print("\nğŸ”§ æ¨¡å‹é…ç½®:")
        print("   ç®—æ³•: XGBoost Classifier")
        print(f"   æ ‘æ•°é‡: {self.model.n_estimators}")
        print(f"   æœ€å¤§æ·±åº¦: {self.model.max_depth}")
        print(f"   å­¦ä¹ ç‡: {self.model.learning_rate}")

        print("\nğŸ“ å·²ä¿å­˜æ–‡ä»¶:")
        models_dir = Path("models")
        for file_path in models_dir.glob(f"{self.model_name}*"):
            size_mb = file_path.stat().st_size / (1024 * 1024)
            print(f"   ğŸ“„ {file_path.name} ({size_mb:.2f} MB)")

        print("\nğŸš€ Phase 2 å®ŒæˆçŠ¶æ€:")
        print("   âœ… æ•°æ®æ³„éœ²ä¿®å¤å®Œæˆ")
        print("   âœ… æ»šåŠ¨ç‰¹å¾å·¥ç¨‹å®Œæˆ")
        print("   âœ… æ—¶åºå®‰å…¨æ¨¡å‹è®­ç»ƒå®Œæˆ")
        print("   âœ… æ¨¡å‹å·¥ä»¶åºåˆ—åŒ–å®Œæˆ")
        print("   âœ… å‡†å¤‡å°±ç»ª: Phase 3 æ¨ç†API")

        print("=" * 80)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ ç”Ÿäº§æ¨¡å‹åºåˆ—åŒ–å¼€å§‹")
    print("Phase 2: Feature Engineering â†’ Production Model")
    print("=" * 60)

    # åˆå§‹åŒ–ç”Ÿäº§æ¨¡å‹ä¿å­˜å™¨
    saver = ProductionModelSaver()

    try:
        # è®­ç»ƒå¹¶ä¿å­˜ç”Ÿäº§æ¨¡å‹
        results = saver.train_production_model(
            data_path="data/processed/features_v2_rolling.csv"
        )

        # æ‰“å°æ‘˜è¦
        saver.print_production_summary(results)

    except Exception as e:
        logger.error(f"âŒ ç”Ÿäº§æ¨¡å‹ä¿å­˜å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()
