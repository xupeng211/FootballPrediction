#!/usr/bin/env python3
"""
å¤©ç½‘è®¡åˆ’ - å…¨åŸŸé‡‡é›†å™¨ (CEOå¼ºåˆ¶ä¿®æ­£ç‰ˆ)
å¼ºåˆ¶ä»æ•°æ®åº“åŠ è½½300+è”èµ›ï¼Œæ— ç¡¬ç¼–ç 
"""

import asyncio
import json
import logging
import random
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.data.collectors.fbref_collector import FBrefCollector
from sqlalchemy import create_engine, text

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("logs/robust_coverage.log"),
        logging.StreamHandler()
    ],
)
logger = logging.getLogger(__name__)


class RobustCoverageCollector:
    """å¤©ç½‘è®¡åˆ’å…¨åŸŸé‡‡é›†å™¨ - CEOå¼ºåˆ¶ç‰ˆ"""

    def __init__(self):
        self.progress_file = "logs/coverage_progress.json"
        self.failed_log_file = "logs/failed_leagues.log"
        self.completed_leagues: Set[str] = set()
        self.failed_leagues: List[Dict] = []

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        Path("logs").mkdir(exist_ok=True)

        # åŠ è½½è¿›åº¦
        self._load_progress()

        # FBrefæ”¶é›†å™¨ - ä½¿ç”¨curl_cffiç‰ˆæœ¬ï¼ˆé¿å…Playwrightä¾èµ–é—®é¢˜ï¼‰
        self.collector = FBrefCollector()

        # æ•°æ®åº“å¼•æ“ - ä½¿ç”¨å®¹å™¨å†…è¿æ¥æˆ–localhost
        # ä¼˜å…ˆå°è¯•å®¹å™¨å†…è¿æ¥ï¼Œå¤±è´¥æ—¶å°è¯•localhost
        try:
            db_url = "postgresql://postgres:postgres-dev-password@db:5432/football_prediction"
            self.engine = create_engine(db_url)
            # æµ‹è¯•è¿æ¥
            with self.engine.connect() as conn:
                conn.execute(text("SELECT 1"))
            logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ (å®¹å™¨å†…)")
        except Exception as e:
            logger.warning(f"âš ï¸ å®¹å™¨å†…è¿æ¥å¤±è´¥ï¼Œå°è¯•localhost: {e}")
            try:
                db_url = "postgresql://postgres:postgres-dev-password@localhost:5432/football_prediction"
                self.engine = create_engine(db_url)
                with self.engine.connect() as conn:
                    conn.execute(text("SELECT 1"))
                logger.info(f"âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ (localhost)")
            except Exception as e2:
                logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e2}")
                raise Exception(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e2}")

        # è”èµ›æ•°æ® - ä»æ•°æ®åº“åŠ¨æ€åŠ è½½æ‰€æœ‰æ´»è·ƒè”èµ›
        logger.info("ğŸ” æ­£åœ¨ä»æ•°æ®åº“åŠ è½½è”èµ›åˆ—è¡¨...")
        self.league_data = self._load_leagues_from_database()

        if not self.league_data:
            logger.error("âŒ æ— æ³•ä»æ•°æ®åº“åŠ è½½è”èµ›åˆ—è¡¨ï¼")
            raise Exception("æ•°æ®åº“è”èµ›åŠ è½½å¤±è´¥")

        logger.info(f"âœ… æˆåŠŸä»æ•°æ®åº“åŠ è½½ {len(self.league_data)} ä¸ªè”èµ›")

        # ä¿å­˜åŠ è½½çš„è”èµ›æ•°é‡åˆ°è¿›åº¦æ–‡ä»¶
        progress_data = {
            "total_leagues": len(self.league_data),
            "loaded_from": "database",
            "load_time": datetime.now().isoformat(),
            "completed_leagues": list(self.completed_leagues),
            "failed_leagues": self.failed_leagues,
        }
        with open(self.progress_file, "w") as f:
            json.dump(progress_data, f, indent=2)
        logger.info(f"ğŸ“Š è”èµ›åˆ—è¡¨å·²ä¿å­˜åˆ°è¿›åº¦æ–‡ä»¶")

    def _load_leagues_from_database(self) -> List[Dict]:
        """
        ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰æ´»è·ƒè”èµ›
        CEOè¦æ±‚ï¼šå¿…é¡»ä½¿ç”¨æ•°æ®åº“ï¼Œä¸èƒ½ç¡¬ç¼–ç 
        """
        import os
        import psycopg2

        # æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        conn_string = os.environ.get('DATABASE_URL', 'postgresql://postgres:postgres@localhost:5432/football_prediction')

        leagues = []

        try:
            # ç›´æ¥è¿æ¥æ•°æ®åº“
            conn = psycopg2.connect(
                host="localhost",
                port=5432,
                user="postgres",
                password="postgres-dev-password",
                database="football_prediction"
            )
            cur = conn.cursor()

            # SQLæŸ¥è¯¢ï¼šè·å–æ‰€æœ‰æœ‰FBref URLçš„è”èµ›
            query = """
                SELECT id, name, fbref_url, tier, category, country
                FROM leagues
                WHERE fbref_url IS NOT NULL
                AND fbref_url != ''
                ORDER BY name
            """

            logger.info("ğŸ“¡ æ‰§è¡ŒSQLæŸ¥è¯¢: è·å–æ‰€æœ‰è”èµ›")
            cur.execute(query)

            rows = cur.fetchall()

            for row in rows:
                league_id, league_name, fbref_url, tier, category, country = row

                # æ„å»ºFBref URL
                if fbref_url.startswith('http'):
                    full_url = fbref_url
                else:
                    # å¤„ç†ç›¸å¯¹URL
                    if 'schedule' not in fbref_url:
                        full_url = f"https://fbref.com{fbref_url}/schedule"
                    else:
                        full_url = f"https://fbref.com{fbref_url}"

                league_info = {
                    'id': league_id,
                    'name': league_name,
                    'url': full_url,
                    'tier': tier,
                    'category': category,
                    'country': country
                }

                leagues.append(league_info)

            cur.close()
            conn.close()

            logger.info(f"âœ… æ•°æ®åº“æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(leagues)} ä¸ªè”èµ›")

            # è®°å½•æ¯ä¸ªè”èµ›çš„åŠ è½½æƒ…å†µ
            for i, league in enumerate(leagues[:10]):  # åªæ‰“å°å‰10ä¸ªä½œä¸ºç¤ºä¾‹
                logger.info(f"  {i+1}. {league['name']} ({league['country']}) - {league['url']}")

            if len(leagues) > 10:
                logger.info(f"  ... è¿˜æœ‰ {len(leagues) - 10} ä¸ªè”èµ›")

            return leagues

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            logger.error("è¯·ç¡®ä¿æ•°æ®åº“æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”è¿æ¥å‚æ•°æ­£ç¡®")
            return []

    def _load_progress(self):
        """åŠ è½½æ–­ç‚¹ç»­ä¼ è¿›åº¦"""
        try:
            if Path(self.progress_file).exists():
                with open(self.progress_file, "r") as f:
                    data = json.load(f)
                    self.completed_leagues = set(data.get("completed_leagues", []))
                    self.failed_leagues = data.get("failed_leagues", [])

                total = data.get("total_leagues", 0)
                logger.info(f"ğŸ“‚ åŠ è½½è¿›åº¦ï¼šå·²å®Œæˆ {len(self.completed_leagues)}/{total} ä¸ªè”èµ›ï¼Œå¤±è´¥ {len(self.failed_leagues)} ä¸ª")
            else:
                logger.info("ğŸ“ é¦–æ¬¡è¿è¡Œï¼Œä»å¤´å¼€å§‹")
                self.completed_leagues = set()
                self.failed_leagues = []
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½è¿›åº¦å¤±è´¥ï¼š{e}")
            self.completed_leagues = set()
            self.failed_leagues = []

    def _save_progress(self):
        """ä¿å­˜è¿›åº¦"""
        try:
            progress_data = {
                "total_leagues": len(self.league_data),
                "completed_leagues": list(self.completed_leagues),
                "failed_leagues": self.failed_leagues,
                "last_update": datetime.now().isoformat(),
            }
            with open(self.progress_file, "w") as f:
                json.dump(progress_data, f, indent=2)
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¿›åº¦å¤±è´¥ï¼š{e}")

    def _log_failure(self, league_id: str, league_name: str, error: str):
        """è®°å½•å¤±è´¥è”èµ›"""
        failure_record = {
            "league_id": league_id,
            "league_name": league_name,
            "error": str(error),
            "timestamp": datetime.now().isoformat(),
        }
        self.failed_leagues.append(failure_record)
        logger.error(f"âŒ è”èµ›å¤±è´¥è®°å½•: {league_name} - {error}")

    async def _wait_between_requests(self):
        """éšæœºä¼‘çœ  15-40 ç§’ - CEOè¦æ±‚ï¼šä¸¥æ ¼æ§åˆ¶é¢‘ç‡"""
        delay = random.uniform(15, 40)
        logger.info(f"â³ ç­‰å¾… {delay:.1f} ç§’ (åçˆ¬è™«ä¿æŠ¤)")
        await asyncio.sleep(delay)

    async def collect_single_league(self, league: Dict) -> bool:
        """
        é‡‡é›†å•ä¸ªè”èµ›
        ä½¿ç”¨FBrefDatabaseSaverå…¥åº“
        """
        league_id = str(league['id'])
        league_name = league['name']
        league_url = league['url']

        # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
        if league_id in self.completed_leagues:
            logger.info(f"â­ï¸ è·³è¿‡å·²å®Œæˆè”èµ›: {league_name}")
            return True

        logger.info(f"\nğŸ† æ­£åœ¨é‡‡é›†è”èµ›: {league_name}")
        logger.info(f"ğŸ“ URL: {league_url}")
        logger.info(f"ğŸ†” ID: {league_id}")

        try:
            # éšæœºä¼‘çœ 
            await self._wait_between_requests()

            # é‡‡é›†æ•°æ®
            logger.info(f"ğŸ“¡ å¼€å§‹é‡‡é›†æ•°æ®...")
            data = await self.collector.get_season_schedule(league_url, season_year=None)

            if data is None or data.empty:
                logger.warning(f"âš ï¸ è”èµ›æ— æ•°æ®: {league_name}")
                self._log_failure(league_id, league_name, "No data returned")
                return False

            logger.info(f"âœ… è·å–åˆ° {len(data)} æ¡æ¯”èµ›è®°å½•")

            # ä½¿ç”¨å¢å¼ºæ•°æ®åº“ä¿å­˜å™¨è¿›è¡ŒUPSERT
            from scripts.enhanced_database_saver import EnhancedDatabaseSaver

            saved_count = 0
            try:
                # åˆå§‹åŒ–å¢å¼ºä¿å­˜å™¨
                saver = EnhancedDatabaseSaver()

                # ç›´æ¥ä¿å­˜DataFrameï¼Œè®©å¢å¼ºä¿å­˜å™¨å¤„ç†æ‰€æœ‰é€»è¾‘
                result = saver.save_matches_dataframe(
                    data,
                    league_name=league_name,
                    season='2025-2026'
                )

                if result['status'] == 'success':
                    saved_count = result['saved_count']
                    logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} åœºæ¯”èµ›: {league_name}")
                    self.completed_leagues.add(league_id)
                    return True
                else:
                    logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {result['message']}")
                    return False

            except Exception as e:
                logger.error(f"âŒ æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
                self._log_failure(league_id, league_name, f"Database save failed: {str(e)}")
                return False

            # æœ€ç»ˆæ£€æŸ¥
            if saved_count > 0:
                logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} åœºæ¯”èµ›: {league_name}")
                self.completed_leagues.add(league_id)
                return True
            else:
                logger.warning(f"âš ï¸ æ— æ–°æ•°æ®ä¿å­˜: {league_name}")
                self._log_failure(league_id, league_name, "No new data to save")
                return False

        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            logger.error(f"âŒ é‡‡é›†è”èµ›å¤±è´¥: {league_name} - {error_msg}")
            self._log_failure(league_id, league_name, error_msg)
            return False

    async def collect_with_403_retry(self, league: Dict, max_retries: int = 3):
        """
        å¸¦403é‡è¯•çš„é‡‡é›†
        """
        for attempt in range(max_retries):
            try:
                success = await self.collect_single_league(league)
                return success
            except Exception as e:
                if "403" in str(e) or "Forbidden" in str(e):
                    if attempt < max_retries - 1:
                        wait_time = (2 ** attempt) * 60
                        logger.warning(f"âš ï¸ 403é”™è¯¯ï¼Œ{wait_time}ç§’åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                        await asyncio.sleep(wait_time)
                    else:
                        logger.error(f"âŒ 403é”™è¯¯é‡è¯•å¤±è´¥: {league['name']}")
                        self._log_failure(str(league['id']), league['name'], f"403 after {max_retries} retries")
                        return False
                else:
                    raise e

        return False

    async def run(self):
        """
        è¿è¡Œå…¨åŸŸé‡‡é›†
        CEOè¦æ±‚ï¼šéå†æ‰€æœ‰300+è”èµ›
        """
        total_leagues = len(self.league_data)
        completed = 0
        failed = 0

        logger.info("\n" + "=" * 80)
        logger.info("ğŸš€ å¤©ç½‘è®¡åˆ’å…¨åŸŸé‡‡é›†å¯åŠ¨ (CEOå¼ºåˆ¶ä¿®æ­£ç‰ˆ)")
        logger.info("=" * 80)
        logger.info(f"ğŸ“Š æ€»è®¡è”èµ›: {total_leagues}")
        logger.info(f"âœ… å·²å®Œæˆ: {len(self.completed_leagues)}")
        logger.info(f"â³ å¾…é‡‡é›†: {total_leagues - len(self.completed_leagues)}")
        logger.info("=" * 80)

        start_time = datetime.now()

        # éå†æ‰€æœ‰è”èµ›
        for i, league in enumerate(self.league_data, 1):
            logger.info(f"\n[{i}/{total_leagues}] è¿›åº¦: {(i/total_leagues)*100:.1f}%")

            # é‡‡é›†è”èµ›
            success = await self.collect_with_403_retry(league)

            if success:
                completed += 1
                logger.info(f"âœ… è¿›åº¦æ›´æ–°: {completed}/{total_leagues} å®Œæˆ")
            else:
                failed += 1
                logger.error(f"âŒ è¿›åº¦æ›´æ–°: {failed}/{total_leagues} å¤±è´¥")

            # ä¿å­˜è¿›åº¦
            if i % 10 == 0 or i == total_leagues:
                self._save_progress()
                elapsed = datetime.now() - start_time
                avg_per_league = elapsed.total_seconds() / i if i > 0 else 0
                eta_seconds = avg_per_league * (total_leagues - i)
                eta_hours = eta_seconds / 3600

                logger.info(f"\nğŸ“Š è¿›åº¦æŠ¥å‘Š:")
                logger.info(f"  å·²å®Œæˆ: {completed}/{total_leagues} ({completed/total_leagues*100:.1f}%)")
                logger.info(f"  å¤±è´¥: {failed}/{total_leagues}")
                logger.info(f"  ç”¨æ—¶: {elapsed.total_seconds()/3600:.1f} å°æ—¶")
                logger.info(f"  é¢„è®¡å‰©ä½™: {eta_hours:.1f} å°æ—¶")
                logger.info(f"  å¹³å‡æ¯è”èµ›: {avg_per_league:.1f} ç§’")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        await self._generate_final_report(start_time, completed, failed)

    async def _generate_final_report(self, start_time: datetime, completed: int, failed: int):
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        end_time = datetime.now()
        total_time = end_time - start_time

        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ å¤©ç½‘è®¡åˆ’å…¨åŸŸé‡‡é›†å®Œæˆ")
        logger.info("=" * 80)
        logger.info(f"â±ï¸ æ€»ç”¨æ—¶: {total_time.total_seconds()/3600:.2f} å°æ—¶")
        logger.info(f"âœ… æˆåŠŸå®Œæˆ: {completed} ä¸ªè”èµ›")
        logger.info(f"âŒ å¤±è´¥: {failed} ä¸ªè”èµ›")
        logger.info(f"ğŸ“Š æˆåŠŸç‡: {completed/(completed+failed)*100:.1f}%")
        logger.info("=" * 80)

        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        report_file = f"logs/final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report = {
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "total_time_hours": total_time.total_seconds() / 3600,
            "total_leagues": len(self.league_data),
            "completed": completed,
            "failed": failed,
            "success_rate": completed / (completed + failed) * 100,
            "completed_leagues": list(self.completed_leagues),
            "failed_leagues": self.failed_leagues,
        }

        with open(report_file, "w") as f:
            json.dump(report, f, indent=2)

        logger.info(f"ğŸ“„ æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        collector = RobustCoverageCollector()
        await collector.run()
        return 0
    except KeyboardInterrupt:
        logger.info("\nâš ï¸ ç”¨æˆ·ä¸­æ–­é‡‡é›†")
        return 1
    except Exception as e:
        logger.error(f"\nâŒ é‡‡é›†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(asyncio.run(main()))
