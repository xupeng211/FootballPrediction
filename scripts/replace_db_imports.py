#!/usr/bin/env python3
"""
æ•°æ®åº“å¯¼å…¥æ›¿æ¢è„šæœ¬
Database Import Replacement Script

è‡ªåŠ¨å°†æ—§çš„æ•°æ®åº“è¿žæŽ¥å¯¼å…¥æ›¿æ¢ä¸ºæ–°çš„å¼‚æ­¥æŽ¥å£
æ”¯æŒå®‰å…¨ã€æ¸è¿›å¼çš„è¿ç§»ç­–ç•¥

ä½¿ç”¨æ–¹æ³•:
python scripts/replace_db_imports.py [--dry-run] [--backup]

é€‰é¡¹:
--dry-run: é¢„è§ˆæ¨¡å¼ï¼Œä¸å®žé™…ä¿®æ”¹æ–‡ä»¶
--backup: ä¿®æ”¹å‰åˆ›å»ºå¤‡ä»½æ–‡ä»¶
--file: æŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œé»˜è®¤å¤„ç†æŠ¥å‘Šä¸­çš„æ‰€æœ‰æ–‡ä»¶ï¼‰
"""

import os
import sys
import re
import shutil
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("patches/import_replacement.log"),
    ],
)
logger = logging.getLogger(__name__)


class DatabaseImportReplacer:
    """æ•°æ®åº“å¯¼å…¥æ›¿æ¢å™¨"""

    def __init__(self, backup: bool = True):
        self.backup = backup
        self.processed_files = []
        self.failed_files = []
        self.replacements = {
            # æ—§å¯¼å…¥æ¨¡å¼ -> æ–°å¯¼å…¥æ¨¡å¼
            "from src.database.connection import get_async_session": "from src.database.async_manager import get_db_session",
            "from src.database.connection import DatabaseManager": "from src.database.async_manager import AsyncDatabaseManager",
            "from src.database.connection import DatabaseManager, get_async_session": "from src.database.async_manager import AsyncDatabaseManager, get_db_session",
            "from ..database.connection import get_session": "from src.database.async_manager import get_db_session",
            # å…¼å®¹æ€§æ›¿æ¢ï¼ˆä¸´æ—¶ï¼‰
            "DatabaseManager()": "DatabaseCompatManager()",
            "get_session()": "get_db_session()",
            # ç‰¹æ®Šçš„CQRSæ¨¡å¼
            "from ..database.connection_mod import get_session": "from src.database.async_manager import get_db_session",
        }

    def analyze_file(self, file_path: Path) -> dict[str, any]:
        """
        åˆ†æžæ–‡ä»¶ï¼Œç¡®å®šæ›¿æ¢ç­–ç•¥

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            åˆ†æžç»“æžœå­—å…¸
        """
        try:
            with open(file_path, encoding="utf-8") as f:
                content = f.read()
        except Exception as e:
            return {"error": str(e), "needs_replacement": False}

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›¿æ¢
        has_old_imports = any(
            pattern in content for pattern in self.replacements.keys()
        )
        is_async = "async def" in content

        # æ£€æŸ¥å‡½æ•°ç­¾å
        sync_functions = re.findall(r"def\s+(\w+)\s*\(", content)
        async_functions = re.findall(r"async def\s+(\w+)\s*\(", content)

        return {
            "needs_replacement": has_old_imports,
            "is_async": is_async,
            "sync_functions": sync_functions,
            "async_functions": async_functions,
            "content": content,
        }

    def generate_replacement_strategy(self, analysis: dict[str, any]) -> str:
        """
        æ ¹æ®æ–‡ä»¶åˆ†æžç»“æžœç”Ÿæˆæ›¿æ¢ç­–ç•¥

        Args:
            analysis: æ–‡ä»¶åˆ†æžç»“æžœ

        Returns:
            ç­–ç•¥æè¿°å­—ç¬¦ä¸²
        """
        if not analysis["needs_replacement"]:
            return "æ— éœ€æ›¿æ¢"

        if analysis["is_async"]:
            return "å¼‚æ­¥æ–‡ä»¶ - ç›´æŽ¥æ›¿æ¢ä¸ºå¼‚æ­¥æŽ¥å£"

        if analysis["sync_functions"] and not analysis["async_functions"]:
            return "åŒæ­¥æ–‡ä»¶ - ä½¿ç”¨å…¼å®¹é€‚é…å™¨"

        if analysis["sync_functions"] and analysis["async_functions"]:
            return "æ··åˆæ–‡ä»¶ - ä¼˜å…ˆä½¿ç”¨å¼‚æ­¥æŽ¥å£ï¼ŒåŒæ­¥éƒ¨åˆ†ç”¨é€‚é…å™¨"

        return "é»˜è®¤ç­–ç•¥ - ä½¿ç”¨å…¼å®¹é€‚é…å™¨"

    def apply_replacements(
        self, file_path: Path, analysis: dict[str, any], dry_run: bool = False
    ) -> bool:
        """
        åº”ç”¨å¯¼å…¥æ›¿æ¢

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            analysis: æ–‡ä»¶åˆ†æžç»“æžœ
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼

        Returns:
            æ›¿æ¢æ˜¯å¦æˆåŠŸ
        """
        content = analysis["content"]
        original_content = content

        try:
            # æ ¹æ®ç­–ç•¥é€‰æ‹©æ›¿æ¢æ–¹å¼
            strategy = self.generate_replacement_strategy(analysis)
            logger.info(f"ðŸ“ {file_path} - {strategy}")

            if "å¼‚æ­¥æ–‡ä»¶" in strategy or "æ··åˆæ–‡ä»¶" in strategy:
                # å¼‚æ­¥æ–‡ä»¶ - ç›´æŽ¥æ›¿æ¢
                for old_pattern, new_pattern in self.replacements.items():
                    if old_pattern in content:
                        content = content.replace(old_pattern, new_pattern)
                        logger.info(
                            f"  âœ… æ›¿æ¢: {old_pattern[:50]}... -> {new_pattern[:50]}..."
                        )

            elif "åŒæ­¥æ–‡ä»¶" in strategy:
                # åŒæ­¥æ–‡ä»¶ - ä½¿ç”¨å…¼å®¹é€‚é…å™¨
                if "from src.database.connection import" in content:
                    # æ›¿æ¢å¯¼å…¥è¯­å¥
                    content = re.sub(
                        r"from src\.database\.connection import ([^\\n]+)",
                        r"from src.database.compat import DatabaseCompatManager, fetch_all_sync, fetch_one_sync, execute_sync",
                        content,
                    )
                    logger.info("  ðŸ”„ åŒæ­¥é€‚é…å™¨: å¯¼å…¥å·²æ›¿æ¢")

                # æ›¿æ¢DatabaseManagerå®žä¾‹åŒ–
                content = re.sub(
                    r"DatabaseManager\(\)", r"DatabaseCompatManager()", content
                )

            # é€šç”¨æ›¿æ¢
            content = re.sub(r"get_session\(\)", r"get_db_session()", content)

            # å¦‚æžœæœ‰å˜åŒ–ï¼Œå†™å…¥æ–‡ä»¶
            if content != original_content:
                if not dry_run:
                    if self.backup:
                        backup_path = file_path.with_suffix(
                            file_path.suffix + ".backup"
                        )
                        shutil.copy2(file_path, backup_path)
                        logger.info(f"  ðŸ’¾ å¤‡ä»½: {backup_path}")

                    with open(file_path, "w", encoding="utf-8") as f:
                        f.write(content)

                    logger.info("  âœ… æ–‡ä»¶å·²æ›´æ–°")
                else:
                    logger.info("  ðŸ” é¢„è§ˆæ¨¡å¼: å°†æ›´æ–°æ­¤æ–‡ä»¶")

                return True
            else:
                logger.info("  â„¹ï¸  æ— éœ€æ›´æ”¹")
                return False

        except Exception as e:
            logger.error(f"  âŒ å¤„ç†å¤±è´¥: {e}")
            self.failed_files.append((str(file_path), str(e)))
            return False

    def process_files(
        self, file_list: list[Path], dry_run: bool = False
    ) -> dict[str, int]:
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶

        Args:
            file_list: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            dry_run: æ˜¯å¦ä¸ºé¢„è§ˆæ¨¡å¼

        Returns:
            å¤„ç†ç»“æžœç»Ÿè®¡
        """
        stats = {
            "total": len(file_list),
            "processed": 0,
            "needs_replacement": 0,
            "failed": 0,
            "skipped": 0,
        }

        logger.info(f"ðŸš€ å¼€å§‹å¤„ç† {len(file_list)} ä¸ªæ–‡ä»¶ (é¢„è§ˆæ¨¡å¼: {dry_run})")

        for file_path in file_list:
            logger.info(f"\n{'='*60}")

            try:
                # åˆ†æžæ–‡ä»¶
                analysis = self.analyze_file(file_path)

                if "error" in analysis:
                    logger.error(f"âŒ åˆ†æžå¤±è´¥: {analysis['error']}")
                    stats["failed"] += 1
                    continue

                if not analysis["needs_replacement"]:
                    logger.info("â„¹ï¸  è·³è¿‡: æ— éœ€æ›¿æ¢")
                    stats["skipped"] += 1
                    continue

                stats["needs_replacement"] += 1

                # åº”ç”¨æ›¿æ¢
                if self.apply_replacements(file_path, analysis, dry_run):
                    stats["processed"] += 1
                    self.processed_files.append(str(file_path))

            except Exception as e:
                logger.error(f"âŒ å¤„ç† {file_path} æ—¶å‡ºé”™: {e}")
                stats["failed"] += 1
                self.failed_files.append((str(file_path), str(e)))

        return stats

    def generate_summary_report(self, stats: dict[str, int]) -> str:
        """
        ç”Ÿæˆå¤„ç†æ‘˜è¦æŠ¥å‘Š

        Args:
            stats: å¤„ç†ç»Ÿè®¡

        Returns:
            æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = f"""
ðŸ“Š æ•°æ®åº“å¯¼å…¥æ›¿æ¢å¤„ç†æŠ¥å‘Š
{'='*50}

ðŸ“ˆ å¤„ç†ç»Ÿè®¡:
- æ€»æ–‡ä»¶æ•°: {stats['total']}
- éœ€è¦æ›¿æ¢: {stats['needs_replacement']}
- æˆåŠŸå¤„ç†: {stats['processed']}
- è·³è¿‡æ–‡ä»¶: {stats['skipped']}
- å¤±è´¥æ–‡ä»¶: {stats['failed']}

âœ… æˆåŠŸå¤„ç†çš„æ–‡ä»¶:
{chr(10).join(f"  â€¢ {f}" for f in self.processed_files[:10])}
{f"  ... è¿˜æœ‰ {len(self.processed_files) - 10} ä¸ªæ–‡ä»¶" if len(self.processed_files) > 10 else ""}

âŒ å¤±è´¥çš„æ–‡ä»¶:
{chr(10).join(f"  â€¢ {f}: {e}" for f, e in self.failed_files[:5])}
{f"  ... è¿˜æœ‰ {len(self.failed_files) - 5} ä¸ªæ–‡ä»¶" if len(self.failed_files) > 5 else ""}

ðŸ’¡ ä¸‹ä¸€æ­¥æ“ä½œ:
1. æ£€æŸ¥æˆåŠŸå¤„ç†çš„æ–‡ä»¶ï¼Œç¡®è®¤æ›¿æ¢æ­£ç¡®
2. æ‰‹åŠ¨ä¿®å¤å¤±è´¥çš„æ–‡ä»¶
3. è¿è¡Œæµ‹è¯•éªŒè¯åŠŸèƒ½æ­£å¸¸
4. æäº¤æ›´æ”¹åˆ°ç‰ˆæœ¬æŽ§åˆ¶
        """
        return report


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ•°æ®åº“å¯¼å…¥æ›¿æ¢è„šæœ¬")
    parser.add_argument(
        "--dry-run", action="store_true", help="é¢„è§ˆæ¨¡å¼ï¼Œä¸å®žé™…ä¿®æ”¹æ–‡ä»¶"
    )
    parser.add_argument("--no-backup", action="store_true", help="ä¸åˆ›å»ºå¤‡ä»½æ–‡ä»¶")
    parser.add_argument("--file", type=str, help="æŒ‡å®šè¦å¤„ç†çš„æ–‡ä»¶")
    parser.add_argument(
        "--limit", type=int, default=3, help="é™åˆ¶å¤„ç†çš„æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰"
    )

    args = parser.parse_args()

    # ç¡®ä¿å·¥ä½œç›®å½•æ­£ç¡®
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    os.chdir(project_root)

    logger.info(f"ðŸ  å·¥ä½œç›®å½•: {project_root}")

    # è¯»å–éœ€è¦å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
    usage_report = project_root / "reports" / "old_db_usage.txt"

    if args.file:
        # å¤„ç†å•ä¸ªæ–‡ä»¶
        file_paths = [Path(args.file)]
    else:
        # ä»ŽæŠ¥å‘Šæ–‡ä»¶è¯»å–
        if not usage_report.exists():
            logger.error(f"âŒ æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {usage_report}")
            sys.exit(1)

        # è§£æžæŠ¥å‘Šæ–‡ä»¶ï¼Œæå–æ–‡ä»¶è·¯å¾„
        file_paths = set()
        with open(usage_report, encoding="utf-8") as f:
            for line in f:
                if ":" in line:
                    file_path = line.split(":")[0]
                    # åªå¤„ç†.pyæ–‡ä»¶ä¸”æŽ’é™¤æµ‹è¯•æ–‡ä»¶ï¼ˆç¬¬ä¸€æ­¥ï¼‰
                    if file_path.endswith(".py") and not any(
                        x in file_path for x in ["test_", "/tests/"]
                    ):
                        file_paths.add(Path(file_path))

        file_paths = list(file_paths)

        # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰
        if args.limit:
            file_paths = file_paths[: args.limit]
            logger.info(f"âš ï¸  é™åˆ¶å¤„ç†æ•°é‡ä¸º: {args.limit}")

    logger.info(f"ðŸ“‹ å°†å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶")

    # åˆ›å»ºæ›¿æ¢å™¨
    replacer = DatabaseImportReplacer(backup=not args.no_backup)

    # å¤„ç†æ–‡ä»¶
    stats = replacer.process_files(file_paths, dry_run=args.dry_run)

    # ç”ŸæˆæŠ¥å‘Š
    report = replacer.generate_summary_report(stats)

    # ä¿å­˜æŠ¥å‘Š
    report_file = (
        Path("patches")
        / f"replacement_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    )
    with open(report_file, "w", encoding="utf-8") as f:
        f.write(report)

    # è¾“å‡ºæŠ¥å‘Š
    print(report)

    logger.info(f"ðŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # è¿”å›žçŠ¶æ€ç 
    if stats["failed"] > 0:
        sys.exit(1)


if __name__ == "__main__":
    from datetime import datetime

    main()
