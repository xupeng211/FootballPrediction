#!/usr/bin/env python3
"""
é«˜çº§æ–‡æ¡£æ¸…ç†å·¥å…·
æ·±åº¦æ¸…ç†docsç›®å½•ä¸­çš„æ‰€æœ‰æ— ç”¨æ–‡ä»¶
"""

import os
import re
import json
import hashlib
import shutil
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Set

class AdvancedDocumentCleaner:
    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path.cwd()
        self.docs_dir = self.project_root / "docs"
        self.archive_dir = self.docs_dir / "_archive"
        self.stats = {
            'total_files': 0,
            'archived': 0,
            'deleted': 0,
            'duplicates': 0,
            'empty_dirs': 0,
            'size_saved_mb': 0
        }
        self.errors = []
        self.dry_run = False

    def log(self, message: str, level: str = "INFO"):
        timestamp = datetime.now().strftime("%H:%M:%S")
        colors = {
            "INFO": "\033[0m",
            "WARN": "\033[0;33m",
            "ERROR": "\033[0;31m",
            "SUCCESS": "\033[0;32m",
            "HIGHLIGHT": "\033[1;34m"
        }
        color = colors.get(level, "\033[0m")
        print(f"{color}[{timestamp}] {level}: {message}\033[0m")

    def get_file_hash(self, file_path: Path) -> str:
        """è·å–æ–‡ä»¶å“ˆå¸Œå€¼ç”¨äºæŸ¥é‡"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except:
            return ""

    def is_likely_obsolete(self, file_path: Path) -> Tuple[bool, str]:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦å¯èƒ½å·²è¿‡æ—¶"""
        name = file_path.name.lower()
        parent_dir = file_path.parent.name.lower()

        # æ˜æ˜¾çš„è¿‡æ—¶æ–‡ä»¶æ¨¡å¼
        obsolete_patterns = [
            # ä¸´æ—¶æ–‡ä»¶
            r'^temp_.*',
            r'^tmp_.*',
            r'^.*_temp$',
            r'^.*_working$',
            r'^.*_draft$',

            # å¤‡ä»½æ–‡ä»¶
            r'^.*\.bak$',
            r'^.*\.backup$',
            r'^.*_backup\.',
            r'^.*_copy\.',

            # æ—§ç‰ˆæœ¬æ ‡è®°
            r'^.*_v[0-9]+.*$',
            r'^.*_old$',
            r'^.*_deprecated$',
            r'^.*_legacy$',

            # è‰ç¨¿å’Œä¸´æ—¶
            r'^draft_.*',
            r'^wip_.*',
            r'^work_in_progress.*',

            # AIç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶
            r'^claude_.*\.md\.bak$',
            r'^gpt_.*',
            r'^ai_.*_temp.*',

            # æµ‹è¯•æ–‡ä»¶
            r'^test_.*\.md$',
            r'^.*_test\.md$',

            # å¤šä½™çš„ç¼–å·
            r'^.*_\d{8}_.*$',  # æ—¥æœŸç¼–å·
            r'^.*_\d{6}_.*$',  # çŸ­æ—¥æœŸç¼–å·
        ]

        for pattern in obsolete_patterns:
            if re.match(pattern, name, re.IGNORECASE):
                return True, f"åŒ¹é…è¿‡æ—¶æ¨¡å¼: {pattern}"

        # ç‰¹å®šç›®å½•çš„æ–‡ä»¶
        if parent_dir in ['_reports', '_tasks', '_archive']:
            # æ£€æŸ¥ä¿®æ”¹æ—¶é—´
            try:
                mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                days_old = (datetime.now() - mtime).days

                # ä¸´æ—¶æŠ¥å‘Š7å¤©åè¿‡æ—¶
                if any(temp in name for temp in ['temp', 'tmp', 'draft', 'wip']):
                    if days_old > 7:
                        return True, f"ä¸´æ—¶æ–‡ä»¶è¶…è¿‡7å¤©"

                # æ™®é€šæŠ¥å‘Š30å¤©åè¿‡æ—¶
                if days_old > 30:
                    return True, f"æŠ¥å‘Šè¶…è¿‡30å¤©"
            except:
                pass

        # å†…å®¹æ£€æŸ¥ - æŸ¥æ‰¾ç©ºæ–‡ä»¶æˆ–æå°‘å†…å®¹
        if file_path.suffix == '.md':
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    # å°‘äº50å­—ç¬¦çš„markdownæ–‡ä»¶å¯èƒ½æ²¡ç”¨
                    if len(content) < 50:
                        return True, "å†…å®¹è¿‡å°‘"
            except:
                pass

        return False, ""

    def find_duplicate_content(self) -> Dict[str, List[Path]]:
        """æŸ¥æ‰¾å†…å®¹é‡å¤çš„æ–‡ä»¶"""
        self.log("\nğŸ” æŸ¥æ‰¾é‡å¤å†…å®¹...", "INFO")

        content_map = {}
        duplicates = {}

        for md_file in self.docs_dir.rglob("*.md"):
            if "_archive" in str(md_file):
                continue

            try:
                # è®¡ç®—å†…å®¹å“ˆå¸Œ
                content_hash = self.get_file_hash(md_file)
                if content_hash:
                    if content_hash not in content_map:
                        content_map[content_hash] = []
                    content_map[content_hash].append(md_file)
            except:
                pass

        # æ‰¾å‡ºé‡å¤çš„
        for hash_val, files in content_map.items():
            if len(files) > 1:
                duplicates[hash_val] = files
                self.log(f"\n  é‡å¤å†…å®¹ç»„ (å“ˆå¸Œ: {hash_val[:8]}...):")
                for f in files:
                    size = f.stat().st_size
                    self.log(f"    - {f.relative_to(self.docs_dir)} ({size} bytes)")

        return duplicates

    def find_similar_files(self) -> List[List[Path]]:
        """æŸ¥æ‰¾æ–‡ä»¶åç›¸ä¼¼çš„æ–‡ä»¶"""
        self.log("\nğŸ” æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶å...", "INFO")

        file_groups = {}

        for md_file in self.docs_dir.rglob("*.md"):
            if "_archive" in str(md_file):
                continue

            # æ ‡å‡†åŒ–æ–‡ä»¶å
            normalized = md_file.stem.lower()
            # ç§»é™¤å¸¸è§åç¼€
            normalized = re.sub(r'(_v\d+|_\d{8}|_old|_new|_backup|_draft|_copy|_temp|_working|_legacy|_deprecated)$', '', normalized)
            # æ›¿æ¢åˆ†éš”ç¬¦
            normalized = re.sub(r'[-_]+', '_', normalized)

            if normalized not in file_groups:
                file_groups[normalized] = []
            file_groups[normalized].append(md_file)

        # æ‰¾å‡ºç›¸ä¼¼çš„
        similar_groups = []
        for normalized, files in file_groups.items():
            if len(files) > 1:
                similar_groups.append(files)
                self.log(f"\n  ç›¸ä¼¼æ–‡ä»¶ç»„: {normalized}")
                for f in sorted(files, key=lambda x: x.stat().st_mtime, reverse=True):
                    mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime('%Y-%m-%d %H:%M')
                    self.log(f"    - {f.name} ({mtime})")

        return similar_groups

    def find_orphaned_images(self) -> List[Path]:
        """æŸ¥æ‰¾æœªè¢«å¼•ç”¨çš„å›¾ç‰‡"""
        self.log("\nğŸ–¼ï¸ æŸ¥æ‰¾å­¤ç«‹å›¾ç‰‡...", "INFO")

        # æ”¶é›†æ‰€æœ‰å›¾ç‰‡
        image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp']
        all_images = []
        for ext in image_extensions:
            all_images.extend(self.docs_dir.rglob(f"*{ext}"))

        # æ”¶é›†æ‰€æœ‰markdownå†…å®¹
        referenced_images = set()
        for md_file in self.docs_dir.rglob("*.md"):
            if "_archive" not in str(md_file):
                try:
                    with open(md_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        # æŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨
                        for img in all_images:
                            if img.name in content:
                                referenced_images.add(img)
                except:
                    pass

        # æ‰¾å‡ºæœªå¼•ç”¨çš„
        orphaned = [img for img in all_images if img not in referenced_images]

        for img in orphaned:
            size = img.stat().st_size
            self.log(f"  æœªå¼•ç”¨: {img.relative_to(self.docs_dir)} ({size} bytes)")

        return orphaned

    def find_empty_files(self) -> List[Path]:
        """æŸ¥æ‰¾ç©ºæ–‡ä»¶"""
        self.log("\nğŸ“„ æŸ¥æ‰¾ç©ºæ–‡ä»¶...", "INFO")

        empty_files = []

        for file_path in self.docs_dir.rglob("*"):
            if not file_path.is_file() or "_archive" in str(file_path):
                continue

            try:
                if file_path.stat().st_size == 0:
                    empty_files.append(file_path)
                    self.log(f"  ç©ºæ–‡ä»¶: {file_path.relative_to(self.docs_dir)}")
                elif file_path.suffix == '.md':
                    # æ£€æŸ¥markdownæ˜¯å¦åªæœ‰æ ‡é¢˜
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = [l.strip() for l in f.readlines() if l.strip()]
                        if len(lines) <= 2:  # åªæœ‰æ ‡é¢˜æˆ–å‡ ä¹ç©º
                            empty_files.append(file_path)
                            self.log(f"  è¿‘ç©ºæ–‡ä»¶: {file_path.relative_to(self.docs_dir)}")
            except:
                pass

        return empty_files

    def find_large_obsolete_files(self, min_size_mb: float = 0.5) -> List[Tuple[Path, float, str]]:
        """æŸ¥æ‰¾å¤§æ–‡ä»¶"""
        self.log(f"\nğŸ“¦ æŸ¥æ‰¾å¤§äº {min_size_mb}MB çš„æ–‡ä»¶...", "INFO")

        large_files = []

        for file_path in self.docs_dir.rglob("*"):
            if not file_path.is_file() or "_archive" in str(file_path):
                continue

            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > min_size_mb:
                is_obsolete, reason = self.is_likely_obsolete(file_path)
                if is_obsolete:
                    large_files.append((file_path, size_mb, reason))
                    self.log(f"  {file_path.relative_to(self.docs_dir)}: {size_mb:.2f}MB ({reason})")

        return sorted(large_files, key=lambda x: x[1], reverse=True)

    def archive_file(self, file_path: Path, reason: str):
        """å½’æ¡£æ–‡ä»¶"""
        if self.dry_run:
            self.log(f"  [DRY] å°†å½’æ¡£: {file_path.relative_to(self.docs_dir)} ({reason})")
            return

        try:
            # åˆ›å»ºå½’æ¡£ç›®å½•
            archive_path = self.archive_dir / reason.replace(" ", "_").replace(":", "")
            archive_path.mkdir(parents=True, exist_ok=True)

            # ä¿ç•™ç›¸å¯¹è·¯å¾„
            rel_path = file_path.relative_to(self.docs_dir)
            target_path = archive_path / rel_path.name

            # é¿å…è¦†ç›–
            counter = 1
            original_target = target_path
            while target_path.exists():
                stem = original_target.stem
                suffix = original_target.suffix
                target_path = archive_path / f"{stem}_{counter}{suffix}"
                counter += 1

            # ç§»åŠ¨æ–‡ä»¶
            shutil.move(str(file_path), str(target_path))

            # æ›´æ–°ç»Ÿè®¡
            size_mb = file_path.stat().st_size / (1024 * 1024)
            self.stats['size_saved_mb'] += size_mb
            self.stats['archived'] += 1

            self.log(f"  å½’æ¡£: {rel_path} ({reason})")

        except Exception as e:
            self.errors.append(f"å½’æ¡£å¤±è´¥ {file_path}: {e}")

    def delete_file(self, file_path: Path, reason: str):
        """åˆ é™¤æ–‡ä»¶"""
        if self.dry_run:
            self.log(f"  [DRY] å°†åˆ é™¤: {file_path.relative_to(self.docs_dir)} ({reason})")
            return

        try:
            size_mb = file_path.stat().st_size / (1024 * 1024)
            file_path.unlink()
            self.stats['size_saved_mb'] += size_mb
            self.stats['deleted'] += 1
            self.log(f"  åˆ é™¤: {file_path.relative_to(self.docs_dir)} ({reason})")
        except Exception as e:
            self.errors.append(f"åˆ é™¤å¤±è´¥ {file_path}: {e}")

    def clean_obsolete_files(self):
        """æ¸…ç†è¿‡æ—¶æ–‡ä»¶"""
        self.log("\nğŸ—‘ï¸ æ¸…ç†è¿‡æ—¶æ–‡ä»¶...", "HIGHLIGHT")

        obsolete_count = 0
        for file_path in self.docs_dir.rglob("*"):
            if not file_path.is_file() or "_archive" in str(file_path):
                continue

            is_obsolete, reason = self.is_likely_obsolete(file_path)
            if is_obsolete:
                obsolete_count += 1
                self.archive_file(file_path, reason)

        self.log(f"  å‘ç°å¹¶å¤„ç† {obsolete_count} ä¸ªè¿‡æ—¶æ–‡ä»¶")

    def process_duplicates(self, duplicates: Dict[str, List[Path]]):
        """å¤„ç†é‡å¤æ–‡ä»¶"""
        self.log("\nğŸ”„ å¤„ç†é‡å¤æ–‡ä»¶...", "HIGHLIGHT")

        for hash_val, files in duplicates.items():
            # ä¿ç•™æœ€æ–°çš„ï¼Œå½’æ¡£å…¶ä»–çš„
            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            keep_file = files[0]

            for dup_file in files[1:]:
                self.archive_file(dup_file, f"é‡å¤æ–‡ä»¶ (ä¿ç•™: {keep_file.name})")
                self.stats['duplicates'] += 1

    def process_similar_files(self, similar_groups: List[List[Path]]):
        """å¤„ç†ç›¸ä¼¼æ–‡ä»¶"""
        self.log("\nğŸ“ å¤„ç†ç›¸ä¼¼æ–‡ä»¶...", "HIGHLIGHT")

        for files in similar_groups:
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            keep_file = files[0]

            for similar_file in files[1:]:
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§ç‰ˆæœ¬
                if any(old in similar_file.name.lower() for old in ['old', 'legacy', 'deprecated', 'backup']):
                    self.archive_file(similar_file, f"æ—§ç‰ˆæœ¬æ–‡ä»¶ (ä¿ç•™: {keep_file.name})")
                elif similar_file.stat().st_size < keep_file.stat().st_size * 0.5:
                    # å¦‚æœæ–‡ä»¶å°å¾ˆå¤šï¼Œå¯èƒ½æ˜¯æœªå®Œæˆçš„ç‰ˆæœ¬
                    self.archive_file(similar_file, f"ä¸å®Œæ•´ç‰ˆæœ¬ (ä¿ç•™: {keep_file.name})")

    def generate_cleanup_report(self):
        """ç”Ÿæˆè¯¦ç»†çš„æ¸…ç†æŠ¥å‘Š"""
        report_dir = self.docs_dir / "_reports"
        report_dir.mkdir(exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"advanced_docs_cleanup_{timestamp}.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# é«˜çº§æ–‡æ¡£æ¸…ç†æŠ¥å‘Š\n\n")
            f.write(f"**æ¸…ç†æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æ¸…ç†å·¥å…·**: scripts/cleanup_docs_advanced.py\n\n")

            f.write("## æ¸…ç†ç»Ÿè®¡\n\n")
            f.write(f"- æ‰«ææ–‡ä»¶æ€»æ•°: {self.stats['total_files']}\n")
            f.write(f"- å½’æ¡£æ–‡ä»¶æ•°: {self.stats['archived']}\n")
            f.write(f"- åˆ é™¤æ–‡ä»¶æ•°: {self.stats['deleted']}\n")
            f.write(f"- é‡å¤æ–‡ä»¶å¤„ç†: {self.stats['duplicates']}\n")
            f.write(f"- ç©ºç›®å½•åˆ é™¤: {self.stats['empty_dirs']}\n")
            f.write(f"- èŠ‚çœç©ºé—´: {self.stats['size_saved_mb']:.2f} MB\n\n")

            if self.errors:
                f.write("## é”™è¯¯åˆ—è¡¨\n\n")
                for error in self.errors:
                    f.write(f"- {error}\n")
                f.write("\n")

            f.write("## æ¸…ç†ç­–ç•¥\n\n")
            f.write("1. **è¿‡æ—¶æ–‡ä»¶è¯†åˆ«**: åŸºäºæ–‡ä»¶åæ¨¡å¼å’Œæ—¶é—´æˆ³\n")
            f.write("2. **å†…å®¹å»é‡**: ä½¿ç”¨MD5å“ˆå¸Œè¯†åˆ«å®Œå…¨ç›¸åŒçš„æ–‡ä»¶\n")
            f.write("3. **ç›¸ä¼¼æ–‡ä»¶åˆå¹¶**: ä¿ç•™æœ€æ–°ç‰ˆæœ¬ï¼Œå½’æ¡£æ—§ç‰ˆæœ¬\n")
            f.write("4. **å­¤ç«‹èµ„æºæ¸…ç†**: åˆ é™¤æœªå¼•ç”¨çš„å›¾ç‰‡å’Œèµ„æº\n")
            f.write("5. **ç©ºæ–‡ä»¶æ¸…ç†**: åˆ é™¤ç©ºæ–‡ä»¶å’Œè¿‘ç©ºæ–‡æ¡£\n\n")

            f.write("## å»ºè®®\n\n")
            f.write("1. å®šæœŸè¿è¡Œæ¸…ç†ï¼ˆå»ºè®®æ¯æœˆä¸€æ¬¡ï¼‰\n")
            f.write("2. ä½¿ç”¨ç‰ˆæœ¬æ§åˆ¶ç®¡ç†é‡è¦æ–‡æ¡£å˜æ›´\n")
            f.write("3. é¿å…åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼Œç›´æ¥ç¼–è¾‘æœ€ç»ˆç‰ˆæœ¬\n")
            f.write("4. å»ºç«‹æ–‡æ¡£å‘½åè§„èŒƒï¼Œé¿å…é‡å¤\n")
            f.write("5. åŠæ—¶åˆ é™¤æ— ç”¨çš„è‰ç¨¿å’Œå¤‡ä»½\n")

        self.log(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file.relative_to(self.docs_dir)}")

    def run_advanced_cleanup(self, dry_run: bool = False):
        """æ‰§è¡Œé«˜çº§æ¸…ç†"""
        self.dry_run = dry_run
        self.log("=" * 70)
        self.log("å¼€å§‹é«˜çº§æ–‡æ¡£æ¸…ç†...", "SUCCESS")
        self.log(f"æ–‡æ¡£ç›®å½•: {self.docs_dir}")
        if dry_run:
            self.log("âš ï¸ è¯•è¿è¡Œæ¨¡å¼ - ä¸ä¼šå®é™…åˆ é™¤æ–‡ä»¶", "WARN")
        self.log("=" * 70)

        # ç»Ÿè®¡åˆå§‹çŠ¶æ€
        self.stats['total_files'] = len(list(self.docs_dir.rglob("*")))

        # 1. æŸ¥æ‰¾å’Œå¤„ç†è¿‡æ—¶æ–‡ä»¶
        self.clean_obsolete_files()

        # 2. æŸ¥æ‰¾é‡å¤å†…å®¹
        duplicates = self.find_duplicate_content()
        if duplicates:
            self.process_duplicates(duplicates)

        # 3. æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶
        similar_files = self.find_similar_files()
        if similar_files:
            self.process_similar_files(similar_files)

        # 4. æŸ¥æ‰¾å­¤ç«‹å›¾ç‰‡
        orphaned_images = self.find_orphaned_images()
        for img in orphaned_images:
            self.archive_file(img, "æœªå¼•ç”¨çš„å›¾ç‰‡")

        # 5. æŸ¥æ‰¾ç©ºæ–‡ä»¶
        empty_files = self.find_empty_files()
        for empty in empty_files:
            self.delete_file(empty, "ç©ºæ–‡ä»¶")

        # 6. æŸ¥æ‰¾å¤§æ–‡ä»¶
        large_files = self.find_large_obsolete_files()
        for large_file, size, reason in large_files:
            self.archive_file(large_file, f"å¤§æ–‡ä»¶ {reason}")

        # 7. æ¸…ç†ç©ºç›®å½•
        self.clean_empty_directories()

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_cleanup_report()

        # è¾“å‡ºæ€»ç»“
        self.log("\n" + "=" * 70)
        self.log("é«˜çº§æ–‡æ¡£æ¸…ç†å®Œæˆï¼", "SUCCESS")
        self.log(f"æ‰«ææ–‡ä»¶: {self.stats['total_files']}")
        self.log(f"å½’æ¡£æ–‡ä»¶: {self.stats['archived']}")
        self.log(f"åˆ é™¤æ–‡ä»¶: {self.stats['deleted']}")
        self.log(f"å¤„ç†é‡å¤: {self.stats['duplicates']}")
        self.log(f"èŠ‚çœç©ºé—´: {self.stats['size_saved_mb']:.2f} MB")
        if self.errors:
            self.log(f"é”™è¯¯æ•°: {len(self.errors)}", "WARN")
        self.log("=" * 70)

    def clean_empty_directories(self):
        """æ¸…ç†ç©ºç›®å½•"""
        self.log("\nğŸ§¹ æ¸…ç†ç©ºç›®å½•...", "INFO")

        # ä»æ·±å±‚å¼€å§‹ï¼Œé€çº§å‘ä¸Š
        all_dirs = sorted(
            [d for d in self.docs_dir.rglob("*") if d.is_dir() and "_archive" not in str(d)],
            key=lambda x: len(x.parts),
            reverse=True
        )

        for dir_path in all_dirs:
            try:
                # æ£€æŸ¥æ˜¯å¦ä¸ºç©º
                if not any(dir_path.iterdir()):
                    dir_path.rmdir()
                    self.stats['empty_dirs'] += 1
                    self.log(f"  åˆ é™¤ç©ºç›®å½•: {dir_path.relative_to(self.docs_dir)}")
            except OSError:
                # ç›®å½•ä¸ä¸ºç©ºæˆ–æƒé™é—®é¢˜
                pass


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="é«˜çº§æ–‡æ¡£æ¸…ç†å·¥å…·")
    parser.add_argument("--project-root", help="é¡¹ç›®æ ¹ç›®å½•", default=None)
    parser.add_argument("--dry-run", action="store_true", help="è¯•è¿è¡Œæ¨¡å¼")
    parser.add_argument("--min-size-mb", type=float, default=0.5, help="å¤§æ–‡ä»¶é˜ˆå€¼(MB)")

    args = parser.parse_args()

    cleaner = AdvancedDocumentCleaner(args.project_root)
    cleaner.run_advanced_cleanup(dry_run=args.dry_run)