#!/usr/bin/env python3
"""
æœåŠ¡æµ‹è¯•ç”Ÿæˆå™¨
è‡ªåŠ¨ä¸ºä¸šåŠ¡æœåŠ¡ç”Ÿæˆå®Œæ•´çš„å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
"""

import os
import sys
import ast
import json
import inspect
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass
from collections import defaultdict
import re
from datetime import datetime

@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    target_module: str
    output_file: str
    test_types: List[str]
    include_mocks: bool = True
    include_fixtures: bool = True
    include_parametrized: bool = True

class ServiceAnalyzer:
    """æœåŠ¡åˆ†æå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.src_dir = project_root / "src"

    def analyze_service_module(self, module_path: str) -> Dict[str, Any]:
        """åˆ†ææœåŠ¡æ¨¡å—"""
        print(f"ğŸ” åˆ†ææœåŠ¡æ¨¡å—: {module_path}")

        try:
            # å¯¼å…¥æ¨¡å—
            sys.path.insert(0, str(self.src_dir))
            module = __import__(module_path, fromlist=['*'])

            analysis = {
                'module_name': module_path,
                'classes': [],
                'functions': [],
                'imports': [],
                'dependencies': set()
            }

            # åˆ†æASTç»“æ„
            module_file = self.src_dir / f"{module_path.replace('.', '/')}.py"
            if module_file.exists():
                with open(module_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        class_info = self._analyze_class(node, module)
                        analysis['classes'].append(class_info)
                    elif isinstance(node, ast.FunctionDef):
                        if not node.name.startswith('_'):
                            func_info = self._analyze_function(node)
                            analysis['functions'].append(func_info)
                    elif isinstance(node, ast.Import):
                        for alias in node.names:
                            analysis['imports'].append(alias.name)
                    elif isinstance(node, ast.ImportFrom):
                        if node.module:
                            analysis['imports'].append(node.module)

            # åˆ†æè¿è¡Œæ—¶ä¿¡æ¯
            for name, obj in module.__dict__.items():
                if inspect.isclass(obj) and not name.startswith('_'):
                    if name not in [c['name'] for c in analysis['classes']]:
                        class_info = self._analyze_runtime_class(obj)
                        analysis['classes'].append(class_info)
                elif inspect.isfunction(obj) and not name.startswith('_'):
                    if name not in [f['name'] for f in analysis['functions']]:
                        func_info = self._analyze_runtime_function(obj)
                        analysis['functions'].append(func_info)

            print(f"âœ… å‘ç° {len(analysis['classes'])} ä¸ªç±»,
    {len(analysis['functions'])} ä¸ªå‡½æ•°")
            return analysis

        except Exception as e:
            print(f"âŒ åˆ†ææ¨¡å—å¤±è´¥: {e}")
            return {}

    def _analyze_class(self, node: ast.ClassDef, module) -> Dict[str, Any]:
        """åˆ†æç±»å®šä¹‰"""
        methods = []
        properties = []
        dependencies = set()

        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                method_info = self._analyze_function(item)
                methods.append(method_info)
                # åˆ†ææ–¹æ³•ä¾èµ–
                for decorator in item.decorator_list:
                    if isinstance(decorator, ast.Name):
                        dependencies.add(decorator.id)

        return {
            'name': node.name,
            'type': 'class',
            'methods': methods,
            'properties': properties,
            'dependencies': list(dependencies),
            'base_classes': [base.id if isinstance(base,
    ast.Name) else str(base) for base in node.bases]
        }

    def _analyze_function(self, node: ast.FunctionDef) -> Dict[str, Any]:
        """åˆ†æå‡½æ•°å®šä¹‰"""
        args = []
        returns = None
        dependencies = set()

        # åˆ†æå‚æ•°
        for arg in node.args.args:
            args.append({
                'name': arg.arg,
                'type': None,  # å¯ä»¥è¿›ä¸€æ­¥åˆ†æç±»å‹æ³¨è§£
                'default': None
            })

        # åˆ†æè¿”å›ç±»å‹
        if node.returns:
            if isinstance(node.returns, ast.Name):
                returns = node.returns.id
            else:
                returns = 'complex_type'

        # åˆ†æå‡½æ•°ä½“ä¸­çš„ä¾èµ–
        for sub_node in ast.walk(node):
            if isinstance(sub_node, ast.Call):
                if isinstance(sub_node.func, ast.Name):
                    dependencies.add(sub_node.func.id)
                elif isinstance(sub_node.func, ast.Attribute):
                    dependencies.add(sub_node.func.attr)

        return {
            'name': node.name,
            'type': 'function',
            'args': args,
            'returns': returns,
            'dependencies': list(dependencies),
            'is_async': isinstance(node, ast.AsyncFunctionDef),
            'decorators': [d.id if isinstance(d,
    ast.Name) else str(d) for d in node.decorator_list if isinstance(d,
    ast.Name)]
        }

    def _analyze_runtime_class(self, cls) -> Dict[str, Any]:
        """åˆ†æè¿è¡Œæ—¶ç±»"""
        methods = []
        dependencies = set()

        for name, method in inspect.getmembers(cls, predicate=inspect.isfunction):
            if not name.startswith('_'):
                method_info = self._analyze_runtime_function(method)
                method_info['name'] = name
                methods.append(method_info)

        return {
            'name': cls.__name__,
            'type': 'class',
            'methods': methods,
            'properties': [],
            'dependencies': list(dependencies),
            'base_classes': [base.__name__ for base in cls.__bases__]
        }

    def _analyze_runtime_function(self, func) -> Dict[str, Any]:
        """åˆ†æè¿è¡Œæ—¶å‡½æ•°"""
        try:
            sig = inspect.signature(func)
            args = []

            for param_name, param in sig.parameters.items():
                args.append({
                    'name': param_name,
                    'type': param.annotation if param.annotation != inspect.Parameter.empty else None,
                    'default': param.default if param.default != inspect.Parameter.empty else None
                })

            return {
                'name': func.__name__,
                'type': 'function',
                'args': args,
                'returns': sig.return_annotation if sig.return_annotation != inspect.Signature.empty else None,
                'dependencies': [],
                'is_async': inspect.iscoroutinefunction(func),
                'decorators': []
            }
        except Exception as e:
            print(f"âš ï¸  åˆ†æå‡½æ•°å¤±è´¥: {func.__name__} - {e}")
            return {
                'name': func.__name__,
                'type': 'function',
                'args': [],
                'returns': None,
                'dependencies': [],
                'is_async': False,
                'decorators': []
            }

class ServiceTestGenerator:
    """æœåŠ¡æµ‹è¯•ç”Ÿæˆå™¨"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.src_dir = project_root / "src"
        self.test_dir = project_root / "tests"

    def generate_tests_for_service(self,
    analysis: Dict[str,
    Any],
    config: TestConfig) -> str:
        """ä¸ºæœåŠ¡ç”Ÿæˆæµ‹è¯•"""
        print(f"ğŸ§ª ä¸ºæœåŠ¡ {analysis['module_name']} ç”Ÿæˆæµ‹è¯•...")

        test_content = f'''"""
è‡ªåŠ¨ç”Ÿæˆçš„æœåŠ¡æµ‹è¯•
æ¨¡å—: {analysis['module_name']}
ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

æ³¨æ„: è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨ç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶ï¼Œè¯·æ ¹æ®å®é™…ä¸šåŠ¡é€»è¾‘è¿›è¡Œè°ƒæ•´å’Œå®Œå–„
"""

import pytest
from unittest.mock import Mock, patch, AsyncMock, MagicMock
import asyncio
from datetime import datetime, timedelta
from typing import Any, Dict, List

# å¯¼å…¥ç›®æ ‡æ¨¡å—
'''

        # æ·»åŠ å¯¼å…¥è¯­å¥
        test_content += f"from {analysis['module_name']} import (\n"

        # å¯¼å…¥ç±»
        for cls in analysis['classes']:
            test_content += f"    {cls['name']},\n"

        # å¯¼å…¥ç‹¬ç«‹å‡½æ•°
        for func in analysis['functions']:
            test_content += f"    {func['name']},\n"

        test_content += ")\n\n"

        # æ·»åŠ fixtures
        if config.include_fixtures:
            test_content += self._generate_fixtures(analysis)

        # ä¸ºæ¯ä¸ªç±»ç”Ÿæˆæµ‹è¯•
        for cls in analysis['classes']:
            test_content += self._generate_class_tests(cls, config)

        # ä¸ºæ¯ä¸ªå‡½æ•°ç”Ÿæˆæµ‹è¯•
        for func in analysis['functions']:
            test_content += self._generate_function_tests(func, config)

        return test_content

    def _generate_fixtures(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•fixtures"""
        fixtures = '''
@pytest.fixture
def sample_data():
    """ç¤ºä¾‹æ•°æ®fixture"""
    return {
        "id": 1,
        "name": "test",
        "created_at": datetime.now(),
        "updated_at": datetime.now()
    }

@pytest.fixture
def mock_repository():
    """æ¨¡æ‹Ÿä»“åº“fixture"""
    repo = Mock()
    repo.get_by_id.return_value = Mock()
    repo.get_all.return_value = []
    repo.save.return_value = Mock()
    repo.delete.return_value = True
    return repo

@pytest.fixture
def mock_service():
    """æ¨¡æ‹ŸæœåŠ¡fixture"""
    service = Mock()
    service.process.return_value = {"status": "success"}
    service.validate.return_value = True
    return service

'''
        return fixtures

    def _generate_class_tests(self, cls: Dict[str, Any], config: TestConfig) -> str:
        """ä¸ºç±»ç”Ÿæˆæµ‹è¯•"""
        class_name = cls['name']
        tests = f"""
class Test{class_name}:
    \"\"\"{class_name} æµ‹è¯•ç±»\"\"\"

    def setup_method(self):
        \"\"\"æ¯ä¸ªæµ‹è¯•æ–¹æ³•å‰çš„è®¾ç½®\"\"\"
        self.instance = {class_name}()

    def teardown_method(self):
        \"\"\"æ¯ä¸ªæµ‹è¯•æ–¹æ³•åçš„æ¸…ç†\"\"\"
        pass

    def test_init(self):
        \"\"\"æµ‹è¯•åˆå§‹åŒ–\"\"\"
        assert self.instance is not None
        assert isinstance(self.instance, {class_name})

"""

        # ä¸ºæ¯ä¸ªæ–¹æ³•ç”Ÿæˆæµ‹è¯•
        for method in cls['methods']:
            tests += self._generate_method_tests(class_name, method, config)

        return tests

    def _generate_method_tests(self,
    class_name: str,
    method: Dict[str,
    Any],
    config: TestConfig) -> str:
        """ä¸ºæ–¹æ³•ç”Ÿæˆæµ‹è¯•"""
        method_name = method['name']
        tests = f"""
    def test_{method_name}_basic(self):
        \"\"\"æµ‹è¯• {method_name} åŸºæœ¬åŠŸèƒ½\"\"\"
        # TODO: å®ç°å…·ä½“çš„æµ‹è¯•é€»è¾‘
        result = self.instance.{method_name}()
        assert result is not None

"""

        # å¦‚æœæ–¹æ³•æœ‰å‚æ•°ï¼Œç”Ÿæˆå‚æ•°åŒ–æµ‹è¯•
        if method['args'] and len(method['args']) > 1:  # æ’é™¤self
            tests += f"""
    @pytest.mark.parametrize("test_input, expected", [
        # TODO: æ·»åŠ æµ‹è¯•å‚æ•°ç»„åˆ
        (None, None),
    ])
    def test_{method_name}_parametrized(self, test_input, expected):
        \"\"\"æµ‹è¯• {method_name} å‚æ•°åŒ–\"\"\"
        # TODO: å®ç°å‚æ•°åŒ–æµ‹è¯•
        if test_input is not None:
            result = self.instance.{method_name}(test_input)
            assert result == expected

"""

        # å¦‚æœæ˜¯å¼‚æ­¥æ–¹æ³•ï¼Œç”Ÿæˆå¼‚æ­¥æµ‹è¯•
        if method['is_async']:
            tests = tests.replace("def test_", "async def test_")
            tests = tests.replace("assert result is not None", "result = await result")

        # å¦‚æœéœ€è¦mockï¼Œç”Ÿæˆmockæµ‹è¯•
        if config.include_mocks and method['dependencies']:
            tests += f"""
    @patch('object_to_mock')
    def test_{method_name}_with_mock(self, mock_obj):
        \"\"\"æµ‹è¯• {method_name} ä½¿ç”¨mock\"\"\"
        # TODO: é…ç½®mockå¯¹è±¡
        mock_obj.return_value = "mocked_result"

        result = self.instance.{method_name}()
        assert result is not None
        mock_obj.assert_called_once()

"""

        return tests

    def _generate_function_tests(self, func: Dict[str, Any], config: TestConfig) -> str:
        """ä¸ºå‡½æ•°ç”Ÿæˆæµ‹è¯•"""
        func_name = func['name']
        tests = f"""

def test_{func_name}_basic():
    \"\"\"æµ‹è¯• {func_name} åŸºæœ¬åŠŸèƒ½\"\"\"
    # TODO: å®ç°å…·ä½“çš„æµ‹è¯•é€»è¾‘
    from {func.get('module', 'src')} import {func_name}

    result = {func_name}()
    assert result is not None

"""

        # å¦‚æœå‡½æ•°æœ‰å‚æ•°ï¼Œç”Ÿæˆå‚æ•°åŒ–æµ‹è¯•
        if func['args'] and config.include_parametrized:
            tests += f"""
@pytest.mark.parametrize("test_input, expected", [
    # TODO: æ·»åŠ æµ‹è¯•å‚æ•°ç»„åˆ
    (None, None),
    ({{"key": "value"}}, {{"processed": True}}),
])
def test_{func_name}_parametrized(test_input, expected):
    \"\"\"æµ‹è¯• {func_name} å‚æ•°åŒ–\"\"\"
    from {func.get('module', 'src')} import {func_name}

    result = {func_name}(test_input)
    assert result == expected

"""

        # å¦‚æœæ˜¯å¼‚æ­¥å‡½æ•°ï¼Œç”Ÿæˆå¼‚æ­¥æµ‹è¯•
        if func['is_async']:
            tests = tests.replace("def test_", "async def test_")
            tests = tests.replace("result = ", "result = await ")

        # å¦‚æœéœ€è¦mockï¼Œç”Ÿæˆmockæµ‹è¯•
        if config.include_mocks and func['dependencies']:
            tests += f"""
@patch('dependency_to_mock')
def test_{func_name}_with_mock(mock_obj):
    \"\"\"æµ‹è¯• {func_name} ä½¿ç”¨mock\"\"\"
    from {func.get('module', 'src')} import {func_name}

    # TODO: é…ç½®mockå¯¹è±¡
    mock_obj.return_value = "mocked_value"

    result = {func_name}()
    assert result is not None
    mock_obj.assert_called_once()

"""

        return tests

class ServiceTestExecutor:
    """æœåŠ¡æµ‹è¯•æ‰§è¡Œå™¨"""

    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path.cwd()
        self.analyzer = ServiceAnalyzer(self.project_root)
        self.generator = ServiceTestGenerator(self.project_root)

    def discover_services(self) -> List[str]:
        """å‘ç°æœåŠ¡æ¨¡å—"""
        print("ğŸ” å‘ç°æœåŠ¡æ¨¡å—...")

        services = []
        src_dir = self.project_root / "src"

        # æŸ¥æ‰¾æœåŠ¡ç›®å½•
        service_dirs = [
            "services",
            "domain/services",
            "business",
            "core"
        ]

        for service_dir in service_dirs:
            service_path = src_dir / service_dir
            if service_path.exists():
                for file_path in service_path.glob("*.py"):
                    if file_path.name != "__init__.py":
                        # æ„å»ºæ¨¡å—è·¯å¾„
                        rel_path = file_path.relative_to(src_dir)
                        module_name = str(rel_path.with_suffix("")).replace("/", ".")
                        services.append(module_name)

        # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æœåŠ¡æ–‡ä»¶
        for pattern in ["*_service.py", "*_service_impl.py", "service_*.py"]:
            for file_path in src_dir.rglob(pattern):
                rel_path = file_path.relative_to(src_dir)
                module_name = str(rel_path.with_suffix("")).replace("/", ".")
                if module_name not in services:
                    services.append(module_name)

        print(f"âœ… å‘ç° {len(services)} ä¸ªæœåŠ¡æ¨¡å—")
        return sorted(services)

    def generate_tests_for_service(self, service_module: str) -> bool:
        """ä¸ºæŒ‡å®šæœåŠ¡ç”Ÿæˆæµ‹è¯•"""
        print(f"ğŸ¯ ä¸ºæœåŠ¡ {service_module} ç”Ÿæˆæµ‹è¯•...")

        # åˆ†ææœåŠ¡æ¨¡å—
        analysis = self.analyzer.analyze_service_module(service_module)
        if not analysis:
            print(f"âŒ æ— æ³•åˆ†ææœåŠ¡æ¨¡å—: {service_module}")
            return False

        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = TestConfig(
            target_module=service_module,
            output_file=f"tests/unit/test_{service_module.replace('.', '_')}.py",
            test_types=["unit", "integration"],
            include_mocks=True,
            include_fixtures=True,
            include_parametrized=True
        )

        # ç”Ÿæˆæµ‹è¯•ä»£ç 
        test_content = self.generator.generate_tests_for_service(analysis, config)

        # ä¿å­˜æµ‹è¯•æ–‡ä»¶
        test_file = self.project_root / config.output_file
        test_file.parent.mkdir(parents=True, exist_ok=True)

        try:
            with open(test_file, 'w', encoding='utf-8') as f:
                f.write(test_content)
            print(f"âœ… æµ‹è¯•æ–‡ä»¶å·²ç”Ÿæˆ: {test_file}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æµ‹è¯•æ–‡ä»¶å¤±è´¥: {e}")
            return False

    def generate_all_service_tests(self) -> Dict[str, bool]:
        """ä¸ºæ‰€æœ‰æœåŠ¡ç”Ÿæˆæµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ä¸ºæ‰€æœ‰æœåŠ¡ç”Ÿæˆæµ‹è¯•")
        print("=" * 50)

        services = self.discover_services()
        results = {}

        for service in services:
            print(f"\nğŸ“‹ å¤„ç†æœåŠ¡: {service}")
            success = self.generate_tests_for_service(service)
            results[service] = success

        # ç”ŸæˆæŠ¥å‘Š
        self._generate_generation_report(results)

        return results

    def _generate_generation_report(self, results: Dict[str, bool]):
        """ç”Ÿæˆæµ‹è¯•ç”ŸæˆæŠ¥å‘Š"""
        total_services = len(results)
        successful_services = sum(1 for success in results.values() if success)
        failed_services = total_services - successful_services

        report = f"""# æœåŠ¡æµ‹è¯•ç”ŸæˆæŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ€»æœåŠ¡æ•°**: {total_services}
**æˆåŠŸç”Ÿæˆ**: {successful_services}
**ç”Ÿæˆå¤±è´¥**: {failed_services}
**æˆåŠŸç‡**: {(successful_services / total_services * 100):.1f}%

## ğŸ“Š è¯¦ç»†ç»“æœ

### âœ… æˆåŠŸç”Ÿæˆçš„æœåŠ¡
"""

        for service, success in results.items():
            if success:
                report += f"- {service}\n"

        if failed_services > 0:
            report += "\n### âŒ ç”Ÿæˆå¤±è´¥çš„æœåŠ¡\n"
            for service, success in results.items():
                if not success:
                    report += f"- {service}\n"

        report += f"""
## ğŸš€ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **æ£€æŸ¥ç”Ÿæˆçš„æµ‹è¯•**: æŸ¥çœ‹ç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶ï¼Œæ ¹æ®å®é™…ä¸šåŠ¡é€»è¾‘è°ƒæ•´
2. **å®Œå–„æµ‹è¯•é€»è¾‘**: è¡¥å……TODOæ ‡è®°çš„æµ‹è¯•å®ç°
3. **è¿è¡Œæµ‹è¯•éªŒè¯**: æ‰§è¡Œç”Ÿæˆçš„æµ‹è¯•ç¡®ä¿å¯æ­£å¸¸è¿è¡Œ
4. **é›†æˆåˆ°CI/CD**: å°†æµ‹è¯•é›†æˆåˆ°æŒç»­é›†æˆæµç¨‹

## ğŸ“ ç”Ÿæˆçš„æµ‹è¯•æ–‡ä»¶

"""

        for service, success in results.items():
            if success:
                test_file = f"tests/unit/test_{service.replace('.', '_')}.py"
                report += f"- `{test_file}`\n"

        # ä¿å­˜æŠ¥å‘Š
        report_dir = self.project_root / "reports"
        report_dir.mkdir(exist_ok=True)
        report_file = report_dir / f"service_test_generation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"\nğŸ“„ ç”ŸæˆæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def create_prediction_service_test():
    """åˆ›å»ºé¢„æµ‹æœåŠ¡æµ‹è¯•"""
    content = '''"""é¢„æµ‹æœåŠ¡æµ‹è¯•"""
import pytest
from unittest.mock import Mock, patch, MagicMock, AsyncMock
from datetime import datetime
from src.models.prediction_service import PredictionService
from src.database.models.match import Match
from src.database.models.team import Team
from src.database.models.prediction import Prediction

class TestPredictionService:
    """é¢„æµ‹æœåŠ¡æµ‹è¯•"""

    @pytest.fixture
    def mock_repository(self):
        """æ¨¡æ‹Ÿé¢„æµ‹ä»“åº“"""
        return Mock()

    @pytest.fixture
    def mock_model(self):
        """æ¨¡æ‹ŸMLæ¨¡å‹"""
        mock_model = Mock()
        mock_model.predict.return_value = {
            "home_win": 0.65,
            "draw": 0.20,
            "away_win": 0.15
        }
        return mock_model

    @pytest.fixture
    def service(self, mock_repository, mock_model):
        """åˆ›å»ºé¢„æµ‹æœåŠ¡"""
        return PredictionService(
            repository=mock_repository,
            model=mock_model
        )

    def test_predict_match(self, service, mock_repository, mock_model):
        """æµ‹è¯•æ¯”èµ›é¢„æµ‹"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        home_team = Team(id=1, name="Team A")
        away_team = Team(id=2, name="Team B")
        match = Match(
            id=1,
            home_team=home_team,
            away_team=away_team,
            date=datetime(2024, 1, 1, 15, 0)
        )

        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.get_match_features.return_value = {
            "home_form": [1, 1, 0],
            "away_form": [0, 0, 1],
            "head_to_head": {"home_wins": 2, "away_wins": 1}
        }

        # è°ƒç”¨æ–¹æ³•
        result = service.predict_match(match.id)

        # éªŒè¯
        assert result["predicted_winner"] in ["home", "draw", "away"]
        assert "confidence" in result
        assert "probabilities" in result
        mock_model.predict.assert_called_once()

    def test_batch_predict(self, service, mock_repository, mock_model):
        """æµ‹è¯•æ‰¹é‡é¢„æµ‹"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        match_ids = [1, 2, 3]

        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.get_matches_by_ids.return_value = [
            Mock(id=1), Mock(id=2), Mock(id=3)
        ]

        # è°ƒç”¨æ–¹æ³•
        results = service.batch_predict(match_ids)

        # éªŒè¯
        assert len(results) == 3
        assert all("predicted_winner" in r for r in results)

    def test_get_prediction_accuracy(self, service, mock_repository):
        """æµ‹è¯•è·å–é¢„æµ‹å‡†ç¡®ç‡"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.get_completed_predictions.return_value = [
            Mock(is_correct=True),
            Mock(is_correct=True),
            Mock(is_correct=False),
            Mock(is_correct=True)
        ]

        # è°ƒç”¨æ–¹æ³•
        accuracy = service.get_accuracy(30)  # æœ€è¿‘30å¤©

        # éªŒè¯
        assert accuracy == 0.75  # 3/4 æ­£ç¡®

    def test_update_prediction(self, service, mock_repository):
        """æµ‹è¯•æ›´æ–°é¢„æµ‹"""
        prediction_id = 1
        update_data = {
            "confidence": 0.90,
            "notes": "Updated prediction"
        }

        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_prediction = Mock(spec=Prediction)
        mock_repository.get_by_id.return_value = mock_prediction

        # è°ƒç”¨æ–¹æ³•
        result = service.update_prediction(prediction_id, update_data)

        # éªŒè¯
        assert result == mock_prediction
        mock_repository.save.assert_called_once()

    def test_validate_prediction_input(self, service):
        """æµ‹è¯•é¢„æµ‹è¾“å…¥éªŒè¯"""
        # æœ‰æ•ˆè¾“å…¥
        valid_input = {
            "match_id": 1,
            "features": {
                "home_form": [1, 1, 0],
                "away_form": [0, 1, 1]
            }
        }
        assert service.validate_input(valid_input) is True

        # æ— æ•ˆè¾“å…¥ï¼ˆç¼ºå°‘å¿…è¦å­—æ®µï¼‰
        invalid_input = {
            "features": {}
        }
        assert service.validate_input(invalid_input) is False

    def test_get_feature_importance(self, service, mock_model):
        """æµ‹è¯•è·å–ç‰¹å¾é‡è¦æ€§"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_model.get_feature_importance.return_value = {
            "home_form": 0.30,
            "away_form": 0.25,
            "head_to_head": 0.20,
            "goals_average": 0.15,
            "injuries": 0.10
        }

        # è°ƒç”¨æ–¹æ³•
        importance = service.get_feature_importance()

        # éªŒè¯
        assert "home_form" in importance
        assert importance["home_form"] == 0.30

    def test_calculate_confidence(self, service):
        """æµ‹è¯•è®¡ç®—ç½®ä¿¡åº¦"""
        probabilities = {
            "home_win": 0.65,
            "draw": 0.20,
            "away_win": 0.15
        }

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆæœ€é«˜æ¦‚ç‡ï¼‰
        confidence = service.calculate_confidence(probabilities)
        assert confidence == 0.65

    def test_predict_with_outcome(self, service, mock_repository):
        """æµ‹è¯•å¸¦ç»“æœçš„é¢„æµ‹"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        match_id = 1
        actual_result = "home"

        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_prediction = Mock(
            predicted_winner="home",
            confidence=0.70,
            is_correct=True
        )
        mock_repository.get_prediction_by_match.return_value = mock_prediction

        # è°ƒç”¨æ–¹æ³•
        result = service.predict_with_outcome(match_id, actual_result)

        # éªŒè¯
        assert result["correct"] is True
        assert result["predicted"] == actual_result

    def test_get_model_performance(self, service, mock_repository):
        """æµ‹è¯•è·å–æ¨¡å‹æ€§èƒ½"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.get_performance_metrics.return_value = {
            "accuracy": 0.75,
            "precision": 0.80,
            "recall": 0.70,
            "f1_score": 0.75
        }

        # è°ƒç”¨æ–¹æ³•
        performance = service.get_model_performance()

        # éªŒè¯
        assert performance["accuracy"] == 0.75
        assert "precision" in performance
        assert "recall" in performance
        assert "f1_score" in performance
'''

    file_path = Path("tests/unit/services/test_prediction_service.py")
    file_path.parent.mkdir(parents=True, exist_ok=True)
    file_path.write_text(content)
    print(f"âœ… åˆ›å»ºæ–‡ä»¶: {file_path}")


def create_data_processing_service_test():
    """åˆ›å»ºæ•°æ®å¤„ç†æœåŠ¡æµ‹è¯•"""
    content = '''"""æ•°æ®å¤„ç†æœåŠ¡æµ‹è¯•"""
import pytest
from unittest.mock import Mock, patch, MagicMock, AsyncMock
import pandas as pd
from datetime import datetime
from src.services.data_processing import DataProcessingService

class TestDataProcessingService:
    """æ•°æ®å¤„ç†æœåŠ¡æµ‹è¯•"""

    @pytest.fixture
    def mock_repository(self):
        """æ¨¡æ‹Ÿæ•°æ®ä»“åº“"""
        return Mock()

    @pytest.fixture
    def mock_cache(self):
        """æ¨¡æ‹Ÿç¼“å­˜"""
        return Mock()

    @pytest.fixture
    def service(self, mock_repository, mock_cache):
        """åˆ›å»ºæ•°æ®å¤„ç†æœåŠ¡"""
        return DataProcessingService(
            repository=mock_repository,
            cache=mock_cache
        )

    def test_process_match_data(self, service, mock_repository):
        """æµ‹è¯•å¤„ç†æ¯”èµ›æ•°æ®"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        raw_data = {
            "match_id": 1,
            "home_team": "Team A",
            "away_team": "Team B",
            "date": "2024-01-01",
            "score": "2-1"
        }

        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.save_processed_data.return_value = True

        # è°ƒç”¨æ–¹æ³•
        result = service.process_match(raw_data)

        # éªŒè¯
        assert result is True
        mock_repository.save_processed_data.assert_called_once()

    def test_clean_player_data(self, service):
        """æµ‹è¯•æ¸…ç†çƒå‘˜æ•°æ®"""
        # å‡†å¤‡è„æ•°æ®
        dirty_data = {
            "name": "John Doe ",
            "age": " 25",
            "position": "  MIDFIELDER  ",
            "salary": "50000.00"
        }

        # è°ƒç”¨æ–¹æ³•
        cleaned_data = service.clean_player_data(dirty_data)

        # éªŒè¯
        assert cleaned_data["name"] == "John Doe"
        assert cleaned_data["age"] == 25
        assert cleaned_data["position"] == "MIDFIELDER"
        assert cleaned_data["salary"] == 50000.0

    def test_validate_match_data(self, service):
        """æµ‹è¯•éªŒè¯æ¯”èµ›æ•°æ®"""
        # æœ‰æ•ˆæ•°æ®
        valid_data = {
            "match_id": 1,
            "home_team_id": 1,
            "away_team_id": 2,
            "date": datetime(2024, 1, 1),
            "league": "Premier League"
        }
        assert service.validate_match_data(valid_data) is True

        # æ— æ•ˆæ•°æ®ï¼ˆç¼ºå°‘å­—æ®µï¼‰
        invalid_data = {
            "match_id": 1,
            "home_team_id": 1
        }
        assert service.validate_match_data(invalid_data) is False

    def test_aggregate_team_stats(self, service, mock_repository):
        """æµ‹è¯•èšåˆçƒé˜Ÿç»Ÿè®¡"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.get_team_matches.return_value = [
            {"team_id": 1, "goals_scored": 2, "goals_conceded": 1},
            {"team_id": 1, "goals_scored": 3, "goals_conceded": 2},
            {"team_id": 1, "goals_scored": 1, "goals_conceded": 1}
        ]

        # è°ƒç”¨æ–¹æ³•
        stats = service.aggregate_team_stats(1)

        # éªŒè¯
        assert stats["total_goals_scored"] == 6
        assert stats["total_goals_conceded"] == 4
        assert stats["matches_played"] == 3
        assert stats["average_goals_scored"] == 2.0

    def test_transform_data_format(self, service):
        """æµ‹è¯•è½¬æ¢æ•°æ®æ ¼å¼"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        data_list = [
            {"match_id": 1, "team": "A", "score": 2},
            {"match_id": 1, "team": "B", "score": 1}
        ]

        # è°ƒç”¨æ–¹æ³•
        transformed = service.transform_to_match_format(data_list)

        # éªŒè¯
        assert transformed["match_id"] == 1
        assert transformed["home_score"] == 2
        assert transformed["away_score"] == 1

    def test_handle_missing_data(self, service):
        """æµ‹è¯•å¤„ç†ç¼ºå¤±æ•°æ®"""
        # å‡†å¤‡å¸¦ç¼ºå¤±å€¼çš„æ•°æ®
        data = pd.DataFrame({
            "id": [1, 2, 3],
            "name": ["Team A", None, "Team C"],
            "score": [2, None, 1]
        })

        # è°ƒç”¨æ–¹æ³•
        cleaned_data = service.handle_missing_data(data)

        # éªŒè¯
        assert None not in cleaned_data["name"].values
        assert None not in cleaned_data["score"].values

    def test_calculate_derived_features(self, service):
        """æµ‹è¯•è®¡ç®—è¡ç”Ÿç‰¹å¾"""
        # å‡†å¤‡åŸºç¡€æ•°æ®
        base_data = {
            "home_goals": 2,
            "away_goals": 1,
            "home_shots": 10,
            "away_shots": 5
        }

        # è°ƒç”¨æ–¹æ³•
        features = service.calculate_features(base_data)

        # éªŒè¯
        assert features["goal_difference"] == 1
        assert features["total_goals"] == 3
        assert features["home_shot_accuracy"] == 0.2  # 2/10
        assert features["away_shot_accuracy"] == 0.2  # 1/5

    def test_batch_process_matches(self, service, mock_repository):
        """æµ‹è¯•æ‰¹é‡å¤„ç†æ¯”èµ›"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        matches = [
            {"id": 1, "home": "A", "away": "B"},
            {"id": 2, "home": "C", "away": "D"}
        ]

        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        mock_repository.batch_save.return_value = True

        # è°ƒç”¨æ–¹æ³•
        result = service.batch_process_matches(matches)

        # éªŒè¯
        assert result is True
        mock_repository.batch_save.assert_called_once()

    def test_data_quality_check(self, service):
        """æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        data = {
            "total_records": 1000,
            "null_values": 50,
            "duplicates": 10,
            "invalid_dates": 5
        }

        # è°ƒç”¨æ–¹æ³•
        quality_score = service.calculate_quality_score(data)

        # éªŒè¯
        assert 0 <= quality_score <= 1
        assert quality_score > 0.9  # æœŸæœ›è¾ƒé«˜çš„è´¨é‡åˆ†æ•°

    def test_cache_processed_data(self, service, mock_cache):
        """æµ‹è¯•ç¼“å­˜å¤„ç†åçš„æ•°æ®"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        data = {"match_id": 1, "processed": True}
        cache_key = "match_1"

        # è°ƒç”¨æ–¹æ³•
        service.cache_data(cache_key, data, ttl=3600)

        # éªŒè¯
        mock_cache.set.assert_called_once_with(cache_key, data, ex=3600)

    def test_get_cached_data(self, service, mock_cache):
        """æµ‹è¯•è·å–ç¼“å­˜æ•°æ®"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        cached_data = {"match_id": 1, "processed": True}
        mock_cache.get.return_value = cached_data

        # è°ƒç”¨æ–¹æ³•
        result = service.get_cached_data("match_1")

        # éªŒè¯
        assert result == cached_data
        mock_cache.get.assert_called_once_with("match_1")
'''

    file_path = Path("tests/unit/services/test_data_processing_service.py")
    file_path.parent.mkdir(parents=True, exist_ok=True)
    file_path.write_text(content)
    print(f"âœ… åˆ›å»ºæ–‡ä»¶: {file_path}")


def create_monitoring_service_test():
    """åˆ›å»ºç›‘æ§æœåŠ¡æµ‹è¯•"""
    content = '''"""ç›‘æ§æœåŠ¡æµ‹è¯•"""
import pytest
from unittest.mock import Mock, patch, MagicMock, AsyncMock
import time
from datetime import datetime, timedelta
from src.monitoring.system_monitor import SystemMonitor
from src.monitoring.metrics_collector import MetricsCollector

class TestSystemMonitor:
    """ç³»ç»Ÿç›‘æ§æµ‹è¯•"""

    @pytest.fixture
    def mock_metrics_collector(self):
        """æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†å™¨"""
        collector = Mock(spec=MetricsCollector)
        collector.collect_cpu_usage.return_value = 45.5
        collector.collect_memory_usage.return_value = 68.2
        collector.collect_disk_usage.return_value = 32.1
        return collector

    @pytest.fixture
    def monitor(self, mock_metrics_collector):
        """åˆ›å»ºç³»ç»Ÿç›‘æ§å™¨"""
        return SystemMonitor(metrics_collector=mock_metrics_collector)

    def test_get_system_metrics(self, monitor, mock_metrics_collector):
        """æµ‹è¯•è·å–ç³»ç»ŸæŒ‡æ ‡"""
        # è°ƒç”¨æ–¹æ³•
        metrics = monitor.get_system_metrics()

        # éªŒè¯
        assert "cpu_usage" in metrics
        assert "memory_usage" in metrics
        assert "disk_usage" in metrics
        assert metrics["cpu_usage"] == 45.5

    def test_health_check(self, monitor):
        """æµ‹è¯•å¥åº·æ£€æŸ¥"""
        # è®¾ç½®æ¨¡æ‹Ÿè¿”å›
        with patch('monitor.database_check') as mock_db,
    patch('monitor.redis_check') as mock_redis:
            mock_db.return_value = {"status": "healthy", "response_time": 10}
            mock_redis.return_value = {"status": "healthy", "response_time": 5}

            # è°ƒç”¨æ–¹æ³•
            health = monitor.check_health()

            # éªŒè¯
            assert health["overall_status"] == "healthy"
            assert "checks" in health
            assert len(health["checks"]) == 2

    def test_performance_monitoring(self, monitor):
        """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        start_time = time.time()
        time.sleep(0.01)  # æ¨¡æ‹Ÿæ“ä½œ
        end_time = time.time()

        # è°ƒç”¨æ–¹æ³•
        performance = monitor.measure_performance(start_time, end_time)

        # éªŒè¯
        assert "duration_ms" in performance
        assert performance["duration_ms"] > 0

    def test_alert_threshold_check(self, monitor):
        """æµ‹è¯•å‘Šè­¦é˜ˆå€¼æ£€æŸ¥"""
        # è®¾ç½®é«˜CPUä½¿ç”¨ç‡
        monitor.metrics_collector.collect_cpu_usage.return_value = 95.0

        # è°ƒç”¨æ–¹æ³•
        alerts = monitor.check_alerts()

        # éªŒè¯
        assert len(alerts) > 0
        assert any(alert["type"] == "high_cpu" for alert in alerts)

    def test_log_anomaly_detection(self, monitor):
        """æµ‹è¯•æ—¥å¿—å¼‚å¸¸æ£€æµ‹"""
        # å‡†å¤‡å¼‚å¸¸æ—¥å¿—
        logs = [
            {"level": "ERROR", "message": "Database connection failed"},
            {"level": "ERROR", "message": "Database connection failed"},
            {"level": "ERROR", "message": "Database connection failed"}
        ]

        # è°ƒç”¨æ–¹æ³•
        anomalies = monitor.detect_log_anomalies(logs)

        # éªŒè¯
        assert len(anomalies) > 0
        assert anomalies[0]["type"] == "repeated_errors"

    def test_api_endpoint_monitoring(self, monitor):
        """æµ‹è¯•APIç«¯ç‚¹ç›‘æ§"""
        # å‡†å¤‡APIæŒ‡æ ‡
        api_metrics = {
            "/api/predictions": {
                "requests": 1000,
                "errors": 10,
                "avg_response_time": 120
            },
            "/api/matches": {
                "requests": 500,
                "errors": 5,
                "avg_response_time": 80
            }
        }

        # è°ƒç”¨æ–¹æ³•
        health = monitor.check_api_health(api_metrics)

        # éªŒè¯
        assert "/api/predictions" in health
        assert health["/api/predictions"]["status"] in ["healthy", "degraded", "unhealthy"]

    def test_resource_usage_trend(self, monitor):
        """æµ‹è¯•èµ„æºä½¿ç”¨è¶‹åŠ¿"""
        # å‡†å¤‡å†å²æ•°æ®
        historical_data = [
            {"timestamp": datetime.now() - timedelta(hours=1), "cpu": 30},
            {"timestamp": datetime.now() - timedelta(minutes=30), "cpu": 45},
            {"timestamp": datetime.now(), "cpu": 60}
        ]

        # è°ƒç”¨æ–¹æ³•
        trend = monitor.analyze_resource_trend(historical_data)

        # éªŒè¯
        assert "direction" in trend  # up/down/stable
        assert "rate" in trend
        assert trend["direction"] == "up"

    def test_generate_monitoring_report(self, monitor):
        """æµ‹è¯•ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        # è®¾ç½®æ¨¡æ‹Ÿæ•°æ®
        with patch.object(monitor,
    'get_system_metrics') as mock_metrics,
    patch.object(monitor,
    'check_health') as mock_health:
            mock_metrics.return_value = {"cpu": 50, "memory": 60}
            mock_health.return_value = {"status": "healthy"}

            # è°ƒç”¨æ–¹æ³•
            report = monitor.generate_report()

            # éªŒè¯
            assert "system_metrics" in report
            assert "health_status" in report
            assert "timestamp" in report
            assert report["health_status"]["status"] == "healthy"


class TestMetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨æµ‹è¯•"""

    @pytest.fixture
    def collector(self):
        """åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨"""
        return MetricsCollector()

    def test_collect_cpu_usage(self, collector):
        """æµ‹è¯•æ”¶é›†CPUä½¿ç”¨ç‡"""
        with patch('psutil.cpu_percent') as mock_cpu:
            mock_cpu.return_value = 45.5

            # è°ƒç”¨æ–¹æ³•
            cpu_usage = collector.collect_cpu_usage()

            # éªŒè¯
            assert isinstance(cpu_usage, (int, float))
            assert 0 <= cpu_usage <= 100

    def test_collect_memory_usage(self, collector):
        """æµ‹è¯•æ”¶é›†å†…å­˜ä½¿ç”¨ç‡"""
        with patch('psutil.virtual_memory') as mock_memory:
            mock_memory_obj = Mock()
            mock_memory_obj.percent = 68.2
            mock_memory.return_value = mock_memory_obj

            # è°ƒç”¨æ–¹æ³•
            memory_usage = collector.collect_memory_usage()

            # éªŒè¯
            assert isinstance(memory_usage, (int, float))
            assert 0 <= memory_usage <= 100

    def test_collect_disk_usage(self, collector):
        """æµ‹è¯•æ”¶é›†ç£ç›˜ä½¿ç”¨ç‡"""
        with patch('psutil.disk_usage') as mock_disk:
            mock_disk_obj = Mock()
            mock_disk_obj.percent = 32.1
            mock_disk.return_value = mock_disk_obj

            # è°ƒç”¨æ–¹æ³•
            disk_usage = collector.collect_disk_usage()

            # éªŒè¯
            assert isinstance(disk_usage, (int, float))
            assert 0 <= disk_usage <= 100

    def test_collect_network_stats(self, collector):
        """æµ‹è¯•æ”¶é›†ç½‘ç»œç»Ÿè®¡"""
        with patch('psutil.net_io_counters') as mock_net:
            mock_net_obj = Mock()
            mock_net_obj.bytes_sent = 1000000
            mock_net_obj.bytes_recv = 2000000
            mock_net.return_value = mock_net_obj

            # è°ƒç”¨æ–¹æ³•
            net_stats = collector.collect_network_stats()

            # éªŒè¯
            assert "bytes_sent" in net_stats
            assert "bytes_recv" in net_stats
            assert net_stats["bytes_sent"] == 1000000

    def test_collect_process_count(self, collector):
        """æµ‹è¯•æ”¶é›†è¿›ç¨‹æ•°é‡"""
        with patch('psutil.pids') as mock_pids:
            mock_pids.return_value = [1, 2, 3, 4, 5]

            # è°ƒç”¨æ–¹æ³•
            process_count = collector.collect_process_count()

            # éªŒè¯
            assert process_count == 5

    def test_collect_active_connections(self, collector):
        """æµ‹è¯•æ”¶é›†æ´»åŠ¨è¿æ¥æ•°"""
        with patch('psutil.net_connections') as mock_connections:
            mock_connections.return_value = [Mock() for _ in range(10)]

            # è°ƒç”¨æ–¹æ³•
            connections = collector.collect_active_connections()

            # éªŒè¯
            assert connections == 10

    def test_collect_system_load(self, collector):
        """æµ‹è¯•æ”¶é›†ç³»ç»Ÿè´Ÿè½½"""
        with patch('os.getloadavg') as mock_loadavg:
            mock_loadavg.return_value = (1.0, 1.5, 2.0)

            # è°ƒç”¨æ–¹æ³•
            load = collector.collect_system_load()

            # éªŒè¯
            assert "1min" in load
            assert "5min" in load
            assert "15min" in load
            assert load["1min"] == 1.0

    def test_collect_all_metrics(self, collector):
        """æµ‹è¯•æ”¶é›†æ‰€æœ‰æŒ‡æ ‡"""
        with patch.object(collector,
    'collect_cpu_usage',
    return_value=50),
    patch.object(collector,
    'collect_memory_usage',
    return_value=60),
    patch.object(collector,
    'collect_disk_usage',
    return_value=30):

            # è°ƒç”¨æ–¹æ³•
            metrics = collector.collect_all()

            # éªŒè¯
            assert "cpu" in metrics
            assert "memory" in metrics
            assert "disk" in metrics
            assert "timestamp" in metrics
'''

    file_path = Path("tests/unit/services/test_monitoring_service.py")
    file_path.parent.mkdir(parents=True, exist_ok=True)
    file_path.write_text(content)
    print(f"âœ… åˆ›å»ºæ–‡ä»¶: {file_path}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹åˆ›å»ºæœåŠ¡å±‚æµ‹è¯•æ–‡ä»¶...")

    # åˆ›å»ºæœåŠ¡æµ‹è¯•ç›®å½•
    service_test_dir = Path("tests/unit/services")
    service_test_dir.mkdir(parents=True, exist_ok=True)

    # åˆ›å»ºå„ä¸ªæµ‹è¯•æ–‡ä»¶
    create_prediction_service_test()
    create_data_processing_service_test()
    create_monitoring_service_test()

    # ä½¿ç”¨è‡ªåŠ¨åŒ–ç”Ÿæˆå™¨
    executor = ServiceTestExecutor()
    results = executor.generate_all_service_tests()

    print(f"\nâœ… å·²åˆ›å»ºæœåŠ¡æµ‹è¯•æ–‡ä»¶!")
    print(f"\nğŸ“ è‡ªåŠ¨ç”Ÿæˆç»“æœ: {sum(1 for success in results.values() if success)}/{len(results)}")

    print("\nğŸƒ è¿è¡Œæµ‹è¯•:")
    print("   make test.unit")


if __name__ == "__main__":
    main()