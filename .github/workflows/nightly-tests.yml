name: Nightly Test Suite

on:
  schedule:
    # æ¯å¤©åŒ—äº¬æ—¶é—´å‡Œæ™¨ 2 ç‚¹ (UTC 18:00) è¿è¡Œ
    - cron: '0 18 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
      send_notification:
        description: 'Send notification to Slack'
        required: false
        default: true
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  # æµ‹è¯•ç¯å¢ƒé…ç½®
  TESTING: true
  TEST_ENV: nightly

jobs:
  # å‡†å¤‡é˜¶æ®µ
  prepare:
    name: å‡†å¤‡æµ‹è¯•ç¯å¢ƒ
    runs-on: ubuntu-latest
    outputs:
      cache-key: ${{ steps.cache-key.outputs.key }}
      should-run: ${{ steps.should-run.outputs.run }}
      test-matrix: ${{ steps.matrix.outputs.json }}
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # è·å–å®Œæ•´å†å²ç”¨äºè¦†ç›–ç‡å¯¹æ¯”

      - name: è®¾ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: è®¾ç½® Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: ç”Ÿæˆç¼“å­˜é”®
        id: cache-key
        run: |
          key="nightly-${{ github.run_number }}-${{ github.sha }}"
          echo "key=$key" >> $GITHUB_OUTPUT

      - name: æ£€æŸ¥æ˜¯å¦éœ€è¦è¿è¡Œ
        id: should-run
        run: |
          # æ£€æŸ¥è¿‡å»24å°æ—¶æ˜¯å¦æœ‰æ–°çš„æäº¤
          last_commit=$(git log -1 --since="24 hours ago" --format="%H")
          if [ -n "$last_commit" ] || [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "run=true" >> $GITHUB_OUTPUT
          else
            echo "run=false" >> $GITHUB_OUTPUT
          fi

      - name: ç”Ÿæˆæµ‹è¯•çŸ©é˜µ
        id: matrix
        run: |
          matrix=$(cat <<EOF
          {
            "include": [
              {
                "name": "unit-tests",
                "test-file": "tests/unit/",
                "timeout": 10,
                "parallel": 4
              },
              {
                "name": "integration-tests",
                "test-file": "tests/integration/",
                "timeout": 20,
                "parallel": 2
              },
              {
                "name": "e2e-tests",
                "test-file": "tests/e2e/",
                "timeout": 30,
                "parallel": 1
              }
            ]
          }
          EOF
          )
          echo "json=$matrix" >> $GITHUB_OUTPUT

  # å•å…ƒæµ‹è¯•
  unit-tests:
    name: å•å…ƒæµ‹è¯•
    runs-on: ubuntu-latest
    needs: prepare
    if: needs.prepare.outputs.should-run == 'true' && (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'unit')
    outputs:
      coverage-report: ${{ steps.coverage.outputs.report }}
      test-results: ${{ steps.results.outputs.json }}
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5432/football_test
      REDIS_URL: redis://localhost:6379/0
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: football_test
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install coverage-badge pytest-cov pytest-html pytest-json-report

      - name: è¿è¡Œå•å…ƒæµ‹è¯•
        id: tests
        timeout-minutes: 10
        run: |
          # è®¾ç½®ç¯å¢ƒå˜é‡
          export TESTING=true
          export TEST_ENV=nightly

          # è¿è¡Œæµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
          pytest tests/unit/ \
            -v \
            --tb=short \
            --junit-xml=reports/unit-junit.xml \
            --html=reports/unit-report.html \
            --self-contained-html \
            --json-report \
            --json-report-file=reports/unit-results.json \
            --cov=src \
            --cov-report=xml:reports/coverage-unit.xml \
            --cov-report=html:reports/html-unit \
            --cov-report=term-missing \
            -m "unit" \
            --maxfail=5 \
            2>&1 | tee tests.log

      - name: å¤„ç†è¦†ç›–ç‡æŠ¥å‘Š
        id: coverage
        if: always()
        run: |
          # æå–è¦†ç›–ç‡æ•°æ®
          coverage=$(python -c "
          import json
          try:
              with open('reports/unit-results.json', 'r') as f:
                  data = json.load(f)
                  coverage = data.get('summary', {}).get('coverage', {}).get 'percent', 0)
                  print(f'coverage={coverage}')
          except:
              print('coverage=0')
          ")
          echo "report=$coverage" >> $GITHUB_OUTPUT

      - name: æ±‡æ€»æµ‹è¯•ç»“æœ
        id: results
        if: always()
        run: |
          python -c "
          import json
          import xml.etree.ElementTree as ET

          # è§£æ JUnit XML
          tree = ET.parse('reports/unit-junit.xml')
          root = tree.getroot()

          results = {
              'total': int(root.attrib.get('tests', 0)),
              'passed': int(root.attrib.get('tests', 0)) - int(root.attrib.get('failures', 0)) - int(root.attrib.get('errors', 0)) - int(root.attrib.get('skipped', 0)),
              'failed': int(root.attrib.get('failures', 0)),
              'errors': int(root.attrib.get('errors', 0)),
              'skipped': int(root.attrib.get('skipped', 0)),
              'time': float(root.attrib.get('time', 0))
          }

          with open('unit-results.json', 'w') as f:
              json.dump(results, f)

          print(f'json={json.dumps(results)}')
          "

      - name: ä¸Šä¼ æµ‹è¯•æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-reports
          path: |
            reports/
            tests.log
          retention-days: 30

  # é›†æˆæµ‹è¯•
  integration-tests:
    name: é›†æˆæµ‹è¯•
    runs-on: ubuntu-latest
    needs: prepare
    if: needs.prepare.outputs.should-run == 'true' && (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration')
    outputs:
      test-results: ${{ steps.results.outputs.json }}
    env:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@localhost:5433/football_test
      REDIS_URL: redis://localhost:6380/0
      KAFKA_BOOTSTRAP_SERVERS: localhost:9093
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å¯åŠ¨æµ‹è¯•ç¯å¢ƒ
        run: |
          docker-compose -f docker-compose.test.yml up -d
          sleep 30  # ç­‰å¾…æœåŠ¡å°±ç»ª

      - name: å®‰è£…ä¾èµ–
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install pytest-asyncio

      - name: ç­‰å¾…æœåŠ¡å¥åº·æ£€æŸ¥
        run: |
          # ç­‰å¾…æ•°æ®åº“å°±ç»ª
          timeout 60 bash -c 'until pg_isready -h localhost -p 5433; do sleep 2; done'

          # ç­‰å¾… Redis å°±ç»ª
          timeout 60 bash -c 'until redis-cli -h localhost -p 6380 ping; do sleep 2; done'

          # ç­‰å¾… Kafka å°±ç»ª
          timeout 60 bash -c 'until nc -z localhost 9093; do sleep 2; done'

      - name: è¿è¡Œé›†æˆæµ‹è¯•
        id: tests
        timeout-minutes: 20
        run: |
          export TESTING=true
          export TEST_ENV=nightly

          pytest tests/integration/ \
            -v \
            --tb=short \
            --junit-xml=reports/integration-junit.xml \
            --html=reports/integration-report.html \
            --self-contained-html \
            --json-report \
            --json-report-file=reports/integration-results.json \
            -m "integration" \
            --maxfail=3 \
            2>&1 | tee integration-tests.log

      - name: æ±‡æ€»æµ‹è¯•ç»“æœ
        id: results
        if: always()
        run: |
          python -c "
          import json
          import xml.etree.ElementTree as ET

          tree = ET.parse('reports/integration-junit.xml')
          root = tree.getroot()

          results = {
              'total': int(root.attrib.get('tests', 0)),
              'passed': int(root.attrib.get('tests', 0)) - int(root.attrib.get('failures', 0)) - int(root.attrib.get('errors', 0)) - int(root.attrib.get('skipped', 0)),
              'failed': int(root.attrib.get('failures', 0)),
              'errors': int(root.attrib.get('errors', 0)),
              'skipped': int(root.attrib.get('skipped', 0)),
              'time': float(root.attrib.get('time', 0))
          }

          with open('integration-results.json', 'w') as f:
              json.dump(results, f)

          print(f'json={json.dumps(results)}')
          "

      - name: æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        if: always()
        run: |
          docker-compose -f docker-compose.test.yml down -v
          docker system prune -f

      - name: ä¸Šä¼ æµ‹è¯•æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-reports
          path: |
            reports/
            integration-tests.log
          retention-days: 30

  # E2E æµ‹è¯•
  e2e-tests:
    name: ç«¯åˆ°ç«¯æµ‹è¯•
    runs-on: ubuntu-latest
    needs: prepare
    if: needs.prepare.outputs.should-run == 'true' && (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'e2e')
    outputs:
      test-results: ${{ steps.results.outputs.json }}
      performance-metrics: ${{ steps.performance.outputs.json }}
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å¯åŠ¨ Staging ç¯å¢ƒ
        run: |
          # åˆ›å»ºæµ‹è¯•ç½‘ç»œ
          docker network create football-test-network

          # å¯åŠ¨ staging ç¯å¢ƒ
          docker-compose -f docker-compose.staging.yml up -d
          echo "ç­‰å¾…æœåŠ¡å¯åŠ¨..."
          sleep 60

      - name: å®‰è£…ä¾èµ–å’Œå·¥å…·
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install playwright
          playwright install --with-deps

          # å®‰è£…é¢å¤–çš„æµ‹è¯•å·¥å…·
          npm install -g wait-on

      - name: ç­‰å¾…æœåŠ¡å°±ç»ª
        run: |
          # ç­‰å¾… API æœåŠ¡
          timeout 120 bash -c 'until curl -f http://localhost:8000/health; do sleep 5; done'

          # ç­‰å¾…å‰ç«¯æœåŠ¡
          timeout 120 bash -c 'until curl -f http://localhost:3000; do sleep 5; done'

      - name: åŠ è½½æµ‹è¯•æ•°æ®
        run: |
          python scripts/load_staging_data.py

      - name: è¿è¡Œ E2E æµ‹è¯•
        id: tests
        timeout-minutes: 30
        run: |
          export TESTING=true
          export TEST_ENV=staging

          # è¿è¡Œ E2E æµ‹è¯•å¥—ä»¶
          python scripts/run_e2e_tests.py \
            --type all \
            --verbose \
            --no-cleanup \
            2>&1 | tee e2e-tests.log

      - name: æ±‡æ€»æµ‹è¯•ç»“æœ
        id: results
        if: always()
        run: |
          # æŸ¥æ‰¾æœ€æ–°çš„æµ‹è¯•æŠ¥å‘Š
          REPORT_DIR=$(ls -td reports/e2e_* | head -1)

          if [ -f "$REPORT_DIR/e2e_*.json" ]; then
            python -c "
          import json
          import glob

          report_file = glob.glob('$REPORT_DIR/e2e_*.json')[0]
          with open(report_file, 'r') as f:
              data = json.load(f)
              summary = data.get('summary', {})

              results = {
                  'total': summary.get('total', 0),
                  'passed': summary.get('passed', 0),
                  'failed': summary.get('failed', 0),
                  'skipped': summary.get('skipped', 0),
                  'error': summary.get('error', 0),
                  'duration': summary.get('duration', 0),
                  'success': summary.get('passed', 0) / max(summary.get('total', 1), 1) * 100
              }

              with open('e2e-results.json', 'w') as f:
                  json.dump(results, f)

              print(f'json={json.dumps(results)}')
          "
          else
            echo "json='{\"total\": 0, \"passed\": 0, \"failed\": 0, \"skipped\": 0, \"error\": 0, \"duration\": 0, \"success\": 0}'"
          fi

      - name: æ”¶é›†æ€§èƒ½æŒ‡æ ‡
        id: performance
        if: always()
        run: |
          # è§£ææ€§èƒ½æ•°æ®
          python -c "
          import json
          import re

          metrics = {
              'avg_response_time': 0,
              'max_response_time': 0,
              'throughput': 0,
              'error_rate': 0
          }

          # ä»æ—¥å¿—ä¸­æå–æ€§èƒ½æŒ‡æ ‡
          try:
              with open('e2e-tests.log', 'r') as f:
                  content = f.read()

              # æå–å“åº”æ—¶é—´
              response_times = re.findall(r'response time: (\d+\.\d+)ms', content)
              if response_times:
                  response_times = [float(t) for t in response_times]
                  metrics['avg_response_time'] = sum(response_times) / len(response_times)
                  metrics['max_response_time'] = max(response_times)

              # æå–ååé‡
              throughput_match = re.search(r'throughput: (\d+\.\d+) req/s', content)
              if throughput_match:
                  metrics['throughput'] = float(throughput_match.group(1))

          except Exception as e:
              print(f'Error parsing metrics: {e}')

          with open('performance-metrics.json', 'w') as f:
              json.dump(metrics, f)

          print(f'json={json.dumps(metrics)}')
          "

      - name: æ¸…ç† Staging ç¯å¢ƒ
        if: always()
        run: |
          docker-compose -f docker-compose.staging.yml down -v
          docker network rm football-test-network || true

      - name: ä¸Šä¼ æµ‹è¯•æŠ¥å‘Šå’Œæˆªå›¾
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-reports
          path: |
            reports/
            e2e-tests.log
            test-results/
          retention-days: 30

  # æ€§èƒ½æµ‹è¯•
  performance-tests:
    name: æ€§èƒ½åŸºå‡†æµ‹è¯•
    runs-on: ubuntu-latest
    needs: prepare
    if: needs.prepare.outputs.should-run == 'true' && (github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance')
    outputs:
      benchmark-results: ${{ steps.benchmark.outputs.json }}
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: å®‰è£…ä¾èµ–
        run: |
          python -m pip install --upgrade pip
          pip install -e .[dev]
          pip install pytest-benchmark

      - name: è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        id: benchmark
        timeout-minutes: 15
        run: |
          export TESTING=true

          pytest tests/performance/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-html=benchmark-report.html \
            --benchmark-autosave \
            -v

      - name: å¯¹æ¯”åŸºå‡†ç»“æœ
        run: |
          python -c "
          import json
          import requests
          from datetime import datetime

          # è¯»å–å½“å‰åŸºå‡†ç»“æœ
          with open('benchmark-results.json', 'r') as f:
              current = json.load(f)

          # ç”ŸæˆåŸºå‡†æŠ¥å‘Š
          report = {
              'date': datetime.now().isoformat(),
              'benchmarks': []
          }

          for name, data in current.get('benchmarks', {}).items():
              report['benchmarks'].append({
                  'name': name,
                  'min': data.get('min', 0),
                  'mean': data.get('mean', 0),
                  'max': data.get('max', 0),
                  'stddev': data.get('stddev', 0),
                  'rounds': data.get('rounds', 0)
              })

          with open('benchmark-summary.json', 'w') as f:
              json.dump(report, f, indent=2)

          print('Benchmark summary generated')
          "

      - name: ä¸Šä¼ åŸºå‡†æŠ¥å‘Š
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-benchmarks
          path: |
            benchmark-results.json
            benchmark-report.html
            benchmark-summary.json
          retention-days: 90

  # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
  generate-report:
    name: ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, performance-tests]
    if: always() && (needs.unit-tests.result != 'skipped' || needs.integration-tests.result != 'skipped' || needs.e2e-tests.result != 'skipped' || needs.performance-tests.result != 'skipped')
    steps:
      - name: æ£€å‡ºä»£ç 
        uses: actions/checkout@v4

      - name: è®¾ç½® Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: ä¸‹è½½æ‰€æœ‰æµ‹è¯•æŠ¥å‘Š
        uses: actions/download-artifact@v4
        with:
          path: all-reports

      - name: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        run: |
          python -c "
          import json
          import os
          from datetime import datetime

          # åˆå§‹åŒ–æŠ¥å‘Šæ•°æ®
          report = {
              'date': datetime.now().isoformat(),
              'run_number': '${{ github.run_number }}',
              'commit_sha': '${{ github.sha }}',
              'branch': '${{ github.ref_name }}',
              'trigger': '${{ github.event_name }}',
              'summary': {
                  'unit': {'total': 0, 'passed': 0, 'failed': 0, 'errors': 0, 'skipped': 0},
                  'integration': {'total': 0, 'passed': 0, 'failed': 0, 'errors': 0, 'skipped': 0},
                  'e2e': {'total': 0, 'passed': 0, 'failed': 0, 'errors': 0, 'skipped': 0},
                  'performance': {'benchmarks': 0}
              },
              'coverage': {'percentage': 0},
              'quality_gate': {'passed': True, 'issues': []},
              'recommendations': []
          }

          # å¤„ç†å•å…ƒæµ‹è¯•ç»“æœ
          if os.path.exists('all-reports/unit-test-reports/unit-results.json'):
              with open('all-reports/unit-test-reports/unit-results.json', 'r') as f:
                  data = json.load(f)
                  report['summary']['unit'] = data

          # å¤„ç†é›†æˆæµ‹è¯•ç»“æœ
          if os.path.exists('all-reports/integration-test-reports/integration-results.json'):
              with open('all-reports/integration-test-reports/integration-results.json', 'r') as f:
                  data = json.load(f)
                  report['summary']['integration'] = data

          # å¤„ç† E2E æµ‹è¯•ç»“æœ
          if os.path.exists('all-reports/e2e-test-reports/e2e-results.json'):
              with open('all-reports/e2e-test-reports/e2e-results.json', 'r') as f:
                  data = json.load(f)
                  report['summary']['e2e'] = data

          # å¤„ç†æ€§èƒ½æµ‹è¯•ç»“æœ
          if os.path.exists('all-reports/performance-benchmarks/benchmark-summary.json'):
              with open('all-reports/performance-benchmarks/benchmark-summary.json', 'r') as f:
                  data = json.load(f)
                  report['summary']['performance']['benchmarks'] = len(data.get('benchmarks', []))

          # è®¡ç®—æ€»ä½“ç»Ÿè®¡
          total_tests = sum(s.get('total', 0) for s in report['summary'].values() if isinstance(s, dict))
          total_passed = sum(s.get('passed', 0) for s in report['summary'].values() if isinstance(s, dict))
          total_failed = sum(s.get('failed', 0) for s in report['summary'].values() if isinstance(s, dict))

          report['overall'] = {
              'total_tests': total_tests,
              'passed': total_passed,
              'failed': total_failed,
              'success_rate': (total_passed / max(total_tests, 1)) * 100
          }

          # è´¨é‡é—¨ç¦æ£€æŸ¥
          issues = []
          if report['summary']['unit'].get('failed', 0) > 0:
              issues.append('Unit tests have failures')
          if report['summary']['integration'].get('failed', 0) > 0:
              issues.append('Integration tests have failures')
          if report['summary']['e2e'].get('failed', 0) > 0:
              issues.append('E2E tests have failures')

          report['quality_gate']['passed'] = len(issues) == 0
          report['quality_gate']['issues'] = issues

          # ç”Ÿæˆå»ºè®®
          if total_tests > 0:
              if report['overall']['success_rate'] < 95:
                  report['recommendations'].append('Overall success rate is below 95%')
              if report['summary']['e2e'].get('passed', 0) == 0:
                  report['recommendations'].append('No E2E tests passed - check critical flows')

          # ä¿å­˜æŠ¥å‘Š
          with open('nightly-test-report.json', 'w') as f:
              json.dump(report, f, indent=2)

          print(f'Generated comprehensive report with {total_tests} total tests')
          "

      - name: ç”Ÿæˆ Markdown æŠ¥å‘Š
        run: |
          python -c "
          import json
          from datetime import datetime

          # è¯»å– JSON æŠ¥å‘Š
          with open('nightly-test-report.json', 'r') as f:
              report = json.load(f)

          # ç”Ÿæˆ Markdown æŠ¥å‘Š
          md_content = f'''# Nightly Test Report

          ## ğŸ“Š æ‰§è¡Œæ¦‚è¦

          - **æ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC
          - **è¿è¡Œç¼–å·**: #{report['run_number']}
          - **æäº¤**: {report['commit_sha'][:7]}
          - **åˆ†æ”¯**: {report['branch']}
          - **è§¦å‘æ–¹å¼**: {report['trigger']}

          ## ğŸ§ª æµ‹è¯•ç»“æœæ±‡æ€»

          | æµ‹è¯•ç±»å‹ | æ€»æ•° | é€šè¿‡ | å¤±è´¥ | é”™è¯¯ | è·³è¿‡ |
          |---------|------|------|------|------|------|
          | å•å…ƒæµ‹è¯• | {report['summary']['unit'].get('total', 0)} | {report['summary']['unit'].get('passed', 0)} | {report['summary']['unit'].get('failed', 0)} | {report['summary']['unit'].get('errors', 0)} | {report['summary']['unit'].get('skipped', 0)} |
          | é›†æˆæµ‹è¯• | {report['summary']['integration'].get('total', 0)} | {report['summary']['integration'].get('passed', 0)} | {report['summary']['integration'].get('failed', 0)} | {report['summary']['integration'].get('errors', 0)} | {report['summary']['integration'].get('skipped', 0)} |
          | E2E æµ‹è¯• | {report['summary']['e2e'].get('total', 0)} | {report['summary']['e2e'].get('passed', 0)} | {report['summary']['e2e'].get('failed', 0)} | {report['summary']['e2e'].get('errors', 0)} | {report['summary']['e2e'].get('skipped', 0)} |
          | **æ€»è®¡** | **{report['overall']['total_tests']}** | **{report['overall']['passed']}** | **{report['overall']['failed']}** | **-** | **-** |

          ### ğŸ“ˆ æ€»ä½“æˆåŠŸç‡
          {report['overall']['success_rate']:.1f}%

          ## ğŸšª è´¨é‡é—¨ç¦

          **çŠ¶æ€**: {'âœ… é€šè¿‡' if report['quality_gate']['passed'] else 'âŒ å¤±è´¥'}

          '''

          if report['quality_gate']['issues']:
              md_content += '### âš ï¸ é—®é¢˜\n\n'
              for issue in report['quality_gate']['issues']:
                  md_content += f'- {issue}\n'

          if report['recommendations']:
              md_content += '\n### ğŸ’¡ å»ºè®®\n\n'
              for rec in report['recommendations']:
                  md_content += f'- {rec}\n'

          md_content += f'''

          ---
          *æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
          '''

          # ä¿å­˜ Markdown æŠ¥å‘Š
          with open('nightly-test-report.md', 'w') as f:
              f.write(md_content)

          print('Markdown report generated')
          "

      - name: ç”Ÿæˆè¦†ç›–ç‡å¾½ç« 
        run: |
          python -c "
          import json

          # è¯»å–æŠ¥å‘Š
          with open('nightly-test-report.json', 'r') as f:
              report = json.load(f)

          coverage = 0  # é»˜è®¤å€¼

          # ç”Ÿæˆå¾½ç« 
          color = 'brightgreen' if coverage >= 80 else 'green' if coverage >= 60 else 'yellow' if coverage >= 40 else 'red'

          badge = f'''
          <svg xmlns='http://www.w3.org/2000/svg' width='120' height='20'>
            <linearGradient id='b' x2='0' y2='100%'>
              <stop offset='0' stop-color='#bbb' stop-opacity='.1'/>
              <stop offset='1' stop-opacity='.1'/>
            </linearGradient>
            <mask id='a'>
              <rect width='120' height='20' rx='3' fill='#fff'/>
            </mask>
            <g mask='url(#a)'>
              <path fill='#555' d='M0 0h39v20H0z'/>
              <path fill='{color}' d='M39 0h81v20H39z'/>
              <path fill='url(#b)' d='M0 0h120v20H0z'/>
            </g>
            <g fill='#fff' text-anchor='middle' font-family='DejaVu Sans,Verdana,Geneva,sans-serif' font-size='11'>
              <text x='19.5' y='15' fill='#010101' fill-opacity='.3'>coverage</text>
              <text x='19.5' y='14'>coverage</text>
              <text x='79.5' y='15' fill='#010101' fill-opacity='.3'>{coverage}%</text>
              <text x='79.5' y='14'>{coverage}%</text>
            </g>
          </svg>
          '''

          with open('coverage-badge.svg', 'w') as f:
              f.write(badge)

          print(f'Coverage badge generated: {coverage}%')
          "

      - name: æ›´æ–° README ä¸­çš„å¾½ç« 
        if: github.ref == 'refs/heads/main'
        run: |
          # å¦‚æœæ˜¯ä¸»åˆ†æ”¯ï¼Œæ›´æ–° README ä¸­çš„å¾½ç« 
          echo "Updating README badges..."

      - name: ä¸Šä¼ ç»¼åˆæŠ¥å‘Š
        uses: actions/upload-artifact@v4
        with:
          name: nightly-test-report
          path: |
            nightly-test-report.json
            nightly-test-report.md
            coverage-badge.svg
          retention-days: 90

  # å‘é€é€šçŸ¥
  notify:
    name: å‘é€é€šçŸ¥
    runs-on: ubuntu-latest
    needs: [generate-report]
    if: always() && (github.event.inputs.send_notification == 'true' || github.event_name == 'schedule')
    steps:
      - name: ä¸‹è½½æŠ¥å‘Š
        uses: actions/download-artifact@v4
        with:
          name: nightly-test-report
          path: report

      - name: å‘é€ Slack é€šçŸ¥
        if: env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          python -c "
          import json
          import os
          import requests
          from datetime import datetime

          # è¯»å–æŠ¥å‘Š
          with open('report/nightly-test-report.json', 'r') as f:
              report = json.load(f)

          # æ„å»º Slack æ¶ˆæ¯
          color = 'good' if report['quality_gate']['passed'] else 'danger'
          success_rate = report['overall']['success_rate']

          message = {
              'attachments': [{
                  'color': color,
                  'title': f'ğŸ§ª Nightly Test Report - #{report[\"run_number\"]}',
                  'title_link': f'https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}',
                  'fields': [
                      {
                          'title': 'æ€»ä½“æˆåŠŸç‡',
                          'value': f'{success_rate:.1f}%',
                          'short': True
                      },
                      {
                          'title': 'æ€»æµ‹è¯•æ•°',
                          'value': str(report['overall']['total_tests']),
                          'short': True
                      },
                      {
                          'title': 'åˆ†æ”¯',
                          'value': report['branch'],
                          'short': True
                      },
                      {
                          'title': 'æäº¤',
                          'value': report['commit_sha'][:7],
                          'short': True
                      }
                  ],
                  'footer': 'Football Prediction System',
                  'ts': int(datetime.now().timestamp())
              }]
          }

          # æ·»åŠ é—®é¢˜è¯¦æƒ…
          if not report['quality_gate']['passed']:
              message['attachments'][0]['fields'].append({
                  'title': 'âŒ è´¨é‡é—¨ç¦é—®é¢˜',
                  'value': '\\n'.join(f'â€¢ {issue}' for issue in report['quality_gate']['issues']),
                  'short': False
              })

          # å‘é€é€šçŸ¥
          if os.getenv('SLACK_WEBHOOK_URL'):
              response = requests.post(
                  os.getenv('SLACK_WEBHOOK_URL'),
                  json=message,
                  headers={'Content-Type': 'application/json'}
              )

              if response.status_code == 200:
                  print('Slack notification sent successfully')
              else:
                  print(f'Failed to send Slack notification: {response.status_code}')
                  print(response.text)
          else:
              print('Slack webhook URL not configured')
          "

      - name: å‘é€é‚®ä»¶é€šçŸ¥
        if: env.SMTP_HOST != ''
        env:
          SMTP_HOST: ${{ secrets.SMTP_HOST }}
          SMTP_PORT: ${{ secrets.SMTP_PORT }}
          SMTP_USER: ${{ secrets.SMTP_USER }}
          SMTP_PASS: ${{ secrets.SMTP_PASS }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
        run: |
          python -c "
          import smtplib
          import json
          from email.mime.text import MIMEText
          from email.mime.multipart import MIMEMultipart
          from datetime import datetime

          # è¯»å–æŠ¥å‘Š
          with open('report/nightly-test-report.md', 'r') as f:
              md_content = f.read()

          # åˆ›å»ºé‚®ä»¶
          msg = MIMEMultipart()
          msg['From'] = os.getenv('SMTP_USER')
          msg['To'] = os.getenv('EMAIL_TO')
          msg['Subject'] = f'Nightly Test Report - #{report[\"run_number\"]} - {datetime.now().strftime(\"%Y-%m-%d\")}'

          # è½¬æ¢ Markdown ä¸º HTML (ç®€å•ç‰ˆæœ¬)
          html_content = f'''
          <html>
          <body>
          <pre style='font-family: monospace; white-space: pre-wrap;'>{md_content}</pre>
          </body>
          </html>
          '''

          msg.attach(MIMEText(html_content, 'html'))

          # å‘é€é‚®ä»¶
          if all(os.getenv(v) for v in ['SMTP_HOST', 'SMTP_PORT', 'SMTP_USER', 'EMAIL_TO']):
              with smtplib.SMTP(os.getenv('SMTP_HOST'), int(os.getenv('SMTP_PORT'))) as server:
                  if os.getenv('SMTP_PASS'):
                      server.starttls()
                      server.login(os.getenv('SMTP_USER'), os.getenv('SMTP_PASS'))

                  server.send_message(msg)
              print('Email notification sent successfully')
          else:
              print('Email configuration incomplete')
          "

      - name: åˆ›å»º GitHub Issue (å¦‚æœå¤±è´¥)
        if: failure() && github.ref == 'refs/heads/main'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');

            // è¯»å–æŠ¥å‘Š
            const report = JSON.parse(fs.readFileSync('report/nightly-test-report.json', 'utf8'));

            // æ„å»º Issue å†…å®¹
            const body = `
            ## ğŸš¨ Nightly Tests Failed

            **Run Number**: #${report.run_number}
            **Commit**: ${report.commit_sha}
            **Date**: ${report.date}

            ### Test Results Summary
            - **Total Tests**: ${report.overall.total_tests}
            - **Passed**: ${report.overall.passed}
            - **Failed**: ${report.overall.failed}
            - **Success Rate**: ${report.overall.success_rate.toFixed(1)}%

            ### Quality Gate Issues
            ${report.quality_gate.issues.map(issue => `- ${issue}`).join('\n')}

            ### Recommendations
            ${report.recommendations.map(rec => `- ${rec}`).join('\n')}

            [View detailed report](${context.payload.repository.html_url}/actions/runs/${context.runId})
            `;

            // åˆ›å»º Issue
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Nightly tests failed - Run #${report.run_number}`,
              body: body,
              labels: ['bug', 'ci/cd', 'tests']
            });

  # æ¸…ç†æ—§å·¥ä»¶
  cleanup:
    name: æ¸…ç†æ—§å·¥ä»¶
    runs-on: ubuntu-latest
    needs: notify
    if: always()
    steps:
      - name: åˆ é™¤æ—§å·¥ä»¶
        uses: actions/github-script@v6
        with:
          script: |
            // è·å–30å¤©å‰çš„å·¥ä»¶
            const artifacts = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: '.github/workflows/nightly-tests.yml',
              created: `<${new Date(Date.now() - 30 * 24 * 60 * 60 * 1000).toISOString()}`,
              per_page: 100
            });

            // åˆ é™¤æ—§çš„è¿è¡Œè®°å½•å’Œå·¥ä»¶
            for (const run of artifacts.data.workflow_runs) {
              if (run.status === 'completed' && run.conclusion !== 'success') {
                try {
                  await github.rest.actions.deleteWorkflowRun({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    run_id: run.id
                  });
                  console.log(`Deleted workflow run #${run.id}`);
                } catch (error) {
                  console.log(`Failed to delete workflow run #${run.id}: ${error.message}`);
                }
              }
            }