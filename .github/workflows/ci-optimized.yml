name: CI Pipeline

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: "0 2 * * *"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  unit-fast:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}-${{ hashFiles('**/requirements*.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m venv .venv
        source .venv/bin/activate
        python -m pip install --upgrade pip
        pip install -r requirements.txt -r requirements-dev.txt
        pip install -e .
        pip install matplotlib
        make check-deps

    - name: Run fast unit tests
      run: |
        source .venv/bin/activate
        export PYTHONPATH="$(pwd):${PYTHONPATH}"
        pytest tests/unit --maxfail=1 --disable-warnings --cov=src --cov-report=term --cov-report=xml --cov-fail-under=0

    - name: Upload coverage to Codecov
      if: github.event_name == 'push'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unit-fast
        name: codecov-umbrella

    - name: Upload fast suite artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: fast-suite-results
        path: |
          coverage.xml

  slow-suite:
    runs-on: ubuntu-latest
    needs: unit-fast
    timeout-minutes: 45
    if: (github.event_name == 'push' && github.ref == 'refs/heads/main') || github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: football_prediction_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      kafka:
        image: bitnami/kafka:3.6.1
        env:
          KAFKA_ENABLE_KRAFT: "yes"
          KAFKA_CFG_PROCESS_ROLES: controller,broker
          KAFKA_CFG_NODE_ID: "1"
          KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
          KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
          KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
          KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
          KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
          KAFKA_CFG_NUM_PARTITIONS: "1"
          KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
          KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
          KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR: "1"
          ALLOW_PLAINTEXT_LISTENER: "yes"
        options: >-
          --health-cmd "kafka-topics.sh --bootstrap-server localhost:9092 --list"
          --health-interval 15s
          --health-timeout 10s
          --health-retries 10
          --health-start-period 30s
        ports:
          - 9092:9092

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}-${{ hashFiles('**/requirements*.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m venv .venv
        source .venv/bin/activate
        python -m pip install --upgrade pip
        pip install -r requirements.txt -r requirements-dev.txt
        pip install -e .
        pip install matplotlib
        make check-deps

    - name: Set up environment variables
      run: |
        echo "TEST_DB_HOST=localhost" >> $GITHUB_ENV
        echo "TEST_DB_PORT=5432" >> $GITHUB_ENV
        echo "TEST_DB_NAME=football_prediction_test" >> $GITHUB_ENV
        echo "TEST_DB_USER=postgres" >> $GITHUB_ENV
        echo "TEST_DB_PASSWORD=postgres" >> $GITHUB_ENV
        echo "ENVIRONMENT=test" >> $GITHUB_ENV

    - name: Configure coverage threshold
      run: |
        if [ "${{ github.event_name }}" = "schedule" ]; then
          echo "COV_FAIL_UNDER=70" >> $GITHUB_ENV
        elif [ "${{ github.ref }}" = "refs/heads/main" ]; then
          echo "COV_FAIL_UNDER=70" >> $GITHUB_ENV
        else
          echo "COV_FAIL_UNDER=20" >> $GITHUB_ENV
        fi

    - name: Wait for database
      run: until pg_isready -h localhost -p 5432; do sleep 2; done

    - name: Start MLflow server
      run: |
        pip install mlflow==2.8.1
        mlflow server --backend-store-uri sqlite:///tmp/mlflow.db --default-artifact-root /tmp/mlflow-artifacts --host 0.0.0.0 --port 5000 &
        sleep 15
        # 使用简单的端口检查而不是健康检查端点
        timeout 60 bash -c 'until nc -z localhost 5000; do echo "Waiting for MLflow port..."; sleep 2; done'
        # 验证MLflow服务是否响应
        curl -s http://localhost:5000/ || echo "MLflow started but not fully ready"

    - name: Prepare test database schema and seed data
      run: |
        source .venv/bin/activate
        export PYTHONPATH="$(pwd):${PYTHONPATH}"
        python scripts/prepare_test_db.py

    - name: Run nightly test suites
      run: |
        set -o pipefail
        source .venv/bin/activate
        export PYTHONPATH="$(pwd):${PYTHONPATH}"
        coverage erase
        pytest tests/unit --cov=src --cov-report=term --cov-fail-under=$COV_FAIL_UNDER \
          --durations=10 --maxfail=1 -vv --full-trace --log-cli-level=INFO | tee fast.log
        pytest tests/slow --cov=src --cov-append --cov-fail-under=$COV_FAIL_UNDER \
          --durations=10 --maxfail=1 -vv --full-trace --log-cli-level=INFO | tee slow.log
        pytest tests/integration -m "integration" --cov=src --cov-append --cov-fail-under=$COV_FAIL_UNDER \
          --durations=10 --maxfail=1 -vv --full-trace --log-cli-level=INFO | tee integration.log
        coverage combine
        coverage report -m > coverage.txt

    - name: Generate CI report
      run: |
        source .venv/bin/activate
        python scripts/generate_ci_report.py

    - name: Validate coverage consistency
      run: |
        source .venv/bin/activate
        python scripts/validate_coverage_consistency.py

    - name: Upload nightly artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: nightly-test-results
        path: |
          fast.log
          slow.log
          integration.log
          coverage.txt
          docs/CI_REPORT.md
    - name: Commit and Push CI Report
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        git config --global user.name "ci-bot"
        git config --global user.email "ci-bot@example.com"
        git add docs/CI_REPORT.md
        git commit -m "ci: update nightly CI_REPORT.md [skip ci]" || echo "No changes to commit"
        git push origin HEAD:${GITHUB_REF}

  integration-tests:
    runs-on: ubuntu-latest
    needs: unit-fast

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: football_prediction_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      kafka:
        image: bitnami/kafka:3.6.1
        env:
          KAFKA_ENABLE_KRAFT: "yes"
          KAFKA_CFG_PROCESS_ROLES: controller,broker
          KAFKA_CFG_NODE_ID: "1"
          KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
          KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093
          KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
          KAFKA_CFG_CONTROLLER_LISTENER_NAMES: CONTROLLER
          KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
          KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE: "true"
          KAFKA_CFG_NUM_PARTITIONS: "1"
          KAFKA_CFG_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
          KAFKA_CFG_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "1"
          KAFKA_CFG_TRANSACTION_STATE_LOG_MIN_ISR: "1"
          ALLOW_PLAINTEXT_LISTENER: "yes"
        options: >-
          --health-cmd "kafka-topics.sh --bootstrap-server localhost:9092 --list"
          --health-interval 15s
          --health-timeout 10s
          --health-retries 10
          --health-start-period 30s
        ports:
          - 9092:9092

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}-${{ hashFiles('**/requirements*.lock') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m venv .venv
        source .venv/bin/activate
        python -m pip install --upgrade pip
        pip install -r requirements.txt -r requirements-dev.txt
        pip install -e .
        make check-deps

    - name: Export integration environment variables
      run: |
        echo "ENVIRONMENT=test" >> $GITHUB_ENV
        echo "TEST_DB_HOST=localhost" >> $GITHUB_ENV
        echo "TEST_DB_PORT=5432" >> $GITHUB_ENV
        echo "TEST_DB_NAME=football_prediction_test" >> $GITHUB_ENV
        echo "TEST_DB_USER=postgres" >> $GITHUB_ENV
        echo "TEST_DB_PASSWORD=postgres" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379/0" >> $GITHUB_ENV
        echo "KAFKA_BOOTSTRAP_SERVERS=localhost:9092" >> $GITHUB_ENV
        echo "MLFLOW_TRACKING_URI=http://localhost:5000" >> $GITHUB_ENV

    - name: Wait for dependent services
      run: |
        until pg_isready -h localhost -p 5432; do echo "Waiting for Postgres..."; sleep 2; done
        until nc -z localhost 6379; do echo "Waiting for Redis..."; sleep 2; done
        until nc -z localhost 9092; do echo "Waiting for Kafka..."; sleep 2; done

        # Start MLflow server
        pip install mlflow==2.8.1
        mlflow server --backend-store-uri sqlite:///tmp/mlflow.db --default-artifact-root /tmp/mlflow-artifacts --host 0.0.0.0 --port 5000 &
        timeout 60 bash -c 'until nc -z localhost 5000; do echo "Waiting for MLflow port..."; sleep 2; done'

    - name: Prepare test database schema and seed data
      run: |
        source .venv/bin/activate
        export PYTHONPATH="$(pwd):${PYTHONPATH}"
        python scripts/prepare_test_db.py

    - name: Run integration test suite
      run: |
        source .venv/bin/activate
        export PYTHONPATH="$(pwd):${PYTHONPATH}"
        pytest tests/integration -m "integration"

  pipeline-smoke:
    runs-on: ubuntu-latest
    needs: unit-fast

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pipeline-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pipeline-

    - name: Install minimal dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pandas scikit-learn

    - name: Run synthetic pipeline smoke test
      run: |
        python scripts/run_pipeline.py --output artifacts/pipeline-smoke --samples 48

    - name: Upload pipeline artefacts
      uses: actions/upload-artifact@v4
      with:
        name: pipeline-smoke-results
        path: artifacts/pipeline-smoke

  ci-verify:
    runs-on: ubuntu-latest
    needs: [unit-fast, slow-suite, pipeline-smoke]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Install Docker Compose
      run: |
        sudo curl -L "https://github.com/docker/compose/releases/download/v2.21.0/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
        sudo chmod +x /usr/local/bin/docker-compose
        docker-compose --version

    - name: Run CI verification script
      run: |
        chmod +x ./ci-verify.sh
        ./ci-verify.sh

    - name: Upload CI verification results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ci-verify-results
        path: |
          coverage.xml
          htmlcov/
