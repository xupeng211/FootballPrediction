name: Comprehensive CI/CD Pipeline

on:
  push:
    branches: [main, develop, "feature/*", "hotfix/*"]
  pull_request:
    branches: [main, develop]
  release:
    types: [published]

env:
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"

jobs:
  # ä»£ç è´¨é‡æ£€æŸ¥
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    outputs:
      quality-score: ${{ steps.quality-report.outputs.score }}
      issues-count: ${{ steps.quality-report.outputs.issues-count }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          make install
          pip install bandit safety pytest-cov pytest-benchmark

      - name: Run Ruff linting and formatting
        run: |
          ruff check src/ tests/ --output-format=github --output-file=ruff-report.json
          ruff format src/ tests/ --check

          # è¾“å‡ºè´¨é‡æŠ¥å‘Š
          python3 << 'EOF'
          import json
          import sys

          try:
              with open('ruff-report.json', 'r') as f:
                  report = json.load(f)

              issues = report.get('results', [])
              error_count = len([i for i in issues if i['type'] == 'error'])
              warning_count = len([i for i in issues if i['type'] == 'warning'])

              # è®¡ç®—è´¨é‡åˆ†æ•° (100 - (error_count * 10) - (warning_count * 2))
              score = max(0, 100 - (error_count * 10) - (warning_count * 2))

              print(f"quality-score={score}")
              print(f"issues-count={len(issues)}")

          except FileNotFoundError:
              print("quality-score=100")
              print("issues-count=0")
          EOF

          echo "âœ… ä»£ç è´¨é‡æ£€æŸ¥å®Œæˆ"

      - name: Run Bandit security scan
        run: |
          bandit -r src/ -f json -o security-report.json

          # åˆ†æå®‰å…¨æŠ¥å‘Š
          python3 << 'EOF'
          import json

          try:
              with open('security-report.json', 'r') as f:
                  report = json.load(f')

              high_severity = report['metrics']['_totals']['SEVERITY.HIGH']
              medium_severity = report['metrics']['_totals']['SEVERITY.MEDIUM']

              print(f"ğŸ”’ å®‰å…¨æ‰«æç»“æœ:")
              print(f"   é«˜å±é—®é¢˜: {high_severity}")
              print(f"   ä¸­å±é—®é¢˜: {medium_severity}")

              if high_severity > 0:
                  print("âŒ å­˜åœ¨é«˜å±å®‰å…¨é—®é¢˜")
                  sys.exit(1)

          except FileNotFoundError:
              print("âš ï¸ å®‰å…¨æ‰«ææŠ¥å‘Šæœªç”Ÿæˆ")
          EOF

      - name: Run safety check
        run: |
          safety check --json --output safety-report.json || true

          # æ£€æŸ¥ä¾èµ–æ¼æ´
          python3 << 'EOF'
          import json

          try:
              with open('safety-report.json', 'r') as f:
                  report = json.load(f)

              vulnerabilities = report.get('vulnerabilities', [])
              critical_count = len([v for v in vulnerabilities if v.get('advisory', {}).get('vulnerability') == 'critical'])
              high_count = len([v for v in vulnerabilities if v.get('advisory', {}).get('vulnerability') == 'high'])

              print(f"ğŸ“¦ ä¾èµ–æ¼æ´æ£€æŸ¥:")
              print(f"   ä¸¥é‡æ¼æ´: {critical_count}")
              print(f"   é«˜å±æ¼æ´: {high_count}")

              if critical_count > 0:
                  print("âŒ å­˜åœ¨ä¸¥é‡ä¾èµ–æ¼æ´")
                  sys.exit(1)

          except FileNotFoundError:
              print("âš ï¸ ä¾èµ–æ¼æ´æ£€æŸ¥æŠ¥å‘Šæœªç”Ÿæˆ")
          EOF

      - name: Save quality report
        id: quality-report
        run: |
          python3 << 'EOF'
          # è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
          score = 85  # é»˜è®¤åˆ†æ•°
          issues_count = 0

          try:
              with open('ruff-report.json', 'r') as f:
                  ruff_report = json.load(f)
                  issues_count = len(ruff_report.get('results', []))

              with open('security-report.json', 'r') as f:
                  security_report = json.load(f)
                  high_severity = security_report['metrics']['_totals']['SEVERITY.HIGH']
                  score -= high_severity * 5

          except FileNotFoundError:
              print("ä½¿ç”¨é»˜è®¤è´¨é‡åˆ†æ•°")

          # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
          score = max(0, min(100, score))

          print(f"quality-score={score}")
          print(f"issues-count={issues_count}")
          EOF

  # å•å…ƒæµ‹è¯•
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: code-quality
    outputs:
      coverage: ${{ steps.coverage.outputs.coverage }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          make install
          pip install pytest pytest-cov pytest-mock pytest-asyncio

      - name: Run Smart Tests
        run: |
          # è¿è¡Œæ ¸å¿ƒç¨³å®šæµ‹è¯•
          pytest tests/unit/utils tests/unit/cache tests/unit/core -v --maxfail=20 || true

      - name: Run unit tests with coverage
        id: coverage
        run: |
          pytest tests/unit/ --cov=src --cov-report=xml --cov-report=html --cov-report=term-missing --junitxml=pytest-results.xml

          # æå–è¦†ç›–ç‡
          COVERAGE=$(python3 -c "
          import re
          with open('coverage.xml', 'r') as f:
              content = f.read()
          match = re.search(r'line-rate=\"(\d+\.\d+)\"', content)
          if match:
              coverage_percent = float(match.group(1)) * 100
              print(f'coverage={coverage_percent:.1f}')
          else:
              print('coverage=0.0')
          ")
          echo $COVERAGE

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
        continue-on-error: true

      - name: Upload test results
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            pytest-results.xml
            htmlcov/
        retention-days: 7

      - name: Check coverage threshold
        run: |
          coverage=$(python3 -c "
          import json
          with open('coverage.xml', 'r') as f:
              content = f.read()
          import re
          match = re.search(r'line-rate=\"(\d+\.\d+)\"', content)
          if match:
              print(float(match.group(1)) * 100)
          else:
              print(0.0)
          ")

          if (( $(echo "$coverage < 30" | bc -l) )); then
            echo "âŒ æµ‹è¯•è¦†ç›–ç‡ ${coverage}% ä½äºè¦æ±‚çš„30%"
            exit 1
          else
            echo "âœ… æµ‹è¯•è¦†ç›–ç‡ ${coverage}% æ»¡è¶³è¦æ±‚"
          fi

  # é›†æˆæµ‹è¯•
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests]
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          make install
          pip install pytest pytest-asyncio httpx

      - name: Wait for services
        run: |
          # ç­‰å¾…æ•°æ®åº“å’ŒRediså°±ç»ª
          sleep 10
          python3 -c "
          import psycopg2
          import redis

          # æµ‹è¯•æ•°æ®åº“è¿æ¥
          conn = psycopg2.connect(
              host='localhost',
              user='testuser',
              password='testpassword',
              dbname='testdb'
          )
          conn.close()

          # æµ‹è¯•Redisè¿æ¥
          r = redis.Redis(host='localhost', port=6379, db=0)
          r.ping()
          print('âœ… æœåŠ¡è¿æ¥æ­£å¸¸')
          "

      - name: Run integration tests
        run: |
          pytest tests/integration/ -v --maxfail=10 --tb=short || true

      - name: Run API integration tests
        run: |
          pytest tests/integration/test_api_* -v --maxfail=5 --tb=short || true

  # å®‰å…¨æµ‹è¯•
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: code-quality

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          make install
          pip install pytest bandit safety

      - name: Run security test suite
        run: |
          pytest tests/security/ -v --cov=src.security --cov-report=xml --cov-report=html || true

      - name: Upload security test results
        uses: actions/upload-artifact@v3
        with:
          name: security-test-results
          path: |
            htmlcov/
          retention-days: 7

  # æ€§èƒ½æµ‹è¯•
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          make install
          pip install pytest pytest-benchmark psutil

      - name: Run performance benchmarks
        run: |
          pytest tests/performance/test_benchmarks.py -v --benchmark-only --benchmark-json=benchmark-results.json || true

      - name: Analyze performance results
        run: |
          python3 << 'EOF'
          import json
          import sys

          try:
              with open('benchmark-results.json', 'r') as f:
                  data = json.load(f')

              benchmarks = data.get('benchmarks', {})
              slow_tests = []

              for name, results in benchmarks.items():
                  if 'stats' in results:
                      mean_time = results['stats'].get('mean', 0)
                      if mean_time > 1.0:  # è¶…è¿‡1ç§’
                          slow_tests.append(f"{name}: {mean_time:.3f}s")

              if slow_tests:
                  print(f"âš ï¸ å‘ç°æ…¢é€Ÿæµ‹è¯•:")
                  for test in slow_tests:
                      print(f"   - {test}")

          except FileNotFoundError:
              print("âš ï¸ æ€§èƒ½æµ‹è¯•ç»“æœæœªæ‰¾åˆ°")
          EOF

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: benchmark-results.json
          retention-days: 7

  # æ„å»ºå’Œéƒ¨ç½²
  build-and-deploy:
    name: Build and Deploy
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests]
    if: github.ref == 'refs/heads/main' || github.event_name == 'release'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: football-prediction-system
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=semver,pattern={{major}}

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Deploy to staging (if main branch)
        if: github.ref == 'refs/heads/main'
        run: |
          echo "ğŸš€ éƒ¨ç½²åˆ°é¢„å‘å¸ƒç¯å¢ƒ"
          # è¿™é‡Œæ·»åŠ å®é™…çš„éƒ¨ç½²é€»è¾‘
          # docker-compose -f docker-compose.staging.yml up -d

  # æ–‡æ¡£æ›´æ–°
  docs-update:
    name: Update Documentation
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests]
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          make install
          pip install mkdocs mkdocs-material

      - name: Generate API docs
        run: |
          # å¦‚æœæœ‰APIæ–‡æ¡£ç”Ÿæˆè„šæœ¬
          if [ -f "scripts/generate_api_docs.py" ]; then
            python scripts/generate_api_docs.py
          fi

      - name: Build documentation
        run: |
          if [ -f "mkdocs.yml" ]; then
            mkdocs build
          fi

      - name: Deploy docs to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./site
        continue-on-error: true

  # è´¨é‡é—¨æ£€æŸ¥
  quality-gate:
    name: Quality Gate
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, security-tests]
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Quality Gate Decision
        run: |
          echo "ğŸ” æ‰§è¡Œè´¨é‡é—¨æ£€æŸ¥..."

          # è·å–å„é¡¹æŒ‡æ ‡
          QUALITY_SCORE=${{ needs.code-quality.outputs.quality-score }}
          ISSUES_COUNT=${{ needs.code-quality.outputs.issues-count }}
          COVERAGE=${{ needs.unit-tests.outputs.coverage }}

          echo "ğŸ“Š è´¨é‡æŒ‡æ ‡:"
          echo "   ä»£ç è´¨é‡åˆ†æ•°: ${QUALITY_SCORE}"
          echo "   é—®é¢˜æ•°é‡: ${ISSUES_COUNT}"
          echo "   æµ‹è¯•è¦†ç›–ç‡: ${COVERAGE}%"

          # è´¨é‡é—¨æ£€æŸ¥
          if (( $(echo "$QUALITY_SCORE < 70" | bc -l) )); then
            echo "âŒ ä»£ç è´¨é‡åˆ†æ•° ${QUALITY_SCORE} ä½äºè¦æ±‚ (70)"
            exit 1
          fi

          if (( $(echo "$COVERAGE < 30" | bc -l) )); then
            echo "âŒ æµ‹è¯•è¦†ç›–ç‡ ${COVERAGE}% ä½äºè¦æ±‚ (30%)"
            exit 1
          fi

          if (( ISSUES_COUNT > 50 )); then
            echo "âŒ ä»£ç é—®é¢˜æ•°é‡ ${ISSUES_COUNT} è¶…è¿‡é˜ˆå€¼ (50)"
            exit 1
          fi

          echo "âœ… æ‰€æœ‰è´¨é‡æ£€æŸ¥é€šè¿‡ï¼"

  # å‘é€é€šçŸ¥
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, security-tests, quality-gate]
    if: always()

    steps:
      - name: Notify on success
        if: needs.quality-gate.result == 'success'
        run: |
          echo "âœ… CI/CD æµæ°´çº¿æˆåŠŸå®Œæˆï¼"
          # è¿™é‡Œå¯ä»¥æ·»åŠ Slackã€é‚®ä»¶æˆ–ä¼ä¸šå¾®ä¿¡é€šçŸ¥

      - name: Notify on failure
        if: needs.quality-gate.result == 'failure'
        run: |
          echo "âŒ CI/CD æµæ°´çº¿å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"
          # å‘é€å¤±è´¥é€šçŸ¥
          curl -X POST "${{ secrets.WEBHOOK_URL }}" \
            -H 'Content-type: application/json' \
            --data '{
              "text": "âŒ CI/CD æµæ°´çº¿å¤±è´¥ï¼\nä»“åº“: ${{ github.repository }}\nåˆ†æ”¯: ${{ github.ref }}\næäº¤: ${{ github.sha }}\nè¯·æ£€æŸ¥ Actions æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚"
            }' || true