name: Test Quality Gate

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # ÊØèÂ§©Êó©‰∏ä8ÁÇπËøêË°åË¥®ÈáèÊ£ÄÊü•
    - cron: '0 8 * * *'

jobs:
  # Âø´ÈÄüÂçïÂÖÉÊµãËØïÊ£ÄÊü•
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        make install

    - name: Run unit tests with coverage
      run: |
        make test.unit

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # ÈõÜÊàêÊµãËØï
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests

    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        make install

    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:test@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379/0
        ENVIRONMENT: testing
      run: |
        make test.integration
      continue-on-error: true

  # E2EÊµãËØïÔºà‰ªÖÂú®‰∏ªÂàÜÊîØÔºâ
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        make install

    - name: Run E2E tests
      run: |
        make test.e2e
      continue-on-error: true

  # ÊµãËØïË¥®ÈáèÁõëÊéß
  quality-monitor:
    name: Test Quality Monitor
    runs-on: ubuntu-latest
    needs: [unit-tests]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        make install

    - name: Generate quality report
      run: |
        make test.monitor

    - name: Upload quality report
      uses: actions/upload-artifact@v3
      with:
        name: quality-report
        path: tests/metrics/report_*.json

    - name: Comment PR with quality report
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          // Êü•ÊâæÊúÄÊñ∞ÁöÑË¥®ÈáèÊä•Âëä
          const reports = fs.readdirSync('tests/metrics')
            .filter(f => f.startsWith('report_') && f.endsWith('.json'))
            .sort()
            .reverse();

          if (reports.length > 0) {
            const reportPath = path.join('tests/metrics', reports[0]);
            const report = JSON.parse(fs.readFileSync(reportPath, 'utf8'));

            const quality = report.quality_score;
            const coverage = report.coverage?.overall_coverage || 0;
            const performance = report.performance?.total_time || 0;

            const comment = `
            ## üìä Test Quality Report

            | Metric | Score | Status |
            |--------|-------|--------|
            | üéØ Overall Quality | ${quality.total_score}/${quality.max_score} (${quality.grade}) | ${quality.grade >= 'B' ? '‚úÖ' : '‚ö†Ô∏è'} |
            | üìà Coverage | ${coverage.toFixed(1)}% | ${coverage >= 20 ? '‚úÖ' : '‚ùå'} |
            | ‚ö° Performance | ${performance.toFixed(1)}s | ${performance <= 60 ? '‚úÖ' : '‚ö†Ô∏è'} |

            ### üí° Recommendations
            ${report.recommendations.map(r => `- ${r}`).join('\n')}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          }

  # Ë¥®ÈáèÈó®Á¶ÅÊ£ÄÊü•
  quality-gate:
    name: Quality Gate
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        make install

    - name: Run quality gate
      run: |
        make test.quality-gate

    - name: Quality gate status
      if: failure()
      run: |
        echo "‚ùå Quality gate failed!"
        echo "Please review the test results and fix the issues."
        exit 1

  # ÊµãËØïÁü©ÈòµÁîüÊàêÔºàÁî®‰∫éÂπ∂Ë°åÂåñÔºâ
  test-matrix:
    name: Generate Test Matrix
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        make install

    - name: Generate test matrix
      id: set-matrix
      run: |
        matrix=$(python scripts/test_runner.py --matrix)
        echo "matrix=$matrix" >> $GITHUB_OUTPUT

  # ÊÄßËÉΩÂõûÂΩíÊµãËØï
  performance-tests:
    name: Performance Regression Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    if: github.event_name == 'push' || github.event_name == 'pull_request'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      fetch-depth: 0  # ÈúÄË¶ÅÂÆåÊï¥ÂéÜÂè≤Êù•ÊØîËæÉÊÄßËÉΩ

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        make install

    - name: Run performance benchmarks
      run: |
        make benchmark-regression

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: tests/performance/results/

  # ÂÆâÂÖ®Êâ´Êèè
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run Bandit Security Scan
      run: |
        pip install bandit[toml]
        bandit -r src/ -f json -o bandit-report.json || true

    - name: Run Safety Check
      run: |
        pip install safety
        safety check --json --output safety-report.json || true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # ÊµãËØïÊä•ÂëäÊ±áÊÄª
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, quality-gate]
    if: always()

    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3

    - name: Generate summary report
      run: |
        echo "# üìä Test Execution Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## ‚úÖ Completed Jobs" >> $GITHUB_STEP_SUMMARY
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Quality Gate: ${{ needs.quality-gate.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## üìà Quality Metrics" >> $GITHUB_STEP_SUMMARY

        if [ -f "quality-report/report_"*.json ]; then
          python -c "
import json
import glob
import os

reports = glob.glob('quality-report/report_*.json')
if reports:
    latest = max(reports, key=os.path.getctime)
    with open(latest) as f:
        data = json.load(f)

    quality = data['quality_score']
    coverage = data.get('coverage', {}).get('overall_coverage', 0)

    print(f'- Overall Quality: {quality[\"total_score\"]}/{quality[\"max_score\"]} ({quality[\"grade\"]})')
    print(f'- Test Coverage: {coverage:.1f}%')
    print(f'- Execution Time: {data.get(\"performance\", {}).get(\"total_time\", 0):.1f}s')
  " >> $GITHUB_STEP_SUMMARY
        fi

    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Test Quality Gate Failed - ${context.sha}`,
            body: `
            ## ‚ùå Test Quality Gate Failed

            **Commit**: ${context.sha}
            **Branch**: ${context.ref}
            **Workflow**: ${context.workflow}

            Please review the [workflow run](${context.payload.repository.html_url}/actions/runs/${context.runId}) and fix the issues.
            `,
            labels: ['bug', 'ci-failure']
          })