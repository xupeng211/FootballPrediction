#!/usr/bin/env python3
"""
æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“
è‡ªåŠ¨åŒ–æ£€æµ‹ã€åˆ†æå’ŒæŠ¥å‘Šç³»ç»Ÿè´¨é‡çš„AIé©±åŠ¨å·¥å…·
åŸºäºIssue #159çš„70.1%è¦†ç›–ç‡æˆå°±ï¼Œå»ºç«‹æ™ºèƒ½åŒ–è´¨é‡ç®¡ç†ä½“ç³»
"""

import ast
import json
import time
from pathlib import Path
from typing import Dict, List, Set, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import threading
import subprocess
import re
import os

@dataclass
class QualityTrend:
    """è´¨é‡è¶‹åŠ¿æ•°æ®"""
    date: datetime
    coverage_percentage: float
    test_count: int
    complexity_score: float
    security_score: float
    performance_score: float
    overall_score: float

@dataclass
class QualityPattern:
    """è´¨é‡æ¨¡å¼è¯†åˆ«"""
    pattern_name: str
    description: str
    indicators: List[str]
    recommendations: List[str]
    auto_fix_available: bool = False

@dataclass
class QualityInsight:
    """è´¨é‡æ´å¯Ÿ"""
    category: str
    insight: str
    impact_level: str  # HIGH, MEDIUM, LOW
    actionable: bool
    evidence: List[str]
    suggested_actions: List[str]

class IntelligentQualityAnalyzer:
    """æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“"""

    def __init__(self):
        self.project_root = Path(__file__).parent.parent
        self.src_root = self.project_root / "src"
        self.tests_root = self.project_root / "tests"

        # è´¨é‡æ¨¡å¼åº“
        self.quality_patterns = self._initialize_quality_patterns()

        # è¶‹åŠ¿æ•°æ®å­˜å‚¨
        self.trend_data = []
        self.trend_data_file = self.project_root / "quality_data" / "quality_trends.json"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.trend_data_file.parent.mkdir(exist_ok=True)

        # åŠ è½½å†å²è¶‹åŠ¿æ•°æ®
        self._load_trend_data()

    def _initialize_quality_patterns(self) -> List[QualityPattern]:
        """åˆå§‹åŒ–è´¨é‡æ¨¡å¼åº“"""
        patterns = [
            QualityPattern(
                pattern_name="ä½è¦†ç›–ç‡æ¨¡å—",
                description="è¯†åˆ«è¦†ç›–ç‡ä½äº50%çš„æ¨¡å—",
                indicators=["coverage_low", "missing_tests", "complex_module"],
                recommendations=[
                    "ä¼˜å…ˆæ·»åŠ å…³é”®è·¯å¾„çš„æµ‹è¯•",
                    "ä½¿ç”¨TDDæ–¹æ³•é‡æ„æ¨¡å—",
                    "å¢åŠ è¾¹ç•Œæ¡ä»¶æµ‹è¯•"
                ],
                auto_fix_available=True
            ),
            QualityPattern(
                pattern_name="å¤æ‚åº¦çƒ­ç‚¹",
                description="è¯†åˆ«å¤æ‚åº¦è¿‡é«˜çš„å‡½æ•°å’Œç±»",
                indicators=["high_cyclomatic_complexity", "deep_nesting", "long_functions"],
                recommendations=[
                    "æ‹†åˆ†å¤§å‡½æ•°",
                    "æå–å­æ–¹æ³•",
                    "ä½¿ç”¨è®¾è®¡æ¨¡å¼ç®€åŒ–ç»“æ„"
                ],
                auto_fix_available=False
            ),
            QualityPattern(
                pattern_name="æµ‹è¯•è´¨é‡ä¸è¶³",
                description="è¯†åˆ«æµ‹è¯•è´¨é‡é—®é¢˜çš„æ¨¡å¼",
                indicators=["weak_assertions", "missing_edge_cases", "no_error_handling"],
                recommendations=[
                    "å¢åŠ æ–­è¨€è¦†ç›–ç‡",
                    "æ·»åŠ è¾¹ç•Œå€¼æµ‹è¯•",
                    "å®Œå–„å¼‚å¸¸å¤„ç†æµ‹è¯•"
                ],
                auto_fix_available=True
            ),
            QualityPattern(
                pattern_name="å®‰å…¨éšæ‚£",
                description="è¯†åˆ«å¸¸è§çš„å®‰å…¨æ¼æ´æ¨¡å¼",
                indicators=["sql_injection_risk", "xss_vulnerability", "hardcoded_secrets"],
                recommendations=[
                    "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢",
                    "è¾“å…¥éªŒè¯å’Œæ¸…ç†",
                    "ç¯å¢ƒå˜é‡å­˜å‚¨æ•æ„Ÿä¿¡æ¯"
                ],
                auto_fix_available=True
            ),
            QualityPattern(
                pattern_name="æ€§èƒ½ç“¶é¢ˆ",
                description="è¯†åˆ«æ€§èƒ½é—®é¢˜çš„æ¨¡å¼",
                indicators=["inefficient_loops", "memory_leaks", "blocking_operations"],
                recommendations=[
                    "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
                    "ä½¿ç”¨å¼‚æ­¥ç¼–ç¨‹",
                    "æ·»åŠ ç¼“å­˜æœºåˆ¶"
                ],
                auto_fix_available=False
            ),
            QualityPattern(
                pattern_name="ä»£ç é‡å¤",
                description="è¯†åˆ«ä»£ç é‡å¤å’Œç»´æŠ¤æ€§é—®é¢˜",
                indicators=["duplicate_blocks", "similar_functions", "magic_numbers"],
                recommendations=[
                    "æå–å…¬å…±æ–¹æ³•",
                    "ä½¿ç”¨å¸¸é‡æ›¿æ¢é­”æ³•æ•°å­—",
                    "é‡æ„ç›¸ä¼¼ä»£ç "
                ],
                auto_fix_available=True
            )
        ]
        return patterns

    def analyze_project_quality(self) -> Dict[str, Any]:
        """åˆ†æé¡¹ç›®æ•´ä½“è´¨é‡"""
        print("ğŸ¤– å¯åŠ¨æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“...")

        start_time = time.time()

        # æ”¶é›†æ•°æ®
        analysis_data = self._collect_project_data()

        # æ¨¡å¼è¯†åˆ«
        patterns_found = self._identify_quality_patterns(analysis_data)

        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_quality_insights(analysis_data, patterns_found)

        # è¶‹åŠ¿åˆ†æ
        current_trend = self._calculate_current_trend(analysis_data)
        self._save_trend_data(current_trend)

        # ç»¼åˆè¯„åˆ†
        quality_score = self._calculate_comprehensive_score(analysis_data)

        analysis_result = {
            "timestamp": datetime.now().isoformat(),
            "analysis_time": time.time() - start_time,
            "coverage_data": analysis_data.get("coverage", {}),
            "test_quality": analysis_data.get("test_quality", {}),
            "code_metrics": analysis_data.get("code_metrics", {}),
            "patterns_found": [
                {
                    "name": p.pattern_name,
                    "description": p.description,
                    "count": len([i for i in patterns_found if i["pattern_name"] == p.pattern_name])
                }
                for p in self.quality_patterns
            ],
            "insights": insights,
            "trend": current_trend,
            "overall_score": quality_score,
            "recommendations": self._generate_comprehensive_recommendations(analysis_data, patterns_found, insights)
        }

        return analysis_result

    def _collect_project_data(self) -> Dict[str, Any]:
        """æ”¶é›†é¡¹ç›®æ•°æ®"""
        data = {}

        # è¿è¡Œè¦†ç›–ç‡åˆ†æ
        print("  ğŸ“Š æ”¶é›†è¦†ç›–ç‡æ•°æ®...")
        try:
            result = subprocess.run([
                "python3", "tests/enhanced_coverage_system_v2.py"
            ], capture_output=True, text=True, cwd=self.project_root)

            # è§£æè¦†ç›–ç‡ç»“æœ
            coverage_data = self._parse_coverage_output(result.stdout)
            data["coverage"] = coverage_data
        except Exception as e:
            print(f"    âš ï¸ è¦†ç›–ç‡åˆ†æå¤±è´¥: {e}")
            data["coverage"] = {"coverage_percentage": 0, "covered_modules": 0}

        # åˆ†ææµ‹è¯•è´¨é‡
        print("  ğŸ§ª åˆ†ææµ‹è¯•è´¨é‡...")
        data["test_quality"] = self._analyze_test_quality()

        # åˆ†æä»£ç æŒ‡æ ‡
        print("  ğŸ“ˆ åˆ†æä»£ç æŒ‡æ ‡...")
        data["code_metrics"] = self._analyze_code_metrics()

        return data

    def _parse_coverage_output(self, output: str) -> Dict[str, Any]:
        """è§£æè¦†ç›–ç‡è¾“å‡º"""
        coverage_data = {}

        # æå–è¦†ç›–ç‡ç™¾åˆ†æ¯”
        coverage_match = re.search(r"ğŸ¯ æœ€ç»ˆè¦†ç›–ç‡: (\d+\.\d+)%", output)
        if coverage_match:
            coverage_data["coverage_percentage"] = float(coverage_match.group(1))

        # æå–è¦†ç›–æ¨¡å—æ•°
        modules_match = re.search(r"âœ… è¦†ç›–æ¨¡å—æ•°: (\d+)", output)
        if modules_match:
            coverage_data["covered_modules"] = int(modules_match.group(1))

        # æå–é¡¹ç›®æ€»æ¨¡å—æ•°
        total_match = re.search(r"ğŸ“ é¡¹ç›®æ€»æ¨¡å—: (\d+)", output)
        if total_match:
            coverage_data["total_modules"] = int(total_match.group(1))

        return coverage_data

    def _analyze_test_quality(self) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•è´¨é‡"""
        test_files = list(self.tests_root.glob("test_*.py"))

        quality_metrics = {
            "total_test_files": len(test_files),
            "total_test_methods": 0,
            "test_classes": 0,
            "files_with_good_assertions": 0,
            "files_with_error_handling": 0,
            "files_with_edge_cases": 0,
            "average_methods_per_file": 0,
            "test_complexity_score": 0.0
        }

        for test_file in test_files:
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                tree = ast.parse(content)

                # ç»Ÿè®¡æµ‹è¯•ç±»å’Œæ–¹æ³•
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef) and node.name.startswith('Test'):
                        quality_metrics["test_classes"] += 1

                        for item in node.body:
                            if isinstance(item, ast.FunctionDef) and item.name.startswith('test_'):
                                quality_metrics["total_test_methods"] += 1

                # æ£€æŸ¥æ–­è¨€è´¨é‡
                if self._has_good_assertions(content):
                    quality_metrics["files_with_good_assertions"] += 1

                # æ£€æŸ¥é”™è¯¯å¤„ç†
                if self._has_error_handling(content):
                    quality_metrics["files_with_error_handling"] += 1

                # æ£€æŸ¥è¾¹ç•Œæ¡ä»¶
                if self._has_edge_case_tests(content):
                    quality_metrics["files_with_edge_cases"] += 1

            except Exception as e:
                print(f"    âš ï¸ åˆ†ææµ‹è¯•æ–‡ä»¶å¤±è´¥ {test_file.name}: {e}")

        # è®¡ç®—å¹³å‡å€¼
        if quality_metrics["total_test_files"] > 0:
            quality_metrics["average_methods_per_file"] = (
                quality_metrics["total_test_methods"] / quality_metrics["total_test_files"]
            )

        # è®¡ç®—æµ‹è¯•å¤æ‚åº¦åˆ†æ•°
        quality_metrics["test_complexity_score"] = min(100, (
            (quality_metrics["files_with_good_assertions"] / quality_metrics["total_test_files"]) * 40 +
            (quality_metrics["files_with_error_handling"] / quality_metrics["total_test_files"]) * 30 +
            (quality_metrics["files_with_edge_cases"] / quality_metrics["total_test_files"]) * 30
        ))

        return quality_metrics

    def _has_good_assertions(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¥½çš„æ–­è¨€"""
        good_assertion_patterns = [
            r"assert\s+\w+\s*==\s*[^,]+",
            r"assert\s+\w+\s*!=\s*[^,]+",
            r"assert\s+\w+\s*>\s*[^,]+",
            r"assert\s+\w+\s*<\s*[^,]+",
            r"assert\s+\w+\s*in\s+[^,]+",
            r"self\.assert[A-Z][a-zA-Z]*\s*\(",
            r"self\.assertEqual\s*\(",
            r"self\.assertTrue\s*\(",
            r"self\.assertFalse\s*\("
        ]

        return any(re.search(pattern, content) for pattern in good_assertion_patterns)

    def _has_error_handling(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯å¤„ç†"""
        error_handling_patterns = [
            r"try\s*:",
            r"except\s+\w+",
            r"raise\s+\w+",
            r"finally\s*:",
            r"with\s+\w+\s+as\s+\w+"
        ]

        return any(re.search(pattern, content) for pattern in error_handling_patterns)

    def _has_edge_case_tests(self, content: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰è¾¹ç•Œæ¡ä»¶æµ‹è¯•"""
        edge_case_patterns = [
            r"test.*empty",
            r"test.*null",
            r"test.*zero",
            r"test.*negative",
            r"test.*invalid",
            r"test.*boundary",
            r"test.*limit",
            r"test.*edge"
        ]

        return any(re.search(pattern, content, re.IGNORECASE) for pattern in edge_case_patterns)

    def _analyze_code_metrics(self) -> Dict[str, Any]:
        """åˆ†æä»£ç æŒ‡æ ‡"""
        code_metrics = {
            "total_python_files": 0,
            "total_lines_of_code": 0,
            "average_file_size": 0,
            "complex_files_count": 0,
            "files_with_docstrings": 0,
            "files_with_type_hints": 0,
            "duplication_score": 0,
            "maintainability_index": 0.0
        }

        python_files = list(self.src_root.rglob("*.py"))
        code_metrics["total_python_files"] = len(python_files)

        total_lines = 0
        docstring_files = 0
        type_hint_files = 0
        complex_files = 0

        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                lines = len(content.split('\n'))
                total_lines += lines

                # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
                if '"""' in content or "'''" in content:
                    docstring_files += 1

                # æ£€æŸ¥ç±»å‹æç¤º
                if ':' in content and ('int:' in content or 'str:' in content or 'bool:' in content):
                    type_hint_files += 1

                # æ£€æŸ¥æ–‡ä»¶å¤æ‚åº¦ï¼ˆåŸºäºè¡Œæ•°ï¼‰
                if lines > 100:
                    complex_files += 1

            except Exception as e:
                print(f"    âš ï¸ åˆ†æä»£ç æ–‡ä»¶å¤±è´¥ {py_file.name}: {e}")

        code_metrics["total_lines_of_code"] = total_lines
        code_metrics["complex_files_count"] = complex_files
        code_metrics["files_with_docstrings"] = docstring_files
        code_metrics["files_with_type_hints"] = type_hint_files

        if code_metrics["total_python_files"] > 0:
            code_metrics["average_file_size"] = total_lines / code_metrics["total_python_files"]

        # è®¡ç®—å¯ç»´æŠ¤æ€§æŒ‡æ•°
        code_metrics["maintainability_index"] = min(100, (
            (code_metrics["files_with_docstrings"] / code_metrics["total_python_files"]) * 30 +
            (code_metrics["files_with_type_hints"] / code_metrics["total_python_files"]) * 25 +
            ((code_metrics["total_python_files"] - code_metrics["complex_files_count"]) / code_metrics["total_python_files"]) * 45
        ))

        return code_metrics

    def _identify_quality_patterns(self, analysis_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«è´¨é‡æ¨¡å¼"""
        patterns_found = []

        # è¦†ç›–ç‡æ¨¡å¼
        coverage_data = analysis_data.get("coverage", {})
        coverage_percentage = coverage_data.get("coverage_percentage", 0)

        if coverage_percentage < 50:
            patterns_found.append({
                "pattern_name": "ä½è¦†ç›–ç‡æ¨¡å—",
                "severity": "HIGH",
                "evidence": [f"è¦†ç›–ç‡: {coverage_percentage}%"],
                "suggested_actions": ["ä¼˜å…ˆæ·»åŠ å…³é”®è·¯å¾„æµ‹è¯•", "ä½¿ç”¨TDDæ–¹æ³•"]
            })

        # æµ‹è¯•è´¨é‡æ¨¡å¼
        test_quality = analysis_data.get("test_quality", {})
        test_complexity_score = test_quality.get("test_complexity_score", 0)

        if test_complexity_score < 50:
            patterns_found.append({
                "pattern_name": "æµ‹è¯•è´¨é‡ä¸è¶³",
                "severity": "MEDIUM",
                "evidence": [f"æµ‹è¯•å¤æ‚åº¦åˆ†æ•°: {test_complexity_score}"],
                "suggested_actions": ["å¢åŠ æ–­è¨€è¦†ç›–ç‡", "æ·»åŠ è¾¹ç•Œæµ‹è¯•"]
            })

        # ä»£ç å¤æ‚åº¦æ¨¡å¼
        code_metrics = analysis_data.get("code_metrics", {})
        complex_files_count = code_metrics.get("complex_files_count", 0)

        if complex_files_count > 10:
            patterns_found.append({
                "pattern_name": "å¤æ‚åº¦çƒ­ç‚¹",
                "severity": "MEDIUM",
                "evidence": [f"å¤æ‚æ–‡ä»¶æ•°é‡: {complex_files_count}"],
                "suggested_actions": ["æ‹†åˆ†å¤§å‡½æ•°", "æå–å­æ–¹æ³•"]
            })

        return patterns_found

    def _generate_quality_insights(self, analysis_data: Dict[str, Any], patterns_found: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè´¨é‡æ´å¯Ÿ"""
        insights = []

        coverage_data = analysis_data.get("coverage", {})
        coverage_percentage = coverage_data.get("coverage_percentage", 0)

        # ç”Ÿæˆè¦†ç›–ç‡æ´å¯Ÿ
        if coverage_percentage > 70:
            insights.append({
                "category": "coverage",
                "insight": f"é¡¹ç›®æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°äº† {coverage_percentage:.1f}%ï¼Œå¤„äºè‰¯å¥½æ°´å¹³",
                "impact_level": "POSITIVE",
                "actionable": False,
                "evidence": [f"è¦†ç›–ç‡: {coverage_percentage}%"],
                "suggested_actions": ["ç»§ç»­ä¿æŒå½“å‰æµ‹è¯•è´¨é‡"]
            })
        elif coverage_percentage < 50:
            insights.append({
                "category": "coverage",
                "insight": f"é¡¹ç›®æµ‹è¯•è¦†ç›–ç‡ä»…ä¸º {coverage_percentage:.1f}%ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨",
                "impact_level": "HIGH",
                "actionable": True,
                "evidence": [f"è¦†ç›–ç‡: {coverage_percentage}%"],
                "suggested_actions": ["å¢åŠ æµ‹è¯•è¦†ç›–ç‡", "ä¼˜å…ˆè¦†ç›–æ ¸å¿ƒæ¨¡å—"]
            })

        # ç”Ÿæˆæµ‹è¯•è´¨é‡æ´å¯Ÿ
        test_quality = analysis_data.get("test_quality", {})
        test_complexity_score = test_quality.get("test_complexity_score", 0)

        if test_complexity_score > 80:
            insights.append({
                "category": "test_quality",
                "insight": f"æµ‹è¯•è´¨é‡è¯„åˆ† {test_complexity_score:.1f}ï¼Œæµ‹è¯•ç”¨ä¾‹è´¨é‡è¾ƒé«˜",
                "impact_level": "POSITIVE",
                "actionable": False,
                "evidence": [f"æµ‹è¯•è´¨é‡åˆ†æ•°: {test_complexity_score}"],
                "suggested_actions": ["ç»§ç»­ä¿æŒæµ‹è¯•è´¨é‡æ ‡å‡†"]
            })

        # ç”Ÿæˆæ¨¡å¼æ´å¯Ÿ
        if patterns_found:
            high_severity_patterns = [p for p in patterns_found if p.get("severity") == "HIGH"]
            if high_severity_patterns:
                insights.append({
                    "category": "patterns",
                    "insight": f"å‘ç° {len(high_severity_patterns)} ä¸ªé«˜ä¸¥é‡æ€§è´¨é‡é—®é¢˜æ¨¡å¼",
                    "impact_level": "HIGH",
                    "actionable": True,
                    "evidence": [p["pattern_name"] for p in high_severity_patterns],
                    "suggested_actions": ["ä¼˜å…ˆè§£å†³é«˜ä¸¥é‡æ€§é—®é¢˜"]
                })

        return insights

    def _calculate_current_trend(self, analysis_data: Dict[str, Any]) -> QualityTrend:
        """è®¡ç®—å½“å‰è¶‹åŠ¿"""
        coverage_data = analysis_data.get("coverage", {})
        test_quality = analysis_data.get("test_quality", {})
        code_metrics = analysis_data.get("code_metrics", {})

        return QualityTrend(
            date=datetime.now(),
            coverage_percentage=coverage_data.get("coverage_percentage", 0),
            test_count=test_quality.get("total_test_methods", 0),
            complexity_score=10.0 - (code_metrics.get("complex_files_count", 0) / max(code_metrics.get("total_python_files", 1), 1)),
            security_score=85.0,  # åŸºäºå½“å‰é¡¹ç›®ä¼°ç®—
            performance_score=90.0,  # åŸºäºå½“å‰é¡¹ç›®ä¼°ç®—
            overall_score=self._calculate_comprehensive_score(analysis_data)
        )

    def _calculate_comprehensive_score(self, analysis_data: Dict[str, Any]) -> float:
        """è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°"""
        coverage_data = analysis_data.get("coverage", {})
        test_quality = analysis_data.get("test_quality", {})
        code_metrics = analysis_data.get("code_metrics", {})

        # å„é¡¹æŒ‡æ ‡åˆ†æ•°
        coverage_score = min(100, coverage_data.get("coverage_percentage", 0))
        test_quality_score = test_quality.get("test_complexity_score", 0)
        maintainability_score = code_metrics.get("maintainability_index", 0)

        # åŠ æƒè®¡ç®—
        comprehensive_score = (
            coverage_score * 0.4 +
            test_quality_score * 0.3 +
            maintainability_score * 0.3
        )

        return comprehensive_score

    def _generate_comprehensive_recommendations(self, analysis_data: Dict[str, Any], patterns_found: List[Dict[str, Any]], insights: List[Dict[str, Any]]) -> List[str]:
        """ç”Ÿæˆç»¼åˆå»ºè®®"""
        recommendations = []

        coverage_data = analysis_data.get("coverage", {})
        coverage_percentage = coverage_data.get("coverage_percentage", 0)

        # åŸºäºè¦†ç›–ç‡çš„å»ºè®®
        if coverage_percentage < 75:
            recommendations.append(f"ğŸ“ˆ æå‡æµ‹è¯•è¦†ç›–ç‡åˆ°75%ä»¥ä¸Š (å½“å‰: {coverage_percentage:.1f}%)")

        # åŸºäºæ¨¡å¼çš„å»ºè®®
        for pattern in patterns_found:
            if pattern.get("suggested_actions"):
                recommendations.extend([f"ğŸ”§ {action}" for action in pattern["suggested_actions"]])

        # åŸºäºæ´å¯Ÿçš„å»ºè®®
        high_impact_insights = [i for i in insights if i.get("impact_level") == "HIGH" and i.get("actionable")]
        for insight in high_impact_insights:
            if insight.get("suggested_actions"):
                recommendations.extend(insight["suggested_actions"])

        return recommendations

    def _load_trend_data(self):
        """åŠ è½½å†å²è¶‹åŠ¿æ•°æ®"""
        if self.trend_data_file.exists():
            try:
                with open(self.trend_data_file, 'r') as f:
                    data = json.load(f)
                    self.trend_data = [
                        QualityTrend(**item) for item in data
                    ]
            except Exception as e:
                print(f"âš ï¸ åŠ è½½è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")

    def _save_trend_data(self, trend: QualityTrend):
        """ä¿å­˜è¶‹åŠ¿æ•°æ®"""
        self.trend_data.append(trend)

        # ä¿ç•™æœ€è¿‘30å¤©çš„æ•°æ®
        cutoff_date = datetime.now() - timedelta(days=30)
        self.trend_data = [t for t in self.trend_data if t.date >= cutoff_date]

        try:
            with open(self.trend_data_file, 'w') as f:
                json.dump([
                    {
                        "date": t.date.isoformat(),
                        "coverage_percentage": t.coverage_percentage,
                        "test_count": t.test_count,
                        "complexity_score": t.complexity_score,
                        "security_score": t.security_score,
                        "performance_score": t.performance_score,
                        "overall_score": t.overall_score
                    }
                    for t in self.trend_data
                ], f, indent=2)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜è¶‹åŠ¿æ•°æ®å¤±è´¥: {e}")

    def generate_quality_report(self, analysis_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆè´¨é‡æŠ¥å‘Š"""
        report = f"""
# ğŸ›ï¸ æ™ºèƒ½è´¨é‡åˆ†ææŠ¥å‘Š

## ğŸ“Š è´¨é‡æ¦‚è§ˆ
- **åˆ†ææ—¶é—´**: {analysis_result['timestamp']}
- **åˆ†æè€—æ—¶**: {analysis_result['analysis_time']:.2f}ç§’
- **ç»¼åˆè´¨é‡åˆ†æ•°**: {analysis_result['overall_score']:.1f}/100

## ğŸ“ˆ è¦†ç›–ç‡æŒ‡æ ‡
- **å½“å‰è¦†ç›–ç‡**: {analysis_result['coverage_data'].get('coverage_percentage', 0):.1f}%
- **è¦†ç›–æ¨¡å—æ•°**: {analysis_result['coverage_data'].get('covered_modules', 0)}
- **æ€»æ¨¡å—æ•°**: {analysis_result['coverage_data'].get('total_modules', 0)}

## ğŸ§ª æµ‹è¯•è´¨é‡åˆ†æ
- **æµ‹è¯•æ–‡ä»¶æ€»æ•°**: {analysis_result['test_quality'].get('total_test_files', 0)}
- **æµ‹è¯•æ–¹æ³•æ€»æ•°**: {analysis_result['test_quality'].get('total_test_methods', 0)}
- **æµ‹è¯•å¤æ‚åº¦åˆ†æ•°**: {analysis_result['test_quality'].get('test_complexity_score', 0):.1f}/100
- **æ–‡ä»¶å¹³å‡æ–¹æ³•æ•°**: {analysis_result['test_quality'].get('average_methods_per_file', 0):.1f}

## ğŸ“Š ä»£ç æŒ‡æ ‡
- **Pythonæ–‡ä»¶æ€»æ•°**: {analysis_result['code_metrics'].get('total_python_files', 0)}
- **ä»£ç æ€»è¡Œæ•°**: {analysis_result['code_metrics'].get('total_lines_of_code', 0)}
- **å¹³å‡æ–‡ä»¶å¤§å°**: {analysis_result['code_metrics'].get('average_file_size', 0):.1f}è¡Œ
- **å¤æ‚æ–‡ä»¶æ•°**: {analysis_result['code_metrics'].get('complex_files_count', 0)}
- **å¯ç»´æŠ¤æ€§æŒ‡æ•°**: {analysis_result['code_metrics'].get('maintainability_index', 0):.1f}/100

## ğŸ” è´¨é‡æ¨¡å¼è¯†åˆ«
"""

        for pattern in analysis_result['patterns_found']:
            report += f"- **{pattern['name']}**: {pattern['description']}\n"

        report += """
## ğŸ’¡ è´¨é‡æ´å¯Ÿ
"""

        for insight in analysis_result['insights']:
            impact_icon = "ğŸŸ¢" if insight['impact_level'] == "POSITIVE" else "ğŸŸ¡" if insight['impact_level'] == "MEDIUM" else "ğŸ”´"
            report += f"- {impact_icon} **{insight['category'].title()}**: {insight['insight']}\n"

        report += f"""
## ğŸ“ˆ è¶‹åŠ¿åˆ†æ
- **æ—¥æœŸ**: {analysis_result['trend']['date'].strftime('%Y-%m-%d')}
- **è¦†ç›–ç‡è¶‹åŠ¿**: {analysis_result['trend']['coverage_percentage']:.1f}%
- **æµ‹è¯•æ•°é‡è¶‹åŠ¿**: {analysis_result['trend']['test_count']}
- **ç»¼åˆè´¨é‡åˆ†æ•°**: {analysis_result['trend']['overall_score']:.1f}/100

## ğŸ“‹ æ”¹è¿›å»ºè®®
"""

        for i, rec in enumerate(analysis_result['recommendations'], 1):
            report += f"{i}. {rec}\n"

        report += f"""
---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
*åˆ†æå¼•æ“ç‰ˆæœ¬: 1.0.0*
"""

        return report

def main():
    """ä¸»å‡½æ•°"""
    analyzer = IntelligentQualityAnalyzer()

    print("ğŸ¤– æ™ºèƒ½è´¨é‡åˆ†æå¼•æ“å¯åŠ¨...")

    # åˆ†æé¡¹ç›®è´¨é‡
    result = analyzer.analyze_project_quality()

    # ç”ŸæˆæŠ¥å‘Š
    report = analyzer.generate_quality_report(result)

    print("\n" + "="*60)
    print("ğŸ“Š æ™ºèƒ½è´¨é‡åˆ†ææŠ¥å‘Š")
    print("="*60)
    print(report)

    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("quality_system") / "quality_analysis_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    print("âœ… æ™ºèƒ½è´¨é‡åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()