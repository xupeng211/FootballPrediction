# ğŸš€ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ä¼˜åŒ–è¯„ä¼°æŠ¥å‘Š

**é¡¹ç›®åç§°**: FootballPrediction - è¶³çƒé¢„æµ‹ç³»ç»Ÿ  
**è¯„ä¼°æ—¥æœŸ**: 2025å¹´10æœˆ11æ—¥  
**è¯„ä¼°ç±»å‹**: ä¸Šçº¿éƒ¨ç½²å‰æ·±åº¦ä»£ç å®¡æŸ¥  
**è¯„ä¼°äººå‘˜**: AI æŠ€æœ¯é¡¾é—®  

---

## ğŸ“‹ æ‰§è¡Œæ¦‚è¦

æœ¬æŠ¥å‘ŠåŸºäºå¯¹é¡¹ç›® **438 ä¸ª Python æºæ–‡ä»¶**å’Œ **411 ä¸ªæµ‹è¯•æ–‡ä»¶**çš„æ·±åº¦ä»£ç å±‚é¢åˆ†æï¼Œè¯†åˆ«äº†åœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰éœ€è¦ä¼˜å…ˆè§£å†³çš„**å…³é”®é—®é¢˜**å’Œ**é‡è¦ä¼˜åŒ–ç‚¹**ã€‚

### ğŸ¯ æ€»ä½“è¯„ä¼°

| ç»´åº¦ | å½“å‰çŠ¶æ€ | ç›®æ ‡çŠ¶æ€ | ä¼˜å…ˆçº§ |
|------|---------|---------|--------|
| æµ‹è¯•è¦†ç›–ç‡ | 18.12% âš ï¸ | â‰¥80% | ğŸ”´ **å…³é”®** |
| ä»£ç è´¨é‡ | è‰¯å¥½ âœ… | ä¼˜ç§€ | ğŸŸ¡ é«˜ |
| å®‰å…¨é…ç½® | æœ‰é£é™© âš ï¸ | å®‰å…¨ | ğŸ”´ **å…³é”®** |
| æ€§èƒ½ä¼˜åŒ– | åŸºç¡€ âš ï¸ | ç”Ÿäº§çº§ | ğŸŸ¡ é«˜ |
| æ–‡æ¡£å®Œæ•´æ€§ | è‰¯å¥½ âœ… | å®Œå–„ | ğŸŸ¢ ä¸­ |
| ç›‘æ§å‘Šè­¦ | åŸºç¡€ âš ï¸ | å®Œå–„ | ğŸŸ¡ é«˜ |

**å»ºè®®éƒ¨ç½²æ—¶é—´çº¿**: å®Œæˆå…³é”®é—®é¢˜ä¿®å¤å 2-3 å‘¨

---

## ğŸ”´ ç¬¬ä¸€éƒ¨åˆ†ï¼šå…³é”®é—®é¢˜ï¼ˆé˜»å¡æ€§ï¼Œå¿…é¡»ä¿®å¤ï¼‰

### 1.1 æµ‹è¯•è¦†ç›–ç‡ä¸¥é‡ä¸è¶³ ğŸ”´

**é—®é¢˜æè¿°**:

- **å®é™…è¦†ç›–ç‡**: 18.12% (5,841/25,947 è¡Œä»£ç )
- **README å£°ç§°**: 96.35% ï¼ˆ**ä¸¥é‡ä¸ä¸€è‡´**ï¼‰
- **å®Œå…¨æœªæµ‹è¯•**: 105 ä¸ªæ–‡ä»¶ (36.5%)
- **åˆ†æ”¯è¦†ç›–ç‡**: 0.48% (31/6,466)

**æ ¸å¿ƒæœªæµ‹è¯•æ¨¡å—**:

| æ¨¡å— | æ–‡ä»¶æ•° | ä»£ç è¡Œæ•° | è¦†ç›–ç‡ | å½±å“ |
|------|-------|---------|--------|------|
| `domain/` | 17 | 2,733 | 0.00% | ğŸ”´ ä¸šåŠ¡æ ¸å¿ƒ |
| `features/` | 4 | 533 | 0.00% | ğŸ”´ ç‰¹å¾å·¥ç¨‹ |
| `collectors/` | 5 | 484 | 1.38% | ğŸ”´ æ•°æ®é‡‡é›† |
| `services/` | 28 | 1,492 | 16.06% | ğŸŸ¡ æœåŠ¡å±‚ |
| `streaming/` | 10 | 741 | 15.58% | ğŸŸ¡ å®æ—¶å¤„ç† |

**å…·ä½“é—®é¢˜æ–‡ä»¶** (Top 10):

1. `src/domain/models/league.py` - 0% (232è¡Œ) - è”èµ›æ¨¡å‹
2. `src/domain/models/team.py` - 0% (225è¡Œ) - é˜Ÿä¼æ¨¡å‹
3. `src/domain/models/prediction.py` - 0% (209è¡Œ) - é¢„æµ‹æ¨¡å‹
4. `src/adapters/registry.py` - 0% (184è¡Œ) - é€‚é…å™¨æ³¨å†Œ
5. `src/cache/mock_redis.py` - 0% (153è¡Œ) - Redis Mock
6. `src/domain/models/match.py` - 0% (160è¡Œ) - æ¯”èµ›æ¨¡å‹
7. `src/adapters/factory.py` - 0% (132è¡Œ) - é€‚é…å™¨å·¥å‚
8. `src/collectors/scores_collector_improved.py` - 0% (304è¡Œ) - æ•°æ®é‡‡é›†å™¨
9. `src/core/di_setup.py` - 0% (101è¡Œ) - ä¾èµ–æ³¨å…¥
10. `src/domain/services/match_service.py` - 0% (69è¡Œ) - æ¯”èµ›æœåŠ¡

**ç”Ÿäº§é£é™©**:

- âŒ æ ¸å¿ƒä¸šåŠ¡é€»è¾‘ï¼ˆdomainå±‚ï¼‰å®Œå…¨æœªæµ‹è¯•ï¼Œä»»ä½•ä¿®æ”¹éƒ½å¯èƒ½å¼•å…¥é‡å¤§bug
- âŒ æ•°æ®é‡‡é›†å™¨æœªæµ‹è¯•ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®è´¨é‡é—®é¢˜
- âŒ æ— æ³•ä¿è¯ä»£ç å˜æ›´çš„å®‰å…¨æ€§
- âŒ é‡æ„å’Œç»´æŠ¤æˆæœ¬æé«˜

**è§£å†³æ–¹æ¡ˆ** (é¢„è®¡æ—¶é—´: 2-3å‘¨):

```markdown
**Phase 1: æ ¸å¿ƒæ¨¡å—æµ‹è¯• (1å‘¨)**
1. domain/models/* - è¡¥å……æ¨¡å‹å•å…ƒæµ‹è¯•
   - league.py, team.py, prediction.py, match.py
   - æµ‹è¯•æ•°æ®éªŒè¯ã€ä¸šåŠ¡è§„åˆ™ã€è¾¹ç•Œæ¡ä»¶
   - ç›®æ ‡è¦†ç›–ç‡: 80%+

2. domain/services/* - è¡¥å……æœåŠ¡å±‚æµ‹è¯•
   - match_service.py, prediction_service.py
   - æµ‹è¯•ä¸šåŠ¡æµç¨‹ã€å¼‚å¸¸å¤„ç†
   - ç›®æ ‡è¦†ç›–ç‡: 75%+

3. features/* - è¡¥å……ç‰¹å¾å·¥ç¨‹æµ‹è¯•
   - æµ‹è¯•ç‰¹å¾è®¡ç®—é€»è¾‘ã€æ•°æ®è½¬æ¢
   - ç›®æ ‡è¦†ç›–ç‡: 80%+

**Phase 2: æ•°æ®é‡‡é›†ä¸é€‚é…å™¨æµ‹è¯• (5å¤©)**
4. collectors/* - è¡¥å……æ•°æ®é‡‡é›†æµ‹è¯•
   - scores_collector_improved.py
   - æµ‹è¯•APIè°ƒç”¨ã€æ•°æ®è§£æã€é”™è¯¯å¤„ç†
   - ä½¿ç”¨ Mock å¤–éƒ¨API
   - ç›®æ ‡è¦†ç›–ç‡: 70%+

5. adapters/* - è¡¥å……é€‚é…å™¨æµ‹è¯•
   - registry.py, factory.py
   - æµ‹è¯•æ³¨å†Œæœºåˆ¶ã€å·¥å‚æ¨¡å¼
   - ç›®æ ‡è¦†ç›–ç‡: 75%+

**Phase 3: æœåŠ¡å±‚ä¸æµå¤„ç†æµ‹è¯• (2-3å¤©)**
6. services/* - æå‡æœåŠ¡å±‚è¦†ç›–ç‡
   - å½“å‰16% -> ç›®æ ‡70%+
   - é‡ç‚¹æµ‹è¯•æ ¸å¿ƒæœåŠ¡æ¥å£

7. streaming/* - è¡¥å……æµå¤„ç†æµ‹è¯•
   - å½“å‰15.58% -> ç›®æ ‡60%+
   - æµ‹è¯• Kafka æ¶ˆæ¯å¤„ç†ï¼ˆä½¿ç”¨ TestContainersï¼‰
```

**éªŒè¯æ ‡å‡†**:

- [ ] æ•´ä½“è¦†ç›–ç‡è¾¾åˆ° â‰¥80%
- [ ] domain/ æ¨¡å—è¾¾åˆ° â‰¥80%
- [ ] æ ¸å¿ƒ API è·¯ç”±è¾¾åˆ° 100%
- [ ] æ‰€æœ‰å…³é”®è·¯å¾„æœ‰é›†æˆæµ‹è¯•
- [ ] ä¿®å¤ README ä¸­çš„è¦†ç›–ç‡æ•°æ®ä¸ä¸€è‡´é—®é¢˜

---

### 1.2 å®‰å…¨é…ç½®å­˜åœ¨é‡å¤§é£é™© ğŸ”´

**é—®é¢˜ 1: æ•æ„Ÿä¿¡æ¯æš´éœ²é£é™©**

æ£€æŸ¥å‘ç°é¡¹ç›®æ ¹ç›®å½•å­˜åœ¨å®é™…çš„ `.env` æ–‡ä»¶ï¼ŒåŒ…å«æ•æ„Ÿå¯†é’¥ï¼š

```bash
# å‘ç°çš„æ•æ„Ÿä¿¡æ¯
SECRET_KEY=D9poRJVoOXt8mh0WtMiI3QU_Rh35tpf1iRkfptX7SlEB98HNw77lzc_nDCifsdXpizfTbEx_egoVphnKkvOxAw
JWT_SECRET_KEY=jdscW3LWlGrDFd_pCIQBXJJsv1vNK6yneZViGbebyafdu8Hqzg4onIp9D6MPHFdwyYRT_765wTC-pDRhl1yC7Q
API_KEY=pov6HCr3IfO--OJoy8s7C915QmMa3k39iFlVyw_quYSlmgLWy8ihWgjdIA8vywLRzTMjjUv_ycoOYMCbJwGwnA
DB_ENCRYPTION_KEY=0f0bd5eafb8e8284de0bd3984537e4f603b79824e0896e5ac3e22e009e9891f1
```

**é£é™©ç­‰çº§**: ğŸ”´ **ä¸¥é‡**

**å½±å“**:

- âŒ å¯†é’¥å¯èƒ½å·²è¢«æäº¤åˆ° Git å†å²
- âŒ å¦‚æœä»£ç è¢«å…¬å¼€ï¼Œæ‰€æœ‰å¯†é’¥éœ€è¦ç«‹å³è½®æ¢
- âŒ æ½œåœ¨çš„å®‰å…¨æ¼æ´æš´éœ²

**è§£å†³æ–¹æ¡ˆ** (ç«‹å³æ‰§è¡Œ):

```bash
# 1. æ£€æŸ¥ .env æ˜¯å¦åœ¨ Git å†å²ä¸­
git log --all --full-history -- .env

# 2. å¦‚æœå‘ç°å†å²è®°å½•ï¼Œç«‹å³æ¸…ç†ï¼ˆä½¿ç”¨ git-filter-repoï¼‰
# å®‰è£…: pip install git-filter-repo
git filter-repo --path .env --invert-paths

# 3. å¼ºåˆ¶è¦æ±‚ .gitignore
echo ".env" >> .gitignore
echo ".env.*" >> .gitignore
echo "!.env.example" >> .gitignore

# 4. æäº¤ .gitignore æ›´æ–°
git add .gitignore
git commit -m "security: ensure .env files are ignored"

# 5. ç”Ÿæˆæ–°çš„ç”Ÿäº§ç¯å¢ƒå¯†é’¥ï¼ˆä½¿ç”¨ç‹¬ç«‹çš„å¯†é’¥ç®¡ç†ç³»ç»Ÿï¼‰
python -c "import secrets; print('SECRET_KEY=' + secrets.token_urlsafe(64))"
```

**ç”Ÿäº§ç¯å¢ƒå»ºè®®**:

1. **ä½¿ç”¨äº‘æœåŠ¡å¯†é’¥ç®¡ç†**:
   - AWS: AWS Secrets Manager / Parameter Store
   - Azure: Azure Key Vault
   - GCP: Secret Manager
   - Kubernetes: External Secrets Operator

2. **12-Factor App åŸåˆ™**:

   ```python
   # src/config/settings.py
   from pydantic_settings import BaseSettings
   
   class Settings(BaseSettings):
       secret_key: str
       jwt_secret_key: str
       database_url: str
       
       class Config:
           env_file = ".env"  # ä»…å¼€å‘ç¯å¢ƒ
           env_file_encoding = "utf-8"
           # ç”Ÿäº§ç¯å¢ƒä»ç¯å¢ƒå˜é‡è¯»å–ï¼Œä¸ä½¿ç”¨ .env æ–‡ä»¶
   ```

3. **æ·»åŠ é¢„æäº¤é’©å­**:

   ```yaml
   # .pre-commit-config.yaml
   repos:
     - repo: https://github.com/Yelp/detect-secrets
       rev: v1.4.0
       hooks:
         - id: detect-secrets
           args: ['--baseline', '.secrets.baseline']
   ```

---

**é—®é¢˜ 2: å¯†ç å’Œæ•æ„Ÿæ•°æ®ç¡¬ç¼–ç æ£€æŸ¥**

å‘ç° 11 ä¸ªæ–‡ä»¶åŒ…å« `SECRET_KEY`, `API_KEY`, `PASSWORD`, `TOKEN` ç­‰å­—ç¬¦ä¸²ï¼š

**éœ€è¦å®¡æŸ¥çš„æ–‡ä»¶**:

```
src/api/dependencies.py
src/collectors/scores_collector.py
src/data/features/feature_store.py
src/tasks/backup_tasks.py
src/database/migrations/versions/004_configure_database_permissions.py
src/database/connection.py
src/database/config.py
```

**è§£å†³æ–¹æ¡ˆ**:

```bash
# 1. é€ä¸€å®¡æŸ¥è¿™äº›æ–‡ä»¶ï¼Œç¡®ä¿æ²¡æœ‰ç¡¬ç¼–ç çš„æ•æ„Ÿä¿¡æ¯
# 2. æ‰€æœ‰æ•æ„Ÿé…ç½®å¿…é¡»ä»ç¯å¢ƒå˜é‡è¯»å–
# 3. æ·»åŠ ä»£ç å®¡æŸ¥æ£€æŸ¥æ¸…å•
```

**ä»£ç å®¡æŸ¥æ£€æŸ¥æ¸…å•**:

- [ ] æ— ç¡¬ç¼–ç çš„ API å¯†é’¥
- [ ] æ— ç¡¬ç¼–ç çš„æ•°æ®åº“å¯†ç 
- [ ] æ— ç¡¬ç¼–ç çš„ JWT å¯†é’¥
- [ ] æ— ç¡¬ç¼–ç çš„åŠ å¯†å¯†é’¥
- [ ] æ‰€æœ‰æ•æ„Ÿé…ç½®é€šè¿‡ç¯å¢ƒå˜é‡æˆ–å¯†é’¥ç®¡ç†ç³»ç»Ÿè·å–

---

**é—®é¢˜ 3: æ•°æ®åº“å¯†ç ç®¡ç†**

å½“å‰é…ç½®ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼Œä½†éœ€è¦ç¡®ä¿ï¼š

```python
# âœ… å¥½çš„åšæ³• (å½“å‰)
DATABASE_URL = os.getenv("DATABASE_URL")

# âŒ é¿å…çš„åšæ³•
DATABASE_URL = "postgresql://user:password@localhost/db"
```

**ç”Ÿäº§ç¯å¢ƒå¢å¼º**:

1. ä½¿ç”¨æ•°æ®åº“è¿æ¥æ± çš„å¯†ç è½®æ¢æœºåˆ¶
2. å¯ç”¨æ•°æ®åº“ SSL/TLS è¿æ¥
3. å®æ–½æœ€å°æƒé™åŸåˆ™ï¼ˆreadonly/readwrite/admin è§’è‰²åˆ†ç¦»ï¼‰

```python
# æ¨è: src/database/config.py å¢å¼º
class DatabaseConfig(BaseSettings):
    db_host: str
    db_port: int = 5432
    db_name: str
    db_user: str
    db_password: str = Field(..., env="DB_PASSWORD")  # ä»ç¯å¢ƒå˜é‡è¯»å–
    
    # ç”Ÿäº§ç¯å¢ƒ SSL é…ç½®
    db_ssl_mode: str = "require"  # ç”Ÿäº§ç¯å¢ƒå¼ºåˆ¶ SSL
    db_ssl_ca: Optional[str] = None
    
    @property
    def database_url(self) -> str:
        return f"postgresql+asyncpg://{self.db_user}:{self.db_password}@{self.db_host}:{self.db_port}/{self.db_name}?ssl={self.db_ssl_mode}"
```

---

### 1.3 ç”Ÿäº§ç¯å¢ƒç›‘æ§ä¸å‘Šè­¦ä¸è¶³ ğŸ”´

**å½“å‰çŠ¶æ€**:

- âœ… æœ‰åŸºç¡€çš„ Prometheus é…ç½®
- âœ… æœ‰ Grafana ä»ªè¡¨æ¿é…ç½®
- âš ï¸ ç¼ºå°‘å®Œæ•´çš„å‘Šè­¦è§„åˆ™
- âš ï¸ ç¼ºå°‘é”™è¯¯è¿½è¸ªé›†æˆï¼ˆSentryï¼‰
- âš ï¸ ç¼ºå°‘æ—¥å¿—èšåˆé…ç½®ï¼ˆè™½æœ‰ Loki ä½†æœªå®Œå…¨é…ç½®ï¼‰

**å…³é”®ç¼ºå¤±**:

1. **é”™è¯¯è¿½è¸ªä¸å¼‚å¸¸ç›‘æ§**

   ```python
   # å½“å‰: src/main.py åªæœ‰åŸºç¡€çš„å¼‚å¸¸å¤„ç†
   @app.exception_handler(Exception)
   async def general_exception_handler(request, exc: Exception):
       logger.error(f"æœªå¤„ç†å¼‚å¸¸: {type(exc).__name__}: {exc}")
       return JSONResponse(...)
   
   # ç¼ºå°‘: Sentry é›†æˆï¼Œæ— æ³•è¿½è¸ªç”Ÿäº§ç¯å¢ƒé”™è¯¯
   ```

2. **æ€§èƒ½ç›‘æ§ä¸å®Œæ•´**
   - æ—  APM (Application Performance Monitoring) é›†æˆ
   - æ— æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ç›‘æ§
   - æ—  API å“åº”æ—¶é—´è¯¦ç»†è¿½è¸ª

3. **ä¸šåŠ¡æŒ‡æ ‡ç›‘æ§ç¼ºå¤±**
   - æ— é¢„æµ‹å‡†ç¡®ç‡å®æ—¶ç›‘æ§
   - æ— æ•°æ®é‡‡é›†æˆåŠŸç‡ç›‘æ§
   - æ— ç”¨æˆ·è¯·æ±‚é‡å’ŒæˆåŠŸç‡ç›‘æ§

**è§£å†³æ–¹æ¡ˆ** (é¢„è®¡æ—¶é—´: 3-5å¤©):

**1. é›†æˆ Sentry é”™è¯¯è¿½è¸ª**

```python
# requirements/requirements.lock æ·»åŠ 
sentry-sdk==2.19.2

# src/main.py é›†æˆ
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.sqlalchemy import SqlalchemyIntegration

if os.getenv("ENVIRONMENT") == "production":
    sentry_sdk.init(
        dsn=os.getenv("SENTRY_DSN"),
        environment=os.getenv("ENVIRONMENT"),
        traces_sample_rate=0.1,  # 10% çš„è¯·æ±‚è¿½è¸ª
        profiles_sample_rate=0.1,
        integrations=[
            FastApiIntegration(),
            SqlalchemyIntegration(),
        ],
        # è¿‡æ»¤æ•æ„Ÿä¿¡æ¯
        before_send=lambda event, hint: event if not contains_pii(event) else None,
    )
```

**2. å®Œå–„ Prometheus æŒ‡æ ‡**

åˆ›å»º `config/prometheus/alerts.yml`:

```yaml
groups:
  - name: football_prediction_alerts
    interval: 30s
    rules:
      # API å¯ç”¨æ€§å‘Šè­¦
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} (> 5%)"
      
      # æ•°æ®åº“è¿æ¥å‘Šè­¦
      - alert: DatabaseConnectionPoolExhausted
        expr: db_pool_connections_in_use / db_pool_connections_max > 0.9
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool nearly exhausted"
      
      # é¢„æµ‹æœåŠ¡æ€§èƒ½å‘Šè­¦
      - alert: PredictionLatencyHigh
        expr: histogram_quantile(0.95, prediction_duration_seconds) > 2.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "95th percentile prediction latency > 2s"
      
      # æ•°æ®é‡‡é›†å¤±è´¥ç‡å‘Šè­¦
      - alert: DataCollectionFailureRate
        expr: rate(data_collection_failures_total[10m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Data collection failure rate > 10%"
```

**3. æ—¥å¿—èšåˆé…ç½®**

æ›´æ–° `config/loki/loki.yml`:

```yaml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2024-01-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: loki_index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/index
    cache_location: /loki/cache
    shared_store: filesystem
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h  # 7 days
  ingestion_rate_mb: 10
  ingestion_burst_size_mb: 20

# æ—¥å¿—ä¿ç•™ç­–ç•¥
chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: true
  retention_period: 2160h  # 90 days
```

**4. åº”ç”¨æ—¥å¿—ç»“æ„åŒ–**

```python
# src/utils/logging_config.py
import structlog

def configure_logging():
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()  # ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ JSON
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )

# ä½¿ç”¨ç¤ºä¾‹
logger = structlog.get_logger()
logger.info("prediction_completed", 
            match_id=123, 
            prediction_result="win", 
            confidence=0.85,
            duration_ms=234)
```

**5. å¥åº·æ£€æŸ¥å¢å¼º**

```python
# src/api/health/router.py å¢å¼º
from fastapi import APIRouter, status
from sqlalchemy import text

router = APIRouter()

@router.get("/health/liveness")
async def liveness():
    """å­˜æ´»æ¢é’ˆ - Kubernetes ä½¿ç”¨"""
    return {"status": "alive"}

@router.get("/health/readiness")
async def readiness(db: AsyncSession = Depends(get_db)):
    """å°±ç»ªæ¢é’ˆ - æ£€æŸ¥ä¾èµ–æœåŠ¡"""
    checks = {}
    
    # æ•°æ®åº“æ£€æŸ¥
    try:
        await db.execute(text("SELECT 1"))
        checks["database"] = "healthy"
    except Exception as e:
        checks["database"] = f"unhealthy: {str(e)}"
    
    # Redis æ£€æŸ¥
    try:
        redis_client = get_redis_client()
        await redis_client.ping()
        checks["redis"] = "healthy"
    except Exception as e:
        checks["redis"] = f"unhealthy: {str(e)}"
    
    # æ•´ä½“çŠ¶æ€
    is_healthy = all(v == "healthy" for v in checks.values())
    status_code = status.HTTP_200_OK if is_healthy else status.HTTP_503_SERVICE_UNAVAILABLE
    
    return JSONResponse(
        status_code=status_code,
        content={"status": "ready" if is_healthy else "not ready", "checks": checks}
    )
```

---

### 1.4 README ä¸å®é™…ä»£ç ä¸¥é‡ä¸ä¸€è‡´ ğŸ”´

**é—®é¢˜**:

- README å£°ç§°æµ‹è¯•è¦†ç›–ç‡ **96.35%**
- å®é™…ä»£ç è¦†ç›–ç‡ **18.12%**
- Badge æ˜¾ç¤º **16.5%**
- ä¸‰ä¸ªæ•°æ®äº’ç›¸çŸ›ç›¾ï¼Œè¯¯å¯¼å¼€å‘è€…å’Œç”¨æˆ·

**è§£å†³æ–¹æ¡ˆ** (ç«‹å³æ‰§è¡Œ):

```bash
# 1. æ›´æ–° README.mdï¼Œä½¿ç”¨å®é™…æ•°æ®
sed -i 's/96.35%/18.12%/g' README.md
sed -i 's/Coverage-16.5%25-yellow/Coverage-18.12%25-red/g' README.md

# 2. æ·»åŠ è¦†ç›–ç‡è‡ªåŠ¨æ›´æ–°è„šæœ¬
cat > scripts/update_coverage_badge.sh << 'EOF'
#!/bin/bash
# ä» coverage.xml è¯»å–å®é™…è¦†ç›–ç‡
coverage_percent=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); print(tree.getroot().attrib['line-rate'])")
coverage_int=$(python3 -c "print(int(float($coverage_percent) * 100))")

# æ›´æ–° README
sed -i "s/Coverage-[0-9.]*%25/Coverage-${coverage_int}%25/g" README.md
echo "âœ… Coverage badge updated to ${coverage_int}%"
EOF

chmod +x scripts/update_coverage_badge.sh

# 3. æ·»åŠ åˆ° CI æµç¨‹
# .github/workflows/ci.yml
```

**å¢åŠ  CI è‡ªåŠ¨éªŒè¯**:

```yaml
# .github/workflows/ci.yml
- name: Verify Coverage Documentation
  run: |
    coverage_actual=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); print(float(tree.getroot().attrib['line-rate']) * 100)")
    coverage_readme=$(grep -oP 'Coverage-\K[0-9.]+' README.md | head -1)
    
    if (( $(echo "$coverage_actual - $coverage_readme > 2" | bc -l) )); then
      echo "âŒ README coverage ($coverage_readme%) does not match actual ($coverage_actual%)"
      exit 1
    fi
    echo "âœ… Coverage documentation is accurate"
```

---

## ğŸŸ¡ ç¬¬äºŒéƒ¨åˆ†ï¼šé‡è¦ä¼˜åŒ–ï¼ˆé«˜ä¼˜å…ˆçº§ï¼Œå¼ºçƒˆå»ºè®®ï¼‰

### 2.1 æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

**å½“å‰é—®é¢˜**:

1. **ç¼ºå°‘è¿æ¥æ± é…ç½®ä¼˜åŒ–**
2. **ç¼ºå°‘æŸ¥è¯¢æ€§èƒ½ç›‘æ§**
3. **ç¼ºå°‘ç´¢å¼•ä¼˜åŒ–åˆ†æ**
4. **ç¼ºå°‘æ…¢æŸ¥è¯¢æ—¥å¿—**

**è§£å†³æ–¹æ¡ˆ**:

**1. è¿æ¥æ± ä¼˜åŒ–**

```python
# src/database/config.py
from sqlalchemy.pool import QueuePool

class DatabaseConfig:
    # è¿æ¥æ± é…ç½®ï¼ˆæ ¹æ®ç”Ÿäº§ç¯å¢ƒè´Ÿè½½è°ƒæ•´ï¼‰
    POOL_SIZE = 20  # å¸¸è§„è¿æ¥æ•°
    MAX_OVERFLOW = 10  # çªå‘æ—¶é¢å¤–è¿æ¥æ•°
    POOL_TIMEOUT = 30  # è·å–è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    POOL_RECYCLE = 3600  # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆ1å°æ—¶ï¼‰
    POOL_PRE_PING = True  # è¿æ¥å‰æµ‹è¯•ï¼Œé¿å…ä½¿ç”¨æ–­å¼€çš„è¿æ¥
    
    # è¯»å†™åˆ†ç¦»é…ç½®
    READER_POOL_SIZE = 30  # è¯»åº“è¿æ¥æ± æ›´å¤§
    WRITER_POOL_SIZE = 10  # å†™åº“è¿æ¥æ± è¾ƒå°

# src/database/connection_mod/manager.py
engine = create_async_engine(
    database_url,
    poolclass=QueuePool,
    pool_size=config.POOL_SIZE,
    max_overflow=config.MAX_OVERFLOW,
    pool_timeout=config.POOL_TIMEOUT,
    pool_recycle=config.POOL_RECYCLE,
    pool_pre_ping=config.POOL_PRE_PING,
    echo=False,  # ç”Ÿäº§ç¯å¢ƒç¦ç”¨SQLæ—¥å¿—
    echo_pool=False,  # ç¦ç”¨è¿æ¥æ± æ—¥å¿—
    # æ€§èƒ½ä¼˜åŒ–
    execution_options={
        "isolation_level": "READ COMMITTED",  # é™ä½é”ç«äº‰
    }
)
```

**2. æŸ¥è¯¢æ€§èƒ½ç›‘æ§**

```python
# src/middleware/database_monitoring.py
from fastapi import Request
import time
import structlog

logger = structlog.get_logger()

@app.middleware("http")
async def monitor_database_queries(request: Request, call_next):
    """ç›‘æ§æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"""
    
    # ä½¿ç”¨ SQLAlchemy äº‹ä»¶ç›‘å¬å™¨
    from sqlalchemy import event
    from sqlalchemy.engine import Engine
    
    query_times = []
    
    @event.listens_for(Engine, "before_cursor_execute")
    def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        conn.info.setdefault('query_start_time', []).append(time.time())
    
    @event.listens_for(Engine, "after_cursor_execute")
    def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
        total_time = time.time() - conn.info['query_start_time'].pop()
        query_times.append(total_time)
        
        # æ…¢æŸ¥è¯¢å‘Šè­¦ï¼ˆè¶…è¿‡1ç§’ï¼‰
        if total_time > 1.0:
            logger.warning("slow_query",
                          query=statement[:200],
                          duration_seconds=total_time,
                          path=request.url.path)
    
    response = await call_next(request)
    
    if query_times:
        response.headers["X-Database-Query-Count"] = str(len(query_times))
        response.headers["X-Database-Query-Time"] = f"{sum(query_times):.3f}s"
    
    return response
```

**3. æ·»åŠ æ•°æ®åº“ç´¢å¼•åˆ†æ**

```sql
-- scripts/sql/analyze_missing_indexes.sql
-- ç”Ÿäº§ç¯å¢ƒè¿è¡Œï¼Œåˆ†æå¯èƒ½ç¼ºå¤±çš„ç´¢å¼•

-- æ‰¾å‡ºæœ€æ…¢çš„æŸ¥è¯¢
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    max_time
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 20;

-- æ‰¾å‡ºè¡¨æ‰«æï¼ˆå¯èƒ½éœ€è¦æ·»åŠ ç´¢å¼•ï¼‰
SELECT 
    schemaname,
    tablename,
    seq_scan,
    seq_tup_read,
    idx_scan,
    seq_tup_read / seq_scan as avg_seq_tup_read
FROM pg_stat_user_tables
WHERE seq_scan > 0
ORDER BY seq_tup_read DESC
LIMIT 20;

-- æ‰¾å‡ºæœªä½¿ç”¨çš„ç´¢å¼•ï¼ˆå¯ä»¥åˆ é™¤ä»¥æå‡å†™æ€§èƒ½ï¼‰
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch,
    pg_size_pretty(pg_relation_size(indexrelid)) as index_size
FROM pg_stat_user_indexes
WHERE idx_scan = 0
    AND indexrelname NOT LIKE 'pg_toast%'
ORDER BY pg_relation_size(indexrelid) DESC;
```

**4. å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—**

```ini
# config/postgresql/postgresql.conf (ç”Ÿäº§ç¯å¢ƒ)
# æ…¢æŸ¥è¯¢æ—¥å¿—
log_min_duration_statement = 1000  # è®°å½•è¶…è¿‡1ç§’çš„æŸ¥è¯¢
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_statement = 'all'  # ç”Ÿäº§ç¯å¢ƒæ”¹ä¸º 'ddl' æˆ– 'mod'

# æŸ¥è¯¢ç»Ÿè®¡
shared_preload_libraries = 'pg_stat_statements'
pg_stat_statements.track = all
pg_stat_statements.max = 10000

# è¿æ¥æ± é…ç½®
max_connections = 200
```

**5. æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤**

```bash
# scripts/database/backup.sh
#!/bin/bash
set -e

BACKUP_DIR="/backups/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/football_prediction_${DATE}.sql.gz"

# åˆ›å»ºå¤‡ä»½
pg_dump -h ${DB_HOST} -U ${DB_USER} -d ${DB_NAME} \
    --format=custom \
    --compress=9 \
    --verbose \
    | gzip > "${BACKUP_FILE}"

# ä»…ä¿ç•™æœ€è¿‘30å¤©çš„å¤‡ä»½
find "${BACKUP_DIR}" -name "*.sql.gz" -mtime +30 -delete

echo "âœ… Backup completed: ${BACKUP_FILE}"

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆç¤ºä¾‹ï¼šAWS S3ï¼‰
if [ -n "${AWS_S3_BACKUP_BUCKET}" ]; then
    aws s3 cp "${BACKUP_FILE}" "s3://${AWS_S3_BACKUP_BUCKET}/postgresql/"
    echo "âœ… Backup uploaded to S3"
fi
```

**æ·»åŠ åˆ° Makefile**:

```makefile
db-backup-automated: ## Database: Create automated backup with timestamp
 @bash scripts/database/backup.sh

db-restore-latest: ## Database: Restore from latest backup
 @bash scripts/database/restore.sh latest
```

---

### 2.2 API æ€§èƒ½ä¼˜åŒ–

**å½“å‰é—®é¢˜**:

1. **ç¼ºå°‘è¯·æ±‚çº§ç¼“å­˜**
2. **ç¼ºå°‘å“åº”å‹ç¼©**
3. **ç¼ºå°‘ API é™æµï¼ˆè™½ç„¶æœ‰ slowapi ä½†æœªå®Œå…¨å¯ç”¨ï¼‰**
4. **ç¼ºå°‘è¯·æ±‚è¶…æ—¶æ§åˆ¶**

**è§£å†³æ–¹æ¡ˆ**:

**1. å¯ç”¨å“åº”å‹ç¼©**

```python
# src/main.py
from fastapi.middleware.gzip import GZipMiddleware

# æ·»åŠ  Gzip å‹ç¼©ä¸­é—´ä»¶ï¼ˆæå‡å“åº”é€Ÿåº¦ï¼‰
app.add_middleware(
    GZipMiddleware,
    minimum_size=1000,  # ä»…å‹ç¼©å¤§äº1KBçš„å“åº”
    compresslevel=6     # å‹ç¼©çº§åˆ«ï¼ˆ1-9ï¼Œ6ä¸ºå¹³è¡¡ï¼‰
)
```

**2. å¢å¼ºè¯·æ±‚è¶…æ—¶æ§åˆ¶**

```python
# src/middleware/timeout.py
from fastapi import Request, HTTPException
import asyncio

class TimeoutMiddleware:
    def __init__(self, app, timeout_seconds: int = 30):
        self.app = app
        self.timeout_seconds = timeout_seconds
    
    async def __call__(self, scope, receive, send):
        if scope["type"] != "http":
            return await self.app(scope, receive, send)
        
        try:
            return await asyncio.wait_for(
                self.app(scope, receive, send),
                timeout=self.timeout_seconds
            )
        except asyncio.TimeoutError:
            # è¿”å› 504 Gateway Timeout
            await send({
                'type': 'http.response.start',
                'status': 504,
                'headers': [[b'content-type', b'application/json']],
            })
            await send({
                'type': 'http.response.body',
                'body': b'{"error": "Request timeout", "status_code": 504}',
            })

# src/main.py
app.add_middleware(TimeoutMiddleware, timeout_seconds=30)
```

**3. è¯·æ±‚çº§ç¼“å­˜ï¼ˆé’ˆå¯¹é¢„æµ‹APIï¼‰**

```python
# src/api/predictions/router.py
from functools import lru_cache
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend
from fastapi_cache.decorator import cache

# åˆå§‹åŒ– Redis ç¼“å­˜åç«¯
@app.on_event("startup")
async def startup():
    redis = aioredis.from_url("redis://localhost", encoding="utf8", decode_responses=True)
    FastAPICache.init(RedisBackend(redis), prefix="fastapi-cache")

# ç¼“å­˜é¢„æµ‹ç»“æœï¼ˆ5åˆ†é’Ÿï¼‰
@router.get("/predictions/{match_id}")
@cache(expire=300)  # 5åˆ†é’Ÿç¼“å­˜
async def get_prediction(match_id: int):
    # è®¡ç®—é¢„æµ‹ï¼ˆæ˜‚è´µæ“ä½œï¼‰
    prediction = await calculate_prediction(match_id)
    return prediction
```

**4. æ‰¹é‡è¯·æ±‚ä¼˜åŒ–**

```python
# src/api/predictions/router.py
from fastapi import BackgroundTasks

@router.post("/predictions/batch")
async def batch_predictions(
    match_ids: List[int],
    background_tasks: BackgroundTasks
):
    """æ‰¹é‡é¢„æµ‹ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰"""
    
    if len(match_ids) > 100:
        raise HTTPException(400, "Maximum 100 match IDs per request")
    
    # åˆ›å»ºæ‰¹é‡ä»»åŠ¡
    task_id = str(uuid.uuid4())
    
    # å¼‚æ­¥æ‰§è¡Œ
    background_tasks.add_task(
        process_batch_predictions,
        task_id=task_id,
        match_ids=match_ids
    )
    
    return {
        "task_id": task_id,
        "status": "processing",
        "match_count": len(match_ids),
        "status_url": f"/predictions/batch/{task_id}/status"
    }

@router.get("/predictions/batch/{task_id}/status")
async def get_batch_status(task_id: str):
    """æŸ¥è¯¢æ‰¹é‡ä»»åŠ¡çŠ¶æ€"""
    # ä» Redis è¯»å–çŠ¶æ€
    status = await redis_client.hgetall(f"batch_task:{task_id}")
    return status
```

---

### 2.3 æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–

**å½“å‰é—®é¢˜**:

1. **æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ä¸å®Œå–„**
2. **ç¼ºå°‘ A/B æµ‹è¯•æœºåˆ¶**
3. **ç¼ºå°‘æ¨¡å‹æ€§èƒ½ç›‘æ§**
4. **ç¼ºå°‘æ¨¡å‹è‡ªåŠ¨é‡è®­ç»ƒæµç¨‹**

**è§£å†³æ–¹æ¡ˆ**:

**1. æ¨¡å‹ç‰ˆæœ¬ç®¡ç†ï¼ˆMLflowï¼‰**

```python
# src/ml/model_registry.py
import mlflow
from mlflow.tracking import MlflowClient

class ModelRegistry:
    def __init__(self):
        self.client = MlflowClient()
        mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000"))
    
    def register_model(self, model, model_name: str, metrics: dict):
        """æ³¨å†Œæ–°æ¨¡å‹ç‰ˆæœ¬"""
        with mlflow.start_run() as run:
            # è®°å½•æ¨¡å‹
            mlflow.sklearn.log_model(model, "model")
            
            # è®°å½•æŒ‡æ ‡
            for key, value in metrics.items():
                mlflow.log_metric(key, value)
            
            # æ³¨å†Œåˆ°æ¨¡å‹åº“
            model_uri = f"runs:/{run.info.run_id}/model"
            mlflow.register_model(model_uri, model_name)
            
            return run.info.run_id
    
    def get_production_model(self, model_name: str):
        """è·å–ç”Ÿäº§ç¯å¢ƒæ¨¡å‹"""
        try:
            model_version = self.client.get_latest_versions(
                model_name,
                stages=["Production"]
            )[0]
            model_uri = f"models:/{model_name}/Production"
            return mlflow.sklearn.load_model(model_uri)
        except IndexError:
            raise ValueError(f"No production model found for {model_name}")
    
    def promote_to_production(self, model_name: str, version: int):
        """æå‡æ¨¡å‹ç‰ˆæœ¬åˆ°ç”Ÿäº§ç¯å¢ƒ"""
        self.client.transition_model_version_stage(
            name=model_name,
            version=version,
            stage="Production",
            archive_existing_versions=True  # å½’æ¡£æ—§ç‰ˆæœ¬
        )
```

**2. æ¨¡å‹ A/B æµ‹è¯•**

```python
# src/ml/ab_testing.py
import random
from enum import Enum

class ModelVariant(Enum):
    CONTROL = "control"  # å½“å‰ç”Ÿäº§æ¨¡å‹
    TREATMENT = "treatment"  # æ–°æ¨¡å‹

class ABTestManager:
    def __init__(self, treatment_ratio: float = 0.1):
        self.treatment_ratio = treatment_ratio
        self.model_registry = ModelRegistry()
    
    def get_model_for_request(self, user_id: str) -> tuple[ModelVariant, Any]:
        """æ ¹æ®ç”¨æˆ·IDåˆ†é…æ¨¡å‹ï¼ˆç¡®ä¿åŒä¸€ç”¨æˆ·å§‹ç»ˆä½¿ç”¨ç›¸åŒæ¨¡å‹ï¼‰"""
        # ä½¿ç”¨å“ˆå¸Œç¡®ä¿ä¸€è‡´æ€§
        hash_val = hash(user_id) % 100
        
        if hash_val < self.treatment_ratio * 100:
            variant = ModelVariant.TREATMENT
            model = self.model_registry.get_model("football_prediction_v2")
        else:
            variant = ModelVariant.CONTROL
            model = self.model_registry.get_production_model("football_prediction")
        
        return variant, model
    
    def log_prediction(self, variant: ModelVariant, user_id: str, 
                      prediction: dict, actual_result: dict = None):
        """è®°å½•é¢„æµ‹ç»“æœç”¨äº A/B æµ‹è¯•åˆ†æ"""
        log_data = {
            "variant": variant.value,
            "user_id": user_id,
            "prediction": prediction,
            "actual_result": actual_result,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # è®°å½•åˆ°æ•°æ®åº“æˆ–åˆ†æç³»ç»Ÿ
        await self._store_ab_test_result(log_data)

# src/api/predictions/router.py
ab_manager = ABTestManager(treatment_ratio=0.1)  # 10% ç”¨æˆ·ä½¿ç”¨æ–°æ¨¡å‹

@router.post("/predictions")
async def create_prediction(match_data: MatchData, user_id: str = Depends(get_user_id)):
    """åˆ›å»ºé¢„æµ‹ï¼ˆæ”¯æŒ A/B æµ‹è¯•ï¼‰"""
    
    # é€‰æ‹©æ¨¡å‹
    variant, model = ab_manager.get_model_for_request(user_id)
    
    # æ‰§è¡Œé¢„æµ‹
    prediction = model.predict(match_data)
    
    # è®°å½•ç»“æœ
    await ab_manager.log_prediction(variant, user_id, prediction)
    
    return {
        "prediction": prediction,
        "model_variant": variant.value  # ä»…ä¾›è°ƒè¯•ï¼Œç”Ÿäº§ç¯å¢ƒå¯éšè—
    }
```

**3. æ¨¡å‹æ€§èƒ½ç›‘æ§**

```python
# src/monitoring/model_monitor.py
from dataclasses import dataclass
from datetime import datetime, timedelta

@dataclass
class ModelPerformanceMetrics:
    accuracy: float
    precision: float
    recall: float
    f1_score: float
    prediction_count: int
    avg_confidence: float
    timestamp: datetime

class ModelMonitor:
    def __init__(self):
        self.metrics_window = timedelta(hours=24)  # 24å°æ—¶çª—å£
    
    async def check_model_health(self) -> dict:
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶æ€"""
        
        # 1. è·å–æœ€è¿‘24å°æ—¶çš„é¢„æµ‹ç»“æœ
        recent_predictions = await self._get_recent_predictions()
        
        # 2. è®¡ç®—å‡†ç¡®ç‡ï¼ˆä»…ç»Ÿè®¡å·²æœ‰å®é™…ç»“æœçš„é¢„æµ‹ï¼‰
        metrics = self._calculate_metrics(recent_predictions)
        
        # 3. æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è®­ç»ƒ
        needs_retraining = self._check_retraining_needed(metrics)
        
        # 4. æ£€æŸ¥æ•°æ®æ¼‚ç§»
        data_drift = await self._detect_data_drift()
        
        return {
            "metrics": metrics,
            "needs_retraining": needs_retraining,
            "data_drift_detected": data_drift,
            "health_status": self._determine_health_status(metrics, data_drift)
        }
    
    def _check_retraining_needed(self, metrics: ModelPerformanceMetrics) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦é‡è®­ç»ƒ"""
        # å‡†ç¡®ç‡ä¸‹é™è¶…è¿‡5%
        if metrics.accuracy < 0.70:
            return True
        
        # F1 åˆ†æ•°ä¸‹é™
        if metrics.f1_score < 0.65:
            return True
        
        # é¢„æµ‹ç½®ä¿¡åº¦ä¸‹é™
        if metrics.avg_confidence < 0.60:
            return True
        
        return False
    
    async def _detect_data_drift(self) -> bool:
        """æ£€æµ‹æ•°æ®æ¼‚ç§»"""
        # æ¯”è¾ƒæœ€è¿‘æ•°æ®åˆ†å¸ƒä¸è®­ç»ƒæ•°æ®åˆ†å¸ƒ
        recent_data = await self._get_recent_feature_data()
        training_data_stats = await self._get_training_data_stats()
        
        # ä½¿ç”¨ Kolmogorov-Smirnov æ£€éªŒ
        from scipy.stats import ks_2samp
        
        drift_detected = False
        for feature in recent_data.columns:
            statistic, p_value = ks_2samp(
                recent_data[feature],
                training_data_stats[feature]
            )
            if p_value < 0.05:  # æ˜¾è‘—æ€§æ°´å¹³
                drift_detected = True
                logger.warning(f"Data drift detected in feature: {feature}")
        
        return drift_detected
```

**4. è‡ªåŠ¨é‡è®­ç»ƒæµç¨‹**

```python
# src/ml/retraining_pipeline.py
from celery import Celery
from celery.schedules import crontab

celery_app = Celery('ml_tasks', broker=os.getenv('CELERY_BROKER_URL'))

@celery_app.task
def check_and_retrain_model():
    """å®šæœŸæ£€æŸ¥å¹¶è§¦å‘æ¨¡å‹é‡è®­ç»ƒ"""
    
    monitor = ModelMonitor()
    health_check = await monitor.check_model_health()
    
    if health_check['needs_retraining']:
        logger.info("Model retraining triggered due to performance degradation")
        
        # è§¦å‘é‡è®­ç»ƒä»»åŠ¡
        retrain_task.delay()
        
        # å‘é€å‘Šè­¦é€šçŸ¥
        await send_alert(
            "Model Retraining Triggered",
            f"Model performance: accuracy={health_check['metrics'].accuracy:.2%}",
            severity="warning"
        )

@celery_app.task
def retrain_model():
    """æ‰§è¡Œæ¨¡å‹é‡è®­ç»ƒ"""
    
    # 1. è·å–æœ€æ–°è®­ç»ƒæ•°æ®
    training_data = fetch_training_data(days=90)
    
    # 2. æ•°æ®é¢„å¤„ç†
    X_train, X_test, y_train, y_test = prepare_training_data(training_data)
    
    # 3. è®­ç»ƒæ–°æ¨¡å‹
    model = train_model(X_train, y_train)
    
    # 4. è¯„ä¼°æ¨¡å‹
    metrics = evaluate_model(model, X_test, y_test)
    
    # 5. å¦‚æœæ–°æ¨¡å‹æ›´å¥½ï¼Œæ³¨å†Œåˆ° MLflow
    if metrics['accuracy'] > current_model_accuracy:
        registry = ModelRegistry()
        run_id = registry.register_model(model, "football_prediction", metrics)
        
        logger.info(f"New model registered with run_id: {run_id}")
        logger.info(f"Accuracy improvement: {metrics['accuracy'] - current_model_accuracy:.2%}")
        
        # 6. è‡ªåŠ¨æå‡åˆ°Stagingç¯å¢ƒè¿›è¡ŒA/Bæµ‹è¯•
        registry.promote_to_staging("football_prediction", run_id)
    else:
        logger.info("New model not better than current, skipping registration")

# Celery Beat å®šæ—¶ä»»åŠ¡é…ç½®
celery_app.conf.beat_schedule = {
    'check-model-health-daily': {
        'task': 'check_and_retrain_model',
        'schedule': crontab(hour=2, minute=0),  # æ¯å¤©å‡Œæ™¨2ç‚¹
    },
}
```

---

### 2.4 ä¾èµ–ç®¡ç†ä¸ä¾›åº”é“¾å®‰å…¨

**å½“å‰é—®é¢˜**:

1. **ä¾èµ–ç‰ˆæœ¬æœªå®Œå…¨é”å®š**
2. **ç¼ºå°‘å®šæœŸçš„æ¼æ´æ‰«æ**
3. **ç¼ºå°‘ä¾èµ–è®¸å¯è¯æ£€æŸ¥**

**è§£å†³æ–¹æ¡ˆ**:

**1. å®Œå–„ä¾èµ–é”å®š**

```bash
# å½“å‰ requirements/requirements.lock å·²ç»é”å®šç‰ˆæœ¬ï¼Œä½†éœ€è¦ç¡®ä¿æ‰€æœ‰ä¾èµ–çš„å­ä¾èµ–ä¹Ÿé”å®š

# ä½¿ç”¨ pip-tools ç¡®ä¿å®Œæ•´é”å®š
pip install pip-tools

# ä» requirements.in ç”Ÿæˆå®Œæ•´çš„ lock æ–‡ä»¶
pip-compile requirements/requirements.in --generate-hashes --output-file=requirements/requirements.lock

# --generate-hashes: ç”ŸæˆåŒ…å“ˆå¸Œå€¼ï¼Œé˜²æ­¢åŒ…è¢«ç¯¡æ”¹
```

**2. æ·»åŠ æ¼æ´æ‰«æåˆ° CI**

```yaml
# .github/workflows/security.yml
name: Security Scan

on:
  push:
    branches: [main, develop]
  pull_request:
  schedule:
    - cron: '0 2 * * *'  # æ¯å¤©å‡Œæ™¨2ç‚¹

jobs:
  dependency-scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      # ä½¿ç”¨ pip-audit æ‰«ææ¼æ´
      - name: Install pip-audit
        run: pip install pip-audit
      
      - name: Audit dependencies
        run: |
          pip-audit -r requirements/requirements.lock --desc --format json --output audit-report.json
          pip-audit -r requirements/requirements.lock --desc
      
      # ä½¿ç”¨ Safety æ‰«æï¼ˆå¤‡é€‰ï¼‰
      - name: Safety check
        run: |
          pip install safety
          safety check --json --output safety-report.json || true
      
      # ä¸Šä¼ æ‰«ææŠ¥å‘Š
      - name: Upload scan reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            audit-report.json
            safety-report.json
      
      # å¦‚æœå‘ç°é«˜å±æ¼æ´ï¼Œä»»åŠ¡å¤±è´¥
      - name: Check for critical vulnerabilities
        run: |
          critical_count=$(jq '[.vulnerabilities[] | select(.severity == "high" or .severity == "critical")] | length' audit-report.json)
          if [ "$critical_count" -gt 0 ]; then
            echo "âŒ Found $critical_count critical/high vulnerabilities"
            exit 1
          fi
          echo "âœ… No critical vulnerabilities found"
```

**3. è®¸å¯è¯æ£€æŸ¥**

```bash
# Makefile æ·»åŠ 
license-audit: ## Security: Audit open source licenses
 @$(ACTIVATE) && \
 pip install pip-licenses && \
 pip-licenses --format=markdown --output-file=docs/LICENSES.md && \
 echo "âœ… License audit completed: docs/LICENSES.md"
 
 # æ£€æŸ¥æ˜¯å¦æœ‰ç¦æ­¢çš„è®¸å¯è¯
 @$(ACTIVATE) && \
 pip-licenses --format=json | python3 -c "\
 import sys, json; \
 data = json.load(sys.stdin); \
 forbidden = ['GPL-3.0', 'AGPL-3.0']; \
 violations = [pkg for pkg in data if pkg.get('License') in forbidden]; \
 if violations: \
     print('âŒ Forbidden licenses found:'); \
     [print(f\"  - {pkg['Name']}: {pkg['License']}\") for pkg in violations]; \
     sys.exit(1); \
 else: \
     print('âœ… No license violations found')"
```

---

### 2.5 é…ç½®ç®¡ç†ä¸ç¯å¢ƒéš”ç¦»

**å½“å‰é—®é¢˜**:

1. **`.env` æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•ï¼ˆå®‰å…¨é£é™©ï¼‰**
2. **ç¼ºå°‘ä¸åŒç¯å¢ƒçš„é…ç½®éªŒè¯**
3. **ç¼ºå°‘æ•æ„Ÿé…ç½®çš„åŠ å¯†å­˜å‚¨**

**è§£å†³æ–¹æ¡ˆ**:

**1. ç¯å¢ƒé…ç½®åˆ†ç¦»**

```python
# src/config/settings.py
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Literal

class BaseConfig(BaseSettings):
    """åŸºç¡€é…ç½®ï¼ˆæ‰€æœ‰ç¯å¢ƒå…±äº«ï¼‰"""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore"
    )
    
    # åº”ç”¨é…ç½®
    app_name: str = "FootballPrediction"
    app_version: str = "1.0.0"
    environment: Literal["development", "staging", "production"] = "development"
    
    # API é…ç½®
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    api_workers: int = 4
    
    # æ•°æ®åº“é…ç½®
    database_url: str
    db_pool_size: int = 20
    db_max_overflow: int = 10
    
    # Redis é…ç½®
    redis_url: str
    redis_max_connections: int = 50
    
    # æ—¥å¿—é…ç½®
    log_level: str = "INFO"
    log_format: Literal["json", "text"] = "json"

class DevelopmentConfig(BaseConfig):
    """å¼€å‘ç¯å¢ƒé…ç½®"""
    environment: Literal["development"] = "development"
    debug: bool = True
    log_level: str = "DEBUG"
    api_reload: bool = True
    
    # å¼€å‘ç¯å¢ƒå¯ä»¥ç»‘å®šæ‰€æœ‰æ¥å£
    api_host: str = "0.0.0.0"

class StagingConfig(BaseConfig):
    """é¢„å‘å¸ƒç¯å¢ƒé…ç½®"""
    environment: Literal["staging"] = "staging"
    debug: bool = False
    log_level: str = "INFO"
    
    # Staging ç¯å¢ƒå¯ç”¨ç›‘æ§
    sentry_dsn: str | None = None
    enable_profiling: bool = True

class ProductionConfig(BaseConfig):
    """ç”Ÿäº§ç¯å¢ƒé…ç½®"""
    environment: Literal["production"] = "production"
    debug: bool = False
    log_level: str = "WARNING"
    
    # ç”Ÿäº§ç¯å¢ƒå®‰å…¨é…ç½®
    api_host: str = "127.0.0.1"  # ä»…ç›‘å¬æœ¬åœ°ï¼ˆé€šè¿‡åå‘ä»£ç†æš´éœ²ï¼‰
    
    # å¿…é¡»é…ç½® Sentry
    sentry_dsn: str
    
    # ç”Ÿäº§ç¯å¢ƒæ€§èƒ½ä¼˜åŒ–
    db_pool_size: int = 50
    db_max_overflow: int = 20
    redis_max_connections: int = 100
    
    # ç”Ÿäº§ç¯å¢ƒå¿…é¡»å¯ç”¨ SSL
    db_ssl_mode: str = "require"

# é…ç½®å·¥å‚
def get_settings() -> BaseConfig:
    """æ ¹æ®ç¯å¢ƒå˜é‡è¿”å›å¯¹åº”é…ç½®"""
    env = os.getenv("ENVIRONMENT", "development").lower()
    
    if env == "production":
        return ProductionConfig()
    elif env == "staging":
        return StagingConfig()
    else:
        return DevelopmentConfig()

# å…¨å±€é…ç½®å®ä¾‹
settings = get_settings()
```

**2. é…ç½®éªŒè¯**

```python
# src/config/validation.py
from typing import List

class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    @staticmethod
    def validate_production_config(config: ProductionConfig) -> List[str]:
        """éªŒè¯ç”Ÿäº§ç¯å¢ƒé…ç½®"""
        errors = []
        
        # æ£€æŸ¥å¿…éœ€çš„å®‰å…¨é…ç½®
        if not config.sentry_dsn:
            errors.append("Production environment requires Sentry DSN")
        
        if config.debug:
            errors.append("Debug mode must be disabled in production")
        
        if config.api_host == "0.0.0.0":
            errors.append("Production API should not bind to 0.0.0.0 (use 127.0.0.1 with reverse proxy)")
        
        # æ£€æŸ¥æ•°æ®åº“ SSL
        if "sslmode=require" not in config.database_url and "ssl=require" not in config.database_url:
            errors.append("Production database must use SSL connection")
        
        # æ£€æŸ¥å¯†é’¥å¼ºåº¦
        if len(config.secret_key) < 64:
            errors.append("SECRET_KEY must be at least 64 characters in production")
        
        return errors
    
    @staticmethod
    def validate_and_exit():
        """éªŒè¯é…ç½®ï¼Œå¦‚æœæœ‰é”™è¯¯åˆ™é€€å‡º"""
        config = get_settings()
        
        if isinstance(config, ProductionConfig):
            errors = ConfigValidator.validate_production_config(config)
            
            if errors:
                print("âŒ Production configuration errors:")
                for error in errors:
                    print(f"  - {error}")
                sys.exit(1)
            
            print("âœ… Production configuration validated")

# åœ¨åº”ç”¨å¯åŠ¨æ—¶éªŒè¯
# src/main.py
if __name__ == "__main__":
    from src.config.validation import ConfigValidator
    ConfigValidator.validate_and_exit()
    
    # ... å¯åŠ¨åº”ç”¨
```

**3. æ•æ„Ÿé…ç½®åŠ å¯†å­˜å‚¨ï¼ˆå¯é€‰ï¼Œé«˜å®‰å…¨åœºæ™¯ï¼‰**

```python
# src/config/encryption.py
from cryptography.fernet import Fernet
import base64
import os

class ConfigEncryption:
    """é…ç½®åŠ å¯†/è§£å¯†å·¥å…·"""
    
    @staticmethod
    def generate_key() -> bytes:
        """ç”ŸæˆåŠ å¯†å¯†é’¥"""
        return Fernet.generate_key()
    
    @staticmethod
    def encrypt_value(value: str, key: bytes) -> str:
        """åŠ å¯†é…ç½®å€¼"""
        f = Fernet(key)
        encrypted = f.encrypt(value.encode())
        return base64.urlsafe_b64encode(encrypted).decode()
    
    @staticmethod
    def decrypt_value(encrypted_value: str, key: bytes) -> str:
        """è§£å¯†é…ç½®å€¼"""
        f = Fernet(key)
        decoded = base64.urlsafe_b64decode(encrypted_value.encode())
        return f.decrypt(decoded).decode()

# ä½¿ç”¨ç¤ºä¾‹
# 1. ç”Ÿæˆä¸»å¯†é’¥ï¼ˆä»…ä¸€æ¬¡ï¼Œä¿å­˜åˆ°å®‰å…¨ä½ç½®ï¼‰
# python -c "from src.config.encryption import ConfigEncryption; print(ConfigEncryption.generate_key())"

# 2. åŠ å¯†æ•æ„Ÿé…ç½®
# python -c "from src.config.encryption import ConfigEncryption; key=b'your-key'; print(ConfigEncryption.encrypt_value('your-secret', key))"

# 3. åœ¨åº”ç”¨ä¸­è§£å¯†
# settings.py
class ProductionConfig(BaseConfig):
    _encryption_key: bytes = Field(default=None, env="CONFIG_ENCRYPTION_KEY")
    
    @property
    def database_password(self) -> str:
        encrypted = os.getenv("DB_PASSWORD_ENCRYPTED")
        return ConfigEncryption.decrypt_value(encrypted, self._encryption_key)
```

---

## ğŸŸ¢ ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸€èˆ¬æ€§ä¼˜åŒ–ï¼ˆä¸­ä¼˜å…ˆçº§ï¼Œå»ºè®®æ”¹è¿›ï¼‰

### 3.1 æ–‡æ¡£å®Œå–„

**å»ºè®®æ·»åŠ çš„æ–‡æ¡£**:

1. **API æ–‡æ¡£å¢å¼º**

   ```markdown
   # docs/api/README.md
   
   ## API ç‰ˆæœ¬ç®¡ç†
   - v1: å½“å‰ç¨³å®šç‰ˆæœ¬
   - v2: å¼€å‘ä¸­ï¼ˆé¢„è®¡ 2025-Q2ï¼‰
   
   ## é€Ÿç‡é™åˆ¶
   - é»˜è®¤: 100 è¯·æ±‚/åˆ†é’Ÿ
   - æ‰¹é‡é¢„æµ‹: 10 è¯·æ±‚/åˆ†é’Ÿ
   
   ## è®¤è¯æ–¹å¼
   - JWT Token (Bearer)
   - API Key (Header: X-API-Key)
   
   ## é”™è¯¯ç è¯´æ˜
   | çŠ¶æ€ç  | å«ä¹‰ | å¤„ç†å»ºè®® |
   |-------|------|---------|
   | 400 | è¯·æ±‚å‚æ•°é”™è¯¯ | æ£€æŸ¥è¯·æ±‚æ ¼å¼ |
   | 401 | æœªè®¤è¯ | æ£€æŸ¥ Token |
   | 429 | é€Ÿç‡é™åˆ¶ | ç¨åé‡è¯• |
   | 500 | æœåŠ¡å™¨é”™è¯¯ | è”ç³»æ”¯æŒ |
   ```

2. **éƒ¨ç½²æ–‡æ¡£**

   ```markdown
   # docs/deployment/PRODUCTION_CHECKLIST.md
   
   ## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ£€æŸ¥æ¸…å•
   
   ### éƒ¨ç½²å‰ (Pre-deployment)
   - [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ˆè¦†ç›–ç‡ â‰¥80%ï¼‰
   - [ ] å®‰å…¨æ‰«ææ— é«˜å±æ¼æ´
   - [ ] é…ç½®æ–‡ä»¶å·²éªŒè¯
   - [ ] æ•°æ®åº“è¿ç§»å·²æµ‹è¯•
   - [ ] ç›‘æ§å‘Šè­¦å·²é…ç½®
   - [ ] å¤‡ä»½ç­–ç•¥å·²éªŒè¯
   
   ### éƒ¨ç½²ä¸­ (During deployment)
   - [ ] ç°åº¦å‘å¸ƒï¼ˆ10% -> 50% -> 100%ï¼‰
   - [ ] å®æ—¶ç›‘æ§æ—¥å¿—
   - [ ] æ€§èƒ½æŒ‡æ ‡æ­£å¸¸
   - [ ] é”™è¯¯ç‡ < 0.1%
   
   ### éƒ¨ç½²å (Post-deployment)
   - [ ] å†’çƒŸæµ‹è¯•é€šè¿‡
   - [ ] å…³é”®è·¯å¾„éªŒè¯
   - [ ] ç›‘æ§ä»ªè¡¨æ¿éªŒè¯
   - [ ] å›æ»šé¢„æ¡ˆå°±ç»ª
   ```

3. **è¿ç»´æ‰‹å†Œ**

   ```markdown
   # docs/operations/RUNBOOK.md
   
   ## å¸¸è§é—®é¢˜å¤„ç†
   
   ### æ•°æ®åº“è¿æ¥æ± è€—å°½
   **ç—‡çŠ¶**: 503 é”™è¯¯ï¼Œæ—¥å¿—æ˜¾ç¤º "connection pool exhausted"
   **å¤„ç†**:
   1. æ£€æŸ¥æ…¢æŸ¥è¯¢: `SELECT * FROM pg_stat_activity WHERE state = 'active' AND query_start < now() - interval '1 minute';`
   2. ä¸´æ—¶å¢åŠ è¿æ¥æ± : `kubectl set env deployment/app DB_POOL_SIZE=50`
   3. åˆ†ææ ¹æœ¬åŸå› ï¼Œä¼˜åŒ–æŸ¥è¯¢
   
   ### Redis è¿æ¥å¤±è´¥
   **ç—‡çŠ¶**: ç¼“å­˜å¤±æ•ˆï¼Œå“åº”å˜æ…¢
   **å¤„ç†**:
   1. æ£€æŸ¥ Redis æœåŠ¡çŠ¶æ€: `kubectl get pods -l app=redis`
   2. é‡å¯ Redis: `kubectl rollout restart deployment/redis`
   3. æ¸…ç†è¿‡æœŸé”®: `redis-cli FLUSHDB`
   
   ### é¢„æµ‹å»¶è¿Ÿå¢åŠ 
   **ç—‡çŠ¶**: P95 å»¶è¿Ÿ > 2ç§’
   **å¤„ç†**:
   1. æ£€æŸ¥æ¨¡å‹æ€§èƒ½: `curl http://localhost:8000/metrics | grep prediction_duration`
   2. æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å®¹: `kubectl get hpa`
   3. å¯ç”¨æ¨¡å‹ç¼“å­˜: è®¾ç½® `ENABLE_MODEL_CACHE=true`
   ```

---

### 3.2 ä»£ç è´¨é‡æå‡

**å»ºè®®æ”¹è¿›**:

1. **æ·»åŠ ä»£ç å¤æ‚åº¦æ£€æŸ¥**

   ```bash
   # Makefile æ·»åŠ 
   complexity-check: ## Quality: Check code complexity
    @$(ACTIVATE) && \
    pip install radon && \
    radon cc src/ -a -nb && \
    radon mi src/ -nb
   ```

2. **æ·»åŠ ä»£ç è¦†ç›–ç‡å·®å¼‚æ£€æŸ¥**

   ```yaml
   # .github/workflows/pr-check.yml
   - name: Coverage diff
     run: |
       # æ£€æŸ¥ PR æ˜¯å¦é™ä½äº†è¦†ç›–ç‡
       coverage_before=$(git show main:coverage.xml | python3 -c "import sys, xml.etree.ElementTree as ET; tree = ET.parse(sys.stdin); print(float(tree.getroot().attrib['line-rate']))")
       coverage_after=$(python3 -c "import xml.etree.ElementTree as ET; tree = ET.parse('coverage.xml'); print(float(tree.getroot().attrib['line-rate']))")
       
       if (( $(echo "$coverage_after < $coverage_before - 0.01" | bc -l) )); then
         echo "âŒ Coverage decreased from ${coverage_before} to ${coverage_after}"
         exit 1
       fi
       echo "âœ… Coverage maintained or improved"
   ```

3. **æ·»åŠ å¯¼å…¥æ’åºæ£€æŸ¥**

   ```toml
   # pyproject.toml
   [tool.isort]
   profile = "black"
   line_length = 88
   known_first_party = ["src"]
   known_third_party = ["fastapi", "sqlalchemy", "pydantic"]
   sections = ["FUTURE", "STDLIB", "THIRDPARTY", "FIRSTPARTY", "LOCALFOLDER"]
   ```

---

### 3.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

**å»ºè®®æ·»åŠ **:

```python
# tests/performance/benchmarks.py
import pytest
from locust import HttpUser, task, between

class PredictionUser(HttpUser):
    """é¢„æµ‹ API æ€§èƒ½æµ‹è¯•"""
    wait_time = between(1, 3)
    
    @task
    def get_prediction(self):
        self.client.get("/api/v1/predictions/123", 
                       headers={"Authorization": f"Bearer {self.token}"})
    
    @task(2)
    def create_prediction(self):
        self.client.post("/api/v1/predictions", 
                        json={"match_id": 123, "features": {...}},
                        headers={"Authorization": f"Bearer {self.token}"})

# è¿è¡Œæ€§èƒ½æµ‹è¯•
# locust -f tests/performance/benchmarks.py --host=http://localhost:8000 --users=100 --spawn-rate=10
```

**æ€§èƒ½ç›®æ ‡**:

| æŒ‡æ ‡ | ç›®æ ‡å€¼ |
|------|--------|
| P50 å»¶è¿Ÿ | < 100ms |
| P95 å»¶è¿Ÿ | < 500ms |
| P99 å»¶è¿Ÿ | < 1000ms |
| ååé‡ | > 1000 req/s |
| é”™è¯¯ç‡ | < 0.1% |

---

## ğŸ“Š ç¬¬å››éƒ¨åˆ†ï¼šä¼˜åŒ–ä¼˜å…ˆçº§ä¸æ—¶é—´è¡¨

### å…³é”®è·¯å¾„ï¼ˆ2-3å‘¨ï¼‰

```mermaid
gantt
    title ä¸Šçº¿éƒ¨ç½²ä¼˜åŒ–æ—¶é—´è¡¨
    dateFormat  YYYY-MM-DD
    section å…³é”®é—®é¢˜
    æµ‹è¯•è¦†ç›–ç‡æå‡          :crit, a1, 2025-10-11, 14d
    å®‰å…¨é…ç½®ä¿®å¤            :crit, a2, 2025-10-11, 2d
    ç›‘æ§å‘Šè­¦å®Œå–„            :crit, a3, 2025-10-13, 3d
    READMEä¿®å¤             :crit, a4, 2025-10-11, 1d
    
    section é‡è¦ä¼˜åŒ–
    æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–          :b1, 2025-10-16, 3d
    APIæ€§èƒ½ä¼˜åŒ–            :b2, 2025-10-18, 2d
    MLæ¨¡å‹ä¼˜åŒ–             :b3, 2025-10-20, 5d
    ä¾èµ–ç®¡ç†               :b4, 2025-10-17, 2d
    
    section ä¸€èˆ¬ä¼˜åŒ–
    æ–‡æ¡£å®Œå–„               :c1, 2025-10-23, 3d
    ä»£ç è´¨é‡æå‡           :c2, 2025-10-25, 2d
    æ€§èƒ½æµ‹è¯•               :c3, 2025-10-26, 2d
    
    section éªŒæ”¶æµ‹è¯•
    é›†æˆæµ‹è¯•               :milestone, m1, 2025-10-28, 2d
    ç”Ÿäº§ç¯å¢ƒé¢„æ¼”           :milestone, m2, 2025-10-30, 1d
    ä¸Šçº¿                   :milestone, m3, 2025-10-31, 1d
```

### ç¬¬1å‘¨ï¼ˆ2025-10-11 è‡³ 2025-10-17ï¼‰

**Day 1-2: ç´§æ€¥å®‰å…¨é—®é¢˜ä¿®å¤**

- [ ] æ£€æŸ¥å¹¶æ¸…ç† `.env` æ–‡ä»¶çš„ Git å†å²
- [ ] æ›´æ–° `.gitignore`
- [ ] ç”Ÿæˆæ–°çš„ç”Ÿäº§ç¯å¢ƒå¯†é’¥
- [ ] ä¿®å¤ README ä¸­çš„è¦†ç›–ç‡ä¸ä¸€è‡´é—®é¢˜
- [ ] å®¡æŸ¥æ‰€æœ‰ç¡¬ç¼–ç çš„æ•æ„Ÿä¿¡æ¯

**Day 3-5: ç›‘æ§å‘Šè­¦ç³»ç»Ÿ**

- [ ] é›†æˆ Sentry é”™è¯¯è¿½è¸ª
- [ ] é…ç½® Prometheus å‘Šè­¦è§„åˆ™
- [ ] é…ç½® Loki æ—¥å¿—èšåˆ
- [ ] å®æ–½ç»“æ„åŒ–æ—¥å¿—
- [ ] å¢å¼ºå¥åº·æ£€æŸ¥ç«¯ç‚¹

**Day 6-7: å¼€å§‹æµ‹è¯•è¦†ç›–ç‡æå‡**

- [ ] Phase 1: domain/models/* æµ‹è¯•ï¼ˆç›®æ ‡ 80%ï¼‰
- [ ] è¡¥å…… league.py, team.py æµ‹è¯•
- [ ] è¡¥å…… prediction.py, match.py æµ‹è¯•

### ç¬¬2å‘¨ï¼ˆ2025-10-18 è‡³ 2025-10-24ï¼‰

**Day 8-10: ç»§ç»­æµ‹è¯•è¦†ç›–ç‡æå‡**

- [ ] Phase 2: domain/services/* æµ‹è¯•ï¼ˆç›®æ ‡ 75%ï¼‰
- [ ] Phase 2: features/* æµ‹è¯•ï¼ˆç›®æ ‡ 80%ï¼‰
- [ ] Phase 2: collectors/* æµ‹è¯•ï¼ˆç›®æ ‡ 70%ï¼‰

**Day 11-12: æ•°æ®åº“ä¸APIæ€§èƒ½ä¼˜åŒ–**

- [ ] æ•°æ®åº“è¿æ¥æ± ä¼˜åŒ–
- [ ] æ…¢æŸ¥è¯¢åˆ†æä¸ç´¢å¼•ä¼˜åŒ–
- [ ] å¯ç”¨å“åº”å‹ç¼©
- [ ] å®æ–½è¯·æ±‚è¶…æ—¶æ§åˆ¶
- [ ] æ·»åŠ è¯·æ±‚çº§ç¼“å­˜

**Day 13-14: ä¾èµ–ç®¡ç†**

- [ ] é”å®šæ‰€æœ‰ä¾èµ–ç‰ˆæœ¬ï¼ˆåŒ…å«å“ˆå¸Œï¼‰
- [ ] é…ç½®è‡ªåŠ¨æ¼æ´æ‰«æ
- [ ] è®¸å¯è¯å®¡è®¡
- [ ] æ›´æ–° CI æµç¨‹

### ç¬¬3å‘¨ï¼ˆ2025-10-25 è‡³ 2025-10-31ï¼‰

**Day 15-17: MLæ¨¡å‹ä¼˜åŒ–**

- [ ] å®æ–½ MLflow æ¨¡å‹æ³¨å†Œ
- [ ] é…ç½® A/B æµ‹è¯•æ¡†æ¶
- [ ] å®æ–½æ¨¡å‹æ€§èƒ½ç›‘æ§
- [ ] é…ç½®è‡ªåŠ¨é‡è®­ç»ƒæµç¨‹

**Day 18-19: æ–‡æ¡£ä¸ä»£ç è´¨é‡**

- [ ] å®Œå–„ API æ–‡æ¡£
- [ ] ç¼–å†™éƒ¨ç½²æ£€æŸ¥æ¸…å•
- [ ] ç¼–å†™è¿ç»´æ‰‹å†Œ
- [ ] ä»£ç å¤æ‚åº¦æ£€æŸ¥
- [ ] å¯¼å…¥æ’åºä¼˜åŒ–

**Day 20-21: é›†æˆæµ‹è¯•ä¸éªŒæ”¶**

- [ ] è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•
- [ ] æœ€ç»ˆå®‰å…¨æ‰«æ

---

## ğŸ¯ ç¬¬äº”éƒ¨åˆ†ï¼šéªŒæ”¶æ ‡å‡†

### ä¸Šçº¿å‰å¿…é¡»è¾¾æˆçš„æ ‡å‡†

#### 1. è´¨é‡æ ‡å‡†

- [x] **æµ‹è¯•è¦†ç›–ç‡**: â‰¥ 80%
  - [ ] domain/ æ¨¡å—: â‰¥ 80%
  - [ ] api/ æ¨¡å—: â‰¥ 90%
  - [ ] services/ æ¨¡å—: â‰¥ 70%
  - [ ] æ ¸å¿ƒä¸šåŠ¡é€»è¾‘: 100%

- [x] **ä»£ç è´¨é‡**
  - [ ] Ruff lint: 0 é”™è¯¯
  - [ ] MyPy: 0 ç±»å‹é”™è¯¯ï¼ˆä¸¥æ ¼æ¨¡å¼ï¼‰
  - [ ] ä»£ç å¤æ‚åº¦: å¹³å‡ < 10

#### 2. å®‰å…¨æ ‡å‡†

- [x] **æ•æ„Ÿä¿¡æ¯**
  - [ ] `.env` æ–‡ä»¶å·²ä» Git å†å²åˆ é™¤
  - [ ] æ— ç¡¬ç¼–ç å¯†é’¥
  - [ ] ç”Ÿäº§å¯†é’¥å·²è½®æ¢
  - [ ] ä½¿ç”¨äº‘æœåŠ¡å¯†é’¥ç®¡ç†

- [x] **æ¼æ´æ‰«æ**
  - [ ] æ— é«˜å±æ¼æ´
  - [ ] æ— ä¸­å±æ¼æ´ï¼ˆæˆ–å·²è¯„ä¼°é£é™©å¯æ¥å—ï¼‰
  - [ ] è®¸å¯è¯æ— è¿è§„

#### 3. æ€§èƒ½æ ‡å‡†

- [x] **API æ€§èƒ½**
  - [ ] P50 å»¶è¿Ÿ < 100ms
  - [ ] P95 å»¶è¿Ÿ < 500ms
  - [ ] P99 å»¶è¿Ÿ < 1000ms
  - [ ] ååé‡ > 500 req/sï¼ˆå•å®ä¾‹ï¼‰

- [x] **æ•°æ®åº“æ€§èƒ½**
  - [ ] è¿æ¥æ± é…ç½®ä¼˜åŒ–
  - [ ] æ…¢æŸ¥è¯¢ < 5 ä¸ªï¼ˆ> 1ç§’ï¼‰
  - [ ] ç´¢å¼•ä¼˜åŒ–å®Œæˆ

#### 4. ç›‘æ§æ ‡å‡†

- [x] **é”™è¯¯è¿½è¸ª**
  - [ ] Sentry å·²é…ç½®
  - [ ] é”™è¯¯å‘Šè­¦å·²æµ‹è¯•
  - [ ] é”™è¯¯ç‡ < 0.1%

- [x] **æŒ‡æ ‡ç›‘æ§**
  - [ ] Prometheus æŒ‡æ ‡å®Œæ•´
  - [ ] å‘Šè­¦è§„åˆ™å·²é…ç½®
  - [ ] Grafana ä»ªè¡¨æ¿å°±ç»ª

- [x] **æ—¥å¿—èšåˆ**
  - [ ] ç»“æ„åŒ–æ—¥å¿—å·²å®æ–½
  - [ ] Loki æ—¥å¿—æ”¶é›†æ­£å¸¸
  - [ ] æ—¥å¿—ä¿ç•™ç­–ç•¥å·²é…ç½®

#### 5. æ–‡æ¡£æ ‡å‡†

- [x] **æŠ€æœ¯æ–‡æ¡£**
  - [ ] API æ–‡æ¡£å®Œæ•´
  - [ ] éƒ¨ç½²æ–‡æ¡£å®Œæ•´
  - [ ] è¿ç»´æ‰‹å†Œå®Œæ•´
  - [ ] æ¶æ„å›¾æ›´æ–°

- [x] **è¿ç»´æ–‡æ¡£**
  - [ ] éƒ¨ç½²æ£€æŸ¥æ¸…å•
  - [ ] å›æ»šé¢„æ¡ˆ
  - [ ] æ•…éšœå¤„ç†æ‰‹å†Œ
  - [ ] ç›‘æ§æŒ‡æ ‡è¯´æ˜

---

## ğŸ“ˆ ç¬¬å…­éƒ¨åˆ†ï¼šä¸Šçº¿åæŒç»­ä¼˜åŒ–å»ºè®®

### çŸ­æœŸï¼ˆä¸Šçº¿å1ä¸ªæœˆï¼‰

1. **ç›‘æ§ä¸å‘Šè­¦è°ƒä¼˜**
   - æ ¹æ®å®é™…æµé‡è°ƒæ•´å‘Šè­¦é˜ˆå€¼
   - ä¼˜åŒ–å‘Šè­¦è§„åˆ™ï¼Œå‡å°‘è¯¯æŠ¥
   - å»ºç«‹å€¼ç­è½®æ¢åˆ¶åº¦

2. **æ€§èƒ½ä¼˜åŒ–**
   - åˆ†æå®é™…æ€§èƒ½ç“¶é¢ˆ
   - ä¼˜åŒ–çƒ­ç‚¹ä»£ç è·¯å¾„
   - è°ƒæ•´ç¼“å­˜ç­–ç•¥

3. **ç”¨æˆ·åé¦ˆ**
   - æ”¶é›†ç”¨æˆ·åé¦ˆ
   - ä¼˜åŒ– API è®¾è®¡
   - å®Œå–„é”™è¯¯æç¤º

### ä¸­æœŸï¼ˆä¸Šçº¿å3ä¸ªæœˆï¼‰

1. **å®¹é‡è§„åˆ’**
   - åˆ†æèµ„æºä½¿ç”¨è¶‹åŠ¿
   - è§„åˆ’æ‰©å®¹æ–¹æ¡ˆ
   - æˆæœ¬ä¼˜åŒ–

2. **åŠŸèƒ½å¢å¼º**
   - A/B æµ‹è¯•åˆ†æ
   - æ¨¡å‹æ€§èƒ½ä¼˜åŒ–
   - æ–°åŠŸèƒ½å¼€å‘

3. **æ¶æ„ä¼˜åŒ–**
   - å¾®æœåŠ¡æ‹†åˆ†è¯„ä¼°
   - å¼‚æ­¥å¤„ç†ä¼˜åŒ–
   - æ¶ˆæ¯é˜Ÿåˆ—å¼•å…¥

### é•¿æœŸï¼ˆä¸Šçº¿å6ä¸ªæœˆ+ï¼‰

1. **æŠ€æœ¯å€ºåŠ¡**
   - å®šæœŸé‡æ„
   - ä¾èµ–å‡çº§
   - æŠ€æœ¯æ ˆæ¼”è¿›

2. **å›¢é˜Ÿå»ºè®¾**
   - ä»£ç è§„èŒƒåŸ¹è®­
   - æœ€ä½³å®è·µåˆ†äº«
   - æŠ€æœ¯æ–‡æ¡£ç»´æŠ¤

3. **ä¸šåŠ¡å‘å±•**
   - å¤šåŒºåŸŸéƒ¨ç½²
   - é«˜å¯ç”¨æ¶æ„
   - ç¾éš¾æ¢å¤

---

## ğŸ”— é™„å½•

### A. å…³é”®è”ç³»äºº

| è§’è‰² | èŒè´£ | è”ç³»æ–¹å¼ |
|------|------|---------|
| Tech Lead | æŠ€æœ¯å†³ç­– | - |
| DevOps | éƒ¨ç½²è¿ç»´ | - |
| QA Lead | è´¨é‡ä¿è¯ | - |
| Security | å®‰å…¨å®¡è®¡ | - |

### B. å‚è€ƒèµ„æº

- [Python æœ€ä½³å®è·µ](https://docs.python-guide.org/)
- [FastAPI ç”Ÿäº§éƒ¨ç½²](https://fastapi.tiangolo.com/deployment/)
- [PostgreSQL æ€§èƒ½ä¼˜åŒ–](https://www.postgresql.org/docs/current/performance-tips.html)
- [OWASP å®‰å…¨æŒ‡å—](https://owasp.org/www-project-top-ten/)
- [12-Factor App](https://12factor.net/)

### C. å·¥å…·æ¸…å•

| å·¥å…· | ç”¨é€” | ä¼˜å…ˆçº§ |
|------|------|--------|
| pytest | å•å…ƒæµ‹è¯• | å¿…éœ€ |
| coverage.py | è¦†ç›–ç‡æµ‹è¯• | å¿…éœ€ |
| Sentry | é”™è¯¯è¿½è¸ª | å¿…éœ€ |
| Prometheus | æŒ‡æ ‡ç›‘æ§ | å¿…éœ€ |
| Grafana | å¯è§†åŒ– | å¿…éœ€ |
| Loki | æ—¥å¿—èšåˆ | æ¨è |
| MLflow | æ¨¡å‹ç®¡ç† | æ¨è |
| Locust | æ€§èƒ½æµ‹è¯• | æ¨è |

---

## ğŸ“ æ€»ç»“

æœ¬æ¬¡è¯„ä¼°è¯†åˆ«äº†é¡¹ç›®åœ¨ä¸Šçº¿éƒ¨ç½²å‰çš„**4ä¸ªå…³é”®é—®é¢˜**ã€**5ä¸ªé‡è¦ä¼˜åŒ–ç‚¹**å’Œ**3ä¸ªä¸€èˆ¬æ€§æ”¹è¿›å»ºè®®**ã€‚

### ğŸ”´ å¿…é¡»ç«‹å³è§£å†³ï¼ˆé˜»å¡ä¸Šçº¿ï¼‰

1. âœ… **æµ‹è¯•è¦†ç›–ç‡** - å½“å‰18%ï¼Œéœ€æå‡è‡³â‰¥80%ï¼ˆé¢„è®¡2å‘¨ï¼‰
2. âœ… **å®‰å…¨é…ç½®** - æ•æ„Ÿä¿¡æ¯æš´éœ²é£é™©ï¼Œå¯†é’¥ç®¡ç†ä¸å½“ï¼ˆé¢„è®¡2å¤©ï¼‰
3. âœ… **ç›‘æ§å‘Šè­¦** - ç¼ºå°‘å®Œæ•´çš„ç”Ÿäº§ç›‘æ§ä½“ç³»ï¼ˆé¢„è®¡3å¤©ï¼‰
4. âœ… **æ–‡æ¡£ä¸ä¸€è‡´** - READMEæ•°æ®é”™è¯¯ï¼ˆé¢„è®¡1å¤©ï¼‰

### ğŸŸ¡ å¼ºçƒˆå»ºè®®å®Œæˆï¼ˆç¡®ä¿ç¨³å®šæ€§ï¼‰

1. æ•°æ®åº“æ€§èƒ½ä¼˜åŒ– - è¿æ¥æ± ã€ç´¢å¼•ã€æ…¢æŸ¥è¯¢ï¼ˆé¢„è®¡3å¤©ï¼‰
2. APIæ€§èƒ½ä¼˜åŒ– - ç¼“å­˜ã€å‹ç¼©ã€é™æµï¼ˆé¢„è®¡2å¤©ï¼‰
3. MLæ¨¡å‹ä¼˜åŒ– - ç‰ˆæœ¬ç®¡ç†ã€A/Bæµ‹è¯•ã€ç›‘æ§ï¼ˆé¢„è®¡5å¤©ï¼‰
4. ä¾èµ–ç®¡ç† - ç‰ˆæœ¬é”å®šã€æ¼æ´æ‰«æã€è®¸å¯è¯å®¡è®¡ï¼ˆé¢„è®¡2å¤©ï¼‰
5. é…ç½®ç®¡ç† - ç¯å¢ƒéš”ç¦»ã€éªŒè¯ã€åŠ å¯†ï¼ˆé¢„è®¡2å¤©ï¼‰

### ğŸŸ¢ æŒç»­æ”¹è¿›ï¼ˆä¸Šçº¿åä¼˜åŒ–ï¼‰

1. æ–‡æ¡£å®Œå–„ - APIæ–‡æ¡£ã€éƒ¨ç½²æ–‡æ¡£ã€è¿ç»´æ‰‹å†Œ
2. ä»£ç è´¨é‡ - å¤æ‚åº¦æ£€æŸ¥ã€è¦†ç›–ç‡å·®å¼‚ã€å¯¼å…¥æ’åº
3. æ€§èƒ½æµ‹è¯• - åŸºå‡†æµ‹è¯•ã€å‹åŠ›æµ‹è¯•

**é¢„è®¡æ€»æ—¶é—´**: 2-3å‘¨  
**å»ºè®®ä¸Šçº¿æ—¥æœŸ**: 2025å¹´10æœˆ31æ—¥

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025-10-11  
**ä¸‹æ¬¡å®¡æŸ¥**: ä¸Šçº¿å1å‘¨  
**æŠ¥å‘Šç‰ˆæœ¬**: v1.0
