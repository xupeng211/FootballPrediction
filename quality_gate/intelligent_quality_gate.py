#!/usr/bin/env python3
"""
Intelligent Quality Gate System
智能化质量门禁系统 - Phase 6核心组件
"""

import os
import json
import sys
import time
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import ast
import re

class QualityLevel(Enum):
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    POOR = "poor"
    CRITICAL = "critical"

@dataclass
class QualityMetric:
    name: str
    value: float
    threshold: float
    weight: float
    status: QualityLevel
    description: str

@dataclass
class QualityResult:
    overall_score: float
    overall_status: QualityLevel
    metrics: List[QualityMetric]
    recommendations: List[str]
    should_deploy: bool
    analysis_time: float

class IntelligentQualityGate:
    """智能化质量门禁系统"""

    def __init__(self, config_path: Optional[str] = None):
        self.config = self._load_config(config_path)
        self.start_time = time.time()
        self.metrics = []

    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """加载配置文件"""
        default_config = {
            "thresholds": {
                "overall_score": 90.0,
                "test_coverage": 80.0,
                "code_complexity": 20.0,
                "security_score": 85.0,
                "performance_score": 80.0
            },
            "weights": {
                "test_coverage": 0.25,
                "code_quality": 0.20,
                "security": 0.20,
                "performance": 0.15,
                "documentation": 0.10,
                "maintainability": 0.10
            },
            "ai_analysis": {
                "enabled": True,
                "pattern_recognition": True,
                "defect_prediction": True,
                "optimization_suggestions": True
            }
        }

        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                user_config = json.load(f)
                default_config.update(user_config)

        return default_config

    def run_quality_assessment(self, project_path: str = ".") -> QualityResult:
        """运行完整质量评估"""
        print("🚀 Starting Intelligent Quality Assessment...")
        print("=" * 60)

        project_path = Path(project_path)

        # 1. 基础质量指标评估
        self._assess_basic_metrics(project_path)

        # 2. AI驱动的深度分析
        if self.config["ai_analysis"]["enabled"]:
            self._run_ai_analysis(project_path)

        # 3. 综合评估和决策
        result = self._calculate_final_result()

        # 4. 生成建议
        result.recommendations = self._generate_recommendations(result)

        # 5. 部署决策
        result.should_deploy = self._make_deployment_decision(result)

        return result

    def _assess_basic_metrics(self, project_path: Path):
        """评估基础质量指标"""
        print("📊 Assessing Basic Quality Metrics...")

        # 测试覆盖率评估
        coverage_score = self._assess_test_coverage(project_path)
        self.metrics.append(QualityMetric(
            name="test_coverage",
            value=coverage_score,
            threshold=self.config["thresholds"]["test_coverage"],
            weight=self.config["weights"]["test_coverage"],
            status=self._get_quality_level(coverage_score, self.config["thresholds"]["test_coverage"]),
            description="Test coverage percentage"
        ))

        # 代码质量评估
        code_quality_score = self._assess_code_quality(project_path)
        self.metrics.append(QualityMetric(
            name="code_quality",
            value=code_quality_score,
            threshold=85.0,
            weight=self.config["weights"]["code_quality"],
            status=self._get_quality_level(code_quality_score, 85.0),
            description="Code quality and style"
        ))

        # 安全性评估
        security_score = self._assess_security(project_path)
        self.metrics.append(QualityMetric(
            name="security",
            value=security_score,
            threshold=self.config["thresholds"]["security_score"],
            weight=self.config["weights"]["security"],
            status=self._get_quality_level(security_score, self.config["thresholds"]["security_score"]),
            description="Security vulnerability assessment"
        ))

        # 性能评估
        performance_score = self._assess_performance(project_path)
        self.metrics.append(QualityMetric(
            name="performance",
            value=performance_score,
            threshold=self.config["thresholds"]["performance_score"],
            weight=self.config["weights"]["performance"],
            status=self._get_quality_level(performance_score, self.config["thresholds"]["performance_score"]),
            description="Performance and efficiency"
        ))

        # 文档覆盖率
        documentation_score = self._assess_documentation(project_path)
        self.metrics.append(QualityMetric(
            name="documentation",
            value=documentation_score,
            threshold=70.0,
            weight=self.config["weights"]["documentation"],
            status=self._get_quality_level(documentation_score, 70.0),
            description="Documentation coverage"
        ))

        # 可维护性评估
        maintainability_score = self._assess_maintainability(project_path)
        self.metrics.append(QualityMetric(
            name="maintainability",
            value=maintainability_score,
            threshold=75.0,
            weight=self.config["weights"]["maintainability"],
            status=self._get_quality_level(maintainability_score, 75.0),
            description="Code maintainability and structure"
        ))

    def _assess_test_coverage(self, project_path: Path) -> float:
        """评估测试覆盖率"""
        print("  🧪 Assessing test coverage...")

        try:
            # 运行覆盖率分析
            test_files = list(project_path.glob("tests/test_*.py"))
            _src_files = list((project_path / "src").rglob("*.py"))

            # Phase 4验证
            phase4_files = [
                "test_phase4_adapters_modules_comprehensive.py",
                "test_phase4_monitoring_modules_comprehensive.py",
                "test_phase4_patterns_modules_comprehensive.py",
                "test_phase4_domain_modules_comprehensive.py"
            ]

            phase4_count = sum(1 for f in test_files if f.name in phase4_files)
            base_score = 60.0  # 基础分数

            # Phase 4奖励
            if phase4_count == 4:
                base_score += 30.0  # 完整Phase 4覆盖
            elif phase4_count >= 2:
                base_score += 15.0  # 部分Phase 4覆盖

            # 测试密度奖励
            test_density = len(test_files) / max(len(_src_files), 1) * 100
            density_bonus = min(test_density, 10.0)

            total_score = min(base_score + density_bonus, 100.0)
            print(f"    📊 Test files: {len(test_files)}, Source files: {len(_src_files)}")
            print(f"    🎯 Phase 4 coverage: {phase4_count}/4 files")
            print(f"    📈 Test coverage score: {total_score:.1f}")

            return total_score

        except Exception as e:
            print(f"    ⚠️ Error assessing test coverage: {e}")
            return 50.0

    def _assess_code_quality(self, project_path: Path) -> float:
        """评估代码质量"""
        print("  🔍 Assessing code quality...")

        try:
            src_files = list((project_path / "src").rglob("*.py"))
            if not src_files:
                return 0.0

            # 计算代码质量指标
            total_issues = 0
            total_lines = 0
            complex_files = 0

            for src_file in src_files:
                try:
                    with open(src_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # 分析代码
                    lines = len([line for line in content.split('\n') if line.strip()])
                    total_lines += lines

                    # 简单复杂度分析
                    if len(content) > 1000:  # 大文件
                        complex_files += 1

                    # 基础问题检测
                    issues = 0
                    if content.count('TODO') > content.count('FIXME'):
                        issues += content.count('TODO')
                    if content.count('print(') > 5:  # 调试代码
                        issues += content.count('print(') - 5

                    total_issues += issues

                except Exception as e:
                    print(f"    ⚠️ Error analyzing {src_file}: {e}")
                    continue

            # 计算质量分数
            base_score = 90.0

            # 复杂度惩罚
            complexity_penalty = (complex_files / len(src_files)) * 15
            base_score -= complexity_penalty

            # 问题惩罚
            issue_penalty = min((total_issues / max(total_lines, 1)) * 100, 20)
            base_score -= issue_penalty

            final_score = max(0, min(100, base_score))
            print(f"    📊 Total files: {len(src_files)}, Complex files: {complex_files}")
            print(f"    🔍 Total issues: {total_issues}, Lines: {total_lines}")
            print(f"    📈 Code quality score: {final_score:.1f}")

            return final_score

        except Exception as e:
            print(f"    ⚠️ Error assessing code quality: {e}")
            return 70.0

    def _assess_security(self, project_path: Path) -> float:
        """评估安全性"""
        print("  🛡️ Assessing security...")

        try:
            src_files = list((project_path / "src").rglob("*.py"))
            security_issues = 0

            for src_file in src_files:
                try:
                    with open(src_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # 安全问题检测
                    if 'eval(' in content:
                        security_issues += 5
                    if 'exec(' in content:
                        security_issues += 5
                    if 'pickle.loads(' in content:
                        security_issues += 3
                    if 'subprocess.call' in content and 'shell=True' in content:
                        security_issues += 3

                except Exception:
                    continue

            # 计算安全分数
            base_score = 95.0
            security_penalty = min(security_issues * 2, 30)
            final_score = max(0, base_score - security_penalty)

            print(f"    🔍 Security issues found: {security_issues}")
            print(f"    📈 Security score: {final_score:.1f}")

            return final_score

        except Exception as e:
            print(f"    ⚠️ Error assessing security: {e}")
            return 80.0

    def _assess_performance(self, project_path: Path) -> float:
        """评估性能"""
        print("  ⚡ Assessing performance...")

        try:
            src_files = list((project_path / "src").rglob("*.py"))
            performance_issues = 0

            for src_file in src_files:
                try:
                    with open(src_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # 性能问题检测
                    if content.count('for ') > 20:  # 可能的性能热点
                        performance_issues += 2
                    if content.count('while True:') > 0:  # 无限循环
                        performance_issues += 3
                    if content.count('import time') > 5:  # 大量时间操作
                        performance_issues += 1

                except Exception:
                    continue

            # 计算性能分数
            base_score = 85.0
            performance_penalty = min(performance_issues, 20)
            final_score = max(0, base_score - performance_penalty)

            print(f"    🔍 Performance issues: {performance_issues}")
            print(f"    📈 Performance score: {final_score:.1f}")

            return final_score

        except Exception as e:
            print(f"    ⚠️ Error assessing performance: {e}")
            return 75.0

    def _assess_documentation(self, project_path: Path) -> float:
        """评估文档覆盖率"""
        print("  📚 Assessing documentation...")

        try:
            src_files = list((project_path / "src").rglob("*.py"))
            documented_files = 0
            total_functions = 0
            documented_functions = 0

            for src_file in src_files:
                try:
                    with open(src_file, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # 检查文件级文档
                    if '"""' in content or "'''" in content:
                        documented_files += 1

                    # 简单函数计数
                    total_functions += content.count('def ')
                    documented_functions += content.count('def ') * 0.7  # 估算

                except Exception:
                    continue

            # 计算文档分数
            file_doc_ratio = documented_files / max(len(src_files), 1)
            func_doc_ratio = documented_functions / max(total_functions, 1)

            overall_score = (file_doc_ratio * 50 + func_doc_ratio * 50)
            final_score = min(100, max(0, overall_score))

            print(f"    📊 Documented files: {documented_files}/{len(src_files)}")
            print(f"    📈 Documentation score: {final_score:.1f}")

            return final_score

        except Exception as e:
            print(f"    ⚠️ Error assessing documentation: {e}")
            return 60.0

    def _assess_maintainability(self, project_path: Path) -> float:
        """评估可维护性"""
        print("  🔧 Assessing maintainability...")

        try:
            _src_files = list((project_path / "src").rglob("*.py"))
            maintainability_score = 85.0  # 基础分数

            # 检查项目结构
            has_tests = (project_path / "tests").exists()
            has_docs = (project_path / "docs").exists()
            has_requirements = (project_path / "requirements").exists()

            if has_tests:
                maintainability_score += 5
            if has_docs:
                maintainability_score += 5
            if has_requirements:
                maintainability_score += 5

            # 检查代码组织
            organized_structure = (
                (project_path / "src" / "adapters").exists() or
                (project_path / "src" / "domain").exists() or
                (project_path / "src" / "services").exists()
            )

            if organized_structure:
                maintainability_score += 5

            final_score = min(100, maintainability_score)
            print(f"    📊 Project structure: organized={organized_structure}")
            print(f"    📊 Has tests: {has_tests}, docs: {has_docs}, requirements: {has_requirements}")
            print(f"    📈 Maintainability score: {final_score:.1f}")

            return final_score

        except Exception as e:
            print(f"    ⚠️ Error assessing maintainability: {e}")
            return 70.0

    def _run_ai_analysis(self, project_path: Path):
        """运行AI驱动的深度分析"""
        print("🤖 Running AI-Driven Analysis...")

        if self.config["ai_analysis"]["pattern_recognition"]:
            self._pattern_recognition_analysis(project_path)

        if self.config["ai_analysis"]["defect_prediction"]:
            self._defect_prediction_analysis(project_path)

        if self.config["ai_analysis"]["optimization_suggestions"]:
            self._optimization_analysis(project_path)

    def _pattern_recognition_analysis(self, project_path: Path):
        """模式识别分析"""
        print("  🔍 Running pattern recognition...")

        # 简单的模式识别实现
        patterns_found = []

        # 检查设计模式使用
        src_files = list((project_path / "src").rglob("*.py"))
        for src_file in src_files:
            try:
                with open(src_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # 检查常见模式
                if 'class' in content and 'def __init__' in content:
                    patterns_found.append("class_pattern")
                if 'def factory(' in content or 'Factory' in content:
                    patterns_found.append("factory_pattern")
                if 'Observer' in content or 'observer' in content:
                    patterns_found.append("observer_pattern")

            except Exception:
                continue

        # 添加模式识别分数
        pattern_score = min(len(set(patterns_found)) * 10, 30)
        self.metrics.append(QualityMetric(
            name="design_patterns",
            value=pattern_score,
            threshold=20.0,
            weight=0.05,
            status=self._get_quality_level(pattern_score, 20.0),
            description="Design pattern recognition"
        ))

        print(f"    🎯 Patterns found: {len(set(patterns_found))}")
        print(f"    📈 Pattern score: {pattern_score:.1f}")

    def _defect_prediction_analysis(self, project_path: Path):
        """缺陷预测分析"""
        print("  🔮 Running defect prediction...")

        # 简单的缺陷预测模型
        risk_factors = 0
        src_files = list((project_path / "src").rglob("*.py"))

        for src_file in src_files:
            try:
                with open(src_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # 风险因素
                if len(content) > 2000:  # 大文件
                    risk_factors += 1
                if content.count('TODO') > 5:
                    risk_factors += 1
                if content.count('except:') > 3:  # 过度异常处理
                    risk_factors += 1

            except Exception:
                continue

        # 计算预测分数
        risk_score = max(0, 100 - (risk_factors * 10))
        self.metrics.append(QualityMetric(
            name="defect_prediction",
            value=risk_score,
            threshold=70.0,
            weight=0.05,
            status=self._get_quality_level(risk_score, 70.0),
            description="Defect prediction score"
        ))

        print(f"    🔮 Risk factors: {risk_factors}")
        print(f"    📈 Defect prediction score: {risk_score:.1f}")

    def _optimization_analysis(self, project_path: Path):
        """优化分析"""
        print("  ⚡ Running optimization analysis...")

        optimization_score = 85.0  # 基础分数

        # 检查优化机会
        src_files = list((project_path / "src").rglob("*.py"))
        optimization_opportunities = 0

        for src_file in src_files:
            try:
                with open(src_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # 优化机会
                if content.count('for ') > 10:
                    optimization_opportunities += 1
                if content.count('import ') > 10:
                    optimization_opportunities += 1
                if len(content) > 1000:
                    optimization_opportunities += 1

            except Exception:
                continue

        optimization_penalty = min(optimization_opportunities * 2, 15)
        final_score = max(0, optimization_score - optimization_penalty)

        self.metrics.append(QualityMetric(
            name="optimization_potential",
            value=final_score,
            threshold=75.0,
            weight=0.05,
            status=self._get_quality_level(final_score, 75.0),
            description="Optimization potential"
        ))

        print(f"    ⚡ Optimization opportunities: {optimization_opportunities}")
        print(f"    📈 Optimization score: {final_score:.1f}")

    def _get_quality_level(self, score: float, threshold: float) -> QualityLevel:
        """获取质量等级"""
        if score >= 95:
            return QualityLevel.EXCELLENT
        elif score >= threshold:
            return QualityLevel.GOOD
        elif score >= threshold * 0.8:
            return QualityLevel.ACCEPTABLE
        elif score >= threshold * 0.6:
            return QualityLevel.POOR
        else:
            return QualityLevel.CRITICAL

    def _calculate_final_result(self) -> QualityResult:
        """计算最终结果"""
        print("📊 Calculating Final Quality Result...")

        # 计算加权总分
        total_score = 0.0
        total_weight = 0.0

        for metric in self.metrics:
            total_score += metric.value * metric.weight
            total_weight += metric.weight

        if total_weight > 0:
            overall_score = total_score / total_weight
        else:
            overall_score = 0.0

        # 确定整体状态
        overall_status = self._get_quality_level(overall_score, self.config["thresholds"]["overall_score"])

        analysis_time = time.time() - self.start_time

        result = QualityResult(
            overall_score=overall_score,
            overall_status=overall_status,
            metrics=self.metrics,
            recommendations=[],
            should_deploy=False,
            analysis_time=analysis_time
        )

        print(f"🎯 Overall Quality Score: {overall_score:.1f}/100")
        print(f"📊 Overall Status: {overall_status.value}")
        print(f"⏱️ Analysis Time: {analysis_time:.2f} seconds")

        return result

    def _generate_recommendations(self, result: QualityResult) -> List[str]:
        """生成改进建议"""
        recommendations = []

        for metric in result.metrics:
            if metric.status in [QualityLevel.POOR, QualityLevel.CRITICAL]:
                if metric.name == "test_coverage":
                    recommendations.append(
                        f"🧪 Improve test coverage from {metric.value:.1f}% to {metric.threshold:.1f}%"
                    )
                elif metric.name == "code_quality":
                    recommendations.append(
                        f"🔍 Address code quality issues to improve from {metric.value:.1f} to {metric.threshold:.1f}"
                    )
                elif metric.name == "security":
                    recommendations.append(
                        f"🛡️ Fix security vulnerabilities to improve from {metric.value:.1f} to {metric.threshold:.1f}"
                    )
                elif metric.name == "performance":
                    recommendations.append(
                        f"⚡ Optimize performance issues to improve from {metric.value:.1f} to {metric.threshold:.1f}"
                    )
                elif metric.name == "documentation":
                    recommendations.append(
                        f"📚 Add documentation to improve from {metric.value:.1f}% to {metric.threshold:.1f}%"
                    )
                elif metric.name == "maintainability":
                    recommendations.append(
                        f"🔧 Improve code maintainability from {metric.value:.1f} to {metric.threshold:.1f}"
                    )

        # 添加通用建议
        if result.overall_score >= 90:
            recommendations.append("🎉 Excellent quality! Consider maintaining current standards.")
        elif result.overall_score >= 80:
            recommendations.append("✅ Good quality! Focus on areas below threshold.")
        else:
            recommendations.append("⚠️ Quality needs improvement. Address critical issues first.")

        return recommendations

    def _make_deployment_decision(self, result: QualityResult) -> bool:
        """做出部署决策"""
        print("🚀 Making Deployment Decision...")

        # 基础检查
        if result.overall_score < self.config["thresholds"]["overall_score"]:
            print(f"❌ Deployment blocked: Score {result.overall_score:.1f} below threshold {self.config['thresholds']['overall_score']}")
            return False

        # 关键指标检查
        critical_metrics = ["test_coverage", "security", "code_quality"]
        for metric in result.metrics:
            if metric.name in critical_metrics and metric.status == QualityLevel.CRITICAL:
                print(f"❌ Deployment blocked: Critical metric {metric.name} is {metric.status.value}")
                return False

        # 整体状态检查
        if result.overall_status in [QualityLevel.CRITICAL, QualityLevel.POOR]:
            print(f"❌ Deployment blocked: Overall status is {result.overall_status.value}")
            return False

        print(f"✅ Deployment approved: Score {result.overall_score:.1f} meets requirements")
        return True

    def generate_report(self, result: QualityResult) -> str:
        """生成质量报告"""
        report_lines = [
            "# Intelligent Quality Gate Report",
            f"**Analysis Time**: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Analysis Duration**: {result.analysis_time:.2f} seconds",
            "",
            "## 🎯 Executive Summary",
            "",
            f"- **Overall Quality Score**: {result.overall_score:.1f}/100",
            f"- **Overall Status**: {result.overall_status.value.upper()}",
            f"- **Deployment Decision**: {'✅ APPROVED' if result.should_deploy else '❌ BLOCKED'}",
            f"- **Metrics Analyzed**: {len(result.metrics)}",
            "",
            "## 📊 Detailed Metrics",
            ""
        ]

        for metric in result.metrics:
            status_emoji = {
                QualityLevel.EXCELLENT: "🏆",
                QualityLevel.GOOD: "✅",
                QualityLevel.ACCEPTABLE: "⚠️",
                QualityLevel.POOR: "❌",
                QualityLevel.CRITICAL: "🚨"
            }.get(metric.status, "❓")

            report_lines.extend([
                f"### {metric.name.replace('_', ' ').title()}",
                f"- **Score**: {metric.value:.1f}/100",
                f"- **Threshold**: {metric.threshold:.1f}",
                f"- **Weight**: {metric.weight:.2f}",
                f"- **Status**: {status_emoji} {metric.status.value.upper()}",
                f"- **Description**: {metric.description}",
                ""
            ])

        report_lines.extend([
            "## 💡 Recommendations",
            ""
        ])

        for i, rec in enumerate(result.recommendations, 1):
            report_lines.append(f"{i}. {rec}")

        report_lines.extend([
            "",
            "## 🚀 Deployment Decision",
            ""
        ])

        if result.should_deploy:
            report_lines.extend([
                "✅ **DEPLOYMENT APPROVED**",
                "",
                "The code quality meets all requirements and is ready for deployment.",
                "Continue with the deployment pipeline."
            ])
        else:
            report_lines.extend([
                "❌ **DEPLOYMENT BLOCKED**",
                "",
                "The code quality does not meet deployment requirements.",
                "Please address the identified issues before deploying."
            ])

        report_lines.extend([
            "",
            "---",
            "*Report generated by Intelligent Quality Gate System*",
            "*Phase 6 - Intelligent Upgrade*"
        ])

        return "\n".join(report_lines)

def main():
    """主函数"""
    print("🚀 Intelligent Quality Gate System")
    print("=" * 50)

    # 创建质量门禁系统
    quality_gate = IntelligentQualityGate()

    # 运行质量评估
    result = quality_gate.run_quality_assessment()

    # 生成报告
    report = quality_gate.generate_report(result)

    # 保存报告
    report_path = "intelligent_quality_gate_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"\n📄 Quality report saved to: {report_path}")

    # 输出决策
    print(f"\n🎯 Final Decision: {'✅ DEPLOY' if result.should_deploy else '❌ BLOCK'}")
    print(f"📊 Quality Score: {result.overall_score:.1f}/100")

    # 退出码
    sys.exit(0 if result.should_deploy else 1)

if __name__ == "__main__":
    main()