# ğŸš€ Football Prediction System - éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•

- [éƒ¨ç½²æ¦‚è§ˆ](#éƒ¨ç½²æ¦‚è§ˆ)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [Dockeréƒ¨ç½²](#dockeréƒ¨ç½²)
- [Kuberneteséƒ¨ç½²](#kuberneteséƒ¨ç½²)
- [äº‘å¹³å°éƒ¨ç½²](#äº‘å¹³å°éƒ¨ç½²)
- [ç›‘æ§å’Œæ—¥å¿—](#ç›‘æ§å’Œæ—¥å¿—)
- [å®‰å…¨é…ç½®](#å®‰å…¨é…ç½®)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
- [å¤‡ä»½å’Œæ¢å¤](#å¤‡ä»½å’Œæ¢å¤)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)

---

## ğŸ¯ éƒ¨ç½²æ¦‚è§ˆ

### æ”¯æŒçš„éƒ¨ç½²æ–¹å¼
- **Docker Compose**: å¼€å‘å’Œå°è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ
- **Kubernetes**: ä¼ä¸šçº§ç”Ÿäº§ç¯å¢ƒ
- **äº‘æœåŠ¡**: AWS, Azure, GCPæ‰˜ç®¡éƒ¨ç½²
- **ä¼ ç»Ÿéƒ¨ç½²**: ç‰©ç†æœº/è™šæ‹Ÿæœºç›´æ¥éƒ¨ç½²

### ç³»ç»Ÿæ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   è´Ÿè½½å‡è¡¡å™¨     â”‚    â”‚   APIç½‘å…³       â”‚    â”‚   åº”ç”¨æœåŠ¡      â”‚
â”‚   (Nginx/HAProxy)â”‚â—„â”€â”€â–ºâ”‚   (Kong/Traefik)â”‚â—„â”€â”€â–ºâ”‚   (FastAPI)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   é™æ€èµ„æº      â”‚    â”‚   ç¼“å­˜å±‚        â”‚    â”‚   æ•°æ®åº“        â”‚
â”‚   (CDN)         â”‚    â”‚   (Redis)       â”‚    â”‚   (PostgreSQL)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### èµ„æºè¦æ±‚

#### æœ€å°é…ç½®
- **CPU**: 2æ ¸å¿ƒ
- **å†…å­˜**: 4GB RAM
- **å­˜å‚¨**: 20GB SSD
- **ç½‘ç»œ**: 100Mbps

#### æ¨èé…ç½®
- **CPU**: 4æ ¸å¿ƒ
- **å†…å­˜**: 8GB RAM
- **å­˜å‚¨**: 100GB SSD
- **ç½‘ç»œ**: 1Gbps

#### ç”Ÿäº§é…ç½®
- **CPU**: 8æ ¸å¿ƒ+
- **å†…å­˜**: 16GB+ RAM
- **å­˜å‚¨**: 500GB+ SSD
- **ç½‘ç»œ**: 10Gbps

---

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 8+ / RHEL 8+
- **Docker**: 20.10+
- **Docker Compose**: 2.0+
- **Git**: 2.30+

### æœåŠ¡å™¨åˆå§‹åŒ–

#### Ubuntu/Debian
```bash
#!/bin/bash
# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…åŸºç¡€å·¥å…·
sudo apt install -y curl wget git htop vim

# å®‰è£…Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo usermod -aG docker $USER

# å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# åˆ›å»ºåº”ç”¨ç›®å½•
sudo mkdir -p /opt/football-prediction
sudo chown $USER:$USER /opt/football-prediction

# é…ç½®é˜²ç«å¢™
sudo ufw allow 22    # SSH
sudo ufw allow 80    # HTTP
sudo ufw allow 443   # HTTPS
sudo ufw --force enable
```

#### CentOS/RHEL
```bash
#!/bin/bash
# æ›´æ–°ç³»ç»Ÿ
sudo yum update -y

# å®‰è£…åŸºç¡€å·¥å…·
sudo yum install -y curl wget git htop vim

# å®‰è£…Docker
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
sudo yum install -y docker-ce docker-ce-cli containerd.io
sudo systemctl start docker
sudo systemctl enable docker
sudo usermod -aG docker $USER

# å®‰è£…Docker Compose
sudo curl -L "https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose

# åˆ›å»ºåº”ç”¨ç›®å½•
sudo mkdir -p /opt/football-prediction
sudo chown $USER:$USER /opt/football-prediction

# é…ç½®é˜²ç«å¢™
sudo firewall-cmd --permanent --add-service=ssh
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo firewall-cmd --reload
```

### SSLè¯ä¹¦å‡†å¤‡

#### Let's Encryptè‡ªåŠ¨è¯ä¹¦
```bash
# å®‰è£…certbot
sudo apt install certbot python3-certbot-nginx  # Ubuntu/Debian
# æˆ–
sudo yum install certbot python3-certbot-nginx   # CentOS/RHEL

# è·å–è¯ä¹¦ï¼ˆéœ€è¦åŸŸåå·²è§£æåˆ°æœåŠ¡å™¨ï¼‰
sudo certbot certonly --nginx -d yourdomain.com -d api.yourdomain.com

# è®¾ç½®è‡ªåŠ¨ç»­æœŸ
sudo crontab -e
# æ·»åŠ ä»¥ä¸‹è¡Œï¼š
# 0 12 * * * /usr/bin/certbot renew --quiet
```

#### è‡ªç­¾åè¯ä¹¦ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
```bash
# åˆ›å»ºè¯ä¹¦ç›®å½•
sudo mkdir -p /etc/ssl/private

# ç”Ÿæˆç§é’¥
sudo openssl genrsa -out /etc/ssl/private/football-prediction.key 2048

# ç”Ÿæˆè¯ä¹¦
sudo openssl req -new -x509 -key /etc/ssl/private/football-prediction.key -out /etc/ssl/certs/football-prediction.crt -days 365
```

---

## ğŸ³ Dockeréƒ¨ç½²

### å¼€å‘ç¯å¢ƒéƒ¨ç½²

#### 1. é¡¹ç›®å…‹éš†å’Œé…ç½®
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /opt/football-prediction

# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/football-prediction.git .

# å¤åˆ¶ç¯å¢ƒé…ç½®
cp .env.example .env

# ç¼–è¾‘é…ç½®æ–‡ä»¶
vim .env
```

#### 2. å¼€å‘ç¯å¢ƒé…ç½®
```bash
# .env
COMPOSE_PROJECT_NAME=football-prediction-dev
ENVIRONMENT=development
DEBUG=true

# æ•°æ®åº“é…ç½®
POSTGRES_DB=football_pred_dev
POSTGRES_USER=dev_user
POSTGRES_PASSWORD=dev_password
DATABASE_URL=postgresql://dev_user:dev_password@postgres:5432/football_pred_dev

# Redisé…ç½®
REDIS_URL=redis://redis:6379

# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# å®‰å…¨é…ç½®
SECRET_KEY=dev-secret-key-change-in-production
CORS_ORIGINS=http://localhost:3000,http://localhost:8080
```

#### 3. å¯åŠ¨å¼€å‘ç¯å¢ƒ
```bash
# æ„å»ºå’Œå¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f app

# åˆå§‹åŒ–æ•°æ®åº“
docker-compose exec app alembic upgrade head
docker-compose exec app python scripts/seed_data.py
```

### ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

#### 1. ç”Ÿäº§ç¯å¢ƒé…ç½®
```bash
# .env.production
COMPOSE_PROJECT_NAME=football-prediction-prod
ENVIRONMENT=production
DEBUG=false

# æ•°æ®åº“é…ç½®ï¼ˆä½¿ç”¨å¼ºå¯†ç ï¼‰
POSTGRES_DB=football_pred
POSTGRES_USER=prod_user
POSTGRES_PASSWORD=$(openssl rand -base64 32)
DATABASE_URL=postgresql://prod_user:${POSTGRES_PASSWORD}@postgres:5432/football_pred

# Redisé…ç½®
REDIS_URL=redis://redis:6379
REDIS_PASSWORD=$(openssl rand -base64 32)

# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# å®‰å…¨é…ç½®
SECRET_KEY=$(openssl rand -base64 64)
CORS_ORIGINS=https://yourdomain.com,https://api.yourdomain.com
ALLOWED_HOSTS=yourdomain.com,api.yourdomain.com

# SSLé…ç½®
SSL_CERT_PATH=/etc/ssl/certs/football-prediction.crt
SSL_KEY_PATH=/etc/ssl/private/football-prediction.key

# ç›‘æ§é…ç½®
PROMETHEUS_ENABLED=true
GRAFANA_ADMIN_PASSWORD=$(openssl rand -base64 16)
```

#### 2. ç”Ÿäº§Docker Compose
```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile.prod
    restart: unless-stopped
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - SECRET_KEY=${SECRET_KEY}
      - ENVIRONMENT=production
    volumes:
      - ./ssl:/etc/ssl:ro
      - ./logs:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app-network
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  nginx:
    image: nginx:alpine
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - app
    networks:
      - app-network

  postgres:
    image: postgres:15
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    networks:
      - app-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 30s
      timeout: 10s
      retries: 3

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: redis-server --requirepass ${REDIS_PASSWORD} --appendonly yes
    volumes:
      - redis_data:/data
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  prometheus:
    image: prom/prometheus:latest
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    networks:
      - app-network

  grafana:
    image: grafana/grafana:latest
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    networks:
      - app-network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  app-network:
    driver: bridge
```

#### 3. ç”Ÿäº§Dockerfile
```dockerfile
# Dockerfile.prod
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash app && \
    chown -R app:app /app
USER app

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["gunicorn", "src.main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000"]
```

#### 4. Nginxé…ç½®
```nginx
# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream app {
        server app:8000;
    }

    # HTTPé‡å®šå‘åˆ°HTTPS
    server {
        listen 80;
        server_name yourdomain.com api.yourdomain.com;
        return 301 https://$server_name$request_uri;
    }

    # HTTPSé…ç½®
    server {
        listen 443 ssl http2;
        server_name yourdomain.com;

        # SSLé…ç½®
        ssl_certificate /etc/ssl/football-prediction.crt;
        ssl_certificate_key /etc/ssl/football-prediction.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES256-GCM-SHA384;
        ssl_prefer_server_ciphers off;

        # å®‰å…¨å¤´
        add_header X-Frame-Options DENY;
        add_header X-Content-Type-Options nosniff;
        add_header X-XSS-Protection "1; mode=block";
        add_header Strict-Transport-Security "max-age=63072000; includeSubDomains; preload";

        # é™æ€æ–‡ä»¶
        location /static/ {
            alias /app/static/;
            expires 1y;
            add_header Cache-Control "public, immutable";
        }

        # APIä»£ç†
        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
    }

    # APIå­åŸŸå
    server {
        listen 443 ssl http2;
        server_name api.yourdomain.com;

        # SSLé…ç½®ï¼ˆåŒä¸Šï¼‰
        ssl_certificate /etc/ssl/football-prediction.crt;
        ssl_certificate_key /etc/ssl/football-prediction.key;
        ssl_protocols TLSv1.2 TLSv1.3;

        # APIä»£ç†
        location / {
            proxy_pass http://app;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

#### 5. éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# deploy.sh

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²Football Prediction System..."

# æ£€æŸ¥ç¯å¢ƒ
if [ ! -f ".env.production" ]; then
    echo "âŒ .env.productionæ–‡ä»¶ä¸å­˜åœ¨"
    exit 1
fi

# æ‹‰å–æœ€æ–°ä»£ç 
echo "ğŸ“¥ æ‹‰å–æœ€æ–°ä»£ç ..."
git pull origin main

# æ„å»ºé•œåƒ
echo "ğŸ”¨ æ„å»ºDockeré•œåƒ..."
docker-compose -f docker-compose.prod.yml build --no-cache

# åœæ­¢æ—§æœåŠ¡
echo "â¹ï¸ åœæ­¢æ—§æœåŠ¡..."
docker-compose -f docker-compose.prod.yml down

# å¯åŠ¨æ–°æœåŠ¡
echo "â–¶ï¸ å¯åŠ¨æ–°æœåŠ¡..."
docker-compose -f docker-compose.prod.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# æ•°æ®åº“è¿ç§»
echo "ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“è¿ç§»..."
docker-compose -f docker-compose.prod.yml exec -T app alembic upgrade head

# å¥åº·æ£€æŸ¥
echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
if curl -f http://localhost/health; then
    echo "âœ… éƒ¨ç½²æˆåŠŸï¼"
else
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
    docker-compose -f docker-compose.prod.yml logs app
    exit 1
fi

echo "ğŸ‰ Football Prediction Systeméƒ¨ç½²å®Œæˆï¼"
echo "ğŸ“Š ç›‘æ§é¢æ¿: http://yourdomain.com:3000"
echo "ğŸ“ˆ Prometheus: http://yourdomain.com:9090"
```

---

## â˜¸ï¸ Kuberneteséƒ¨ç½²

### é›†ç¾¤å‡†å¤‡

#### 1. èŠ‚ç‚¹è¦æ±‚
- **MasterèŠ‚ç‚¹**: 2CPU, 4GB RAM, 20GBå­˜å‚¨
- **WorkerèŠ‚ç‚¹**: 4CPU, 8GB RAM, 100GBå­˜å‚¨
- **ç½‘ç»œ**: Calico/Flannel CNI

#### 2. å®‰è£…ä¾èµ–
```bash
# å®‰è£…kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

# å®‰è£…Helm
curl https://get.helm.sh/helm-v3.12.0-linux-amd64.tar.gz | tar xz
sudo mv linux-amd64/helm /usr/local/bin/

# æ·»åŠ Helmä»“åº“
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
```

### åº”ç”¨éƒ¨ç½²

#### 1. å‘½åç©ºé—´å’Œé…ç½®
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: football-prediction
  labels:
    name: football-prediction

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: football-prediction
data:
  ENVIRONMENT: "production"
  DEBUG: "false"
  API_HOST: "0.0.0.0"
  API_PORT: "8000"
  CORS_ORIGINS: "https://yourdomain.com"
  ALLOWED_HOSTS: "yourdomain.com"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: football-prediction
type: Opaque
data:
  DATABASE_URL: <base64-encoded-database-url>
  REDIS_URL: <base64-encoded-redis-url>
  SECRET_KEY: <base64-encoded-secret-key>
  SSL_CERT_PATH: <base64-encoded-ssl-cert-path>
  SSL_KEY_PATH: <base64-encoded-ssl-key-path>
```

#### 2. æ•°æ®åº“éƒ¨ç½²
```yaml
# k8s/postgres.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: football-prediction
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15
        env:
        - name: POSTGRES_DB
          value: "football_pred"
        - name: POSTGRES_USER
          value: "postgres"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret
              key: password
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 20Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: football-prediction
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  type: ClusterIP
```

#### 3. Rediséƒ¨ç½²
```yaml
# k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: football-prediction
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        command: ["redis-server", "--requirepass", "$(REDIS_PASSWORD)", "--appendonly", "yes"]
        env:
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: redis-secret
              key: password
        ports:
        - containerPort: 6379
        volumeMounts:
        - name: redis-storage
          mountPath: /data
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
      volumes:
      - name: redis-storage
        persistentVolumeClaim:
          claimName: redis-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: football-prediction
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
  type: ClusterIP

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-pvc
  namespace: football-prediction
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

#### 4. åº”ç”¨éƒ¨ç½²
```yaml
# k8s/app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: football-prediction-api
  namespace: football-prediction
spec:
  replicas: 3
  selector:
    matchLabels:
      app: football-prediction-api
  template:
    metadata:
      labels:
        app: football-prediction-api
    spec:
      containers:
      - name: api
        image: football-prediction:latest
        ports:
        - containerPort: 8000
        envFrom:
        - configMapRef:
            name: app-config
        - secretRef:
            name: app-secrets
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3

---
apiVersion: v1
kind: Service
metadata:
  name: football-prediction-api
  namespace: football-prediction
spec:
  selector:
    app: football-prediction-api
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: football-prediction-api-hpa
  namespace: football-prediction
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: football-prediction-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

#### 5. Ingressé…ç½®
```yaml
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: football-prediction-ingress
  namespace: football-prediction
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  tls:
  - hosts:
    - yourdomain.com
    - api.yourdomain.com
    secretName: football-prediction-tls
  rules:
  - host: yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: football-prediction-api
            port:
              number: 80
  - host: api.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: football-prediction-api
            port:
              number: 80
```

#### 6. éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# k8s-deploy.sh

set -e

echo "ğŸš€ å¼€å§‹Kuberneteséƒ¨ç½²..."

# åº”ç”¨é…ç½®
echo "ğŸ“ åº”ç”¨é…ç½®..."
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml

# éƒ¨ç½²æ•°æ®åº“
echo "ğŸ—„ï¸ éƒ¨ç½²æ•°æ®åº“..."
kubectl apply -f k8s/postgres.yaml
kubectl apply -f k8s/redis.yaml

# ç­‰å¾…æ•°æ®åº“å°±ç»ª
echo "â³ ç­‰å¾…æ•°æ®åº“å°±ç»ª..."
kubectl wait --for=condition=ready pod -l app=postgres -n football-prediction --timeout=300s
kubectl wait --for=condition=ready pod -l app=redis -n football-prediction --timeout=300s

# éƒ¨ç½²åº”ç”¨
echo "ğŸš€ éƒ¨ç½²åº”ç”¨..."
kubectl apply -f k8s/app.yaml

# ç­‰å¾…åº”ç”¨å°±ç»ª
echo "â³ ç­‰å¾…åº”ç”¨å°±ç»ª..."
kubectl wait --for=condition=ready pod -l app=football-prediction-api -n football-prediction --timeout=300s

# é…ç½®Ingress
echo "ğŸŒ é…ç½®Ingress..."
kubectl apply -f k8s/ingress.yaml

# æ•°æ®åº“è¿ç§»
echo "ğŸ—„ï¸ æ‰§è¡Œæ•°æ®åº“è¿ç§»..."
kubectl exec -n football-prediction deployment/football-prediction-api -- alembic upgrade head

# éªŒè¯éƒ¨ç½²
echo "âœ… éªŒè¯éƒ¨ç½²..."
kubectl get pods -n football-prediction
kubectl get services -n football-prediction
kubectl get ingress -n football-prediction

echo "ğŸ‰ Kuberneteséƒ¨ç½²å®Œæˆï¼"
```

---

## â˜ï¸ äº‘å¹³å°éƒ¨ç½²

### AWSéƒ¨ç½²

#### 1. ECSéƒ¨ç½²
```yaml
# aws/ecs-task-definition.json
{
  "family": "football-prediction",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "512",
  "memory": "1024",
  "executionRoleArn": "arn:aws:iam::account:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::account:role/ecsTaskRole",
  "containerDefinitions": [
    {
      "name": "football-prediction-api",
      "image": "your-account.dkr.ecr.region.amazonaws.com/football-prediction:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "ENVIRONMENT",
          "value": "production"
        }
      ],
      "secrets": [
        {
          "name": "DATABASE_URL",
          "valueFrom": "arn:aws:secretsmanager:region:account:secret:football-prediction/db-url"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/football-prediction",
          "awslogs-region": "us-west-2",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "healthCheck": {
        "command": ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"],
        "interval": 30,
        "timeout": 5,
        "retries": 3
      }
    }
  ]
}
```

#### 2. AWS CDKéƒ¨ç½²
```typescript
// aws/lib/football-prediction-stack.ts
import * as cdk from 'aws-cdk-lib';
import { Construct } from 'constructs';
import * as ecs from 'aws-cdk-lib/aws-ecs';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as rds from 'aws-cdk-lib/aws-rds';
import * as elasticache from 'aws-cdk-lib/aws-elasticache';

export class FootballPredictionStack extends cdk.Stack {
  constructor(scope: Construct, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    // VPC
    const vpc = new ec2.Vpc(this, 'FootballPredictionVPC', {
      maxAzs: 2,
      natGateways: 1,
    });

    // ECS Cluster
    const cluster = new ecs.Cluster(this, 'FootballPredictionCluster', {
      vpc,
      clusterName: 'football-prediction',
    });

    // RDS PostgreSQL
    const database = new rds.DatabaseInstance(this, 'FootballPredictionDB', {
      engine: rds.DatabaseInstanceEngine.postgres({
        version: rds.PostgresEngineVersion.VER_15,
      }),
      instanceType: ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.MICRO),
      vpc,
      databaseName: 'football_pred',
      allocatedStorage: 20,
      storageType: rds.StorageType.GP2,
      backupRetention: cdk.Duration.days(7),
      deletionProtection: false,
    });

    // ElastiCache Redis
    const redis = new elasticache.CfnCacheCluster(this, 'FootballPredictionRedis', {
      cacheNodeType: 'cache.t3.micro',
      engine: 'redis',
      numCacheNodes: 1,
      vpcSecurityGroupIds: [vpc.vpcDefaultSecurityGroup],
    });

    // Fargate Task Definition
    const taskDefinition = new ecs.FargateTaskDefinition(this, 'FootballPredictionTask', {
      memoryLimitMiB: 1024,
      cpu: 512,
    });

    const container = taskDefinition.addContainer('football-prediction-api', {
      image: ecs.ContainerImage.fromRegistry('your-account.dkr.ecr.region.amazonaws.com/football-prediction:latest'),
      portMappings: [{ containerPort: 8000 }],
      logging: new ecs.AwsLogDriver({
        streamPrefix: 'football-prediction',
      }),
    });

    container.addEnvironment('DATABASE_URL', database.instanceEndpoint.socketAddress);
    container.addEnvironment('REDIS_URL', `redis://${redis.attrRedisEndpoint.address}:${redis.attrRedisEndpoint.port}`);

    // Fargate Service
    const service = new ecs.FargateService(this, 'FootballPredictionService', {
      cluster,
      taskDefinition,
      desiredCount: 2,
      assignPublicIp: false,
    });
  }
}
```

### Azureéƒ¨ç½²

#### 1. Azure Container Instances
```bash
#!/bin/bash
# azure/deploy-aci.sh

# å˜é‡å®šä¹‰
RESOURCE_GROUP="football-prediction-rg"
LOCATION="eastus"
CONTAINER_NAME="football-prediction-api"
IMAGE="yourregistry.azurecr.io/football-prediction:latest"

# åˆ›å»ºèµ„æºç»„
az group create --name $RESOURCE_GROUP --location $LOCATION

# éƒ¨ç½²å®¹å™¨å®ä¾‹
az container create \
  --resource-group $RESOURCE_GROUP \
  --name $CONTAINER_NAME \
  --image $IMAGE \
  --cpu 1 \
  --memory 2 \
  --ports 8000 \
  --environment-variables \
    ENVIRONMENT=production \
    DATABASE_URL=$DATABASE_URL \
    REDIS_URL=$REDIS_URL \
    SECRET_KEY=$SECRET_KEY \
  --dns-name-label football-prediction-$RANDOM

# è·å–FQDN
FQDN=$(az container show --resource-group $RESOURCE_GROUP --name $CONTAINER_NAME --query "ipAddress.fqdn" --output tsv)

echo "ğŸŒ åº”ç”¨å·²éƒ¨ç½²: https://$FQDN:8000"
```

### GCPéƒ¨ç½²

#### 1. Cloud Runéƒ¨ç½²
```bash
#!/bin/bash
# gcp/deploy-cloudrun.sh

# å˜é‡å®šä¹‰
PROJECT_ID="your-project-id"
REGION="us-central1"
SERVICE_NAME="football-prediction-api"
IMAGE_NAME="football-prediction"

# æ„å»ºå’Œæ¨é€é•œåƒ
gcloud builds submit --tag gcr.io/$PROJECT_ID/$IMAGE_NAME

# éƒ¨ç½²åˆ°Cloud Run
gcloud run deploy $SERVICE_NAME \
  --image gcr.io/$PROJECT_ID/$IMAGE_NAME \
  --platform managed \
  --region $REGION \
  --allow-unauthenticated \
  --memory 1Gi \
  --cpu 1 \
  --max-instances 10 \
  --set-env-vars ENVIRONMENT=production \
  --set-secrets DATABASE_URL=football-prediction-db-url:latest \
  --set-secrets REDIS_URL=football-prediction-redis-url:latest \
  --set-secrets SECRET_KEY=football-prediction-secret-key:latest

# è·å–æœåŠ¡URL
SERVICE_URL=$(gcloud run services describe $SERVICE_NAME --region $REGION --format "value(status.url)")

echo "ğŸŒ åº”ç”¨å·²éƒ¨ç½²: $SERVICE_URL"
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### Prometheusé…ç½®

#### 1. Prometheusé…ç½®æ–‡ä»¶
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'football-prediction-api'
    static_configs:
      - targets: ['app:8000']
    metrics_path: /metrics
    scrape_interval: 15s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx:80']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

#### 2. å‘Šè­¦è§„åˆ™
```yaml
# monitoring/alert_rules.yml
groups:
  - name: football-prediction
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }} seconds"

      - alert: DatabaseConnectionFailure
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection failure"
          description: "PostgreSQL database is down"

      - alert: RedisConnectionFailure
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis connection failure"
          description: "Redis cache is down"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"

      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is {{ $value }}%"
```

### Grafanaä»ªè¡¨æ¿

#### 1. Grafanaé…ç½®
```json
{
  "dashboard": {
    "title": "Football Prediction System",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"4..\"}[5m])",
            "legendFormat": "4xx errors"
          },
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "5xx errors"
          }
        ]
      },
      {
        "title": "Database Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "pg_stat_database_numbackends",
            "legendFormat": "Active connections"
          }
        ]
      },
      {
        "title": "Redis Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "redis_memory_used_bytes",
            "legendFormat": "Memory used"
          }
        ]
      }
    ]
  }
}
```

### æ—¥å¿—èšåˆ

#### 1. ELK Stacké…ç½®
```yaml
# logging/elasticsearch.yml
version: '3.8'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

#### 2. Logstashé…ç½®
```ruby
# logstash/pipeline/logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "football-prediction" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    if [level] == "ERROR" {
      mutate {
        add_tag => [ "error" ]
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "football-prediction-%{+YYYY.MM.dd}"
  }

  if "error" in [tags] {
    email {
      to => "admin@yourdomain.com"
      subject => "Football Prediction Error Alert"
      body => "Error occurred: %{message}"
    }
  }
}
```

---

## ğŸ”’ å®‰å…¨é…ç½®

### åº”ç”¨å®‰å…¨

#### 1. ç¯å¢ƒå˜é‡åŠ å¯†
```bash
# ä½¿ç”¨Docker secretsæˆ–Kubernetes secrets
# ä¸è¦åœ¨ç¯å¢ƒå˜é‡ä¸­å­˜å‚¨æ•æ„Ÿä¿¡æ¯

# åŠ å¯†æ•æ„Ÿé…ç½®
echo "your-secret-key" | openssl enc -aes-256-cbc -base64 -salt > secret.key

# è§£å¯†é…ç½®
openssl enc -aes-256-cbc -d -base64 -in secret.key -out secret.decrypted
```

#### 2. APIå®‰å…¨é…ç½®
```python
# src/security/middleware.py
from fastapi import Request, HTTPException
from slowapi import Limiter
from slowapi.util import get_remote_address
import time

# é™æµé…ç½®
limiter = Limiter(key_func=get_remote_address)

# å®‰å…¨ä¸­é—´ä»¶
async def security_middleware(request: Request, call_next):
    # æ·»åŠ å®‰å…¨å¤´
    response = await call_next(request)

    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Strict-Transport-Security"] = "max-age=63072000; includeSubDomains; preload"

    return response

# è¯·æ±‚éªŒè¯
async def validate_request(request: Request):
    # éªŒè¯User-Agent
    user_agent = request.headers.get("User-Agent", "")
    if not user_agent or len(user_agent) < 10:
        raise HTTPException(status_code=400, detail="Invalid User-Agent")

    # éªŒè¯Content-Length
    content_length = request.headers.get("Content-Length")
    if content_length and int(content_length) > 10 * 1024 * 1024:  # 10MB
        raise HTTPException(status_code=413, detail="Payload too large")
```

#### 3. æ•°æ®åº“å®‰å…¨
```sql
-- åˆ›å»ºä¸“ç”¨æ•°æ®åº“ç”¨æˆ·
CREATE USER api_user WITH PASSWORD 'strong_password';
GRANT CONNECT ON DATABASE football_pred TO api_user;
GRANT USAGE ON SCHEMA public TO api_user;
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO api_user;
GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO api_user;

-- è¡Œçº§å®‰å…¨ç­–ç•¥
ALTER TABLE predictions ENABLE ROW LEVEL SECURITY;

CREATE POLICY user_predictions ON predictions
    FOR ALL TO api_user
    USING (user_id = current_setting('app.current_user_id')::uuid);

-- å®¡è®¡æ—¥å¿—
CREATE TABLE audit_log (
    id SERIAL PRIMARY KEY,
    user_id UUID,
    action VARCHAR(50),
    table_name VARCHAR(50),
    record_id UUID,
    old_values JSONB,
    new_values JSONB,
    timestamp TIMESTAMP DEFAULT NOW()
);

CREATE OR REPLACE FUNCTION audit_trigger()
RETURNS TRIGGER AS $$
BEGIN
    INSERT INTO audit_log (user_id, action, table_name, record_id, old_values, new_values)
    VALUES (
        current_setting('app.current_user_id')::uuid,
        TG_OP,
        TG_TABLE_NAME,
        COALESCE(NEW.id, OLD.id),
        CASE WHEN TG_OP = 'DELETE' THEN row_to_json(OLD) ELSE NULL END,
        CASE WHEN TG_OP IN ('INSERT', 'UPDATE') THEN row_to_json(NEW) ELSE NULL END
    );
    RETURN COALESCE(NEW, OLD);
END;
$$ LANGUAGE plpgsql;
```

### ç½‘ç»œå®‰å…¨

#### 1. é˜²ç«å¢™é…ç½®
```bash
# UFWé…ç½®
sudo ufw default deny incoming
sudo ufw default allow outgoing

# å…è®¸SSH
sudo ufw allow 22/tcp

# å…è®¸HTTP/HTTPS
sudo ufw allow 80/tcp
sudo ufw allow 443/tcp

# é™åˆ¶æ•°æ®åº“è®¿é—®
sudo ufw allow from 10.0.0.0/8 to any port 5432
sudo ufw allow from 10.0.0.0/8 to any port 6379

# å¯ç”¨é˜²ç«å¢™
sudo ufw enable
```

#### 2. Fail2bané…ç½®
```bash
# /etc/fail2ban/jail.local
[DEFAULT]
bantime = 3600
findtime = 600
maxretry = 3

[sshd]
enabled = true
port = ssh
filter = sshd
logpath = /var/log/auth.log

[nginx-http-auth]
enabled = true
port = http,https
filter = nginx-http-auth
logpath = /var/log/nginx/error.log

[nginx-limit-req]
enabled = true
port = http,https
filter = nginx-limit-req
logpath = /var/log/nginx/error.log
maxretry = 10
findtime = 600
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### åº”ç”¨ä¼˜åŒ–

#### 1. è¿æ¥æ± ä¼˜åŒ–
```python
# src/config/database.py
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.pool import QueuePool

engine = create_async_engine(
    settings.database_url,
    poolclass=QueuePool,
    pool_size=20,          # è¿æ¥æ± å¤§å°
    max_overflow=30,       # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
    pool_pre_ping=True,    # è¿æ¥å‰pingæ£€æŸ¥
    pool_recycle=3600,     # è¿æ¥å›æ”¶æ—¶é—´
    echo=settings.debug
)
```

#### 2. ç¼“å­˜ç­–ç•¥
```python
# src/cache/strategy.py
from typing import Optional
import redis.asyncio as redis
import json
import hashlib

class CacheManager:
    def __init__(self):
        self.redis = redis.from_url(settings.redis_url)

    async def get_or_set(
        self,
        key: str,
        func,
        expire: int = 3600,
        cache_null: bool = False
    ):
        """è·å–ç¼“å­˜æˆ–è®¾ç½®æ–°å€¼"""
        # å°è¯•ä»ç¼“å­˜è·å–
        cached = await self.redis.get(key)
        if cached is not None:
            return json.loads(cached)

        # æ‰§è¡Œå‡½æ•°è·å–æ•°æ®
        result = await func()

        # ç¼“å­˜ç»“æœï¼ˆåŒ…æ‹¬nullå€¼å¦‚æœéœ€è¦ï¼‰
        if result is not None or cache_null:
            await self.redis.setex(
                key,
                expire,
                json.dumps(result, default=str)
            )

        return result

    async def invalidate_pattern(self, pattern: str):
        """æŒ‰æ¨¡å¼æ‰¹é‡åˆ é™¤ç¼“å­˜"""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)

# ä½¿ç”¨è£…é¥°å™¨
def cache_result(expire: int = 3600, key_prefix: str = ""):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = f"{key_prefix}:{func.__name__}:{hashlib.md5(str(args + tuple(kwargs.items())).encode()).hexdigest()}"

            # è·å–æˆ–è®¾ç½®ç¼“å­˜
            return await cache_manager.get_or_set(
                cache_key,
                lambda: func(*args, **kwargs),
                expire
            )
        return wrapper
    return decorator
```

#### 3. å¼‚æ­¥ä¼˜åŒ–
```python
# src/utils/async_utils.py
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import List, Callable, Any

class AsyncTaskManager:
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)

    async def gather_with_concurrency(
        self,
        tasks: List[Callable],
        max_concurrency: int = 10
    ) -> List[Any]:
        """å¹¶å‘æ‰§è¡Œä»»åŠ¡ï¼Œé™åˆ¶å¹¶å‘æ•°"""
        semaphore = asyncio.Semaphore(max_concurrency)

        async def limited_task(task):
            async with semaphore:
                if asyncio.iscoroutinefunction(task):
                    return await task()
                else:
                    loop = asyncio.get_event_loop()
                    return await loop.run_in_executor(self.executor, task)

        return await asyncio.gather(*(limited_task(task) for task in tasks))

    async def batch_process(
        self,
        items: List[Any],
        processor: Callable,
        batch_size: int = 100
    ) -> List[Any]:
        """æ‰¹é‡å¤„ç†æ•°æ®"""
        results = []

        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_results = await self.gather_with_concurrency([
                lambda item=item: processor(item) for item in batch
            ])
            results.extend(batch_results)

        return results

# ä½¿ç”¨ç¤ºä¾‹
async def process_predictions_batch(predictions: List[PredictionRequest]):
    task_manager = AsyncTaskManager()

    # æ‰¹é‡å¤„ç†é¢„æµ‹è¯·æ±‚
    results = await task_manager.batch_process(
        predictions,
        process_single_prediction,
        batch_size=50
    )

    return results
```

### æ•°æ®åº“ä¼˜åŒ–

#### 1. ç´¢å¼•ä¼˜åŒ–
```sql
-- åˆ›å»ºå¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY idx_predictions_user_created
ON predictions(user_id, created_at DESC);

CREATE INDEX CONCURRENTLY idx_matches_status_date
ON matches(status, match_date)
WHERE status IN ('scheduled', 'live');

-- éƒ¨åˆ†ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_active_users
ON users(id)
WHERE subscription_plan != 'free';

-- è¡¨è¾¾å¼ç´¢å¼•
CREATE INDEX CONCURRENTLY idx_predictions_date_trunc
ON predictions(date_trunc('day', created_at));

-- è‡ªåŠ¨åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
CREATE OR REPLACE FUNCTION auto_analyze_tables()
RETURNS void AS $$
BEGIN
    ANALYZE predictions;
    ANALYZE matches;
    ANALYZE users;
END;
$$ LANGUAGE plpgsql;

-- è®¾ç½®å®šæ—¶ä»»åŠ¡
SELECT cron.schedule('auto-analyze', '0 2 * * *', 'SELECT auto_analyze_tables();');
```

#### 2. æŸ¥è¯¢ä¼˜åŒ–
```python
# src/repositories/optimized_queries.py
from sqlalchemy import select, and_, or_, func
from sqlalchemy.orm import selectinload, joinedload

class OptimizedPredictionRepository:
    async def get_user_predictions_paginated(
        self,
        user_id: str,
        page: int = 1,
        page_size: int = 20
    ):
        """åˆ†é¡µè·å–ç”¨æˆ·é¢„æµ‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        offset = (page - 1) * page_size

        # ä½¿ç”¨çª—å£å‡½æ•°ä¼˜åŒ–åˆ†é¡µ
        query = select(
            Prediction,
            func.count().over().label('total_count')
        ).where(
            Prediction.user_id == user_id
        ).order_by(
            Prediction.created_at.desc()
        ).offset(offset).limit(page_size)

        result = await self.session.execute(query)
        rows = result.all()

        if not rows:
            return [], 0

        total_count = rows[0].total_count
        predictions = [row.Prediction for row in rows]

        return predictions, total_count

    async def get_popular_predictions(
        self,
        limit: int = 10,
        time_range_days: int = 7
    ):
        """è·å–çƒ­é—¨é¢„æµ‹ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        cutoff_date = datetime.now() - timedelta(days=time_range_days)

        # ä½¿ç”¨CTEä¼˜åŒ–å¤æ‚æŸ¥è¯¢
        cte = select(
            Prediction.match_id,
            func.count().label('prediction_count')
        ).where(
            and_(
                Prediction.created_at >= cutoff_date,
                Prediction.predicted_winner.isnot(None)
            )
        ).group_by(
            Prediction.match_id
        ).order_by(
            func.count().desc()
        ).limit(limit).cte('popular_predictions')

        query = select(
            Match,
            cte.c.prediction_count
        ).join(
            cte, Match.id == cte.c.match_id
        ).options(
            selectinload(Match.home_team),
            selectinload(Match.away_team)
        )

        result = await self.session.execute(query)
        return result.all()
```

---

## ğŸ’¾ å¤‡ä»½å’Œæ¢å¤

### æ•°æ®åº“å¤‡ä»½

#### 1. è‡ªåŠ¨å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# scripts/backup_database.sh

set -e

# é…ç½®
BACKUP_DIR="/backups/postgresql"
RETENTION_DAYS=30
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="football_pred_backup_${TIMESTAMP}.sql"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ‰§è¡Œå¤‡ä»½
echo "ğŸ“¦ å¼€å§‹æ•°æ®åº“å¤‡ä»½..."
pg_dump -h localhost -U postgres -d football_pred | gzip > "${BACKUP_DIR}/${BACKUP_FILE}.gz"

# éªŒè¯å¤‡ä»½æ–‡ä»¶
if [ -f "${BACKUP_DIR}/${BACKUP_FILE}.gz" ]; then
    echo "âœ… å¤‡ä»½æˆåŠŸ: ${BACKUP_FILE}.gz"
else
    echo "âŒ å¤‡ä»½å¤±è´¥"
    exit 1
fi

# æ¸…ç†æ—§å¤‡ä»½
echo "ğŸ§¹ æ¸…ç†${RETENTION_DAYS}å¤©å‰çš„å¤‡ä»½..."
find $BACKUP_DIR -name "*.gz" -mtime +$RETENTION_DAYS -delete

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
if [ ! -z "$AWS_S3_BUCKET" ]; then
    echo "â˜ï¸ ä¸Šä¼ å¤‡ä»½åˆ°S3..."
    aws s3 cp "${BACKUP_DIR}/${BACKUP_FILE}.gz" "s3://${AWS_S3_BUCKET}/backups/"
fi

echo "ğŸ‰ æ•°æ®åº“å¤‡ä»½å®Œæˆï¼"
```

#### 2. å¢é‡å¤‡ä»½
```bash
#!/bin/bash
# scripts/incremental_backup.sh

WAL_ARCHIVE_DIR="/backups/wal_archive"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºWALå½’æ¡£ç›®å½•
mkdir -p $WAL_ARCHIVE_DIR

# é…ç½®PostgreSQL WALå½’æ¡£
echo "archive_command = 'cp %p ${WAL_ARCHIVE_DIR}/%f'" >> /var/lib/postgresql/data/postgresql.conf

# é‡å¯PostgreSQLä»¥åº”ç”¨é…ç½®
docker-compose restart postgres

# åˆ›å»ºåŸºç¡€å¤‡ä»½
pg_basebackup -h localhost -U postgres -D "${WAL_ARCHIVE_DIR}/base_backup_${TIMESTAMP}" -Ft -z -P

echo "âœ… å¢é‡å¤‡ä»½é…ç½®å®Œæˆ"
```

### æ•°æ®æ¢å¤

#### 1. å®Œæ•´æ¢å¤
```bash
#!/bin/bash
# scripts/restore_database.sh

set -e

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "âŒ è¯·æä¾›å¤‡ä»½æ–‡ä»¶è·¯å¾„"
    echo "ç”¨æ³•: $0 <backup_file>"
    exit 1
fi

echo "ğŸ”„ å¼€å§‹æ•°æ®åº“æ¢å¤..."

# åœæ­¢åº”ç”¨æœåŠ¡
docker-compose stop app

# åˆ é™¤ç°æœ‰æ•°æ®åº“
echo "ğŸ—‘ï¸ åˆ é™¤ç°æœ‰æ•°æ®åº“..."
docker-compose exec postgres psql -U postgres -c "DROP DATABASE IF EXISTS football_pred;"

# åˆ›å»ºæ–°æ•°æ®åº“
echo "ğŸ“ åˆ›å»ºæ–°æ•°æ®åº“..."
docker-compose exec postgres psql -U postgres -c "CREATE DATABASE football_pred;"

# æ¢å¤æ•°æ®
echo "ğŸ“¥ æ¢å¤æ•°æ®..."
if [[ $BACKUP_FILE == *.gz ]]; then
    gunzip -c $BACKUP_FILE | docker-compose exec -T postgres psql -U postgres -d football_pred
else
    docker-compose exec -T postgres psql -U postgres -d football_pred < $BACKUP_FILE
fi

# é‡å¯åº”ç”¨æœåŠ¡
docker-compose start app

echo "âœ… æ•°æ®åº“æ¢å¤å®Œæˆï¼"
```

#### 2. æ—¶é—´ç‚¹æ¢å¤
```bash
#!/bin/bash
# scripts/point_in_time_recovery.sh

TARGET_TIME=$1  # æ ¼å¼: "2025-11-10 15:30:00"

if [ -z "$TARGET_TIME" ]; then
    echo "âŒ è¯·æä¾›ç›®æ ‡æ—¶é—´"
    echo "ç”¨æ³•: $0 \"YYYY-MM-DD HH:MM:SS\""
    exit 1
fi

echo "ğŸ•’ å¼€å§‹æ—¶é—´ç‚¹æ¢å¤åˆ°: $TARGET_TIME"

# åœæ­¢PostgreSQL
docker-compose stop postgres

# åˆ›å»ºæ¢å¤é…ç½®
cat > /var/lib/postgresql/data/recovery.conf << EOF
restore_command = 'cp ${WAL_ARCHIVE_DIR}/%f %p'
recovery_target_time = '$TARGET_TIME'
EOF

# å¯åŠ¨PostgreSQLï¼ˆæ¢å¤æ¨¡å¼ï¼‰
docker-compose start postgres

# ç›‘æ§æ¢å¤è¿›ç¨‹
echo "â³ ç­‰å¾…æ¢å¤å®Œæˆ..."
while docker-compose exec postgres pg_isready -q; do
    sleep 5
    echo "æ¢å¤ä¸­..."
done

echo "âœ… æ—¶é—´ç‚¹æ¢å¤å®Œæˆï¼"
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜è¯Šæ–­

#### 1. æœåŠ¡å¥åº·æ£€æŸ¥
```bash
#!/bin/bash
# scripts/health_check.sh

echo "ğŸ¥ Football Prediction System å¥åº·æ£€æŸ¥"

# æ£€æŸ¥DockeræœåŠ¡
echo "ğŸ“‹ DockeræœåŠ¡çŠ¶æ€:"
docker-compose ps

# æ£€æŸ¥APIå¥åº·çŠ¶æ€
echo "ğŸŒ APIå¥åº·çŠ¶æ€:"
curl -f http://localhost:8000/health || echo "âŒ APIæœåŠ¡å¼‚å¸¸"

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "ğŸ—„ï¸ æ•°æ®åº“è¿æ¥:"
docker-compose exec postgres pg_isready || echo "âŒ æ•°æ®åº“è¿æ¥å¤±è´¥"

# æ£€æŸ¥Redisè¿æ¥
echo "ğŸ’¾ Redisè¿æ¥:"
docker-compose exec redis redis-cli ping || echo "âŒ Redisè¿æ¥å¤±è´¥"

# æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "ğŸ’½ ç£ç›˜ä½¿ç”¨æƒ…å†µ:"
df -h

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
echo "ğŸ§  å†…å­˜ä½¿ç”¨æƒ…å†µ:"
free -h

# æ£€æŸ¥ç½‘ç»œè¿æ¥
echo "ğŸŒ ç½‘ç»œè¿æ¥:"
netstat -tlnp | grep -E ':(80|443|8000|5432|6379)'

# æŸ¥çœ‹æœ€è¿‘çš„é”™è¯¯æ—¥å¿—
echo "ğŸ“‹ æœ€è¿‘çš„é”™è¯¯æ—¥å¿—:"
docker-compose logs --tail=50 app | grep -i error || echo "âœ… æ— é”™è¯¯æ—¥å¿—"
```

#### 2. æ€§èƒ½è¯Šæ–­
```bash
#!/bin/bash
# scripts/performance_diagnosis.sh

echo "ğŸ“Š æ€§èƒ½è¯Šæ–­æŠ¥å‘Š"

# æ£€æŸ¥CPUä½¿ç”¨ç‡
echo "ğŸ”¥ CPUä½¿ç”¨ç‡:"
top -bn1 | grep "Cpu(s)" | awk '{print "CPUä½¿ç”¨ç‡:", $2}'

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
echo "ğŸ§  å†…å­˜ä½¿ç”¨:"
free -h

# æ£€æŸ¥æ•°æ®åº“æ€§èƒ½
echo "ğŸ—„ï¸ æ•°æ®åº“æ€§èƒ½:"
docker-compose exec postgres psql -U postgres -d football_pred -c "
SELECT
    schemaname,
    tablename,
    n_tup_ins as inserts,
    n_tup_upd as updates,
    n_tup_del as deletes,
    n_live_tup as live_tuples,
    n_dead_tup as dead_tuples
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC
LIMIT 10;"

# æ£€æŸ¥æ…¢æŸ¥è¯¢
echo "ğŸŒ æ…¢æŸ¥è¯¢:"
docker-compose exec postgres psql -U postgres -d football_pred -c "
SELECT query, mean_time, calls, total_time
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 5;"

# æ£€æŸ¥Redisæ€§èƒ½
echo "ğŸ’¾ Redisæ€§èƒ½:"
docker-compose exec redis redis-cli info memory | grep used_memory_human
docker-compose exec redis redis-cli info stats | grep keyspace

# æ£€æŸ¥APIå“åº”æ—¶é—´
echo "ğŸŒ APIå“åº”æ—¶é—´:"
curl -w "å“åº”æ—¶é—´: %{time_total}s\n" -o /dev/null -s http://localhost:8000/health
```

#### 3. æ—¥å¿—åˆ†æ
```bash
#!/bin/bash
# scripts/log_analysis.sh

LOG_DIR="./logs"
TODAY=$(date +%Y-%m-%d)

echo "ğŸ“‹ æ—¥å¿—åˆ†ææŠ¥å‘Š - $TODAY"

# APIè®¿é—®ç»Ÿè®¡
echo "ğŸ“Š APIè®¿é—®ç»Ÿè®¡:"
if [ -f "$LOG_DIR/access.log" ]; then
    awk '{print $1}' "$LOG_DIR/access.log" | sort | uniq -c | sort -nr | head -10
fi

# é”™è¯¯ç»Ÿè®¡
echo "âŒ é”™è¯¯ç»Ÿè®¡:"
if [ -f "$LOG_DIR/error.log" ]; then
    grep -c "ERROR" "$LOG_DIR/error.log" || echo "æ— é”™è¯¯è®°å½•"
fi

# æ•°æ®åº“æ…¢æŸ¥è¯¢
echo "ğŸŒ æ•°æ®åº“æ…¢æŸ¥è¯¢:"
if [ -f "$LOG_DIR/postgresql.log" ]; then
    grep "slow query" "$LOG_DIR/postgresql.log" | tail -5
fi

# åº”ç”¨å¼‚å¸¸
echo "âš ï¸ åº”ç”¨å¼‚å¸¸:"
if [ -f "$LOG_DIR/app.log" ]; then
    grep -i "exception\|error" "$LOG_DIR/app.log" | tail -10
fi
```

### ç´§æ€¥æ¢å¤ç¨‹åº

#### 1. æœåŠ¡é‡å¯è„šæœ¬
```bash
#!/bin/bash
# scripts/emergency_restart.sh

set -e

echo "ğŸš¨ ç´§æ€¥é‡å¯ç¨‹åº"

# è®°å½•é‡å¯æ—¶é—´
echo "é‡å¯æ—¶é—´: $(date)" >> /var/log/football-prediction-restarts.log

# åœæ­¢æ‰€æœ‰æœåŠ¡
echo "â¹ï¸ åœæ­¢æ‰€æœ‰æœåŠ¡..."
docker-compose down

# æ¸…ç†ç³»ç»Ÿèµ„æº
echo "ğŸ§¹ æ¸…ç†ç³»ç»Ÿèµ„æº..."
docker system prune -f

# é‡æ–°å¯åŠ¨æœåŠ¡
echo "â–¶ï¸ é‡æ–°å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# æ‰§è¡Œå¥åº·æ£€æŸ¥
echo "ğŸ¥ æ‰§è¡Œå¥åº·æ£€æŸ¥..."
./scripts/health_check.sh

echo "âœ… ç´§æ€¥é‡å¯å®Œæˆï¼"
```

#### 2. æ•°æ®åº“ä¿®å¤
```bash
#!/bin/bash
# scripts/repair_database.sh

echo "ğŸ”§ æ•°æ®åº“ä¿®å¤ç¨‹åº"

# æ£€æŸ¥æ•°æ®åº“ä¸€è‡´æ€§
echo "ğŸ” æ£€æŸ¥æ•°æ®åº“ä¸€è‡´æ€§..."
docker-compose exec postgres psql -U postgres -d football_pred -c "
SELECT
    schemaname,
    tablename,
    attname,
    n_distinct,
    correlation
FROM pg_stats
WHERE schemaname = 'public'
ORDER BY tablename, attname;"

# é‡å»ºç´¢å¼•
echo "ğŸ”¨ é‡å»ºç´¢å¼•..."
docker-compose exec postgres psql -U postgres -d football_pred -c "
REINDEX DATABASE football_pred;"

# æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯
echo "ğŸ“Š æ›´æ–°ç»Ÿè®¡ä¿¡æ¯..."
docker-compose exec postgres psql -U postgres -d football_pred -c "
ANALYZE;"

# æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
echo "âœ… æ£€æŸ¥æ•°æ®å®Œæ•´æ€§..."
docker-compose exec postgres psql -U postgres -d football_pred -c "
SELECT
    conname,
    conrelid::regclass as table_name,
    conkey
FROM pg_constraint
WHERE contype = 'f'
AND convalidated = false;"

echo "âœ… æ•°æ®åº“ä¿®å¤å®Œæˆï¼"
```

---

## ğŸ“ æ”¯æŒå’Œç»´æŠ¤

### ç›‘æ§å‘Šè­¦
- **Slacké›†æˆ**: é”™è¯¯é€šçŸ¥å‘é€åˆ°Slacké¢‘é“
- **é‚®ä»¶é€šçŸ¥**: å…³é”®é”™è¯¯å‘é€é‚®ä»¶å‘Šè­¦
- **çŸ­ä¿¡é€šçŸ¥**: ç´§æ€¥æ•…éšœå‘é€çŸ­ä¿¡æé†’

### å®šæœŸç»´æŠ¤
- **æ¯æ—¥**: å¤‡ä»½æ•°æ®åº“ï¼Œæ¸…ç†æ—¥å¿—æ–‡ä»¶
- **æ¯å‘¨**: åˆ†ææ€§èƒ½æŒ‡æ ‡ï¼Œæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
- **æ¯æœˆ**: å®‰å…¨æ›´æ–°ï¼Œæ€§èƒ½è°ƒä¼˜
- **æ¯å­£åº¦**: å®¹é‡è§„åˆ’ï¼Œæ¶æ„ä¼˜åŒ–

### è”ç³»æ”¯æŒ
- **æŠ€æœ¯æ”¯æŒ**: support@football-prediction.com
- **ç´§æ€¥å“åº”**: emergency@football-prediction.com
- **æ–‡æ¡£**: https://docs.football-prediction.com

---

**éƒ¨ç½²æŒ‡å—ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-11-10
**é€‚ç”¨äº**: Football Prediction System v1.0+
