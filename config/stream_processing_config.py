"""
流处理系统配置
生成时间：2025-10-26 20:57:22
"""

# Kafka配置
KAFKA_CONFIG = {
    "bootstrap_servers": ["localhost:9092"],
    "group_id": "football_prediction_group",
    "auto_offset_reset": True,
    "enable_auto_commit": False,
    "value_serializer": "org.apache.kafka.common.serialization.StringSerializer",
        "value_deserializer": "org.apache.kafka.common.serialization.StringDeserializer",
        "key_serializer": "org.apache.kafka.common.serialization.StringSerializer",
        "key_deserializer": "org.apache.kafka.common.serialization.StringDeserializer"
}

# 流处理主题配置
STREAM_TOPICS = {
    "match_events": {
        "topic": "match_events",
        "partitions": 3,
        "replication_factor": 1
    },
    "prediction_updates": {
        "topic": "prediction_updates",
        "partitions": 2,
        "replication_factor": 1
    },
    "user_activities": {
        "topic": "user_activities",
        "partitions": 2,
        "replication_factor": 1
    },
    "system_metrics": {
        "topic": "system_metrics",
        "partitions": 1,
        "replication_factor": 1
    }
}

# 流处理消费者配置
STREAM_CONSUMER_CONFIG = {
    "group_id": "football_prediction_consumer",
    "auto_offset_reset": True,
    "enable_auto_commit": False,
    "session_timeout_ms": 30000,
    "heartbeat_interval_ms": 3000,
    "max_poll_records": 100,
        "fetch_max_wait_ms": 500
}

# 流处理配置
STREAM_PROCESSING_CONFIG = {
    "batch_size": 100,
    "flush_interval": 1.0,
    "processing_timeout": 30.0,
    "error_handling": {
        "retry_attempts": 3,
        "retry_delay": 1.0,
        "dead_letter_queue": "dlq_stream_processing"
    },
    "monitoring": {
        "enabled": True,
        "metrics_interval": 60,
        "performance_tracking": True
    }
}

# 流处理使用示例
import asyncio
from kafka import KafkaConsumer, KafkaProducer
import json

class StreamProcessor:
    def __init__(self):
        self.consumer_config = STREAM_CONSUMER_CONFIG
        self.producer_config = KAFKA_CONFIG

    async def consume_events(self, topic: str, callback):
        """消费流事件"""
        print(f"开始消费主题: {topic}")

        # 这里应该是实际的Kafka消费者实现
        pass

    async def produce_events(self, topic: str, events: List[Dict]):
        """生产流事件"""
        print(f"生产事件到主题: {topic}")

        # 这里应该是实际的Kafka生产者实现
        for event in events:
            pass
