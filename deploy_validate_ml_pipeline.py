#!/usr/bin/env python3
"""
P0-4 ML Pipeline éƒ¨ç½²éªŒè¯è„šæœ¬
éªŒè¯æ–°ML Pipelineçš„æ ¸å¿ƒåŠŸèƒ½å’Œé›†æˆ
"""

import sys
import logging
import tempfile
from pathlib import Path
from typing import List, Dict, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ·»åŠ srcè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

class DeploymentValidator:
    """ML Pipelineéƒ¨ç½²éªŒè¯å™¨"""

    def __init__(self):
        self.test_results = []
        self.temp_dir = None

    def log_result(self, test_name: str, passed: bool, details: str = ""):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        status = "âœ… PASS" if passed else "âŒ FAIL"
        logger.info(f"{status} {test_name}: {details}")
        self.test_results.append({
            "test": test_name,
            "passed": passed,
            "details": details
        })

    def test_configuration_system(self) -> bool:
        """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
        logger.info("ğŸ”§ Testing Configuration System...")

        try:
            # æµ‹è¯•åŸºæœ¬é…ç½®åˆ›å»º
            from src.pipeline.config import PipelineConfig, ModelConfig, FeatureConfig

            # æµ‹è¯•é»˜è®¤é…ç½®
            config = PipelineConfig()
            assert config.model.default_algorithm == "xgboost"
            assert len(config.model.supported_algorithms) > 0

            # æµ‹è¯•é…ç½®éªŒè¯
            try:
                bad_config = PipelineConfig(
                    model={"default_algorithm": "invalid_algorithm"}
                )
                self.log_result("Config Validation", False, "Should reject invalid algorithm")
                return False
            except ValueError:
                self.log_result("Config Validation", True, "Correctly rejects invalid algorithm")

            # æµ‹è¯•dictè½¬æ¢ä¿®å¤
            dict_config = PipelineConfig(
                features={"required_features": ["test_feature"]},
                model={"default_algorithm": "lightgbm"}
            )
            assert isinstance(dict_config.features, FeatureConfig)
            assert isinstance(dict_config.model, ModelConfig)

            self.log_result("Configuration System", True, "All config tests passed")
            return True

        except Exception as e:
            self.log_result("Configuration System", False, str(e))
            return False

    def test_model_registry(self) -> bool:
        """æµ‹è¯•æ¨¡å‹æ³¨å†Œè¡¨"""
        logger.info("ğŸ—„ï¸ Testing Model Registry...")

        try:
            from src.pipeline.config import PipelineConfig
            from src.pipeline.model_registry import ModelRegistry

            # åˆ›å»ºä¸´æ—¶ç›®å½•
            self.temp_dir = tempfile.mkdtemp()
            config = PipelineConfig()
            config.models_dir = self.temp_dir

            # æµ‹è¯•æ³¨å†Œè¡¨åˆ›å»º
            registry = ModelRegistry(config)

            # æµ‹è¯•æ¨¡å‹ä¿å­˜
            from sklearn.ensemble import RandomForestClassifier
            import numpy as np

            # åˆ›å»ºç®€å•æ¨¡å‹
            model = RandomForestClassifier(n_estimators=2, random_state=42)
            X_train = np.random.random((10, 5))
            y_train = np.random.randint(0, 2, 10)
            model.fit(X_train, y_train)

            metadata = {
                "algorithm": "random_forest",
                "accuracy": 0.85,
                "features": ["feature1", "feature2"]
            }

            model_path = registry.save_model(model, "test_model", metadata)
            assert Path(model_path).exists()

            # æµ‹è¯•æ¨¡å‹åŠ è½½
            loaded_model, loaded_metadata = registry.load_model("test_model")
            assert loaded_model is not None
            assert loaded_metadata["algorithm"] == "random_forest"

            # æµ‹è¯•æ¨¡å‹æ¯”è¾ƒ
            comparison = registry.compare_models("test_model")
            assert len(comparison) > 0

            self.log_result("Model Registry", True, "Model save/load/compare working")
            return True

        except Exception as e:
            self.log_result("Model Registry", False, str(e))
            return False

    def test_feature_loader_interface(self) -> bool:
        """æµ‹è¯•ç‰¹å¾åŠ è½½å™¨æ¥å£"""
        logger.info("ğŸ”„ Testing Feature Loader Interface...")

        try:
            # æµ‹è¯•å¯¼å…¥æ˜¯å¦æ­£ç¡®
            from src.pipeline.feature_loader import FeatureLoader
            from src.features.feature_store_interface import FeatureStoreProtocol

            # éªŒè¯ç±»å®šä¹‰
            assert hasattr(FeatureLoader, 'load_training_data')
            assert hasattr(FeatureLoader, '_load_training_data_async')

            self.log_result("Feature Loader Interface", True, "Imports and class structure correct")
            return True

        except Exception as e:
            self.log_result("Feature Loader Interface", False, str(e))
            return False

    def test_trainer_interface(self) -> bool:
        """æµ‹è¯•è®­ç»ƒå™¨æ¥å£"""
        logger.info("ğŸ‹ï¸ Testing Trainer Interface...")

        try:
            from src.pipeline.config import PipelineConfig
            from src.pipeline.trainer import Trainer

            # æµ‹è¯•è®­ç»ƒå™¨åˆ›å»º
            config = PipelineConfig()
            trainer = Trainer(config)

            # éªŒè¯å¿…è¦æ–¹æ³•å­˜åœ¨
            assert hasattr(trainer, 'train')
            assert hasattr(trainer, 'training_history')

            # æµ‹è¯•å†å²è®°å½•åˆå§‹åŒ–
            assert isinstance(trainer.training_history, list)

            self.log_result("Trainer Interface", True, "Trainer class structure correct")
            return True

        except Exception as e:
            self.log_result("Trainer Interface", False, str(e))
            return False

    def test_pipeline_module_structure(self) -> bool:
        """æµ‹è¯•Pipelineæ¨¡å—ç»“æ„"""
        logger.info("ğŸ“ Testing Pipeline Module Structure...")

        try:
            from src.pipeline import FeatureLoader, Trainer, ModelRegistry, PipelineConfig

            # éªŒè¯æ¨¡å—å¯¼å‡º
            assert FeatureLoader is not None
            assert Trainer is not None
            assert ModelRegistry is not None
            assert PipelineConfig is not None

            # éªŒè¯å·¥ä½œæµæ¨¡å—
            try:
                from src.pipeline.flows import train_flow, eval_flow
                self.log_result("Pipeline Flows", True, "Flows module accessible")
            except ImportError as e:
                self.log_result("Pipeline Flows", False, f"Flow import error: {e}")
                return False

            self.log_result("Pipeline Module Structure", True, "All core components accessible")
            return True

        except Exception as e:
            self.log_result("Pipeline Module Structure", False, str(e))
            return False

    def test_integration_readiness(self) -> bool:
        """æµ‹è¯•é›†æˆå‡†å¤‡åº¦"""
        logger.info("ğŸ”— Testing Integration Readiness...")

        try:
            # æ£€æŸ¥å…³é”®æ–‡ä»¶å­˜åœ¨
            required_files = [
                "src/pipeline/__init__.py",
                "src/pipeline/config.py",
                "src/pipeline/feature_loader.py",
                "src/pipeline/trainer.py",
                "src/pipeline/model_registry.py",
                "src/features/feature_store_interface.py",
                "patches/pr_p0_4_ml_pipeline_fix.md"
            ]

            missing_files = []
            for file_path in required_files:
                if not Path(file_path).exists():
                    missing_files.append(file_path)

            if missing_files:
                self.log_result("Integration Readiness", False, f"Missing files: {missing_files}")
                return False

            # æ£€æŸ¥æ–‡æ¡£å®Œæ•´æ€§
            doc_content = Path("patches/pr_p0_4_ml_pipeline_fix.md").read_text()
            required_sections = ["ä¿®å¤å†…å®¹", "Root Cause", "éªŒè¯æ­¥éª¤", "éƒ¨ç½²æŒ‡å—"]

            missing_sections = []
            for section in required_sections:
                if section not in doc_content:
                    missing_sections.append(section)

            if missing_sections:
                self.log_result("Documentation Completeness", False, f"Missing sections: {missing_sections}")
            else:
                self.log_result("Documentation Completeness", True, "All required sections present")

            self.log_result("Integration Readiness", True, "All files and documentation ready")
            return True

        except Exception as e:
            self.log_result("Integration Readiness", False, str(e))
            return False

    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        if self.temp_dir and Path(self.temp_dir).exists():
            import shutil
            shutil.rmtree(self.temp_dir, ignore_errors=True)

    def run_all_tests(self) -> dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰éªŒè¯æµ‹è¯•"""
        logger.info("ğŸš€ Starting P0-4 ML Pipeline Deployment Validation")
        logger.info("=" * 60)

        tests = [
            ("Configuration System", self.test_configuration_system),
            ("Model Registry", self.test_model_registry),
            ("Feature Loader Interface", self.test_feature_loader_interface),
            ("Trainer Interface", self.test_trainer_interface),
            ("Pipeline Module Structure", self.test_pipeline_module_structure),
            ("Integration Readiness", self.test_integration_readiness),
        ]

        passed_tests = 0
        total_tests = len(tests)

        for test_name, test_func in tests:
            try:
                if test_func():
                    passed_tests += 1
            except Exception as e:
                logger.error(f"Test {test_name} crashed: {e}")
                self.log_result(test_name, False, f"Test crash: {e}")

        logger.info("=" * 60)
        logger.info(f"ğŸ“Š Validation Results: {passed_tests}/{total_tests} tests passed")

        # ç”ŸæˆæŠ¥å‘Š
        success_rate = passed_tests / total_tests
        if success_rate >= 0.8:
            logger.info("âœ… P0-4 ML Pipeline Deployment Validation: PASSED")
            status = "PASSED"
        else:
            logger.error("âŒ P0-4 ML Pipeline Deployment Validation: FAILED")
            status = "FAILED"

        return {
            "status": status,
            "passed_tests": passed_tests,
            "total_tests": total_tests,
            "success_rate": success_rate,
            "test_results": self.test_results
        }

def main():
    """ä¸»å‡½æ•°"""
    validator = DeploymentValidator()

    try:
        results = validator.run_all_tests()

        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = f"""
# P0-4 ML Pipeline éƒ¨ç½²éªŒè¯æŠ¥å‘Š

**éªŒè¯æ—¶é—´**: {Path(__file__).stat().st_mtime}
**éªŒè¯çŠ¶æ€**: {results['status']}
**é€šè¿‡ç‡**: {results['success_rate']:.1%} ({results['passed_tests']}/{results['total_tests']})

## è¯¦ç»†ç»“æœ
"""

        for result in results['test_results']:
            status_icon = "âœ…" if result['passed'] else "âŒ"
            report += f"{status_icon} **{result['test']}**: {result['details']}\n"

        # ä¿å­˜æŠ¥å‘Š
        report_path = Path("P0_4_DEPLOYMENT_VALIDATION_REPORT.md")
        report_path.write_text(report)

        logger.info(f"ğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        return results['status'] == "PASSED"

    finally:
        validator.cleanup()

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
