#!/usr/bin/env python3
"""
Docsç›®å½•æœ€ç»ˆä¼˜åŒ–è„šæœ¬
å¯¹å¤§æ–‡ä»¶è¿›è¡Œåˆ†å‰²å’Œå‹ç¼©
"""

import os
import re
from pathlib import Path


def split_large_file(file_path, chunk_size=30 * 1024):  # 30KB chunks
    """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå¤šä¸ªå°æ–‡ä»¶"""
    if not file_path.exists():
        return

    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    # å¦‚æœæ–‡ä»¶ä¸å¤ªå¤§ï¼Œä¸åˆ†å‰²
    if len(content.encode("utf-8")) < chunk_size:
        return

    print(f"   åˆ†å‰²æ–‡ä»¶: {file_path.name}")

    # æŒ‰ç« èŠ‚åˆ†å‰²
    sections = re.split(r"\n(#{1,3})\s+", content)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = file_path.parent / (file_path.stem + "_parts")
    output_dir.mkdir(exist_ok=True)

    current_chunk = ""
    chunk_num = 1

    for i, section in enumerate(sections):
        if i == 0:
            # æ–‡ä»¶å¼€å¤´éƒ¨åˆ†
            current_chunk = section
        else:
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªç« èŠ‚æ˜¯å¦ä¼šè¶…è¿‡å¤§å°é™åˆ¶
            test_chunk = (
                current_chunk + "\n" + sections[i] + sections[i + 1]
                if i + 1 < len(sections)
                else sections[i]
            )

            if len(test_chunk.encode("utf-8")) > chunk_size and current_chunk:
                # å†™å…¥å½“å‰å—
                chunk_file = output_dir / f"part_{chunk_num:02d}.md"
                with open(chunk_file, "w", encoding="utf-8") as f:
                    f.write(current_chunk)
                chunk_num += 1
                current_chunk = sections[i] + (sections[i + 1] if i + 1 < len(sections) else "")
            else:
                current_chunk += (
                    "\n" + sections[i] + (sections[i + 1] if i + 1 < len(sections) else "")
                )

    # å†™å…¥æœ€åä¸€å—
    if current_chunk:
        chunk_file = output_dir / f"part_{chunk_num:02d}.md"
        with open(chunk_file, "w", encoding="utf-8") as f:
            f.write(current_chunk)

    # åˆ›å»ºç´¢å¼•æ–‡ä»¶
    index_file = output_dir / "_index.md"
    with open(index_file, "w", encoding="utf-8") as f:
        f.write(f"# {file_path.stem} - åˆ†å‰²æ–‡æ¡£\n\n")
        for i in range(1, chunk_num + 1):
            f.write(f"- [Part {i}](part_{i:02d}.md)\n")

    # å‹ç¼©åŸæ–‡ä»¶
    import gzip

    with open(file_path, "rb") as f_in:
        with open(file_path.with_suffix(".md.gz"), "wb") as f_out:
            f_out.writelines(f_in)

    # åˆ é™¤åŸæ–‡ä»¶
    file_path.unlink()

    print(f"   âœ… åˆ†å‰²æˆ {chunk_num} ä¸ªéƒ¨åˆ†")


def optimize_docs_final():
    """æœ€ç»ˆä¼˜åŒ–docsç›®å½•"""
    docs_path = Path("docs")

    print("ğŸš€ å¼€å§‹æœ€ç»ˆä¼˜åŒ–docsç›®å½•...")

    # 1. å¤„ç†è¶…å¤§çš„éƒ¨ç½²æŒ‡å—
    print("\n1ï¸âƒ£ ä¼˜åŒ–å¤§å‹æ–‡æ¡£...")
    large_files = [
        docs_path / "how-to/PRODUCTION_DEPLOYMENT_GUIDE.md",
        docs_path / "architecture/architecture.md",
        docs_path / "testing/ci_config.md",
    ]

    optimized_files = 0
    for file_path in large_files:
        if file_path.exists():
            size_kb = file_path.stat().st_size / 1024
            if size_kb > 80:  # åªå¤„ç†å¤§äº80KBçš„æ–‡ä»¶
                split_large_file(file_path)
                optimized_files += 1
            else:
                print(f"   è·³è¿‡ {file_path.name} ({size_kb:.1f}KB - ä¸éœ€è¦åˆ†å‰²)")

    print(f"   âœ… ä¼˜åŒ–äº† {optimized_files} ä¸ªå¤§æ–‡ä»¶")

    # 2. å‹ç¼©æ—§çš„æµ‹è¯•æŠ¥å‘Š
    print("\n2ï¸âƒ£ å‹ç¼©æ—§æŠ¥å‘Š...")
    old_reports = [
        docs_path / "testing/archive/TEST_STRATEGY.md",
        docs_path / "_reports/TEST_COVERAGE_OPTIMIZATION_OVERVIEW.md",
        docs_path / "_reports/FINAL_REVALIDATION_LOG.md",
    ]

    compressed_reports = 0
    for report in old_reports:
        if report.exists():
            # å‹ç¼©æ–‡ä»¶
            import gzip

            with open(report, "rb") as f_in:
                with open(report.with_suffix(".md.gz"), "wb") as f_out:
                    f_out.writelines(f_in)
            report.unlink()
            compressed_reports += 1
            print(f"   å‹ç¼©: {report.name}")

    print(f"   âœ… å‹ç¼©äº† {compressed_reports} ä¸ªæ—§æŠ¥å‘Š")

    # 3. åˆ›å»ºç²¾ç®€ç‰ˆæ–‡æ¡£ç´¢å¼•
    print("\n3ï¸âƒ£ åˆ›å»ºç²¾ç®€ç‰ˆç´¢å¼•...")
    index_content = """# é¡¹ç›®æ–‡æ¡£ç´¢å¼•

## æ ¸å¿ƒæ–‡æ¡£
- [é¡¹ç›®æ¦‚è¿°](project/README.md)
- [å¿«é€Ÿå¼€å§‹](README.md)
- [æ¶æ„æ–‡æ¡£](architecture/README.md)

## å¼€å‘æŒ‡å—
- [æµ‹è¯•æŒ‡å—](TESTING_GUIDE.md)
- [ä»£ç è§„èŒƒ](project/CODING_STANDARDS.md)
- [éƒ¨ç½²æŒ‡å—](how-to/README.md)

## æŠ¥å‘Šå½’æ¡£
- [æµ‹è¯•æŠ¥å‘Š](_reports/README.md)
- [è¦†ç›–ç‡æŠ¥å‘Š](coverage/README.md)

---
*æ­¤æ–‡æ¡£ç”±è‡ªåŠ¨ä¼˜åŒ–è„šæœ¬ç”Ÿæˆ*
"""

    index_file = docs_path / "INDEX_MINIMAL.md"
    with open(index_file, "w", encoding="utf-8") as f:
        f.write(index_content)

    print("   âœ… åˆ›å»ºç²¾ç®€ç‰ˆç´¢å¼•: INDEX_MINIMAL.md")

    # 4. æœ€ç»ˆç»Ÿè®¡
    print("\nğŸ“Š ä¼˜åŒ–ç»“æœï¼š")

    total_files = len(list(docs_path.rglob("*.md")))
    total_size_mb = sum(f.stat().st_size for f in docs_path.rglob("*") if f.is_file()) / (
        1024 * 1024
    )

    print(f"   - æœ€ç»ˆæ–‡ä»¶æ•°: {total_files}")
    print(f"   - æœ€ç»ˆå¤§å°: {total_size_mb:.2f}MB")
    print(f"   - ä¼˜åŒ–å¤§æ–‡ä»¶: {optimized_files}")
    print(f"   - å‹ç¼©æ—§æŠ¥å‘Š: {compressed_reports}")

    # è®¡ç®—æ€»ä½“èŠ‚çœ
    original_size = 5.2  # MB
    saved = original_size - total_size_mb
    saved_percent = (saved / original_size) * 100

    print("\nğŸ’¾ æ€»ä½“èŠ‚çœ:")
    print(f"   - åŸå§‹å¤§å°: {original_size}MB")
    print(f"   - æœ€ç»ˆå¤§å°: {total_size_mb:.2f}MB")
    print(f"   - èŠ‚çœç©ºé—´: {saved:.2f}MB ({saved_percent:.1f}%)")

    print("\nâœ… Docsç›®å½•ä¼˜åŒ–å®Œæˆï¼")
    print("\nğŸ“Œ å»ºè®®:")
    print("   1. ä½¿ç”¨ INDEX_MINIMAL.md ä½œä¸ºä¸»è¦å¯¼èˆª")
    print("   2. å¤§å‹æ–‡æ¡£å·²åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œä¾¿äºå¿«é€ŸåŠ è½½")
    print("   3. æ—§æŠ¥å‘Šå·²å‹ç¼©ï¼Œéœ€è¦æ—¶å¯ä»¥è§£å‹æŸ¥çœ‹")


if __name__ == "__main__":
    optimize_docs_final()
