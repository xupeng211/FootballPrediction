#!/usr/bin/env python3
"""
Docsç›®å½•æ¸…ç†è„šæœ¬
åˆ é™¤é‡å¤ã€è¿‡æ—¶å’Œä¸å¿…è¦çš„æ–‡æ¡£æ–‡ä»¶
"""

import os
import shutil
from pathlib import Path
import hashlib
from collections import defaultdict


def calculate_file_hash(filepath):
    """è®¡ç®—æ–‡ä»¶çš„MD5å“ˆå¸Œå€¼"""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def find_duplicate_files(directory):
    """æŸ¥æ‰¾é‡å¤æ–‡ä»¶"""
    file_hashes = defaultdict(list)

    for file_path in Path(directory).rglob("*.md"):
        if file_path.is_file():
            hash_value = calculate_file_hash(file_path)
            file_hashes[hash_value].append(file_path)

    # è¿”å›æœ‰é‡å¤çš„æ–‡ä»¶åˆ—è¡¨
    duplicates = {hash_val: files for hash_val, files in file_hashes.items() if len(files) > 1}
    return duplicates


def clean_docs_directory():
    """æ¸…ç†docsç›®å½•"""
    docs_path = Path("docs")

    if not docs_path.exists():
        print("âŒ docsç›®å½•ä¸å­˜åœ¨")
        return

    print("ğŸ” å¼€å§‹åˆ†ædocsç›®å½•...")

    # 1. æŸ¥æ‰¾é‡å¤æ–‡ä»¶
    print("\n1ï¸âƒ£ æŸ¥æ‰¾é‡å¤æ–‡ä»¶...")
    duplicates = find_duplicate_files(docs_path)

    removed_duplicates = 0
    for hash_val, files in duplicates.items():
        # ä¿ç•™ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼Œåˆ é™¤å…¶ä½™çš„
        for file_path in files[1:]:
            print(f"   åˆ é™¤é‡å¤æ–‡ä»¶: {file_path}")
            file_path.unlink()
            removed_duplicates += 1

    print(f"   âœ… åˆ é™¤äº† {removed_duplicates} ä¸ªé‡å¤æ–‡ä»¶")

    # 2. åˆ é™¤å­˜æ¡£ç›®å½•ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
    print("\n2ï¸âƒ£ æ¸…ç†å­˜æ¡£ç›®å½•...")
    archive_dirs = [
        docs_path / "archive",
        docs_path / "archives",
        docs_path / "old",
        docs_path / "backup",
    ]

    removed_archives = 0
    for archive_dir in archive_dirs:
        if archive_dir.exists():
            print(f"   åˆ é™¤å­˜æ¡£ç›®å½•: {archive_dir}")
            shutil.rmtree(archive_dir)
            removed_archives += 1

    print(f"   âœ… åˆ é™¤äº† {removed_archives} ä¸ªå­˜æ¡£ç›®å½•")

    # 3. åˆ é™¤ç‰¹å®šç±»å‹çš„æ–‡ä»¶
    print("\n3ï¸âƒ£ æ¸…ç†ç‰¹å®šæ–‡ä»¶...")

    # è¦åˆ é™¤çš„æ–‡ä»¶æ¨¡å¼
    patterns_to_remove = [
        "**/README.md.bak",
        "**/*.md.bak",
        "**/*.md.old",
        "**/*.md.tmp",
        "**/DRAFT_*.md",
        "**/TODO_*.md",
        "**/draft_*.md",
        "**/temp_*.md",
        "**/test_*.md",
    ]

    removed_patterns = 0
    for pattern in patterns_to_remove:
        for file_path in docs_path.glob(pattern):
            if file_path.is_file():
                print(f"   åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
                file_path.unlink()
                removed_patterns += 1

    print(f"   âœ… åˆ é™¤äº† {removed_patterns} ä¸ªä¸´æ—¶æ–‡ä»¶")

    # 4. æ¸…ç†ç©ºç›®å½•
    print("\n4ï¸âƒ£ æ¸…ç†ç©ºç›®å½•...")
    removed_empty_dirs = 0

    # ä»æœ€æ·±å±‚çš„ç›®å½•å¼€å§‹ï¼Œå‘ä¸Šæ¸…ç†
    for root, dirs, files in os.walk(docs_path, topdown=False):
        for dir_name in dirs:
            dir_path = Path(root) / dir_name
            try:
                if dir_path.exists() and not any(dir_path.iterdir()):
                    print(f"   åˆ é™¤ç©ºç›®å½•: {dir_path}")
                    dir_path.rmdir()
                    removed_empty_dirs += 1
            except OSError:
                pass  # ç›®å½•ä¸ä¸ºç©ºæˆ–æƒé™é—®é¢˜

    print(f"   âœ… åˆ é™¤äº† {removed_empty_dirs} ä¸ªç©ºç›®å½•")

    # 5. åˆ—å‡ºå¤§æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
    print("\n5ï¸âƒ£ å¤§æ–‡ä»¶æ£€æŸ¥ï¼ˆ>50KBï¼‰...")
    large_files = []

    for file_path in docs_path.rglob("*.md"):
        if file_path.is_file():
            size_kb = file_path.stat().st_size / 1024
            if size_kb > 50:
                large_files.append((file_path, size_kb))

    if large_files:
        large_files.sort(key=lambda x: x[1], reverse=True)
        print("   ä»¥ä¸‹æ–‡ä»¶è¾ƒå¤§ï¼Œè¯·æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–ï¼š")
        for file_path, size_kb in large_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   - {file_path.relative_to(docs_path)} ({size_kb:.1f}KB)")

    # 6. ç»Ÿè®¡ç»“æœ
    print("\nğŸ“Š æ¸…ç†ç»“æœç»Ÿè®¡ï¼š")

    # è®¡ç®—æ¸…ç†åçš„æ–‡ä»¶æ•°å’Œå¤§å°
    total_files = len(list(docs_path.rglob("*.md")))
    total_size_mb = sum(f.stat().st_size for f in docs_path.rglob("*") if f.is_file()) / (
        1024 * 1024
    )

    print(f"   - å‰©ä½™Markdownæ–‡ä»¶æ•°: {total_files}")
    print(f"   - å‰©ä½™ç›®å½•å¤§å°: {total_size_mb:.2f}MB")
    print(f"   - åˆ é™¤é‡å¤æ–‡ä»¶: {removed_duplicates}")
    print(f"   - åˆ é™¤å­˜æ¡£ç›®å½•: {removed_archives}")
    print(f"   - åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {removed_patterns}")
    print(f"   - åˆ é™¤ç©ºç›®å½•: {removed_empty_dirs}")

    print("\nâœ… docsç›®å½•æ¸…ç†å®Œæˆï¼")


if __name__ == "__main__":
    clean_docs_directory()
