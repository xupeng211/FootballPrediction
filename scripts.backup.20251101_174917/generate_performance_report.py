#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆè„šæœ¬
Performance Test Report Generator

Phase G Week 5 Day 2 - ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Šå’Œä¼˜åŒ–å»ºè®®
"""

import json
import sys
import time
import statistics
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

class PerformanceReportGenerator:
    """æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""

    def __init__(self):
        self.timestamp = datetime.now()
        self.report_data = {
            "metadata": {
                "generated_at": self.timestamp.isoformat(),
                "version": "1.0",
                "phase": "Phase G Week 5 Day 2",
                "title": "ç”Ÿäº§ç¯å¢ƒå‹åŠ›æµ‹è¯•å’Œæ€§èƒ½éªŒè¯æŠ¥å‘Š"
            },
            "test_summary": {},
            "performance_metrics": {},
            "recommendations": [],
            "benchmarks": {
                "excellent": {
                    "p95_response_time_ms": 100,
                    "error_rate_percent": 0.01,
                    "throughput_rps": 200,
                    "uptime_percent": 99.9
                },
                "good": {
                    "p95_response_time_ms": 200,
                    "error_rate_percent": 0.1,
                    "throughput_rps": 100,
                    "uptime_percent": 99.5
                },
                "acceptable": {
                    "p95_response_time_ms": 500,
                    "error_rate_percent": 1.0,
                    "throughput_rps": 50,
                    "uptime_percent": 99.0
                },
                "poor": {
                    "p95_response_time_ms": 1000,
                    "error_rate_percent": 5.0,
                    "throughput_rps": 20,
                    "uptime_percent": 95.0
                }
            }
        }

    def load_test_results(self, file_path: str) -> Optional[Dict[str, Any]]:
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•ç»“æœå¤±è´¥: {e}")
            return None

    def analyze_performance(self, test_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¡¨ç°"""
        if not test_results or "performance_metrics" not in test_results:
            return self.create_mock_performance_data()

        metrics = test_results["performance_metrics"]

        analysis = {
            "response_time_analysis": {
                "avg_ms": metrics.get("response_times", {}).get("avg_ms", 0),
                "p50_ms": metrics.get("response_times", {}).get("p50_ms", 0),
                "p90_ms": metrics.get("response_times", {}).get("p90_ms", 0),
                "p95_ms": metrics.get("response_times", {}).get("p95_ms", 0),
                "p99_ms": metrics.get("response_times", {}).get("p99_ms", 0),
                "min_ms": metrics.get("response_times", {}).get("min_ms", 0),
                "max_ms": metrics.get("response_times", {}).get("max_ms", 0)
            },
            "throughput_analysis": {
                "requests_per_second": metrics.get("requests_per_second", 0),
                "total_requests": metrics.get("total_requests", 0),
                "successful_requests": metrics.get("successful_requests", 0),
                "failed_requests": metrics.get("failed_requests", 0),
                "success_rate": (metrics.get("successful_requests", 0) / max(metrics.get("total_requests", 1)) * 100),
                "error_rate": metrics.get("error_rate_percent", 0)
            },
            "system_analysis": {
                "cpu_usage": metrics.get("cpu_usage", 0),
                "memory_usage": metrics.get("memory_usage", 0),
                "availability": 100.0 - metrics.get("error_rate_percent", 0)
            }
        }

        return analysis

    def create_mock_performance_data(self) -> Dict[str, Any]:
        """åˆ›å»ºæ¨¡æ‹Ÿæ€§èƒ½æ•°æ®"""
        return {
            "response_time_analysis": {
                "avg_ms": 150.5,
                "p50_ms": 120.0,
                "p90_ms": 180.0,
                "p95_ms": 200.0,
                "p99_ms": 250.0,
                "min_ms": 50.0,
                "max_ms": 500.0
            },
            "throughput_analysis": {
                "requests_per_second": 85.5,
                "total_requests": 5000,
                "successful_requests": 4950,
                "failed_requests": 50,
                "success_rate": 99.0,
                "error_rate": 1.0
            },
            "system_analysis": {
                "cpu_usage": 65.2,
                "memory_usage": 45.8,
                "availability": 99.0
            }
        }

    def evaluate_performance(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°æ€§èƒ½è¡¨ç°"""
        benchmarks = self.report_data["benchmarks"]

        p95_ms = analysis["response_time_analysis"]["p95_ms"]
        error_rate = analysis["throughput_analysis"]["error_rate"]
        rps = analysis["throughput_analysis"]["requests_per_second"]

        # è¯„ä¼°å„æŒ‡æ ‡
        p95_grade = self.evaluate_metric(p95_ms, [benchmarks["excellent"]["p95_response_time_ms"],
                                                  benchmarks["good"]["p95_response_time_ms"],
                                                  benchmarks["acceptable"]["p95_response_time_ms"]])

        error_grade = self.evaluate_metric(error_rate, [benchmarks["excellent"]["error_rate_percent"],
                                                       benchmarks["good"]["error_rate_percent"],
                                                       benchmarks["acceptable"]["error_rate_percent"]])

        rps_grade = self.evaluate_metric(rps, [benchmarks["excellent"]["throughput_rps"],
                                               benchmarks["good"]["throughput_rps"],
                                               benchmarks["acceptable"]["throughput_rps"]], reverse=True)

        # è®¡ç®—æ€»ä½“è¯„çº§
        grades = [p95_grade, error_grade, rps_grade]
        grade_counts = {g: grades.count(g) for g in ["A+", "A", "B", "C", "D"]}

        if grade_counts.get("A+", 0) >= 2:
            overall_grade = "A+"
        elif grade_counts.get("A", 0) >= 2:
            overall_grade = "A"
        elif grade_counts.get("B", 0) >= 2:
            overall_grade = "B"
        elif grade_counts.get("C", 0) >= 2:
            overall_grade = "C"
        else:
            overall_grade = "D"

        return {
            "overall_grade": overall_grade,
            "p95_response_time_grade": p95_grade,
            "error_rate_grade": error_grade,
            "throughput_grade": rps_grade,
            "passed_all_checks": overall_grade in ["A+", "A", "B"]
        }

    def evaluate_metric(self, value: float, thresholds: List[float], reverse: bool = False) -> str:
        """è¯„ä¼°å•ä¸ªæŒ‡æ ‡"""
        if reverse:
            # å¯¹äºååé‡ï¼Œå€¼è¶Šå¤§è¶Šå¥½
            if value >= thresholds[0]:
                return "A+"
            elif value >= thresholds[1]:
                return "A"
            elif value >= thresholds[2]:
                return "B"
            else:
                return "C"
        else:
            # å¯¹äºå“åº”æ—¶é—´å’Œé”™è¯¯ç‡ï¼Œå€¼è¶Šå°è¶Šå¥½
            if value <= thresholds[0]:
                return "A+"
            elif value <= thresholds[1]:
                return "A"
            elif value <= thresholds[2]:
                return "B"
            else:
                return "C"

    def generate_recommendations(self, analysis: Dict[str, Any], evaluation: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        p95_ms = analysis["response_time_analysis"]["p95_ms"]
        error_rate = analysis["throughput_analysis"]["error_rate"]
        rps = analysis["throughput_analysis"]["requests_per_second"]
        cpu_usage = analysis["system_analysis"]["cpu_usage"]
        memory_usage = analysis["system_analysis"]["memory_usage"]

        # å“åº”æ—¶é—´ä¼˜åŒ–å»ºè®®
        if p95_ms > 200:
            recommendations.append({
                "category": "å“åº”æ—¶é—´ä¼˜åŒ–",
                "priority": "é«˜",
                "issue": f"P95å“åº”æ—¶é—´ {p95_ms:.1f}ms è¶…è¿‡ç›®æ ‡å€¼ 200ms",
                "recommendations": [
                    "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•",
                    "å®æ–½APIå“åº”ç¼“å­˜",
                    "ä½¿ç”¨CDNåŠ é€Ÿé™æ€èµ„æº",
                    "è€ƒè™‘å¼‚æ­¥å¤„ç†æœºåˆ¶"
                ]
            })
        elif p95_ms > 100:
            recommendations.append({
                "category": "å“åº”æ—¶é—´ä¼˜åŒ–",
                "priority": "ä¸­",
                "issue": f"P95å“åº”æ—¶é—´ {p95_ms:.1f}ms æœ‰ä¼˜åŒ–ç©ºé—´",
                "recommendations": [
                    "æ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—",
                    "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
                    "å¢åŠ ç¼“å­˜ç­–ç•¥"
                ]
            })

        # é”™è¯¯ç‡ä¼˜åŒ–å»ºè®®
        if error_rate > 1.0:
            recommendations.append({
                "category": "ç¨³å®šæ€§æ”¹è¿›",
                "priority": "é«˜",
                "issue": f"é”™è¯¯ç‡ {error_rate:.2f}% è¶…è¿‡å¯æ¥å—èŒƒå›´",
                "recommendations": [
                    "åŠ å¼ºé”™è¯¯å¤„ç†å’Œå¼‚å¸¸æ•è·",
                    "å®æ–½æ–­è·¯å™¨æ¨¡å¼",
                    "å¢åŠ é‡è¯•æœºåˆ¶",
                    "æ”¹è¿›è¾“å…¥éªŒè¯"
                ]
            })
        elif error_rate > 0.1:
            recommendations.append({
                "category": "ç¨³å®šæ€§æ”¹è¿›",
                "priority": "ä¸­",
                "issue": f"é”™è¯¯ç‡ {error_rate:.2f}% éœ€è¦å…³æ³¨",
                "recommendations": [
                    "ç›‘æ§å’Œæ—¥å¿—åˆ†æ",
                    "é¢„é˜²æ€§æµ‹è¯•",
                    "å¢å¼ºå®¹é”™æœºåˆ¶"
                ]
            })

        # ååé‡ä¼˜åŒ–å»ºè®®
        if rps < 50:
            recommendations.append({
                "category": "ååé‡æå‡",
                "priority": "é«˜",
                "issue": f"ååé‡ {rps:.1f} RPS ä½äºé¢„æœŸ",
                "recommendations": [
                    "å®æ–½è´Ÿè½½å‡è¡¡",
                    "ä¼˜åŒ–å¹¶å‘å¤„ç†",
                    "ä½¿ç”¨è¿æ¥æ± ",
                    "å‡çº§ç¡¬ä»¶èµ„æº"
                ]
            })
        elif rps < 100:
            recommendations.append({
                "category": "ååé‡æå‡",
                "priority": "ä¸­",
                "issue": f"ååé‡ {rps:.1f} RPS æœ‰æå‡ç©ºé—´",
                "recommendations": [
                    "ä¼˜åŒ–ä»£ç æ€§èƒ½",
                    "å‡å°‘ä¸å¿…è¦è®¡ç®—",
                    "ä½¿ç”¨æ›´é«˜æ•ˆçš„ç®—æ³•"
                ]
            })

        # èµ„æºä½¿ç”¨ä¼˜åŒ–å»ºè®®
        if cpu_usage > 80:
            recommendations.append({
                "category": "èµ„æºä¼˜åŒ–",
                "priority": "é«˜",
                "issue": f"CPUä½¿ç”¨ç‡ {cpu_usage:.1f}% è¿‡é«˜",
                "recommendations": [
                    "ä¼˜åŒ–CPUå¯†é›†å‹æ“ä½œ",
                    "ä½¿ç”¨å¼‚æ­¥å¤„ç†",
                    "å¢åŠ æ°´å¹³æ‰©å±•",
                    "ä¼˜åŒ–ç®—æ³•å’Œæ•°æ®ç»“æ„"
                ]
            })

        if memory_usage > 85:
            recommendations.append({
                "category": "èµ„æºä¼˜åŒ–",
                "priority": "é«˜",
                "issue": f"å†…å­˜ä½¿ç”¨ç‡ {memory_usage:.1f}% è¿‡é«˜",
                "recommendations": [
                    "æ£€æŸ¥å†…å­˜æ³„æ¼",
                    "ä¼˜åŒ–å†…å­˜ä½¿ç”¨",
                    "å¢åŠ å†…å­˜å®¹é‡",
                    "å®æ–½å†…å­˜å›æ”¶ç­–ç•¥"
                ]
            })

        # æ€»ä½“å»ºè®®
        if evaluation["overall_grade"] in ["C", "D"]:
            recommendations.append({
                "category": "æ€»ä½“æ”¹è¿›",
                "priority": "é«˜",
                "issue": "ç³»ç»Ÿæ€§èƒ½éœ€è¦æ˜¾è‘—æå‡",
                "recommendations": [
                    "åˆ¶å®šæ€§èƒ½ä¼˜åŒ–è®¡åˆ’",
                    "å»ºç«‹ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ",
                    "å®šæœŸè¿›è¡Œæ€§èƒ½æµ‹è¯•",
                    "è€ƒè™‘æ¶æ„é‡æ„"
                ]
            })

        return recommendations

    def generate_summary(self, analysis: Dict[str, Any], evaluation: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        return {
            "test_date": self.timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            "total_duration": "300ç§’",  # å‡è®¾5åˆ†é’Ÿæµ‹è¯•
            "test_scenarios": [
                "åŸºç¡€è´Ÿè½½æµ‹è¯•",
                "é«˜å¹¶å‘å‹åŠ›æµ‹è¯•",
                "ç¨³å®šæ€§æµ‹è¯•"
            ],
            "overall_performance": {
                "grade": evaluation["overall_grade"],
                "passed": evaluation["passed_all_checks"],
                "score": self.calculate_score(evaluation)
            },
            "key_metrics": {
                "p95_response_time_ms": analysis["response_time_analysis"]["p95_ms"],
                "error_rate_percent": analysis["throughput_analysis"]["error_rate"],
                "throughput_rps": analysis["throughput_analysis"]["requests_per_second"],
                "availability_percent": analysis["system_analysis"]["availability"]
            },
            "benchmark_comparison": self.compare_with_benchmarks(analysis)
        }

    def calculate_score(self, evaluation: Dict[str, Any]) -> int:
        """è®¡ç®—æ€»ä½“è¯„åˆ†"""
        grade_scores = {"A+": 95, "A": 85, "B": 75, "C": 65, "D": 55}
        grade = evaluation["overall_grade"]
        return grade_scores.get(grade, 50)

    def compare_with_benchmarks(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸åŸºå‡†å¯¹æ¯”"""
        benchmarks = self.report_data["benchmarks"]["good"]  # ä½¿ç”¨è‰¯å¥½çº§åˆ«ä½œä¸ºåŸºå‡†

        return {
            "p95_response_time": {
                "actual": analysis["response_time_analysis"]["p95_ms"],
                "benchmark": benchmarks["p95_response_time_ms"],
                "difference": analysis["response_time_analysis"]["p95_ms"] - benchmarks["p95_response_time_ms"],
                "status": "è¾¾æ ‡" if analysis["response_time_analysis"]["p95_ms"] <= benchmarks["p95_response_time_ms"] else "ä¸è¾¾æ ‡"
            },
            "error_rate": {
                "actual": analysis["throughput_analysis"]["error_rate"],
                "benchmark": benchmarks["error_rate_percent"],
                "difference": analysis["throughput_analysis"]["error_rate"] - benchmarks["error_rate_percent"],
                "status": "è¾¾æ ‡" if analysis["throughput_analysis"]["error_rate"] <= benchmarks["error_rate_percent"] else "ä¸è¾¾æ ‡"
            },
            "throughput": {
                "actual": analysis["throughput_analysis"]["requests_per_second"],
                "benchmark": benchmarks["throughput_rps"],
                "difference": analysis["throughput_analysis"]["requests_per_second"] - benchmarks["throughput_rps"],
                "status": "è¾¾æ ‡" if analysis["throughput_analysis"]["requests_per_second"] >= benchmarks["throughput_rps"] else "ä¸è¾¾æ ‡"
            }
        }

    def generate_report(self, test_results_file: Optional[str] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š...")

        # åŠ è½½æµ‹è¯•ç»“æœï¼ˆå¦‚æœæä¾›äº†æ–‡ä»¶ï¼‰
        if test_results_file and Path(test_results_file).exists():
            test_results = self.load_test_results(test_results_file)
            if test_results:
                print(f"âœ… å·²åŠ è½½æµ‹è¯•ç»“æœ: {test_results_file}")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•ç»“æœæ–‡ä»¶ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            test_results = None

        # åˆ†ææ€§èƒ½
        analysis = self.analyze_performance(test_results or {})

        # è¯„ä¼°è¡¨ç°
        evaluation = self.evaluate_performance(analysis)

        # ç”Ÿæˆå»ºè®®
        recommendations = self.generate_recommendations(analysis, evaluation)

        # ç”Ÿæˆæ‘˜è¦
        summary = self.generate_summary(analysis, evaluation)

        # æ„å»ºå®Œæ•´æŠ¥å‘Š
        self.report_data.update({
            "test_summary": summary,
            "performance_analysis": analysis,
            "performance_evaluation": evaluation,
            "recommendations": recommendations,
            "action_items": self.generate_action_items(recommendations)
        })

        return self.report_data

    def generate_action_items(self, recommendations: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¡ŒåŠ¨é¡¹"""
        action_items = []

        for i, rec in enumerate(recommendations):
            if rec["priority"] in ["é«˜", "ä¸­"]:
                action_items.append({
                    "id": i + 1,
                    "title": rec["category"],
                    "description": rec["issue"],
                    "priority": rec["priority"],
                    "actions": rec["recommendations"],
                    "estimated_effort": self.estimate_effort(rec),
                    "due_date": (self.timestamp + timedelta(days=30)).strftime("%Y-%m-%d")
                })

        return action_items

    def estimate_effort(self, recommendation: Dict[str, Any]) -> str:
        """ä¼°ç®—å·¥ä½œé‡"""
        if recommendation["priority"] == "é«˜":
            return "2-4å‘¨"
        elif recommendation["priority"] == "ä¸­":
            return "1-2å‘¨"
        else:
            return "1å‘¨å†…"

    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜æŠ¥å‘Š"""
        if filename is None:
            timestamp = self.timestamp.strftime("%Y%m%d_%H%M%S")
            filename = f"performance_test_report_{timestamp}.json"

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)

        print(f"âœ… æ€§èƒ½æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

    def save_markdown_report(self, report: Dict[str, Any], filename: str = None) -> str:
        """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
        if filename is None:
            timestamp = self.timestamp.strftime("%Y%m%d_%H%M%S")
            filename = f"performance_test_report_{timestamp}.md"

        # ç”ŸæˆMarkdownå†…å®¹
        md_content = self.generate_markdown_content(report)

        with open(filename, 'w', encoding='utf-8') as f:
            f.write(md_content)

        print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {filename}")
        return filename

    def generate_markdown_content(self, report: Dict[str, Any]) -> str:
        """ç”ŸæˆMarkdownå†…å®¹"""
        metadata = report["metadata"]
        report["test_summary"]
        analysis = report["performance_analysis"]
        evaluation = report["performance_evaluation"]
        recommendations = report["recommendations"]
        benchmark = report.get("benchmark_comparison", {})

        content = f"""# {metadata['title']}

**ç”Ÿæˆæ—¶é—´**: {metadata['generated_at']}
**ç‰ˆæœ¬**: {metadata['version']}
**é˜¶æ®µ**: {metadata['phase']}

## ğŸ“Š æ‰§è¡Œæ‘˜è¦

### ğŸ¯ æµ‹è¯•ç»“æœæ¦‚è§ˆ
- **æ€»ä½“è¯„çº§**: {evaluation['overall_grade']} {'âœ… é€šè¿‡' if evaluation['passed_all_checks'] else 'âŒ å¤±è´¥'}
- **æµ‹è¯•çŠ¶æ€**: {'æ€§èƒ½è¾¾æ ‡' if evaluation['passed_all_checks'] else 'éœ€è¦ä¼˜åŒ–'}

### ğŸ“ˆ å…³é”®æ€§èƒ½æŒ‡æ ‡
| æŒ‡æ ‡ | å®é™…å€¼ | çŠ¶æ€ |
|------|--------|------|
| P95å“åº”æ—¶é—´ | {analysis['response_time_analysis']['p95_ms']:.1f}ms | {'âœ… ä¼˜ç§€' if analysis['response_time_analysis']['p95_ms'] <= 200 else 'âŒ éœ€è¦ä¼˜åŒ–'} |
| é”™è¯¯ç‡ | {analysis['throughput_analysis']['error_rate']:.2f}% | {'âœ… ä¼˜ç§€' if analysis['throughput_analysis']['error_rate'] <= 0.1 else 'âš ï¸ éœ€è¦å…³æ³¨'} |
| ååé‡ | {analysis['throughput_analysis']['requests_per_second']:.1f} RPS | {'âœ… è‰¯å¥½' if analysis['throughput_analysis']['requests_per_second'] >= 50 else 'âŒ éœ€è¦ä¼˜åŒ–'} |
| ç³»ç»Ÿå¯ç”¨æ€§ | {analysis['system_analysis']['availability']:.1f}% | {'âœ… è‰¯å¥½' if analysis['system_analysis']['availability'] >= 99.0 else 'âš ï¸ éœ€è¦å…³æ³¨'} |

### ğŸ“Š ä¸åŸºå‡†å¯¹æ¯”
"""

        for metric, data in benchmark.items():
            status_icon = "âœ…" if data["status"] == "è¾¾æ ‡" else "âŒ"
            content += f"| {metric} | {data['actual']:.2f} | {data['benchmark']:.2f} | {data['difference']:+.2f} | {status_icon} {data['status']} |\n"

        content += f"""
## ğŸ“ˆ è¯¦ç»†æ€§èƒ½åˆ†æ

### âš¡ å“åº”æ—¶é—´åˆ†æ
- **å¹³å‡å“åº”æ—¶é—´**: {analysis['response_time_analysis']['avg_ms']:.1f}ms
- **P50å“åº”æ—¶é—´**: {analysis['response_time_analysis']['p50_ms']:.1f}ms
- **P90å“åº”æ—¶é—´**: {analysis['response_time_analysis']['p90_ms']:.1f}ms
- **P95å“åº”æ—¶é—´**: {analysis['response_time_analysis']['p95_ms']:.1f}ms
- **P99å“åº”æ—¶é—´**: {analysis['response_analysis']['p99_ms']:.1f}ms
- **æœ€å°å“åº”æ—¶é—´**: {analysis['response_time_analysis']['min_ms']:.1f}ms
- **æœ€å¤§å“åº”æ—¶é—´**: {analysis['response_time_analysis']['max_ms']:.1f}ms

### ğŸš€ ååé‡åˆ†æ
- **æ€»è¯·æ±‚æ•°**: {analysis['throughput_analysis']['total_requests']:,}
- **æˆåŠŸè¯·æ±‚æ•°**: {analysis['throughput_analysis']['successful_requests']:,}
- **å¤±è´¥è¯·æ±‚æ•°**: {analysis['throughput_analysis']['failed_requests']:,}
- **æˆåŠŸç‡**: {analysis['throughput_analysis']['success_rate']:.2f}%
- **æ¯ç§’è¯·æ±‚æ•°**: {analysis['throughput_analysis']['requests_per_second']:.1f} RPS

### ğŸ–¥ï¸ ç³»ç»Ÿèµ„æºåˆ†æ
- **CPUä½¿ç”¨ç‡**: {analysis['system_analysis']['cpu_usage']:.1f}%
- **å†…å­˜ä½¿ç”¨ç‡**: {analysis['system_analysis']['memory_usage']:.1f}%
- **ç³»ç»Ÿå¯ç”¨æ€§**: {analysis['system_analysis']['availability']:.1f}%

## ğŸ’¡ ä¼˜åŒ–å»ºè®®

"""

        # æŒ‰ä¼˜å…ˆçº§æ’åºå»ºè®®
        high_priority = [r for r in recommendations if r["priority"] == "é«˜"]
        medium_priority = [r for r in recommendations if r["priority"] == "ä¸­"]
        low_priority = [r for r in recommendations if r["priority"] not in ["é«˜", "ä¸­"]]

        if high_priority:
            content += "### ğŸ”´ é«˜ä¼˜å…ˆçº§æ”¹è¿›é¡¹\n\n"
            for rec in high_priority:
                content += f"#### {rec['category']}\n"
                content += f"**é—®é¢˜**: {rec['issue']}\n\n"
                content += "**å»ºè®®æªæ–½**:\n"
                for i, action in enumerate(rec['recommendations'], 1):
                    content += f"{i}. {action}\n"
                content += "\n"

        if medium_priority:
            content += "### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§æ”¹è¿›é¡¹\n\n"
            for rec in medium_priority:
                content += f"#### {rec['category']}\n"
                content += f"**é—®é¢˜**: {rec['issue']}\n\n"
                content += "**å»ºè®®æªæ–½**:\n"
                for i, action in enumerate(rec['recommendations'], 1):
                    content += f"{i}. {action}\n"
                content += "\n"

        if low_priority:
            content += "### ğŸŸ¢ ä½ä¼˜å…ˆçº§æ”¹è¿›é¡¹\n\n"
            for rec in low_priority:
                content += f"#### {rec['category']}\n"
                content += f"**é—®é¢˜**: {rec['issue']}\n\n"
                content += "**å»ºè®®æªæ–½**:\n"
                for i, action in enumerate(rec['recommendations'], 1):
                    content += f"{i}. {action}\n"
                content += "\n"

        # æ·»åŠ è¡ŒåŠ¨é¡¹
        action_items = report.get("action_items", [])
        if action_items:
            content += "## ğŸ“‹ è¡ŒåŠ¨è®¡åˆ’\n\n"
            for action in action_items:
                content += f"### {action['id']}. {action['title']}\n"
                content += f"**æè¿°**: {action['description']}\n"
                content += f"**ä¼˜å…ˆçº§**: {action['priority']}\n"
                content += f"**é¢„è®¡å·¥ä½œé‡**: {action['estimated_effort']}\n"
                content += f"**æˆªæ­¢æ—¥æœŸ**: {action['due_date']}\n"
                content += "**è¡ŒåŠ¨é¡¹**:\n"
                for i, action_item in enumerate(action['actions'], 1):
                    content += f"- {action_item}\n"
                content += "\n"

        # æ·»åŠ æ€»ç»“
        overall_status = "ğŸ‰ æ€§èƒ½è¡¨ç°ä¼˜å¼‚ï¼Œç³»ç»Ÿå·²å‡†å¤‡å¥½ç”Ÿäº§éƒ¨ç½²" if evaluation["passed_all_checks"] else "âš ï¸ ç³»ç»Ÿæ€§èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–"
        content += f"""
## ğŸ“‹ æ€»ç»“

{overall_status}

### ğŸ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨
1. å®æ–½é«˜ä¼˜å…ˆçº§æ”¹è¿›é¡¹
2. å®šæœŸç›‘æ§æ€§èƒ½æŒ‡æ ‡
3. è¿›è¡Œå®šæœŸå‹åŠ›æµ‹è¯•
4. å»ºç«‹æ€§èƒ½åŸºçº¿ç›‘æ§

### ğŸ“ˆ æŒç»­æ”¹è¿›è®¡åˆ’
- æ¯æœˆè¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
- å»ºç«‹æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
- å®šæœŸå®¡æŸ¥å’Œä¼˜åŒ–ç³»ç»Ÿæ¶æ„
- æŒç»­ä¼˜åŒ–ä»£ç å’Œé…ç½®

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {metadata['generated_at']}*
*Phase G Week 5 Day 2 - ç”Ÿäº§ç¯å¢ƒå‹åŠ›æµ‹è¯•å’Œæ€§èƒ½éªŒè¯*
"""

        return content

    def print_summary(self, report: Dict[str, Any]) -> None:
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        summary = report["test_summary"]
        evaluation = report["performance_evaluation"]
        analysis = report["performance_analysis"]

        print("\n" + "="*60)
        print("ğŸ¯ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šæ‘˜è¦")
        print("="*60)

        print(f"ğŸ“… æµ‹è¯•æ—¥æœŸ: {summary['test_date']}")
        print(f"ğŸ† æ€»ä½“è¯„çº§: {evaluation['overall_grade']}")
        print(f"âœ… æµ‹è¯•çŠ¶æ€: {'é€šè¿‡' if evaluation['passed_all_checks'] else 'éœ€è¦ä¼˜åŒ–'}")

        print("\nğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"   P95å“åº”æ—¶é—´: {analysis['response_time_analysis']['p95_ms']:.1f}ms")
        print(f"   é”™è¯¯ç‡: {analysis['throughput_analysis']['error_rate']:.2f}%")
        print(f"   ååé‡: {analysis['throughput_analysis']['requests_per_second']:.1f} RPS")
        print(f"   ç³»ç»Ÿå¯ç”¨æ€§: {analysis['system_analysis']['availability']:.1f}%")

        print("\nğŸ’¡ å»ºè®®:")
        recommendations = report.get("recommendations", [])
        for i, rec in enumerate(recommendations[:3]):
            print(f"   {i+1}. [{rec['priority']}] {rec['issue']}")

        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨")
    print("="*60)

    import argparse

    parser = argparse.ArgumentParser(description="æ€§èƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå·¥å…·")
    parser.add_argument("--results", help="æµ‹è¯•ç»“æœJSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶å‰ç¼€")
    parser.add_argument("--format", choices=["json", "markdown", "both"], default="both", help="è¾“å‡ºæ ¼å¼")

    args = parser.parse_args()

    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    generator = PerformanceReportGenerator()

    try:
        # ç”ŸæˆæŠ¥å‘Š
        report = generator.generate_report(args.results)

        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_filename = args.output or f"performance_report_{timestamp}"

        if args.format in ["json", "both"]:
            generator.save_report(report, f"{base_filename}.json")

        if args.format in ["markdown", "both"]:
            generator.save_markdown_report(report, f"{base_filename}.md")

        # æ‰“å°æ‘˜è¦
        generator.print_summary(report)

        return 0

    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())