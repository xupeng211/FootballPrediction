#!/usr/bin/env python3
"""
OddsPortalçˆ¬è™«æµ‹è¯•è„šæœ¬
æµ‹è¯•OddsPortalæ•°æ®æŠ“å–åŠŸèƒ½
"""

import asyncio
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.collectors.oddsportal_scraper import (OddsPortalMatch,
                                               OddsPortalScraper)
from src.core.logging_system import get_logger

logger = get_logger(__name__)


async def test_scraper_initialization():
    """æµ‹è¯•çˆ¬è™«åˆå§‹åŒ–"""
    logger.info("Testing OddsPortal Scraper Initialization...")

    try:
        scraper = OddsPortalScraper()

        # æµ‹è¯•åŸºæœ¬å±æ€§
        assert scraper.base_url == "https://www.oddsportal.com"
        assert scraper.min_request_interval == 1.0  # é»˜è®¤é€Ÿç‡é™åˆ¶
        assert hasattr(scraper, "session")
        assert scraper.robots_parser is not None

        logger.info("âœ… Scraper initialization test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ Scraper initialization test failed: {e}")
        return False


async def test_robots_txt():
    """æµ‹è¯•robots.txtæ£€æŸ¥"""
    logger.info("Testing robots.txt compliance...")

    try:
        scraper = OddsPortalScraper()

        # éªŒè¯robots_parserå·²åˆå§‹åŒ–
        assert hasattr(scraper, "robots_parser")
        assert scraper.robots_parser is not None

        logger.info("âœ… robots.txt compliance test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ robots.txt compliance test failed: {e}")
        return False


async def test_data_parsing():
    """æµ‹è¯•æ•°æ®è§£æåŠŸèƒ½"""
    logger.info("Testing data parsing functionality...")

    try:
        OddsPortalScraper()

        # æ¨¡æ‹ŸHTMLå“åº”è¿›è¡Œè§£ææµ‹è¯•
        sample_html = """
        <table class="table-main">
            <tr>
                <td class="datet">13:00</td>
                <td class="name">
                    <a href="/soccer/england/premier-league/manchester-city-vs-liverpool/">Manchester City - Liverpool</a>
                </td>
                <td class="odds">
                    <span class="odds">2.45</span>
                    <span class="odds">3.20</span>
                    <span class="odds">2.80</span>
                </td>
            </tr>
        </table>
        """

        # æµ‹è¯•è§£æåŠŸèƒ½
        from bs4 import BeautifulSoup

        soup = BeautifulSoup(sample_html, "html.parser")

        # éªŒè¯åŸºæœ¬è§£æ
        table = soup.find("table", class_="table-main")
        assert table is not None

        logger.info("âœ… Data parsing test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ Data parsing test failed: {e}")
        return False


async def test_url_construction():
    """æµ‹è¯•URLæ„é€ """
    logger.info("Testing URL construction...")

    try:
        from urllib.parse import urljoin

        scraper = OddsPortalScraper()

        # æµ‹è¯•URLæ„é€ 
        league_url = urljoin(scraper.base_url, "/soccer/england/premier-league/")
        assert league_url == "https://www.oddsportal.com/soccer/england/premier-league/"

        match_url = urljoin(scraper.base_url, "/match/123456/")
        assert match_url == "https://www.oddsportal.com/match/123456/"

        logger.info("âœ… URL construction test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ URL construction test failed: {e}")
        return False


async def test_rate_limiting():
    """æµ‹è¯•é€Ÿç‡é™åˆ¶"""
    logger.info("Testing rate limiting...")

    try:
        scraper = OddsPortalScraper()

        # æµ‹è¯•é€Ÿç‡é™åˆ¶å±æ€§
        assert hasattr(scraper, "min_request_interval")
        assert scraper.min_request_interval == 1.0
        assert hasattr(scraper, "rate_limit_requests_per_minute")
        assert scraper.rate_limit_requests_per_minute == 60
        assert hasattr(scraper, "request_times")
        assert isinstance(scraper.request_times, list)

        logger.info("âœ… Rate limiting test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ Rate limiting test failed: {e}")
        return False


async def test_data_structure():
    """æµ‹è¯•æ•°æ®ç»“æ„"""
    logger.info("Testing OddsPortalMatch data structure...")

    try:
        from dataclasses import asdict
        from datetime import datetime

        # åˆ›å»ºæµ‹è¯•æ•°æ®
        match = OddsPortalMatch(
            match_id="test_001",
            home_team="Manchester City",
            away_team="Liverpool",
            league="Premier League",
            match_date=datetime.now(),
            status="upcoming",
            odds_home_win=2.45,
            odds_draw=3.20,
            odds_away_win=2.80,
        )

        # éªŒè¯æ•°æ®ç»“æ„
        assert match.home_team == "Manchester City"
        assert match.away_team == "Liverpool"
        assert match.odds_home_win == 2.45
        assert match.odds_draw == 3.20
        assert match.odds_away_win == 2.80

        # æµ‹è¯•è½¬æ¢ä¸ºå­—å…¸
        match_dict = asdict(match)
        assert isinstance(match_dict, dict)
        assert "home_team" in match_dict
        assert "odds_home_win" in match_dict

        logger.info("âœ… Data structure test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ Data structure test failed: {e}")
        return False


async def test_error_handling():
    """æµ‹è¯•é”™è¯¯å¤„ç†"""
    logger.info("Testing error handling...")

    try:
        scraper = OddsPortalScraper()

        # æµ‹è¯•é”™è¯¯å¤„ç†å±æ€§
        assert hasattr(scraper, "max_retries")
        assert scraper.max_retries == 3
        assert hasattr(scraper, "retry_delay")
        assert scraper.retry_delay == 2

        logger.info("âœ… Error handling test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ Error handling test failed: {e}")
        return False


async def test_adapter_interface():
    """æµ‹è¯•é€‚é…å™¨æ¥å£"""
    logger.info("Testing DataSourceAdapter interface...")

    try:

        scraper = OddsPortalScraper()

        # æµ‹è¯•åŸºæœ¬æ–¹æ³•æ˜¯å¦å­˜åœ¨
        assert hasattr(scraper, "base_url")
        assert scraper.base_url == "https://www.oddsportal.com"
        assert hasattr(scraper, "user_agent")
        assert "Mozilla" in scraper.user_agent

        logger.info("âœ… Adapter interface test passed")
        return True
    except Exception as e:
        logger.error(f"âŒ Adapter interface test failed: {e}")
        return False


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("Starting OddsPortal Scraper Tests...")

    tests = [
        ("Scraper Initialization", test_scraper_initialization),
        ("Robots.txt Compliance", test_robots_txt),
        ("Data Parsing", test_data_parsing),
        ("URL Construction", test_url_construction),
        ("Rate Limiting", test_rate_limiting),
        ("Data Structure", test_data_structure),
        ("Error Handling", test_error_handling),
        ("Adapter Interface", test_adapter_interface),
    ]

    results = {}
    passed = 0
    total = len(tests)

    for test_name, test_func in tests:
        logger.info(f"\n{'='*50}")
        logger.info(f"Running Test: {test_name}")
        logger.info(f"{'='*50}")

        try:
            result = await test_func()
            results[test_name] = result
            if result:
                passed += 1
                logger.info(f"âœ… {test_name}: PASSED")
            else:
                logger.error(f"âŒ {test_name}: FAILED")
        except Exception as e:
            logger.error(f"âŒ {test_name}: ERROR - {e}")
            results[test_name] = False

    # æµ‹è¯•æ€»ç»“
    logger.info(f"\n{'='*50}")
    logger.info("TEST SUMMARY")
    logger.info(f"{'='*50}")
    logger.info(f"Total Tests: {total}")
    logger.info(f"Passed: {passed}")
    logger.info(f"Failed: {total - passed}")
    logger.info(f"Success Rate: {passed/total*100:.1f}%")

    for test_name, result in results.items():
        status = "âœ… PASSED" if result else "âŒ FAILED"
        logger.info(f"  {test_name}: {status}")

    if passed == total:
        logger.info("\nğŸ‰ All tests passed! OddsPortal scraper is working correctly.")
    else:
        logger.warning(
            f"\nâš ï¸ {total - passed} test(s) failed. Please check the implementation."
        )

    return results


async def main():
    """ä¸»å‡½æ•°"""
    try:
        results = await run_all_tests()
        return results
    except Exception as e:
        logger.error(f"Test execution failed: {e}")
        return None


if __name__ == "__main__":
    asyncio.run(main())
