#!/usr/bin/env python3
"""
ğŸ” APIé›†æˆæµ‹è¯•ï¼šæ•°æ®ä¸€è‡´æ€§éªŒè¯

æµ‹è¯•APIæ•°æ®çš„ä¸€è‡´æ€§ï¼ŒåŒ…æ‹¬ï¼š
1. é¢„æµ‹åˆ›å»ºåçš„æ•°æ®å­˜å‚¨å’Œæ£€ç´¢ä¸€è‡´æ€§
2. ç”¨æˆ·ä¼šè¯çŠ¶æ€ç»´æŠ¤
3. ç¼“å­˜æ•°æ®çš„ä¸€è‡´æ€§
4. å¹¶å‘æ“ä½œçš„æ•°æ®ä¸€è‡´æ€§
5. æ•°æ®æ ¼å¼å’Œçº¦æŸçš„ä¸€è‡´æ€§éªŒè¯
"""

import asyncio
import logging
import time
from datetime import datetime
from typing import Any

import httpx
import pytest

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataConsistencyTester:
    """APIæ•°æ®ä¸€è‡´æ€§æµ‹è¯•å™¨"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.test_results = []
        self.auth_token: str | None = None
        self.test_data: dict[str, Any] = {}
        self.consistency_errors: list[str] = []

    def log_test(
        self, test_name: str, success: bool, details: str = "", duration: float = 0
    ):
        """è®°å½•æµ‹è¯•ç»“æœ"""
        result = {
            "test_name": test_name,
            "success": success,
            "details": details,
            "duration": duration,
            "timestamp": datetime.now().isoformat(),
        }
        self.test_results.append(result)

        if details:
            pass
        if duration > 0:
            pass

    def log_consistency_error(self, error_msg: str):
        """è®°å½•ä¸€è‡´æ€§é”™è¯¯"""
        self.consistency_errors.append(error_msg)
        logger.error(f"æ•°æ®ä¸€è‡´æ€§é”™è¯¯: {error_msg}")

    async def setup_test_user(self) -> bool:
        """è®¾ç½®æµ‹è¯•ç”¨æˆ·"""
        start_time = time.time()
        try:
            # æ³¨å†Œæµ‹è¯•ç”¨æˆ·
            user_data = {
                "username": f"consistency_test_{int(time.time())}",
                "email": f"consistency_{int(time.time())}@example.com",
                "password": "testpassword123",
                "full_name": "ä¸€è‡´æ€§æµ‹è¯•ç”¨æˆ·",
            }

            async with httpx.AsyncClient() as client:
                register_response = await client.post(
                    f"{self.base_url}/auth/register", json=user_data, timeout=10.0
                )

                if register_response.status_code not in [200, 201]:
                    raise Exception(f"ç”¨æˆ·æ³¨å†Œå¤±è´¥: {register_response.status_code}")

                # ç™»å½•è·å–token
                login_response = await client.post(
                    f"{self.base_url}/auth/login",
                    json={
                        "username": user_data["username"],
                        "password": user_data["password"],
                    },
                    timeout=10.0,
                )

                if login_response.status_code != 200:
                    raise Exception(f"ç”¨æˆ·ç™»å½•å¤±è´¥: {login_response.status_code}")

                token_data = login_response.json()
                self.auth_token = token_data.get("access_token")
                self.test_data["user"] = user_data

            duration = time.time() - start_time
            self.log_test("æµ‹è¯•ç”¨æˆ·è®¾ç½®", True, "æµ‹è¯•ç”¨æˆ·åˆ›å»ºå’Œç™»å½•æˆåŠŸ", duration)
            return True

        except Exception as e:
            duration = time.time() - start_time
            self.log_test("æµ‹è¯•ç”¨æˆ·è®¾ç½®", False, f"å¼‚å¸¸: {str(e)}", duration)
            return False

    @pytest.mark.asyncio

    async def test_prediction_creation_consistency(self) -> bool:
        """æµ‹è¯•é¢„æµ‹åˆ›å»ºçš„æ•°æ®ä¸€è‡´æ€§"""
        start_time = time.time()
        try:
            if not self.auth_token:
                raise Exception("è®¤è¯tokenä¸å­˜åœ¨")

            headers = {"Authorization": f"Bearer {self.auth_token}"}

            # åˆ›å»ºé¢„æµ‹è¯·æ±‚
            prediction_request = {"model_version": "default", "include_details": True}

            async with httpx.AsyncClient() as client:
                # å‘é€åˆ›å»ºè¯·æ±‚
                create_response = await client.post(
                    f"{self.base_url}/predictions/",
                    headers=headers,
                    json=prediction_request,
                )

                if create_response.status_code not in [200, 201]:
                    raise Exception(f"åˆ›å»ºé¢„æµ‹å¤±è´¥: {create_response.status_code}")

                created_prediction = create_response.json()
                match_id = created_prediction.get("match_id")

                if not match_id:
                    raise Exception("åˆ›å»ºçš„é¢„æµ‹ç¼ºå°‘match_id")

                # ç«‹å³æŸ¥è¯¢éªŒè¯æ•°æ®ä¸€è‡´æ€§
                get_response = await client.get(
                    f"{self.base_url}/predictions/{match_id}", headers=headers
                )

                if get_response.status_code != 200:
                    raise Exception(f"æŸ¥è¯¢é¢„æµ‹å¤±è´¥: {get_response.status_code}")

                retrieved_prediction = get_response.json()

                # éªŒè¯æ•°æ®ä¸€è‡´æ€§
                consistency_checks = [
                    (
                        "match_idä¸€è‡´æ€§",
                        created_prediction.get("match_id")
                        == retrieved_prediction.get("match_id"),
                    ),
                    (
                        "model_versionä¸€è‡´æ€§",
                        created_prediction.get("model_version")
                        == retrieved_prediction.get("model_version"),
                    ),
                    (
                        "æ¦‚ç‡å’Œæ¥è¿‘1.0",
                        abs(
                            (
                                created_prediction.get("home_win_prob", 0)
                                + created_prediction.get("draw_prob", 0)
                                + created_prediction.get("away_win_prob", 0)
                            )
                            - 1.0
                        )
                        < 0.01,
                    ),
                    (
                        "ç½®ä¿¡åº¦åœ¨æœ‰æ•ˆèŒƒå›´å†…",
                        0 <= created_prediction.get("confidence", 0) <= 1,
                    ),
                    (
                        "é¢„æµ‹ç»“æœæœ‰æ•ˆ",
                        created_prediction.get("predicted_outcome")
                        in ["home", "draw", "away"],
                    ),
                ]

                failed_checks = [
                    check_name
                    for check_name, passed in consistency_checks
                    if not passed
                ]

                if failed_checks:
                    error_msg = f"æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {', '.join(failed_checks)}"
                    self.log_consistency_error(error_msg)
                    raise Exception(error_msg)

                # ä¿å­˜æµ‹è¯•æ•°æ®
                self.test_data["prediction"] = created_prediction
                self.test_data["retrieved_prediction"] = retrieved_prediction

            duration = time.time() - start_time
            details = f"é¢„æµ‹ID: {match_id}, æ‰€æœ‰ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡"
            self.log_test("é¢„æµ‹åˆ›å»ºä¸€è‡´æ€§", True, details, duration)
            return True

        except Exception as e:
            duration = time.time() - start_time
            self.log_test("é¢„æµ‹åˆ›å»ºä¸€è‡´æ€§", False, f"å¼‚å¸¸: {str(e)}", duration)
            return False

    @pytest.mark.asyncio

    async def test_batch_prediction_consistency(self) -> bool:
        """æµ‹è¯•æ‰¹é‡é¢„æµ‹çš„æ•°æ®ä¸€è‡´æ€§"""
        start_time = time.time()
        try:
            if not self.auth_token:
                raise Exception("è®¤è¯tokenä¸å­˜åœ¨")

            headers = {"Authorization": f"Bearer {self.auth_token}"}

            # åˆ›å»ºæ‰¹é‡é¢„æµ‹è¯·æ±‚
            batch_request = {
                "match_ids": [11111, 22222, 33333],
                "model_version": "default",
            }

            async with httpx.AsyncClient() as client:
                # å‘é€æ‰¹é‡åˆ›å»ºè¯·æ±‚
                batch_response = await client.post(
                    f"{self.base_url}/predictions/batch",
                    headers=headers,
                    json=batch_request,
                )

                if batch_response.status_code != 200:
                    raise Exception(f"æ‰¹é‡åˆ›å»ºé¢„æµ‹å¤±è´¥: {batch_response.status_code}")

                batch_result = batch_response.json()
                predictions = batch_result.get("predictions", [])
                total_count = batch_result.get("total", 0)

                # éªŒè¯æ‰¹é‡æ•°æ®ä¸€è‡´æ€§
                consistency_checks = [
                    (
                        "è¿”å›æ•°é‡åŒ¹é…",
                        len(predictions) == len(batch_request["match_ids"]),
                    ),
                    ("æ€»æ•°åŒ¹é…", total_count == len(predictions)),
                    (
                        "æ‰€æœ‰é¢„æµ‹éƒ½æœ‰match_id",
                        all(
                            p.get("match_id") in batch_request["match_ids"]
                            for p in predictions
                        ),
                    ),
                    (
                        "æ¨¡å‹ç‰ˆæœ¬ä¸€è‡´",
                        all(
                            p.get("model_version") == batch_request["model_version"]
                            for p in predictions
                        ),
                    ),
                    (
                        "æ¦‚ç‡å’Œéƒ½æ¥è¿‘1.0",
                        all(
                            abs(
                                (
                                    p.get("home_win_prob", 0)
                                    + p.get("draw_prob", 0)
                                    + p.get("away_win_prob", 0)
                                )
                                - 1.0
                            )
                            < 0.01
                            for p in predictions
                        ),
                    ),
                ]

                failed_checks = [
                    check_name
                    for check_name, passed in consistency_checks
                    if not passed
                ]

                if failed_checks:
                    error_msg = f"æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {', '.join(failed_checks)}"
                    self.log_consistency_error(error_msg)
                    raise Exception(error_msg)

                # ä¿å­˜æµ‹è¯•æ•°æ®
                self.test_data["batch_predictions"] = predictions

            duration = time.time() - start_time
            details = f"æ‰¹é‡é¢„æµ‹æ•°é‡: {len(predictions)}, æ‰€æœ‰ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡"
            self.log_test("æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§", True, details, duration)
            return True

        except Exception as e:
            duration = time.time() - start_time
            self.log_test("æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§", False, f"å¼‚å¸¸: {str(e)}", duration)
            return False

    @pytest.mark.asyncio

    async def test_history_data_consistency(self) -> bool:
        """æµ‹è¯•å†å²æ•°æ®çš„ä¸€è‡´æ€§"""
        start_time = time.time()
        try:
            if not self.auth_token:
                raise Exception("è®¤è¯tokenä¸å­˜åœ¨")

            headers = {"Authorization": f"Bearer {self.auth_token}"}

            async with httpx.AsyncClient() as client:
                # è·å–é¢„æµ‹å†å²
                history_response = await client.get(
                    f"{self.base_url}/predictions/history", headers=headers
                )

                if history_response.status_code != 200:
                    raise Exception(f"è·å–å†å²æ•°æ®å¤±è´¥: {history_response.status_code}")

                history_data = history_response.json()
                predictions = history_data.get("predictions", [])

                # éªŒè¯å†å²æ•°æ®ä¸€è‡´æ€§
                consistency_checks = [
                    ("å†å²æ•°æ®æ˜¯åˆ—è¡¨", isinstance(predictions, list)),
                    (
                        "æ¯ä¸ªé¢„æµ‹éƒ½æœ‰å¿…è¦å­—æ®µ",
                        all(
                            all(
                                key in p
                                for key in [
                                    "match_id",
                                    "predicted_outcome",
                                    "confidence",
                                ]
                            )
                            for p in predictions
                        ),
                    ),
                    (
                        "é¢„æµ‹ç»“æœæœ‰æ•ˆ",
                        all(
                            p.get("predicted_outcome") in ["home", "draw", "away"]
                            for p in predictions
                        ),
                    ),
                    (
                        "ç½®ä¿¡åº¦èŒƒå›´æœ‰æ•ˆ",
                        all(0 <= p.get("confidence", 0) <= 1 for p in predictions),
                    ),
                ]

                failed_checks = [
                    check_name
                    for check_name, passed in consistency_checks
                    if not passed
                ]

                if failed_checks:
                    error_msg = f"å†å²æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {', '.join(failed_checks)}"
                    self.log_consistency_error(error_msg)
                    raise Exception(error_msg)

                # ä¿å­˜æµ‹è¯•æ•°æ®
                self.test_data["history"] = history_data

            duration = time.time() - start_time
            details = f"å†å²é¢„æµ‹æ•°é‡: {len(predictions)}, æ‰€æœ‰ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡"
            self.log_test("å†å²æ•°æ®ä¸€è‡´æ€§", True, details, duration)
            return True

        except Exception as e:
            duration = time.time() - start_time
            self.log_test("å†å²æ•°æ®ä¸€è‡´æ€§", False, f"å¼‚å¸¸: {str(e)}", duration)
            return False

    @pytest.mark.asyncio

    async def test_concurrent_operations_consistency(self) -> bool:
        """æµ‹è¯•å¹¶å‘æ“ä½œçš„æ•°æ®ä¸€è‡´æ€§"""
        start_time = time.time()
        try:
            if not self.auth_token:
                raise Exception("è®¤è¯tokenä¸å­˜åœ¨")

            headers = {"Authorization": f"Bearer {self.auth_token}"}

            # å¹¶å‘åˆ›å»ºå¤šä¸ªé¢„æµ‹
            async def create_prediction_async(
                match_id: int,
            ) -> dict[str, Any] | None:
                async with httpx.AsyncClient() as client:
                    response = await client.post(
                        f"{self.base_url}/predictions/",
                        headers=headers,
                        json={"model_version": "default", "include_details": True},
                    )
                    if response.status_code in [200, 201]:
                        return response.json()
                    return None

            # å¯åŠ¨å¹¶å‘ä»»åŠ¡
            match_ids = [10001, 10002, 10003, 10004, 10005]
            tasks = [create_prediction_async(mid) for mid in match_ids]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
            successful_predictions = [r for r in results if isinstance(r, dict)]
            failed_operations = [r for r in results if isinstance(r, Exception)]

            # éªŒè¯å¹¶å‘æ“ä½œä¸€è‡´æ€§
            consistency_checks = [
                ("å¤§éƒ¨åˆ†æ“ä½œæˆåŠŸ", len(successful_predictions) >= len(match_ids) * 0.8),
                (
                    "æ²¡æœ‰é‡å¤çš„match_id",
                    len({p.get("match_id") for p in successful_predictions})
                    == len(successful_predictions),
                ),
                (
                    "æ‰€æœ‰é¢„æµ‹éƒ½æœ‰æœ‰æ•ˆæ•°æ®",
                    all(
                        all(
                            key in p
                            for key in ["match_id", "predicted_outcome", "confidence"]
                        )
                        for p in successful_predictions
                    ),
                ),
            ]

            failed_checks = [
                check_name for check_name, passed in consistency_checks if not passed
            ]

            if failed_checks:
                error_msg = f"å¹¶å‘æ“ä½œä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {', '.join(failed_checks)}"
                self.log_consistency_error(error_msg)
                # å¯¹äºå¹¶å‘æµ‹è¯•ï¼Œæˆ‘ä»¬è®°å½•é”™è¯¯ä½†ä¸ä¸€å®šå¤±è´¥
                logger.warning(error_msg)

            duration = time.time() - start_time
            details = f"æˆåŠŸ: {len(successful_predictions)}/{len(match_ids)}, å¤±è´¥: {len(failed_operations)}"
            self.log_test("å¹¶å‘æ“ä½œä¸€è‡´æ€§", len(failed_checks) == 0, details, duration)
            return len(failed_checks) == 0

        except Exception as e:
            duration = time.time() - start_time
            self.log_test("å¹¶å‘æ“ä½œä¸€è‡´æ€§", False, f"å¼‚å¸¸: {str(e)}", duration)
            return False

    @pytest.mark.asyncio

    async def test_data_format_consistency(self) -> bool:
        """æµ‹è¯•æ•°æ®æ ¼å¼çš„ä¸€è‡´æ€§"""
        start_time = time.time()
        try:
            # æµ‹è¯•ä¸åŒAPIç«¯ç‚¹è¿”å›çš„æ•°æ®æ ¼å¼ä¸€è‡´æ€§
            endpoints_to_test = [
                "/predictions/",
                "/predictions/history",
                "/predictions/recent",
            ]

            headers = {"Authorization": f"Bearer {self.auth_token}"}
            format_errors = []

            async with httpx.AsyncClient() as client:
                for endpoint in endpoints_to_test:
                    try:
                        response = await client.get(
                            f"{self.base_url}{endpoint}", headers=headers, timeout=5.0
                        )

                        if response.status_code == 200:
                            data = response.json()

                            # éªŒè¯å“åº”æ ¼å¼
                            if endpoint == "/predictions/":
                                # æ ¹ç«¯ç‚¹åº”è¯¥æœ‰åŸºæœ¬ä¿¡æ¯
                                if not isinstance(data, dict):
                                    format_errors.append(
                                        f"{endpoint}: å“åº”ä¸æ˜¯å­—å…¸æ ¼å¼"
                                    )
                            elif endpoint in [
                                "/predictions/history",
                                "/predictions/recent",
                            ]:
                                # åˆ—è¡¨ç«¯ç‚¹åº”è¯¥æœ‰predictionsæ•°ç»„
                                if (
                                    not isinstance(data, dict)
                                    or "predictions" not in data
                                ):
                                    format_errors.append(
                                        f"{endpoint}: ç¼ºå°‘predictionså­—æ®µ"
                                    )
                                elif not isinstance(data.get("predictions"), list):
                                    format_errors.append(
                                        f"{endpoint}: predictionsä¸æ˜¯æ•°ç»„æ ¼å¼"
                                    )

                    except Exception as e:
                        logger.warning(f"æµ‹è¯•ç«¯ç‚¹ {endpoint} æ—¶å¼‚å¸¸: {e}")

            success = len(format_errors) == 0
            duration = time.time() - start_time
            details = f"æµ‹è¯•ç«¯ç‚¹æ•°: {len(endpoints_to_test)}, æ ¼å¼é”™è¯¯æ•°: {len(format_errors)}"

            if format_errors:
                details += f", é”™è¯¯: {'; '.join(format_errors[:2])}"

            self.log_test("æ•°æ®æ ¼å¼ä¸€è‡´æ€§", success, details, duration)
            return success

        except Exception as e:
            duration = time.time() - start_time
            self.log_test("æ•°æ®æ ¼å¼ä¸€è‡´æ€§", False, f"å¼‚å¸¸: {str(e)}", duration)
            return False

    async def run_all_consistency_tests(self) -> dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ•°æ®ä¸€è‡´æ€§æµ‹è¯•"""

        # é¦–å…ˆè®¾ç½®æµ‹è¯•ç”¨æˆ·
        if not await self.setup_test_user():
            return {
                "total_tests": 0,
                "passed_tests": 0,
                "failed_tests": 0,
                "success_rate": 0,
                "consistency_errors": ["æµ‹è¯•ç”¨æˆ·è®¾ç½®å¤±è´¥"],
                "timestamp": datetime.now().isoformat(),
            }

        test_methods = [
            ("é¢„æµ‹åˆ›å»ºä¸€è‡´æ€§", self.test_prediction_creation_consistency),
            ("æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§", self.test_batch_prediction_consistency),
            ("å†å²æ•°æ®ä¸€è‡´æ€§", self.test_history_data_consistency),
            ("å¹¶å‘æ“ä½œä¸€è‡´æ€§", self.test_concurrent_operations_consistency),
            ("æ•°æ®æ ¼å¼ä¸€è‡´æ€§", self.test_data_format_consistency),
        ]

        passed_tests = 0
        total_tests = len(test_methods)

        for test_name, test_method in test_methods:
            try:
                if await test_method():
                    passed_tests += 1
                await asyncio.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«
            except Exception as e:
                logger.error(f"æµ‹è¯•æ–¹æ³• {test_name} æ‰§è¡Œå¼‚å¸¸: {e}")

        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        success_rate = (passed_tests / total_tests) * 100
        report = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "success_rate": success_rate,
            "consistency_errors": self.consistency_errors,
            "test_results": self.test_results,
            "test_data_summary": {
                "has_user": "user" in self.test_data,
                "has_prediction": "prediction" in self.test_data,
                "has_batch_predictions": "batch_predictions" in self.test_data,
                "has_history": "history" in self.test_data,
            },
            "timestamp": datetime.now().isoformat(),
        }

        if self.consistency_errors:
            for _error in self.consistency_errors:
                pass

        return report


# Pytestæµ‹è¯•ç”¨ä¾‹
@pytest.mark.integration
@pytest.mark.asyncio
class TestDataConsistency:
    """APIæ•°æ®ä¸€è‡´æ€§é›†æˆæµ‹è¯•"""

    @pytest.fixture
    async def consistency_tester(self):
        """åˆ›å»ºæ•°æ®ä¸€è‡´æ€§æµ‹è¯•å™¨å®ä¾‹"""
        return DataConsistencyTester()

    @pytest.mark.asyncio

    async def test_prediction_creation_consistency(self, consistency_tester):
        """æµ‹è¯•é¢„æµ‹åˆ›å»ºçš„æ•°æ®ä¸€è‡´æ€§"""
        await consistency_tester.setup_test_user()
        result = await consistency_tester.test_prediction_creation_consistency()
        assert result, "é¢„æµ‹åˆ›å»ºä¸€è‡´æ€§æµ‹è¯•å¤±è´¥"

    @pytest.mark.asyncio

    async def test_batch_prediction_consistency(self, consistency_tester):
        """æµ‹è¯•æ‰¹é‡é¢„æµ‹çš„æ•°æ®ä¸€è‡´æ€§"""
        await consistency_tester.setup_test_user()
        result = await consistency_tester.test_batch_prediction_consistency()
        assert result, "æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥"

    @pytest.mark.asyncio

    async def test_history_data_consistency(self, consistency_tester):
        """æµ‹è¯•å†å²æ•°æ®çš„ä¸€è‡´æ€§"""
        await consistency_tester.setup_test_user()
        result = await consistency_tester.test_history_data_consistency()
        assert result, "å†å²æ•°æ®ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥"

    @pytest.mark.asyncio

    async def test_data_format_consistency(self, consistency_tester):
        """æµ‹è¯•æ•°æ®æ ¼å¼çš„ä¸€è‡´æ€§"""
        await consistency_tester.setup_test_user()
        result = await consistency_tester.test_data_format_consistency()
        assert result, "æ•°æ®æ ¼å¼ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥"

    @pytest.mark.asyncio

    async def test_all_consistency_checks(self, consistency_tester):
        """æµ‹è¯•æ‰€æœ‰ä¸€è‡´æ€§æ£€æŸ¥"""
        report = await consistency_tester.run_all_consistency_tests()
        assert (
            report["success_rate"] >= 80
        ), f"ä¸€è‡´æ€§æµ‹è¯•æˆåŠŸç‡ä¸è¶³80%: {report['success_rate']:.1f}%"
        assert (
            len(report["consistency_errors"]) == 0
        ), f"å‘ç°ä¸€è‡´æ€§é”™è¯¯: {report['consistency_errors']}"


# ç‹¬ç«‹è¿è¡Œæµ‹è¯•çš„ä¸»å‡½æ•°
async def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ•°æ®ä¸€è‡´æ€§æµ‹è¯•"""
    tester = DataConsistencyTester()
    report = await tester.run_all_consistency_tests()

    if report["success_rate"] >= 80 and len(report["consistency_errors"]) == 0:
        return 0
    else:
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)
