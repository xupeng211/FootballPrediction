#!/usr/bin/env python3
"""
ç¬¬å››é˜¶æ®µç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
Stage 4 End-to-End Test Script - Complete System Integration
"""

import asyncio
import sys
import os
import logging
import time
import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from concurrent.futures import ThreadPoolExecutor

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, "/home/user/projects/FootballPrediction")

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["FOOTBALL_DATA_API_KEY"] = "ed809154dc1f422da46a18d8961a98a0"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger(__name__)

# å¯¼å…¥åŸºç¡€ç»„ä»¶
try:
    from test_stage2_fixed import SimpleDataCollector
    from test_stage3_simple import SimpleCacheManager
except ImportError as e:
    logger.error(f"å¯¼å…¥åŸºç¡€ç»„ä»¶å¤±è´¥: {e}")
    sys.exit(1)


class Stage4E2ETester:
    """ç¬¬å››é˜¶æ®µç«¯åˆ°ç«¯æµ‹è¯•å™¨"""

    def __init__(self):
        self.cache_manager = SimpleCacheManager()
        self.data_collector = None
        self.test_results = {
            "total_tests": 0,
            "passed_tests": 0,
            "failed_tests": 0,
            "errors": [],
            "performance_metrics": {},
            "data_quality_metrics": {},
        }
        self.performance_data = []

    async def initialize(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        try:
            self.data_collector = SimpleDataCollector()
            await self.data_collector.__aenter__()
            logger.info("âœ… æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    async def cleanup(self):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        try:
            if self.data_collector:
                await self.data_collector.__aexit__(None, None, None)
            self.cache_manager.clear_all()
            logger.info("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ æµ‹è¯•ç¯å¢ƒæ¸…ç†å¤±è´¥: {e}")

    def record_performance(self, operation: str, duration: float, data_size: int = 0):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        self.performance_data.append(
            {
                "operation": operation,
                "duration": duration,
                "data_size": data_size,
                "timestamp": datetime.utcnow().isoformat(),
            }
        )

    async def test_complete_data_flow(self) -> bool:
        """æµ‹è¯•å®Œæ•´æ•°æ®æµï¼šAPI â†’ ç¼“å­˜ â†’ å¤„ç† â†’ å­˜å‚¨"""
        try:
            logger.info("ğŸ” æµ‹è¯•å®Œæ•´æ•°æ®æµ...")

            # ç¬¬ä¸€æ­¥ï¼šAPIæ•°æ®è·å–
            start_time = time.time()
            competitions_data = await self.data_collector._make_request_with_retry(
                "competitions"
            )
            api_duration = time.time() - start_time
            self.record_performance(
                "api_competitions_fetch",
                api_duration,
                len(competitions_data.get("competitions", [])),
            )

            if not competitions_data or "competitions" not in competitions_data:
                raise Exception("APIæ•°æ®è·å–å¤±è´¥")

            competitions = competitions_data["competitions"]
            premier_league = next(
                (comp for comp in competitions if comp.get("code") == "PL"), None
            )
            if not premier_league:
                raise Exception("æœªæ‰¾åˆ°è‹±è¶…è”èµ›æ•°æ®")

            logger.info(
                f"  âœ… APIæ•°æ®è·å–æˆåŠŸ: {len(competitions)}ä¸ªè”èµ›ï¼Œè€—æ—¶{api_duration:.3f}ç§’"
            )

            # ç¬¬äºŒæ­¥ï¼šæ•°æ®ç¼“å­˜
            start_time = time.time()
            cache_success = self.cache_manager.set_cache(
                "competitions", competitions, ttl_seconds=300
            )
            cache_duration = time.time() - start_time
            self.record_performance(
                "cache_competitions_set", cache_duration, len(competitions)
            )

            if not cache_success:
                raise Exception("æ•°æ®ç¼“å­˜å¤±è´¥")

            logger.info(f"  âœ… æ•°æ®ç¼“å­˜æˆåŠŸï¼Œè€—æ—¶{cache_duration:.3f}ç§’")

            # ç¬¬ä¸‰æ­¥ï¼šç¼“å­˜æ•°æ®è¯»å–
            start_time = time.time()
            cached_competitions = self.cache_manager.get_cache("competitions")
            cache_read_duration = time.time() - start_time
            self.record_performance(
                "cache_competitions_get",
                cache_read_duration,
                len(cached_competitions or []),
            )

            if not cached_competitions or len(cached_competitions) != len(competitions):
                raise Exception("ç¼“å­˜æ•°æ®è¯»å–å¤±è´¥")

            logger.info(f"  âœ… ç¼“å­˜æ•°æ®è¯»å–æˆåŠŸï¼Œè€—æ—¶{cache_read_duration:.3f}ç§’")

            # ç¬¬å››æ­¥ï¼šæ•°æ®è½¬æ¢å’Œå¤„ç†
            start_time = time.time()
            processed_leagues = []
            for comp in competitions[:5]:  # å¤„ç†å‰5ä¸ªè”èµ›
                processed_league = {
                    "external_id": str(comp.get("id")),
                    "name": comp.get("name"),
                    "code": comp.get("code"),
                    "type": comp.get("type", "LEAGUE"),
                    "area_name": comp.get("area", {}).get("name", "Unknown"),
                    "current_matchday": comp.get("currentSeason", {}).get(
                        "currentMatchday"
                    ),
                    "processed_at": datetime.utcnow().isoformat(),
                }
                processed_leagues.append(processed_league)

            processing_duration = time.time() - start_time
            self.record_performance(
                "data_processing", processing_duration, len(processed_leagues)
            )

            logger.info(
                f"  âœ… æ•°æ®å¤„ç†å®Œæˆ: {len(processed_leagues)}ä¸ªè”èµ›ï¼Œè€—æ—¶{processing_duration:.3f}ç§’"
            )

            # ç¬¬äº”æ­¥ï¼šå¤„ç†åæ•°æ®ç¼“å­˜
            start_time = time.time()
            final_cache_success = self.cache_manager.set_cache(
                "processed_leagues", processed_leagues, ttl_seconds=600
            )
            final_cache_duration = time.time() - start_time
            self.record_performance(
                "final_cache_set", final_cache_duration, len(processed_leagues)
            )

            if not final_cache_success:
                raise Exception("å¤„ç†åæ•°æ®ç¼“å­˜å¤±è´¥")

            logger.info(f"  âœ… æœ€ç»ˆæ•°æ®ç¼“å­˜æˆåŠŸï¼Œè€—æ—¶{final_cache_duration:.3f}ç§’")

            # æ•°æ®è´¨é‡éªŒè¯
            quality_score = self._calculate_data_quality(processed_leagues)
            self.test_results["data_quality_metrics"]["leagues_quality"] = quality_score

            if quality_score < 80:
                raise Exception(f"æ•°æ®è´¨é‡è¯„åˆ†è¿‡ä½: {quality_score}")

            logger.info(f"  âœ… æ•°æ®è´¨é‡éªŒè¯é€šè¿‡: {quality_score}/100")

            # æ€§èƒ½éªŒè¯
            total_duration = (
                api_duration
                + cache_duration
                + cache_read_duration
                + processing_duration
                + final_cache_duration
            )
            if total_duration > 10.0:  # è¶…è¿‡10ç§’è®¤ä¸ºæ€§èƒ½ä¸ä½³
                logger.warning(f"  âš ï¸ å®Œæ•´æ•°æ®æµè€—æ—¶è¾ƒé•¿: {total_duration:.3f}ç§’")

            logger.info(f"  âœ… å®Œæ•´æ•°æ®æµæµ‹è¯•é€šè¿‡ï¼Œæ€»è€—æ—¶: {total_duration:.3f}ç§’")

            return True

        except Exception as e:
            logger.error(f"  âŒ å®Œæ•´æ•°æ®æµæµ‹è¯•å¤±è´¥: {e}")
            return False

    async def test_concurrent_data_access(self) -> bool:
        """æµ‹è¯•å¹¶å‘æ•°æ®è®¿é—®"""
        try:
            logger.info("ğŸ” æµ‹è¯•å¹¶å‘æ•°æ®è®¿é—®...")

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            test_data = [
                {
                    "id": i,
                    "name": f"Team {i}",
                    "points": i * 3,
                    "created_at": datetime.utcnow().isoformat(),
                }
                for i in range(100)
            ]

            # æ‰¹é‡è®¾ç½®ç¼“å­˜
            for i, data in enumerate(test_data):
                self.cache_manager.set_cache(f"team_{i}", data, ttl_seconds=300)

            logger.info(f"  âœ… å‡†å¤‡äº†{len(test_data)}æ¡æµ‹è¯•æ•°æ®")

            # å¹¶å‘è¯»å–æµ‹è¯•
            async def concurrent_read(team_id: int) -> bool:
                try:
                    cached_data = self.cache_manager.get_cache(f"team_{team_id}")
                    if cached_data and cached_data.get("id") == team_id:
                        return True
                    return False
                except Exception:
                    return False

            # å¹¶å‘å†™å…¥æµ‹è¯•
            async def concurrent_write(team_id: int) -> bool:
                try:
                    data = {
                        "id": team_id,
                        "name": f"Concurrent Team {team_id}",
                        "updated_at": datetime.utcnow().isoformat(),
                    }
                    return self.cache_manager.set_cache(
                        f"concurrent_team_{team_id}", data, ttl_seconds=300
                    )
                except Exception:
                    return False

            # æ‰§è¡Œå¹¶å‘æµ‹è¯•
            concurrent_read_count = 50
            concurrent_write_count = 30

            start_time = time.time()

            # å¹¶å‘è¯»å–
            read_tasks = [
                concurrent_read(i % 100) for i in range(concurrent_read_count)
            ]
            read_results = await asyncio.gather(*read_tasks, return_exceptions=True)

            # å¹¶å‘å†™å…¥
            write_tasks = [concurrent_write(i) for i in range(concurrent_write_count)]
            write_results = await asyncio.gather(*write_tasks, return_exceptions=True)

            total_duration = time.time() - start_time

            # ç»Ÿè®¡ç»“æœ
            successful_reads = sum(1 for result in read_results if result is True)
            successful_writes = sum(1 for result in write_results if result is True)

            read_success_rate = (successful_reads / concurrent_read_count) * 100
            write_success_rate = (successful_writes / concurrent_write_count) * 100

            logger.info(
                f"  âœ… å¹¶å‘è¯»å–æµ‹è¯•: {successful_reads}/{concurrent_read_count} æˆåŠŸ ({read_success_rate:.1f}%)"
            )
            logger.info(
                f"  âœ… å¹¶å‘å†™å…¥æµ‹è¯•: {successful_writes}/{concurrent_write_count} æˆåŠŸ ({write_success_rate:.1f}%)"
            )
            logger.info(f"  âœ… å¹¶å‘æµ‹è¯•æ€»è€—æ—¶: {total_duration:.3f}ç§’")

            # æ€§èƒ½æŒ‡æ ‡è®°å½•
            self.record_performance(
                "concurrent_operations",
                total_duration,
                concurrent_read_count + concurrent_write_count,
            )

            # éªŒè¯å¹¶å‘æ€§èƒ½
            if read_success_rate < 90 or write_success_rate < 90:
                raise Exception(
                    f"å¹¶å‘æ€§èƒ½ä¸è¾¾æ ‡: è¯»å–{read_success_rate:.1f}%, å†™å…¥{write_success_rate:.1f}%"
                )

            return True

        except Exception as e:
            logger.error(f"  âŒ å¹¶å‘æ•°æ®è®¿é—®æµ‹è¯•å¤±è´¥: {e}")
            return False

    async def test_data_consistency_across_components(self) -> bool:
        """æµ‹è¯•ç»„ä»¶é—´æ•°æ®ä¸€è‡´æ€§"""
        try:
            logger.info("ğŸ” æµ‹è¯•ç»„ä»¶é—´æ•°æ®ä¸€è‡´æ€§...")

            # ä»APIè·å–åŸå§‹æ•°æ®
            teams_data = await self.data_collector._make_request_with_retry(
                "competitions/2021/teams"
            )
            if not teams_data or "teams" not in teams_data:
                raise Exception("æ— æ³•è·å–çƒé˜Ÿæ•°æ®")

            original_teams = teams_data["teams"]
            logger.info(f"  âœ… è·å–åŸå§‹æ•°æ®: {len(original_teams)}æ”¯çƒé˜Ÿ")

            # ç¬¬ä¸€å±‚ç¼“å­˜ï¼šåŸå§‹APIæ•°æ®ç¼“å­˜
            api_cache_key = "api:teams:raw"
            self.cache_manager.set_cache(api_cache_key, original_teams, ttl_seconds=300)

            # ç¬¬äºŒå±‚ç¼“å­˜ï¼šå¤„ç†åæ•°æ®ç¼“å­˜
            processed_teams = []
            for team in original_teams:
                processed_team = {
                    "external_id": str(team.get("id")),
                    "name": team.get("name"),
                    "short_name": team.get("shortName"),
                    "tla": team.get("tla"),
                    "founded": team.get("founded"),
                    "venue": team.get("venue"),
                    "processed_at": datetime.utcnow().isoformat(),
                }
                processed_teams.append(processed_team)

            self.cache_manager.set_cache(
                "processed:teams", processed_teams, ttl_seconds=600
            )

            # ç¬¬ä¸‰å±‚ç¼“å­˜ï¼šæ‘˜è¦æ•°æ®ç¼“å­˜
            summary_data = {
                "total_teams": len(processed_teams),
                "teams_with_venue": len([t for t in processed_teams if t.get("venue")]),
                "teams_with_founded": len(
                    [t for t in processed_teams if t.get("founded")]
                ),
                "generated_at": datetime.utcnow().isoformat(),
            }

            self.cache_manager.set_cache("summary:teams", summary_data, ttl_seconds=900)

            # ä¸€è‡´æ€§éªŒè¯
            api_cached = self.cache_manager.get_cache(api_cache_key)
            processed_cached = self.cache_manager.get_cache("processed:teams")
            summary_cached = self.cache_manager.get_cache("summary:teams")

            # éªŒè¯æ•°æ®é‡ä¸€è‡´æ€§
            if len(api_cached) != len(original_teams):
                raise Exception("APIç¼“å­˜æ•°æ®é‡ä¸ä¸€è‡´")

            if len(processed_cached) != len(processed_teams):
                raise Exception("å¤„ç†åç¼“å­˜æ•°æ®é‡ä¸ä¸€è‡´")

            if len(api_cached) != len(processed_cached):
                raise Exception("åŸå§‹æ•°æ®å’Œå¤„ç†åæ•°æ®é‡ä¸ä¸€è‡´")

            # éªŒè¯æ•°æ®å†…å®¹ä¸€è‡´æ€§
            original_ids = set(str(team.get("id")) for team in original_teams)
            processed_ids = set(team.get("external_id") for team in processed_cached)

            if original_ids != processed_ids:
                missing_in_processed = original_ids - processed_ids
                extra_in_processed = processed_ids - original_ids
                logger.warning(
                    f"  âš ï¸ æ•°æ®IDä¸ä¸€è‡´: ç¼ºå¤±{len(missing_in_processed)}, å¤šä½™{len(extra_in_processed)}"
                )

            # éªŒè¯æ‘˜è¦æ•°æ®ä¸€è‡´æ€§
            if summary_cached["total_teams"] != len(processed_cached):
                raise Exception("æ‘˜è¦æ•°æ®ä¸å®é™…æ•°æ®ä¸ä¸€è‡´")

            logger.info(f"  âœ… æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
            logger.info(f"    - APIç¼“å­˜: {len(api_cached)}æ¡")
            logger.info(f"    - å¤„ç†åç¼“å­˜: {len(processed_cached)}æ¡")
            logger.info(f"    - æ‘˜è¦ç¼“å­˜: {summary_cached['total_teams']}æ¡")

            return True

        except Exception as e:
            logger.error(f"  âŒ æ•°æ®ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
            return False

    async def test_error_recovery_and_robustness(self) -> bool:
        """æµ‹è¯•é”™è¯¯æ¢å¤å’Œå¥å£®æ€§"""
        try:
            logger.info("ğŸ” æµ‹è¯•é”™è¯¯æ¢å¤å’Œå¥å£®æ€§...")

            # æµ‹è¯•1: æ— æ•ˆAPIç«¯ç‚¹å¤„ç†
            try:
                invalid_data = await self.data_collector._make_request_with_retry(
                    "invalid/endpoint"
                )
                if invalid_data.get("status") != 404:
                    logger.warning("  âš ï¸ æ— æ•ˆç«¯ç‚¹å¤„ç†å¯èƒ½æœ‰é—®é¢˜")
                else:
                    logger.info("  âœ… æ— æ•ˆç«¯ç‚¹å¤„ç†æ­£ç¡®")
            except Exception as e:
                logger.warning(f"  âš ï¸ æ— æ•ˆç«¯ç‚¹å¤„ç†å¼‚å¸¸: {e}")

            # æµ‹è¯•2: ç¼“å­˜é”®å†²çªå¤„ç†
            test_data_1 = {"name": "Team 1", "value": 100}
            test_data_2 = {"name": "Team 2", "value": 200}

            self.cache_manager.set_cache("test_key", test_data_1)
            self.cache_manager.set_cache("test_key", test_data_2)  # è¦†ç›–

            final_data = self.cache_manager.get_cache("test_key")
            if final_data.get("name") != "Team 2":
                raise Exception("ç¼“å­˜é”®è¦†ç›–å¤„ç†å¤±è´¥")

            logger.info("  âœ… ç¼“å­˜é”®è¦†ç›–å¤„ç†æ­£ç¡®")

            # æµ‹è¯•3: å¤§æ•°æ®é‡å¤„ç†
            large_dataset = [
                {
                    "id": i,
                    "data": "x" * 1000,
                    "timestamp": datetime.utcnow().isoformat(),
                }
                for i in range(500)
            ]

            start_time = time.time()
            self.cache_manager.set_cache(
                "large_dataset", large_dataset, ttl_seconds=300
            )
            cache_duration = time.time() - start_time

            retrieved_data = self.cache_manager.get_cache("large_dataset")
            if len(retrieved_data) != 500:
                raise Exception("å¤§æ•°æ®é‡ç¼“å­˜å¤±è´¥")

            logger.info(f"  âœ… å¤§æ•°æ®é‡å¤„ç†æˆåŠŸ: 500æ¡è®°å½•ï¼Œè€—æ—¶{cache_duration:.3f}ç§’")

            # æµ‹è¯•4: å†…å­˜å‹åŠ›æµ‹è¯•
            memory_test_data = []
            for i in range(100):
                data = {
                    "id": i,
                    "payload": "x" * 10000,  # 10KB per record
                    "metadata": {
                        "created": datetime.utcnow().isoformat(),
                        "type": "test",
                    },
                }
                memory_test_data.append(data)

            start_time = time.time()
            success_count = 0
            for i, data in enumerate(memory_test_data):
                if self.cache_manager.set_cache(
                    f"memory_test_{i}", data, ttl_seconds=60
                ):
                    success_count += 1

            memory_duration = time.time() - start_time
            logger.info(
                f"  âœ… å†…å­˜å‹åŠ›æµ‹è¯•: {success_count}/100æ¡è®°å½•æˆåŠŸï¼Œè€—æ—¶{memory_duration:.3f}ç§’"
            )

            if success_count < 95:  # è‡³å°‘95%æˆåŠŸç‡
                raise Exception(f"å†…å­˜å‹åŠ›æµ‹è¯•æˆåŠŸç‡è¿‡ä½: {success_count}%")

            return True

        except Exception as e:
            logger.error(f"  âŒ é”™è¯¯æ¢å¤å’Œå¥å£®æ€§æµ‹è¯•å¤±è´¥: {e}")
            return False

    async def test_system_performance_benchmarks(self) -> bool:
        """æµ‹è¯•ç³»ç»Ÿæ€§èƒ½åŸºå‡†"""
        try:
            logger.info("ğŸ” æµ‹è¯•ç³»ç»Ÿæ€§èƒ½åŸºå‡†...")

            benchmark_results = {}

            # åŸºå‡†1: APIå“åº”æ—¶é—´
            api_times = []
            for _ in range(5):
                start_time = time.time()
                await self.data_collector._make_request_with_retry("competitions")
                api_times.append(time.time() - start_time)

            avg_api_time = sum(api_times) / len(api_times)
            benchmark_results["avg_api_response_time"] = avg_api_time
            logger.info(f"  âœ… APIå¹³å‡å“åº”æ—¶é—´: {avg_api_time:.3f}ç§’")

            # åŸºå‡†2: ç¼“å­˜å†™å…¥æ€§èƒ½
            cache_write_times = []
            test_data = {"test": "data", "timestamp": datetime.utcnow().isoformat()}

            for _ in range(100):
                start_time = time.time()
                self.cache_manager.set_cache(
                    f"benchmark_{_}", test_data, ttl_seconds=300
                )
                cache_write_times.append(time.time() - start_time)

            avg_cache_write_time = sum(cache_write_times) / len(cache_write_times)
            benchmark_results["avg_cache_write_time"] = avg_cache_write_time
            logger.info(f"  âœ… ç¼“å­˜å¹³å‡å†™å…¥æ—¶é—´: {avg_cache_write_time:.6f}ç§’")

            # åŸºå‡†3: ç¼“å­˜è¯»å–æ€§èƒ½
            cache_read_times = []
            for i in range(100):
                start_time = time.time()
                self.cache_manager.get_cache(f"benchmark_{i}")
                cache_read_times.append(time.time() - start_time)

            avg_cache_read_time = sum(cache_read_times) / len(cache_read_times)
            benchmark_results["avg_cache_read_time"] = avg_cache_read_time
            logger.info(f"  âœ… ç¼“å­˜å¹³å‡è¯»å–æ—¶é—´: {avg_cache_read_time:.6f}ç§’")

            # åŸºå‡†4: æ•°æ®å¤„ç†æ€§èƒ½
            processing_times = []
            for _ in range(10):
                sample_data = [{"id": i, "value": i * 2} for i in range(100)]

                start_time = time.time()
                processed = [
                    {
                        "id": item["id"],
                        "doubled_value": item["value"] * 2,
                        "processed": True,
                    }
                    for item in sample_data
                ]
                processing_times.append(time.time() - start_time)

            avg_processing_time = sum(processing_times) / len(processing_times)
            benchmark_results["avg_processing_time"] = avg_processing_time
            logger.info(
                f"  âœ… æ•°æ®å¤„ç†å¹³å‡æ—¶é—´: {avg_processing_time:.6f}ç§’ (100æ¡è®°å½•)"
            )

            # æ€§èƒ½è¯„ä¼°
            performance_score = 100

            if avg_api_time > 3.0:
                performance_score -= 20
                logger.warning(f"  âš ï¸ APIå“åº”æ—¶é—´è¾ƒæ…¢: {avg_api_time:.3f}ç§’")

            if avg_cache_write_time > 0.001:
                performance_score -= 15
                logger.warning(f"  âš ï¸ ç¼“å­˜å†™å…¥æ—¶é—´è¾ƒæ…¢: {avg_cache_write_time:.6f}ç§’")

            if avg_cache_read_time > 0.0005:
                performance_score -= 15
                logger.warning(f"  âš ï¸ ç¼“å­˜è¯»å–æ—¶é—´è¾ƒæ…¢: {avg_cache_read_time:.6f}ç§’")

            if avg_processing_time > 0.01:
                performance_score -= 10
                logger.warning(f"  âš ï¸ æ•°æ®å¤„ç†æ—¶é—´è¾ƒæ…¢: {avg_processing_time:.6f}ç§’")

            benchmark_results["performance_score"] = performance_score
            self.test_results["performance_metrics"] = benchmark_results

            logger.info(f"  âœ… ç³»ç»Ÿæ€§èƒ½è¯„åˆ†: {performance_score}/100")

            if performance_score < 60:
                raise Exception(f"ç³»ç»Ÿæ€§èƒ½è¯„åˆ†è¿‡ä½: {performance_score}")

            return True

        except Exception as e:
            logger.error(f"  âŒ ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            return False

    def _calculate_data_quality(self, data: List[Dict[str, Any]]) -> int:
        """è®¡ç®—æ•°æ®è´¨é‡è¯„åˆ†"""
        if not data:
            return 0

        quality_score = 0
        total_items = len(data)

        # å¿…éœ€å­—æ®µå®Œæ•´æ€§ (40åˆ†)
        required_fields = ["external_id", "name"]
        for item in data:
            completeness = sum(1 for field in required_fields if item.get(field))
            quality_score += (completeness / len(required_fields)) * 40 / total_items

        # æ•°æ®ç±»å‹æ­£ç¡®æ€§ (20åˆ†)
        type_correct_count = 0
        for item in data:
            if isinstance(item.get("external_id"), str) and isinstance(
                item.get("name"), str
            ):
                type_correct_count += 1
        quality_score += (type_correct_count / total_items) * 20

        # æ•°æ®ä¸°å¯Œåº¦ (20åˆ†)
        richness_score = 0
        optional_fields = ["code", "type", "area_name", "current_matchday"]
        for item in data:
            richness = sum(1 for field in optional_fields if item.get(field))
            richness_score += richness / len(optional_fields)
        quality_score += (richness_score / total_items) * 20

        # æ—¶é—´æˆ³ä¸€è‡´æ€§ (20åˆ†)
        timestamp_count = sum(1 for item in data if item.get("processed_at"))
        quality_score += (timestamp_count / total_items) * 20

        return int(quality_score)

    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰ç«¯åˆ°ç«¯æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹ç¬¬å››é˜¶æ®µç«¯åˆ°ç«¯æµ‹è¯•")
        print("=" * 60)

        start_time = datetime.now()

        tests = [
            ("å®Œæ•´æ•°æ®æµæµ‹è¯•", self.test_complete_data_flow),
            ("å¹¶å‘æ•°æ®è®¿é—®æµ‹è¯•", self.test_concurrent_data_access),
            ("æ•°æ®ä¸€è‡´æ€§æµ‹è¯•", self.test_data_consistency_across_components),
            ("é”™è¯¯æ¢å¤å’Œå¥å£®æ€§æµ‹è¯•", self.test_error_recovery_and_robustness),
            ("ç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•", self.test_system_performance_benchmarks),
        ]

        for test_name, test_func in tests:
            print(f"\nğŸ” æ‰§è¡Œ {test_name}...")
            self.test_results["total_tests"] += 1

            try:
                if await test_func():
                    print(f"âœ… {test_name}é€šè¿‡")
                    self.test_results["passed_tests"] += 1
                else:
                    print(f"âŒ {test_name}å¤±è´¥")
                    self.test_results["failed_tests"] += 1
            except Exception as e:
                print(f"âŒ {test_name}å¼‚å¸¸: {e}")
                self.test_results["failed_tests"] += 1
                self.test_results["errors"].append(f"{test_name}: {e}")

        end_time = datetime.now()
        duration = end_time - start_time

        print("\n" + "=" * 60)
        print(f"ğŸ“Š ç¬¬å››é˜¶æ®µç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆ!")
        print(f"   æ€»è®¡: {self.test_results['total_tests']}")
        print(f"   é€šè¿‡: {self.test_results['passed_tests']}")
        print(f"   å¤±è´¥: {self.test_results['failed_tests']}")
        print(f"   è€—æ—¶: {duration.total_seconds():.2f} ç§’")

        # æ€§èƒ½æŒ‡æ ‡æ‘˜è¦
        if self.test_results["performance_metrics"]:
            print(f"\nâš¡ æ€§èƒ½æŒ‡æ ‡æ‘˜è¦:")
            metrics = self.test_results["performance_metrics"]
            for key, value in metrics.items():
                if isinstance(value, float):
                    print(f"   {key}: {value:.6f}")
                else:
                    print(f"   {key}: {value}")

        # æ•°æ®è´¨é‡æŒ‡æ ‡
        if self.test_results["data_quality_metrics"]:
            print(f"\nğŸ“ˆ æ•°æ®è´¨é‡æŒ‡æ ‡:")
            for key, value in self.test_results["data_quality_metrics"].items():
                print(f"   {key}: {value}/100")

        if self.test_results["errors"]:
            print("\nâŒ é”™è¯¯è¯¦æƒ…:")
            for error in self.test_results["errors"]:
                print(f"   - {error}")

        success_rate = 0
        if self.test_results["total_tests"] > 0:
            success_rate = (
                self.test_results["passed_tests"] / self.test_results["total_tests"]
            ) * 100

        print(f"\nğŸ¯ æˆåŠŸç‡: {success_rate:.1f}%")

        if self.test_results["failed_tests"] == 0:
            print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
            print("âœ… å®Œæ•´æ•°æ®æµæ­£å¸¸å·¥ä½œ")
            print("âœ… å¹¶å‘è®¿é—®æ€§èƒ½ä¼˜ç§€")
            print("âœ… æ•°æ®ä¸€è‡´æ€§ä¿è¯")
            print("âœ… é”™è¯¯æ¢å¤æœºåˆ¶å®Œå–„")
            print("âœ… ç³»ç»Ÿæ€§èƒ½è¾¾åˆ°åŸºå‡†")
            print("ğŸš€ ç¬¬å››é˜¶æ®µç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆï¼")
            return True
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
            return False


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    tester = Stage4E2ETester()

    try:
        # åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ
        await tester.initialize()

        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        success = await tester.run_all_tests()

        # æ¸…ç†æµ‹è¯•ç¯å¢ƒ
        await tester.cleanup()

        print(f"\né€€å‡ºç : {0 if success else 1}")
        return success

    except Exception as e:
        logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
