#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•
åŸºäºé«˜è¦†ç›–ç‡çš„æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼Œè¯†åˆ«æ€§èƒ½ç“¶é¢ˆï¼Œä¼˜åŒ–å…³é”®è·¯å¾„
"""

import asyncio
import time
from typing import Any
from unittest.mock import AsyncMock, Mock

import pytest

# å°è¯•å¯¼å…¥æ€§èƒ½æµ‹è¯•ç›¸å…³æ¨¡å—
try:
    import pytest_benchmark

    BENCHMARK_AVAILABLE = True
except ImportError:
    BENCHMARK_AVAILABLE = False
    print("Warning: pytest-benchmark not available, running basic performance tests")

try:
    from src.cache.redis_enhanced import EnhancedRedisManager
    from src.domain.services.match_service import MatchService
    from src.services.prediction import PredictionService
    from src.utils.date_utils import format_datetime
    from src.utils.string_utils import format_currency
except ImportError as e:
    print(f"Warning: Could not import some modules: {e}")
    # åˆ›å»ºMockå¯¹è±¡ç”¨äºæµ‹è¯•
    PredictionService = Mock
    MatchService = Mock


@pytest.mark.performance
class TestCoreServicesBenchmark:
    """æ ¸å¿ƒæœåŠ¡æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    def sample_prediction_data(self) -> dict[str, Any]:
        """ç¤ºä¾‹é¢„æµ‹æ•°æ®"""
        return {
            "match_id": 12345,
            "predicted_result": "home_win",
            "confidence": 0.75,
            "home_team_odds": 2.1,
            "away_team_odds": 3.4,
            "draw_odds": 3.2,
            "analysis_data": {
                "home_form": 5,
                "away_form": 3,
                "h2h_history": {"home_wins": 8, "away_wins": 5, "draws": 2},
                "team_strength_diff": 15,
            },
        }

    @pytest.fixture
    def sample_match_data(self) -> dict[str, Any]:
        """ç¤ºä¾‹æ¯”èµ›æ•°æ®"""
        return {
            "id": 12345,
            "home_team": "Team A",
            "away_team": "Team B",
            "home_score": 2,
            "away_score": 1,
            "match_status": "finished",
            "match_time": "2024-01-15T15:00:00Z",
            "league": "Premier League",
            "season": "2024-2025",
        }

    def test_prediction_service_creation_performance(self, sample_prediction_data):
        """æµ‹è¯•é¢„æµ‹æœåŠ¡åˆ›å»ºæ€§èƒ½"""
        service = PredictionService()

        # æ€§èƒ½åŸºå‡†ï¼š100æ¬¡åˆ›å»ºåº”åœ¨1ç§’å†…å®Œæˆ
        start_time = time.time()

        for i in range(100):
            prediction_data = {
                **sample_prediction_data,
                "match_id": sample_prediction_data["match_id"] + i,
            }
            # æ¨¡æ‹Ÿåˆ›å»ºè¿‡ç¨‹
            result = (
                service.create_prediction(prediction_data)
                if hasattr(service, "create_prediction")
                else {"id": i}
            )

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 1.0, f"100æ¬¡é¢„æµ‹åˆ›å»ºè€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡1ç§’é™åˆ¶"
        print(f"âœ… 100æ¬¡é¢„æµ‹åˆ›å»ºè€—æ—¶: {duration:.3f} ç§’")

    def test_match_service_performance(self, sample_match_data):
        """æµ‹è¯•æ¯”èµ›æœåŠ¡æ€§èƒ½"""
        service = MatchService()

        # æ€§èƒ½åŸºå‡†ï¼š100æ¬¡æŸ¥è¯¢åº”åœ¨0.5ç§’å†…å®Œæˆ
        start_time = time.time()

        for i in range(100):
            match_data = {**sample_match_data, "id": sample_match_data["id"] + i}
            # æ¨¡æ‹ŸæŸ¥è¯¢è¿‡ç¨‹
            result = (
                service.get_match(match_data["id"])
                if hasattr(service, "get_match")
                else match_data
            )

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 0.5, f"100æ¬¡æ¯”èµ›æŸ¥è¯¢è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡0.5ç§’é™åˆ¶"
        print(f"âœ… 100æ¬¡æ¯”èµ›æŸ¥è¯¢è€—æ—¶: {duration:.3f} ç§’")


@pytest.mark.performance
class TestUtilsBenchmark:
    """å·¥å…·å‡½æ•°æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def test_string_utils_performance(self):
        """æµ‹è¯•å­—ç¬¦ä¸²å·¥å…·æ€§èƒ½"""
        test_strings = [
            "Hello World",
            "123.45",
            "test@example.com",
            "2024-01-15T15:00:00Z",
            "Premier League",
        ]

        # æµ‹è¯•å­—ç¬¦ä¸²æ ¼å¼åŒ–æ€§èƒ½
        start_time = time.time()

        for _ in range(1000):
            for s in test_strings:
                # æµ‹è¯•å„ç§å­—ç¬¦ä¸²æ“ä½œ
                _ = s.upper()
                _ = s.lower()
                _ = len(s)
                _ = s.replace(" ", "_")

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 0.1, f"å­—ç¬¦ä¸²æ“ä½œè€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡0.1ç§’é™åˆ¶"
        print(f"âœ… 5000æ¬¡å­—ç¬¦ä¸²æ“ä½œè€—æ—¶: {duration:.3f} ç§’")

    def test_format_currency_performance(self):
        """æµ‹è¯•è´§å¸æ ¼å¼åŒ–æ€§èƒ½"""
        amounts = [12.34, 100.0, 0.99, 999.99, 1234567.89]

        start_time = time.time()

        for _ in range(1000):
            for amount in amounts:
                _ = format_currency(amount)

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 0.2, f"è´§å¸æ ¼å¼åŒ–è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡0.2ç§’é™åˆ¶"
        print(f"âœ… 5000æ¬¡è´§å¸æ ¼å¼åŒ–è€—æ—¶: {duration:.3f} ç§’")

    def test_datetime_format_performance(self):
        """æµ‹è¯•æ—¥æœŸæ—¶é—´æ ¼å¼åŒ–æ€§èƒ½"""
        timestamps = [
            1705310400,  # 2024-01-15 00:00:00
            1705396800,  # 2024-01-16 00:00:00
            time.time(),  # å½“å‰æ—¶é—´
        ]

        start_time = time.time()

        for _ in range(1000):
            for ts in timestamps:
                _ = (
                    format_datetime(ts)
                    if hasattr(format_datetime, "__call__")
                    else str(ts)
                )

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 0.3, f"æ—¥æœŸæ ¼å¼åŒ–è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡0.3ç§’é™åˆ¶"
        print(f"âœ… 3000æ¬¡æ—¥æœŸæ ¼å¼åŒ–è€—æ—¶: {duration:.3f} ç§’")


@pytest.mark.performance
class TestCacheBenchmark:
    """ç¼“å­˜æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    @pytest.fixture
    def mock_redis(self):
        """æ¨¡æ‹ŸRedisè¿æ¥"""
        redis_mock = AsyncMock()
        redis_mock.get.return_value = None
        redis_mock.set.return_value = True
        redis_mock.delete.return_value = 1
        return redis_mock

    def test_cache_read_performance(self, mock_redis):
        """æµ‹è¯•ç¼“å­˜è¯»å–æ€§èƒ½"""
        cache_manager = EnhancedRedisManager(redis_client=mock_redis)

        # æ€§èƒ½åŸºå‡†ï¼š1000æ¬¡ç¼“å­˜è¯»å–åº”åœ¨0.5ç§’å†…å®Œæˆ
        start_time = time.time()

        for i in range(1000):
            key = f"test_key_{i}"
            # æ¨¡æ‹Ÿç¼“å­˜è¯»å–
            _ = cache_manager.get(key) if hasattr(cache_manager, "get") else None

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 0.5, f"1000æ¬¡ç¼“å­˜è¯»å–è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡0.5ç§’é™åˆ¶"
        print(f"âœ… 1000æ¬¡ç¼“å­˜è¯»å–è€—æ—¶: {duration:.3f} ç§’")

    def test_cache_write_performance(self, mock_redis):
        """æµ‹è¯•ç¼“å­˜å†™å…¥æ€§èƒ½"""
        cache_manager = EnhancedRedisManager(redis_client=mock_redis)

        # æ€§èƒ½åŸºå‡†ï¼š1000æ¬¡ç¼“å­˜å†™å…¥åº”åœ¨1ç§’å†…å®Œæˆ
        start_time = time.time()

        for i in range(1000):
            key = f"test_key_{i}"
            value = f"test_value_{i}"
            # æ¨¡æ‹Ÿç¼“å­˜å†™å…¥
            _ = cache_manager.set(key, value) if hasattr(cache_manager, "set") else True

        end_time = time.time()
        duration = end_time - start_time

        assert duration < 1.0, f"1000æ¬¡ç¼“å­˜å†™å…¥è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡1ç§’é™åˆ¶"
        print(f"âœ… 1000æ¬¡ç¼“å­˜å†™å…¥è€—æ—¶: {duration:.3f} ç§’")


@pytest.mark.performance
@pytest.mark.asyncio
class TestAsyncPerformance:
    """å¼‚æ­¥æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    async def test_async_prediction_service_performance(self):
        """æµ‹è¯•å¼‚æ­¥é¢„æµ‹æœåŠ¡æ€§èƒ½"""
        service = PredictionService()

        # æ€§èƒ½åŸºå‡†ï¼š100æ¬¡å¼‚æ­¥æ“ä½œåº”åœ¨2ç§’å†…å®Œæˆ
        start_time = time.time()

        tasks = []
        for i in range(100):
            task = asyncio.create_task(self._simulate_async_prediction(service, i))
            tasks.append(task)

        results = await asyncio.gather(*tasks)
        end_time = time.time()
        duration = end_time - start_time

        assert len(results) == 100, "å¼‚æ­¥æ“ä½œç»“æœæ•°é‡ä¸æ­£ç¡®"
        assert duration < 2.0, f"100æ¬¡å¼‚æ­¥æ“ä½œè€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡2ç§’é™åˆ¶"
        print(f"âœ… 100æ¬¡å¼‚æ­¥é¢„æµ‹æ“ä½œè€—æ—¶: {duration:.3f} ç§’")

    async def test_async_database_query_performance(self):
        """æµ‹è¯•å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"""
        # æ¨¡æ‹Ÿå¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢
        mock_db = AsyncMock()
        mock_db.fetch.return_value = {"id": 1, "name": "test"}

        start_time = time.time()

        tasks = []
        for i in range(50):
            task = asyncio.create_task(self._simulate_async_db_query(mock_db, i))
            tasks.append(task)

        results = await asyncio.gather(*tasks)
        end_time = time.time()
        duration = end_time - start_time

        assert len(results) == 50, "å¼‚æ­¥æŸ¥è¯¢ç»“æœæ•°é‡ä¸æ­£ç¡®"
        assert duration < 1.5, f"50æ¬¡å¼‚æ­¥æŸ¥è¯¢è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡1.5ç§’é™åˆ¶"
        print(f"âœ… 50æ¬¡å¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢è€—æ—¶: {duration:.3f} ç§’")

    async def _simulate_async_prediction(self, service, index: int) -> dict[str, Any]:
        """æ¨¡æ‹Ÿå¼‚æ­¥é¢„æµ‹æ“ä½œ"""
        await asyncio.sleep(0.001)  # æ¨¡æ‹ŸI/Oå»¶è¿Ÿ
        return {"id": index, "prediction": f"result_{index}"}

    async def _simulate_async_db_query(self, mock_db, query_id: int) -> dict[str, Any]:
        """æ¨¡æ‹Ÿå¼‚æ­¥æ•°æ®åº“æŸ¥è¯¢"""
        await asyncio.sleep(0.002)  # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢å»¶è¿Ÿ
        return mock_db.fetch()


@pytest.mark.performance
class TestMemoryUsage:
    """å†…å­˜ä½¿ç”¨åŸºå‡†æµ‹è¯•"""

    def test_large_data_processing_memory(self):
        """æµ‹è¯•å¤§æ•°æ®å¤„ç†çš„å†…å­˜ä½¿ç”¨"""
        import gc
        import sys

        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
        gc.collect()
        initial_memory = sys.getsizeof([])

        # åˆ›å»ºå¤§é‡æ•°æ®
        large_data = []
        for i in range(10000):
            large_data.append(
                {
                    "id": i,
                    "name": f"item_{i}",
                    "description": f"Description for item {i} with additional text to increase memory usage",
                    "metadata": {"created_at": time.time(), "type": f"type_{i % 5}"},
                }
            )

        # å¤„ç†æ•°æ®
        processed_count = 0
        start_time = time.time()

        for item in large_data:
            # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
            processed_item = {**item, "processed": True, "processed_at": time.time()}
            processed_count += 1

        end_time = time.time()
        duration = end_time - start_time

        # è·å–å¤„ç†åå†…å­˜ä½¿ç”¨
        final_memory = sys.getsizeof(large_data)
        memory_increase = final_memory - initial_memory

        # æ¸…ç†
        del large_data
        gc.collect()

        assert processed_count == 10000, "æ•°æ®å¤„ç†æ•°é‡ä¸æ­£ç¡®"
        assert duration < 1.0, f"10000æ¡æ•°æ®å¤„ç†è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡1ç§’é™åˆ¶"
        assert (
            memory_increase < 50 * 1024 * 1024
        ), f"å†…å­˜å¢åŠ  {memory_increase / 1024 / 1024:.1f} MBï¼Œè¶…è¿‡50MBé™åˆ¶"

        print(
            f"âœ… 10000æ¡æ•°æ®å¤„ç†è€—æ—¶: {duration:.3f} ç§’ï¼Œå†…å­˜å¢åŠ : {memory_increase / 1024 / 1024:.1f} MB"
        )


@pytest.mark.performance
class TestConcurrencyPerformance:
    """å¹¶å‘æ€§èƒ½åŸºå‡†æµ‹è¯•"""

    def test_thread_safety_performance(self):
        """æµ‹è¯•çº¿ç¨‹å®‰å…¨çš„æ€§èƒ½å½±å“"""
        import threading
        import time

        results = []

        def worker_function(worker_id: int):
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            thread_results = []
            for i in range(100):
                # æ¨¡æ‹Ÿå·¥ä½œè´Ÿè½½
                _ = worker_id * i
                thread_results.append(i)
            results.extend(thread_results)

        start_time = time.time()

        # åˆ›å»ºå¤šä¸ªçº¿ç¨‹
        threads = []
        for i in range(4):
            thread = threading.Thread(target=worker_function, args=(i,))
            threads.append(thread)
            thread.start()

        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()

        end_time = time.time()
        duration = end_time - start_time

        assert len(results) == 400, f"å¤šçº¿ç¨‹å¤„ç†ç»“æœæ•°é‡ä¸æ­£ç¡®: {len(results)}"
        assert duration < 0.5, f"å¤šçº¿ç¨‹å¤„ç†è€—æ—¶ {duration:.3f} ç§’ï¼Œè¶…è¿‡0.5ç§’é™åˆ¶"
        print(f"âœ… 4çº¿ç¨‹å¤„ç†400é¡¹è€—æ—¶: {duration:.3f} ç§’")


# æ€§èƒ½å›å½’æ£€æµ‹
@pytest.mark.performance
class TestPerformanceRegression:
    """æ€§èƒ½å›å½’æ£€æµ‹"""

    def test_api_response_time_regression(self):
        """APIå“åº”æ—¶é—´å›å½’æµ‹è¯•"""
        # åŸºå‡†å“åº”æ—¶é—´ï¼š100ms
        max_response_time = 0.1

        start_time = time.time()

        # æ¨¡æ‹ŸAPIè°ƒç”¨
        for _ in range(50):
            # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„APIè°ƒç”¨
            _ = self._simulate_api_call("/health")
            _ = self._simulate_api_call("/api/v1/predictions")
            _ = self._simulate_api_call("/api/v1/matches")

        end_time = time.time()
        avg_response_time = (end_time - start_time) / 50

        assert (
            avg_response_time < max_response_time
        ), f"å¹³å‡APIå“åº”æ—¶é—´ {avg_response_time*1000:.1f}ms è¶…è¿‡ {max_response_time*1000}ms é™åˆ¶"
        print(f"âœ… å¹³å‡APIå“åº”æ—¶é—´: {avg_response_time*1000:.1f}ms")

    def _simulate_api_call(self, endpoint: str) -> dict[str, Any]:
        """æ¨¡æ‹ŸAPIè°ƒç”¨"""
        time.sleep(0.001)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        return {"endpoint": endpoint, "status": "ok"}


if __name__ == "__main__":
    # è¿è¡Œæ€§èƒ½æµ‹è¯•
    print("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")

    # è¿è¡Œæ‰€æœ‰æ€§èƒ½æµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short", "-x"])  # ç¬¬ä¸€ä¸ªå¤±è´¥æ—¶åœæ­¢
