"""
æµ‹è¯•è´¨é‡ç›‘æ§ç³»ç»Ÿ
æä¾›æµ‹è¯•è¦†ç›–ç‡ã€æ€§èƒ½ã€ç¨³å®šæ€§ç­‰æŒ‡æ ‡çš„å®æ—¶ç›‘æ§
"""

import json
import time
import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import subprocess
import sys
import os


class TestQualityMonitor:
    """æµ‹è¯•è´¨é‡ç›‘æ§å™¨"""

    def __init__(self, project_root: str = None):
        self.project_root = Path(project_root) if project_root else Path(__file__).parent.parent.parent
        self.metrics_dir = self.project_root / "tests" / "metrics"
        self.metrics_dir.mkdir(exist_ok=True)
        self.current_metrics = {}

    def collect_coverage_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æµ‹è¯•è¦†ç›–ç‡æŒ‡æ ‡"""
        try:
            # è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
            result = subprocess.run(
                [
                    sys.executable, "-m", "pytest",
                    "tests/unit/",
                    "--cov=src",
                    "--cov-report=json",
                    "--cov-report=term-missing",
                    "--quiet"
                ],
                cwd=self.project_root,
                capture_output=True,
                text=True
            )

            # è¯»å–è¦†ç›–ç‡æŠ¥å‘Š
            coverage_file = self.project_root / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file) as f:
                    coverage_data = json.load(f)

                metrics = {
                    "overall_coverage": coverage_data["totals"]["percent_covered"],
                    "covered_lines": coverage_data["totals"]["covered_lines"],
                    "missing_lines": coverage_data["totals"]["missing_lines"],
                    "total_lines": coverage_data["totals"]["num_statements"],
                    "files": {}
                }

                # å„æ¨¡å—è¦†ç›–ç‡
                for filename, file_data in coverage_data["files"].items():
                    if "src/" in filename:
                        module_name = filename.replace("src/", "").replace(".py", "")
                        metrics["files"][module_name] = {
                            "coverage": file_data["summary"]["percent_covered"],
                            "covered_lines": file_data["summary"]["covered_lines"],
                            "missing_lines": file_data["summary"]["missing_lines"]
                        }

                return metrics

        except Exception as e:
            print(f"æ”¶é›†è¦†ç›–ç‡æŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}

    def collect_performance_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
        try:
            start_time = time.time()

            # è¿è¡Œå•å…ƒæµ‹è¯•å¹¶è®¡æ—¶
            result = subprocess.run(
                [
                    sys.executable, "-m", "pytest",
                    "tests/unit/",
                    "--durations=0",
                    "--quiet"
                ],
                cwd=self.project_root,
                capture_output=True,
                text=True
            )

            total_time = time.time() - start_time

            # è§£ææµ‹è¯•æ—¶é•¿
            durations = []
            lines = result.stdout.split('\n')
            for line in lines:
                if 'test_' in line and '::' in line:
                    try:
                        parts = line.split()
                        duration = float(parts[-1])
                        test_name = parts[-2]
                        durations.append({
                            "test": test_name,
                            "duration": duration
                        })
                    except:
                        continue

            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            if durations:
                durations.sort(key=lambda x: x["duration"], reverse=True)
                avg_duration = sum(d["duration"] for d in durations) / len(durations)
                max_duration = durations[0]["duration"]
                min_duration = durations[-1]["duration"]

                return {
                    "total_time": total_time,
                    "test_count": len(durations),
                    "average_duration": avg_duration,
                    "max_duration": max_duration,
                    "min_duration": min_duration,
                    "slowest_tests": durations[:5],  # æœ€æ…¢çš„5ä¸ªæµ‹è¯•
                    "passed": result.returncode == 0
                }

        except Exception as e:
            print(f"æ”¶é›†æ€§èƒ½æŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}

    def collect_stability_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æµ‹è¯•ç¨³å®šæ€§æŒ‡æ ‡"""
        try:
            # å¤šæ¬¡è¿è¡Œæµ‹è¯•æ£€æŸ¥ç¨³å®šæ€§
            runs = []
            for i in range(3):
                start_time = time.time()
                result = subprocess.run(
                    [
                        sys.executable, "-m", "pytest",
                        "tests/unit/",
                        "--quiet"
                    ],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True
                )
                run_time = time.time() - start_time

                # è§£ææµ‹è¯•ç»“æœ
                output = result.stdout + result.stderr
                passed = output.count("passed")
                failed = output.count("failed")
                skipped = output.count("skipped")
                errors = output.count("ERROR")

                runs.append({
                    "run": i + 1,
                    "time": run_time,
                    "passed": passed,
                    "failed": failed,
                    "skipped": skipped,
                    "errors": errors,
                    "exit_code": result.returncode
                })

            # è®¡ç®—ç¨³å®šæ€§æŒ‡æ ‡
            if runs:
                avg_time = sum(r["time"] for r in runs) / len(runs)
                all_passed = all(r["exit_code"] == 0 for r in runs)
                consistency = 1.0 if all_passed else 0.5

                return {
                    "runs": runs,
                    "average_time": avg_time,
                    "all_passed": all_passed,
                    "stability_score": consistency,
                    "total_tests": sum(r["passed"] + r["failed"] for r in runs) / len(runs)
                }

        except Exception as e:
            print(f"æ”¶é›†ç¨³å®šæ€§æŒ‡æ ‡å¤±è´¥: {e}")
            return {"error": str(e)}

    def analyze_test_trends(self) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•è¶‹åŠ¿"""
        try:
            # è¯»å–å†å²æ•°æ®
            history_file = self.metrics_dir / "history.json"
            if not history_file.exists():
                return {"trend": "no_data"}

            with open(history_file) as f:
                history = json.load(f)

            # åˆ†ææœ€è¿‘7å¤©çš„æ•°æ®
            recent_data = history[-7:] if len(history) >= 7 else history

            if len(recent_data) < 2:
                return {"trend": "insufficient_data"}

            # è¦†ç›–ç‡è¶‹åŠ¿
            coverage_trend = []
            for record in recent_data:
                if "coverage" in record and "overall_coverage" in record["coverage"]:
                    coverage_trend.append(record["coverage"]["overall_coverage"])

            # æ€§èƒ½è¶‹åŠ¿
            performance_trend = []
            for record in recent_data:
                if "performance" in record and "total_time" in record["performance"]:
                    performance_trend.append(record["performance"]["total_time"])

            trend_analysis = {
                "coverage_trend": self._calculate_trend(coverage_trend),
                "performance_trend": self._calculate_trend(performance_trend, reverse=True),
                "data_points": len(recent_data)
            }

            return trend_analysis

        except Exception as e:
            print(f"åˆ†ææµ‹è¯•è¶‹åŠ¿å¤±è´¥: {e}")
            return {"error": str(e)}

    def _calculate_trend(self, data: List[float], reverse: bool = False) -> str:
        """è®¡ç®—æ•°æ®è¶‹åŠ¿"""
        if len(data) < 2:
            return "stable"

        # è®¡ç®—è¶‹åŠ¿
        if reverse:  # å¯¹äºæ—¶é—´æ•°æ®ï¼Œè¶Šå°è¶Šå¥½
            if data[-1] < data[0] * 0.95:
                return "improving"
            elif data[-1] > data[0] * 1.05:
                return "degrading"
        else:  # å¯¹äºè¦†ç›–ç‡ç­‰ï¼Œè¶Šå¤§è¶Šå¥½
            if data[-1] > data[0] * 1.02:
                return "improving"
            elif data[-1] < data[0] * 0.98:
                return "degrading"

        return "stable"

    def generate_quality_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæµ‹è¯•è´¨é‡æŠ¥å‘Š"""
        print("ğŸ” å¼€å§‹æ”¶é›†æµ‹è¯•è´¨é‡æŒ‡æ ‡...")

        # æ”¶é›†å„é¡¹æŒ‡æ ‡
        coverage_metrics = self.collect_coverage_metrics()
        performance_metrics = self.collect_performance_metrics()
        stability_metrics = self.collect_stability_metrics()
        trend_analysis = self.analyze_test_trends()

        # è®¡ç®—è´¨é‡è¯„åˆ†
        quality_score = self._calculate_quality_score(
            coverage_metrics,
            performance_metrics,
            stability_metrics
        )

        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.datetime.now().isoformat(),
            "quality_score": quality_score,
            "coverage": coverage_metrics,
            "performance": performance_metrics,
            "stability": stability_metrics,
            "trends": trend_analysis,
            "recommendations": self._generate_recommendations(
                coverage_metrics,
                performance_metrics,
                stability_metrics
            )
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.metrics_dir / f"report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2, default=str)

        # æ›´æ–°å†å²è®°å½•
        self._update_history(report)

        print(f"âœ… æµ‹è¯•è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report

    def _calculate_quality_score(self, coverage: Dict, performance: Dict, stability: Dict) -> Dict[str, Any]:
        """è®¡ç®—æµ‹è¯•è´¨é‡è¯„åˆ†"""
        score = 0
        max_score = 100
        details = {}

        # è¦†ç›–ç‡è¯„åˆ† (40åˆ†)
        if "overall_coverage" in coverage:
            cov = coverage["overall_coverage"]
            if cov >= 25:
                cov_score = 40
            elif cov >= 20:
                cov_score = 30
            elif cov >= 15:
                cov_score = 20
            else:
                cov_score = 10
            score += cov_score
            details["coverage"] = {"score": cov_score, "value": cov, "weight": 40}

        # æ€§èƒ½è¯„åˆ† (30åˆ†)
        if "total_time" in performance:
            perf_time = performance["total_time"]
            if perf_time <= 60:  # 1åˆ†é’Ÿå†…
                perf_score = 30
            elif perf_time <= 180:  # 3åˆ†é’Ÿå†…
                perf_score = 25
            elif perf_time <= 300:  # 5åˆ†é’Ÿå†…
                perf_score = 20
            else:
                perf_score = 10
            score += perf_score
            details["performance"] = {"score": perf_score, "value": perf_time, "weight": 30}

        # ç¨³å®šæ€§è¯„åˆ† (30åˆ†)
        if "stability_score" in stability:
            stab_score = stability["stability_score"] * 30
            score += stab_score
            details["stability"] = {"score": stab_score, "value": stability["stability_score"], "weight": 30}

        # è¯„çº§
        if score >= 90:
            grade = "A+"
        elif score >= 80:
            grade = "A"
        elif score >= 70:
            grade = "B"
        elif score >= 60:
            grade = "C"
        else:
            grade = "D"

        return {
            "total_score": score,
            "max_score": max_score,
            "grade": grade,
            "details": details
        }

    def _generate_recommendations(self, coverage: Dict, performance: Dict, stability: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # è¦†ç›–ç‡å»ºè®®
        if "overall_coverage" in coverage:
            cov = coverage["overall_coverage"]
            if cov < 20:
                recommendations.append("ğŸ“Š è¦†ç›–ç‡åä½ï¼Œå»ºè®®å¢åŠ å•å…ƒæµ‹è¯•ä»¥è¾¾åˆ°20%åŸºçº¿")
            elif cov < 25:
                recommendations.append("ğŸ“ˆ è¦†ç›–ç‡æ¥è¿‘åŸºçº¿ï¼Œå†å¢åŠ å°‘é‡æµ‹è¯•å³å¯è¾¾æ ‡")

            # æ£€æŸ¥ä½è¦†ç›–ç‡æ¨¡å—
            if "files" in coverage:
                low_coverage_files = [
                    f for f, data in coverage["files"].items()
                    if data["coverage"] < 15
                ]
                if low_coverage_files:
                    recommendations.append(f"ğŸ” ä»¥ä¸‹æ¨¡å—éœ€è¦é¢å¤–æµ‹è¯•è¦†ç›–: {', '.join(low_coverage_files)}")

        # æ€§èƒ½å»ºè®®
        if "total_time" in performance:
            perf_time = performance["total_time"]
            if perf_time > 300:
                recommendations.append("âš¡ æµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•æˆ–ä½¿ç”¨å¹¶è¡Œæ‰§è¡Œ")

            if "slowest_tests" in performance:
                slow_tests = performance["slowest_tests"][:3]
                if slow_tests and slow_tests[0]["duration"] > 5:
                    recommendations.append(f"ğŸŒ æœ€æ…¢çš„æµ‹è¯•éœ€è¦ä¼˜åŒ–: {slow_tests[0]['test']} ({slow_tests[0]['duration']:.2f}s)")

        # ç¨³å®šæ€§å»ºè®®
        if "all_passed" in stability and not stability["all_passed"]:
            recommendations.append("âš ï¸ æµ‹è¯•å­˜åœ¨ä¸ç¨³å®šæ€§ï¼Œæ£€æŸ¥å¤±è´¥çš„æµ‹è¯•å¹¶ä¿®å¤")

        return recommendations

    def _update_history(self, report: Dict[str, Any]):
        """æ›´æ–°å†å²è®°å½•"""
        history_file = self.metrics_dir / "history.json"

        # è¯»å–ç°æœ‰å†å²
        history = []
        if history_file.exists():
            with open(history_file) as f:
                history = json.load(f)

        # æ·»åŠ æ–°è®°å½•
        summary = {
            "timestamp": report["timestamp"],
            "quality_score": report["quality_score"]["total_score"],
            "grade": report["quality_score"]["grade"]
        }

        if "coverage" in report and "overall_coverage" in report["coverage"]:
            summary["coverage"] = {"overall_coverage": report["coverage"]["overall_coverage"]}

        if "performance" in report and "total_time" in report["performance"]:
            summary["performance"] = {"total_time": report["performance"]["total_time"]}

        history.append(summary)

        # ä¿ç•™æœ€è¿‘30å¤©çš„è®°å½•
        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=30)
        history = [
            record for record in history
            if datetime.datetime.fromisoformat(record["timestamp"]) > cutoff_date
        ]

        # ä¿å­˜å†å²
        with open(history_file, 'w') as f:
            json.dump(history, f, indent=2)

    def print_report_summary(self, report: Dict[str, Any]):
        """æ‰“å°æŠ¥å‘Šæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æµ‹è¯•è´¨é‡æŠ¥å‘Šæ‘˜è¦")
        print("="*60)

        # è´¨é‡è¯„åˆ†
        quality = report["quality_score"]
        print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {quality['total_score']}/{quality['max_score']} ({quality['grade']})")

        # è¦†ç›–ç‡
        if "coverage" in report and "overall_coverage" in report["coverage"]:
            cov = report["coverage"]["overall_coverage"]
            print(f"ğŸ“ˆ è¦†ç›–ç‡: {cov:.1f}%")

        # æ€§èƒ½
        if "performance" in report and "total_time" in report["performance"]:
            perf = report["performance"]["total_time"]
            print(f"âš¡ æ‰§è¡Œæ—¶é—´: {perf:.1f}ç§’")

        # ç¨³å®šæ€§
        if "stability" in report and "stability_score" in report["stability"]:
            stab = report["stability"]["stability_score"]
            print(f"ğŸ›¡ï¸ ç¨³å®šæ€§: {stab*100:.1f}%")

        # è¶‹åŠ¿
        if "trends" in report:
            trends = report["trends"]
            if "coverage_trend" in trends:
                trend_icon = {"improving": "ğŸ“ˆ", "degrading": "ğŸ“‰", "stable": "â¡ï¸"}
                print(f"ğŸ“Š è¦†ç›–ç‡è¶‹åŠ¿: {trend_icon.get(trends['coverage_trend'], 'â¡ï¸')} {trends['coverage_trend']}")

        # å»ºè®®
        if "recommendations" in report and report["recommendations"]:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for rec in report["recommendations"]:
                print(f"   {rec}")

        print("\n" + "="*60)


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="æµ‹è¯•è´¨é‡ç›‘æ§å·¥å…·")
    parser.add_argument("--output", "-o", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--quiet", "-q", action="store_true", help="é™é»˜æ¨¡å¼")
    args = parser.parse_args()

    # åˆ›å»ºç›‘æ§å™¨
    monitor = TestQualityMonitor()

    # ç”ŸæˆæŠ¥å‘Š
    report = monitor.generate_quality_report()

    # è¾“å‡ºæ‘˜è¦
    if not args.quiet:
        monitor.print_report_summary(report)

    # ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")

    # è¿”å›è¯„åˆ†ï¼ˆç”¨äºCIï¼‰
    return report["quality_score"]["total_score"]


if __name__ == "__main__":
    sys.exit(0 if main() >= 60 else 1)