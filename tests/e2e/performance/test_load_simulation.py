"""
è´Ÿè½½æ¨¡æ‹Ÿ E2E æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿåœ¨é«˜å¹¶å‘ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import asyncio
import statistics
import time
from datetime import datetime, timedelta, timezone
from typing import Any, Dict, List

import aiohttp
import pytest
from httpx import AsyncClient


@pytest.mark.e2e
@pytest.mark.performance
class TestLoadSimulation:
    """è´Ÿè½½æ¨¡æ‹Ÿæ€§èƒ½æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_concurrent_user_predictions(
        self, api_client: AsyncClient, test_data_loader, performance_metrics
    ):
        """æµ‹è¯•å¹¶å‘ç”¨æˆ·åˆ›å»ºé¢„æµ‹çš„æ€§èƒ½"""
        # 1. å‡†å¤‡æµ‹è¯•æ•°æ®
        _teams = await test_data_loader.create_teams()

        # åˆ›å»ºå¤šä¸ªå³å°†åˆ°æ¥çš„æ¯”èµ›
        matches_data = []
        for i in range(10):
            match_data = {
                "home_team_id": teams[i % len(teams)]["id"],
                "away_team_id": teams[(i + 1) % len(teams)]["id"],
                "match_date": (
                    datetime.now(timezone.utc) + timedelta(hours=i + 1)
                ).isoformat(),
                "competition": "Performance League",
                "season": "2024/2025",
                "status": "UPCOMING",
            }
            matches_data.append(match_data)

        # ä½¿ç”¨ç®¡ç†å‘˜åˆ›å»ºæ¯”èµ›
        admin_token = (
            await api_client.post(
                "/api/v1/auth/login",
                _data={"username": "e2e_admin", "password": "E2EAdminPass123!"},
            )
        ).json()["access_token"]
        admin_headers = {"Authorization": f"Bearer {admin_token}"}

        created_matches = []
        for match_data in matches_data:
            response = await api_client.post(
                "/api/v1/matches", json=match_data, headers=admin_headers
            )
            if response.status_code == 201:
                created_matches.append(response.json())

        assert len(created_matches) >= 5, "éœ€è¦è‡³å°‘5åœºæ¯”èµ›"

        # 2. å‡†å¤‡å¹¶å‘ç”¨æˆ·
        concurrent_users = 50
        predictions_per_user = 10

        # åˆ›å»ºæµ‹è¯•ç”¨æˆ·ä¼šè¯
        sessions = []
        for i in range(concurrent_users):
            # æ³¨å†Œç”¨æˆ·
            user_data = {
                "username": f"perf_user_{i:03d}",
                "email": f"perfuser{i:03d}@example.com",
                "password": "PerfTest123!",
            }

            response = await api_client.post("/api/v1/auth/register", json=user_data)
            if response.status_code != 201:
                continue  # ç”¨æˆ·å¯èƒ½å·²å­˜åœ¨

            # ç™»å½•
            login_data = {
                "username": user_data["username"],
                "password": user_data["password"],
            }
            response = await api_client.post("/api/v1/auth/login", _data=login_data)
            if response.status_code == 200:
                token = response.json()["access_token"]
                sessions.append(
                    {
                        "user_id": i,
                        "token": token,
                        "headers": {"Authorization": f"Bearer {token}"},
                    }
                )

        assert len(sessions) >= 30, f"è‡³å°‘éœ€è¦30ä¸ªç”¨æˆ·ï¼Œå½“å‰åªæœ‰{len(sessions)}ä¸ª"
        sessions = sessions[:30]  # é™åˆ¶ç”¨æˆ·æ•°ä»¥é¿å…è¿‡è½½

        print(f"âœ… å‡†å¤‡äº† {len(sessions)} ä¸ªå¹¶å‘ç”¨æˆ·")

        # 3. å¹¶å‘åˆ›å»ºé¢„æµ‹
        performance_metrics.start_timer("concurrent_predictions")

        async def create_predictions(session: Dict[str, Any]):
            """å•ä¸ªç”¨æˆ·çš„é¢„æµ‹åˆ›å»ºä»»åŠ¡"""
            response_times = []
            success_count = 0

            for i in range(predictions_per_user):
                match = created_matches[i % len(created_matches)]
                prediction_types = ["HOME_WIN", "DRAW", "AWAY_WIN"]

                pred_data = {
                    "match_id": match["id"],
                    "prediction": prediction_types[i % 3],
                    "confidence": round(0.5 + (i * 0.05), 2),
                }

                start_time = time.time()
                response = await api_client.post(
                    "/api/v1/predictions", json=pred_data, headers=session["headers"]
                )
                end_time = time.time()

                response_times.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

                if response.status_code == 201:
                    success_count += 1
                elif response.status_code == 400:
                    # å¯èƒ½æ˜¯é‡å¤é¢„æµ‹ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª
                    pass

            return {
                "user_id": session["user_id"],
                "response_times": response_times,
                "success_count": success_count,
                "avg_response_time": (
                    statistics.mean(response_times) if response_times else 0
                ),
            }

        # æ‰§è¡Œå¹¶å‘ä»»åŠ¡
        print(f"ğŸš€ å¼€å§‹å¹¶å‘æµ‹è¯•: {len(sessions)} ç”¨æˆ· x {predictions_per_user} é¢„æµ‹")

        results = await asyncio.gather(
            *[create_predictions(session) for session in sessions],
            return_exceptions=True,
        )

        # ç»Ÿè®¡ç»“æœ
        total_predictions = 0
        total_successful = 0
        all_response_times = []

        for result in results:
            if isinstance(result, dict):
                total_predictions += predictions_per_user
                total_successful += result["success_count"]
                all_response_times.extend(result["response_times"])

        duration = performance_metrics.end_timer("concurrent_predictions")

        # æ€§èƒ½æŒ‡æ ‡
        success_rate = (
            total_successful / total_predictions if total_predictions > 0 else 0
        )
        avg_response_time = (
            statistics.mean(all_response_times) if all_response_times else 0
        )
        p95_response_time = (
            statistics.quantiles(all_response_times, n=20)[18]
            if len(all_response_times) >= 20
            else max(all_response_times) if all_response_times else 0
        )
        p99_response_time = (
            statistics.quantiles(all_response_times, n=100)[98]
            if len(all_response_times) >= 100
            else max(all_response_times) if all_response_times else 0
        )
        throughput = total_successful / duration if duration > 0 else 0

        print("\nğŸ“Š å¹¶å‘æµ‹è¯•ç»“æœ:")
        print(f"   - æ€»è¯·æ±‚æ•°: {total_predictions}")
        print(f"   - æˆåŠŸæ•°: {total_successful}")
        print(f"   - æˆåŠŸç‡: {success_rate * 100:.1f}%")
        print(f"   - æ€»è€—æ—¶: {duration:.2f}s")
        print(f"   - ååé‡: {throughput:.1f} req/s")
        print(f"   - å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.1f}ms")
        print(f"   - P95å“åº”æ—¶é—´: {p95_response_time:.1f}ms")
        print(f"   - P99å“åº”æ—¶é—´: {p99_response_time:.1f}ms")

        # æ€§èƒ½æ–­è¨€
        assert success_rate >= 0.95, f"æˆåŠŸç‡è¿‡ä½: {success_rate * 100:.1f}%"
        assert avg_response_time < 1000, f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.1f}ms"
        assert p95_response_time < 2000, f"P95å“åº”æ—¶é—´è¿‡é•¿: {p95_response_time:.1f}ms"
        assert throughput >= 10, f"ååé‡è¿‡ä½: {throughput:.1f} req/s"

    @pytest.mark.asyncio
    async def test_api_endpoints_performance(
        self, api_client: AsyncClient, test_data_loader, performance_metrics
    ):
        """æµ‹è¯•å„APIç«¯ç‚¹çš„æ€§èƒ½"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        _teams = await test_data_loader.create_teams()
        _matches = await test_data_loader.create_matches()

        # è·å–token
        user_token = (
            await api_client.post(
                "/api/v1/auth/login",
                _data={"username": "e2e_user", "password": "E2ETestPass123!"},
            )
        ).json()["access_token"]
        headers = {"Authorization": f"Bearer {user_token}"}

        # å®šä¹‰è¦æµ‹è¯•çš„ç«¯ç‚¹
        endpoints = [
            {
                "name": "è·å–æ¯”èµ›åˆ—è¡¨",
                "method": "GET",
                "url": "/api/v1/matches",
                "params": {"limit": 20},
            },
            {
                "name": "è·å–æ¯”èµ›è¯¦æƒ…",
                "method": "GET",
                "url": f"/api/v1/matches/{matches[0]['id']}",
            },
            {
                "name": "è·å–é¢„æµ‹åˆ—è¡¨",
                "method": "GET",
                "url": "/api/v1/predictions",
                "params": {"limit": 20},
            },
            {"name": "è·å–ç”¨æˆ·ä¿¡æ¯", "method": "GET", "url": "/api/v1/users/me"},
            {
                "name": "è·å–ç”¨æˆ·ç»Ÿè®¡",
                "method": "GET",
                "url": "/api/v1/users/me/statistics",
            },
            {"name": "å¥åº·æ£€æŸ¥", "method": "GET", "url": "/health"},
        ]

        # é¢„çƒ­ï¼ˆé¿å…å†·å¯åŠ¨å½±å“ï¼‰
        for endpoint in endpoints:
            await api_client.request(
                endpoint["method"],
                endpoint["url"],
                params=endpoint.get("params"),
                headers=headers if "health" not in endpoint["url"] else None,
            )

        # æ€§èƒ½æµ‹è¯•
        print("\nğŸ” APIç«¯ç‚¹æ€§èƒ½æµ‹è¯•:")
        results = []

        for endpoint in endpoints:
            response_times = []

            # æ¯ä¸ªç«¯ç‚¹æµ‹è¯•10æ¬¡
            for _ in range(10):
                start_time = time.time()

                response = await api_client.request(
                    endpoint["method"],
                    endpoint["url"],
                    params=endpoint.get("params"),
                    headers=headers if "health" not in endpoint["url"] else None,
                )

                end_time = time.time()
                response_times.append((end_time - start_time) * 1000)

                assert response.status_code < 500, f"ç«¯ç‚¹é”™è¯¯: {endpoint['name']}"

            avg_time = statistics.mean(response_times)
            min_time = min(response_times)
            max_time = max(response_times)

            results.append(
                {
                    "endpoint": endpoint["name"],
                    "avg": avg_time,
                    "min": min_time,
                    "max": max_time,
                }
            )

            print(
                f"   - {endpoint['name']}: {avg_time:.1f}ms (min: {min_time:.1f}ms, max: {max_time:.1f}ms)"
            )

        # æ€§èƒ½æ–­è¨€
        for result in results:
            assert (
                result["avg"] < 500
            ), f"{result['endpoint']} å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {result['avg']:.1f}ms"
            assert (
                result["max"] < 2000
            ), f"{result['endpoint']} æœ€å¤§å“åº”æ—¶é—´è¿‡é•¿: {result['max']:.1f}ms"

    @pytest.mark.asyncio
    async def test_database_query_performance(
        self, api_client: AsyncClient, test_data_loader, performance_metrics
    ):
        """æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"""
        # å‡†å¤‡å¤§é‡æ•°æ®
        _teams = await test_data_loader.create_teams()

        # åˆ›å»ºå¤šä¸ªç”¨æˆ·
        users = []
        for i in range(20):
            user_data = {
                "username": f"db_test_user_{i}",
                "email": f"dbtest{i}@example.com",
                "password": "DBTestPass123!",
            }
            response = await api_client.post("/api/v1/auth/register", json=user_data)
            if response.status_code == 201:
                users.append(user_data)

        # ä¸ºæ¯ä¸ªç”¨æˆ·åˆ›å»ºå¤šä¸ªé¢„æµ‹
        user_tokens = []
        for user in users:
            login_data = {"username": user["username"], "password": user["password"]}
            response = await api_client.post("/api/v1/auth/login", _data=login_data)
            if response.status_code == 200:
                user_tokens.append(response.json()["access_token"])

        # åˆ›å»ºæ¯”èµ›
        _matches = await test_data_loader.create_matches()

        # æ‰¹é‡åˆ›å»ºé¢„æµ‹
        total_predictions = 0
        for i, token in enumerate(user_tokens[:10]):
            headers = {"Authorization": f"Bearer {token}"}
            for j, match in enumerate(matches[:5]):
                pred_data = {
                    "match_id": match["id"],
                    "prediction": ["HOME_WIN", "DRAW", "AWAY_WIN"][j % 3],
                    "confidence": 0.6 + (j * 0.08),
                }
                response = await api_client.post(
                    "/api/v1/predictions", json=pred_data, headers=headers
                )
                if response.status_code == 201:
                    total_predictions += 1

        print(f"âœ… åˆ›å»ºäº† {total_predictions} ä¸ªé¢„æµ‹ç”¨äºæ€§èƒ½æµ‹è¯•")

        # æµ‹è¯•å„ç§æŸ¥è¯¢çš„æ€§èƒ½
        queries = [
            {
                "name": "è·å–æ‰€æœ‰é¢„æµ‹ï¼ˆåˆ†é¡µï¼‰",
                "url": "/api/v1/predictions",
                "params": {"limit": 20, "page": 1},
            },
            {
                "name": "æŒ‰çŠ¶æ€ç­›é€‰é¢„æµ‹",
                "url": "/api/v1/predictions",
                "params": {"status": "PENDING", "limit": 50},
            },
            {
                "name": "è·å–æ¯”èµ›åˆ—è¡¨ï¼ˆå¸¦ç­›é€‰ï¼‰",
                "url": "/api/v1/matches",
                "params": {"status": "UPCOMING", "limit": 50},
            },
            {
                "name": "æœç´¢æ¯”èµ›",
                "url": "/api/v1/matches/search",
                "params": {"q": "E2E"},
            },
            {
                "name": "è·å–ç”¨æˆ·é¢„æµ‹å†å²",
                "url": "/api/v1/users/me/predictions",
                "params": {"limit": 100},
            },
        ]

        # ç™»å½•ä¸€ä¸ªç”¨æˆ·è¿›è¡Œæµ‹è¯•
        test_token = user_tokens[0] if user_tokens else None
        test_headers = {"Authorization": f"Bearer {test_token}"} if test_token else {}

        print("\nğŸ—„ï¸ æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æµ‹è¯•:")

        for query in queries:
            response_times = []

            # æ¯ä¸ªæŸ¥è¯¢æµ‹è¯•5æ¬¡
            for _ in range(5):
                start_time = time.time()

                response = await api_client.get(
                    query["url"], params=query.get("params"), headers=test_headers
                )

                end_time = time.time()
                response_times.append((end_time - start_time) * 1000)

                assert response.status_code < 500

            avg_time = statistics.mean(response_times)
            print(f"   - {query['name']}: {avg_time:.1f}ms")

            # æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½æ–­è¨€
            assert avg_time < 1000, f"æŸ¥è¯¢æ€§èƒ½è¿‡æ…¢: {query['name']} - {avg_time:.1f}ms"

    @pytest.mark.asyncio
    async def test_cache_performance(
        self, api_client: AsyncClient, test_data_loader, performance_metrics
    ):
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½å½±å“"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        _teams = await test_data_loader.create_teams()
        _matches = await test_data_loader.create_matches()

        # è·å–token
        token = (
            await api_client.post(
                "/api/v1/auth/login",
                _data={"username": "e2e_user", "password": "E2ETestPass123!"},
            )
        ).json()["access_token"]
        headers = {"Authorization": f"Bearer {token}"}

        # æµ‹è¯•ç¼“å­˜æ•ˆæœï¼ˆå¤šæ¬¡è¯·æ±‚åŒä¸€æ•°æ®ï¼‰
        cacheable_url = f"/api/v1/matches/{matches[0]['id']}"

        print("\nğŸ’¾ ç¼“å­˜æ€§èƒ½æµ‹è¯•:")

        # ç¬¬ä¸€æ¬¡è¯·æ±‚ï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        start_time = time.time()
        response1 = await api_client.get(cacheable_url, headers=headers)
        first_request_time = (time.time() - start_time) * 1000

        assert response1.status_code == 200

        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿ç¼“å­˜å†™å…¥
        await asyncio.sleep(0.1)

        # åç»­è¯·æ±‚ï¼ˆåº”è¯¥å‘½ä¸­ç¼“å­˜ï¼‰
        cache_times = []
        for _ in range(10):
            start_time = time.time()
            response = await api_client.get(cacheable_url, headers=headers)
            end_time = time.time()
            cache_times.append((end_time - start_time) * 1000)
            assert response.status_code == 200

        avg_cache_time = statistics.mean(cache_times)

        print(f"   - é¦–æ¬¡è¯·æ±‚: {first_request_time:.1f}ms")
        print(f"   - ç¼“å­˜è¯·æ±‚å¹³å‡: {avg_cache_time:.1f}ms")
        print(
            f"   - ç¼“å­˜æå‡: {(first_request_time - avg_cache_time) / first_request_time * 100:.1f}%"
        )

        # éªŒè¯ç¼“å­˜æ•ˆæœ
        assert avg_cache_time < first_request_time * 0.5, "ç¼“å­˜æ•ˆæœä¸æ˜æ˜¾"

    @pytest.mark.asyncio
    async def test_stress_load(
        self, api_client: AsyncClient, test_data_loader, performance_metrics
    ):
        """å‹åŠ›æµ‹è¯•ï¼šæ¨¡æ‹Ÿé«˜è´Ÿè½½åœºæ™¯"""
        print("\nğŸ”¥ å‹åŠ›æµ‹è¯•ï¼šæ¨¡æ‹Ÿé«˜è´Ÿè½½åœºæ™¯")

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        _teams = await test_data_loader.create_teams()
        _matches = await test_data_loader.create_matches()

        # åˆ›å»ºç”¨æˆ·æ± 
        user_pool = []
        for i in range(100):
            user_data = {
                "username": f"stress_user_{i}",
                "email": f"stress{i}@example.com",
                "password": "StressTest123!",
            }
            response = await api_client.post("/api/v1/auth/register", json=user_data)
            if response.status_code != 201:
                continue

            # ç™»å½•
            login_data = {
                "username": user_data["username"],
                "password": user_data["password"],
            }
            response = await api_client.post("/api/v1/auth/login", _data=login_data)
            if response.status_code == 200:
                user_pool.append(
                    {
                        "username": user_data["username"],
                        "token": response.json()["access_token"],
                        "headers": {
                            "Authorization": f"Bearer {response.json()['access_token']}"
                        },
                    }
                )

        assert len(user_pool) >= 20, f"ç”¨æˆ·æ± å¤§å°ä¸è¶³: {len(user_pool)}"

        # å®šä¹‰ä¸åŒçš„æ“ä½œç±»å‹
        async def read_operation(user):
            """è¯»æ“ä½œ"""
            response = await api_client.get("/api/v1/matches", headers=user["headers"])
            return response.status_code == 200

        async def write_operation(user):
            """å†™æ“ä½œï¼šåˆ›å»ºé¢„æµ‹"""
            pred_data = {
                "match_id": matches[user["username"][-1] % len(matches)]["id"],
                "prediction": "HOME_WIN",
                "confidence": 0.75,
            }
            response = await api_client.post(
                "/api/v1/predictions", json=pred_data, headers=user["headers"]
            )
            return response.status_code in [201, 400]  # 400å¯èƒ½æ˜¯é‡å¤

        async def auth_operation(user):
            """è®¤è¯æ“ä½œï¼šè·å–ç”¨æˆ·ä¿¡æ¯"""
            response = await api_client.get("/api/v1/users/me", headers=user["headers"])
            return response.status_code == 200

        # å¹¶å‘æ‰§è¡Œæ··åˆæ“ä½œ
        operations = [read_operation, write_operation, auth_operation]
        total_operations = 0
        successful_operations = 0
        errors = []

        performance_metrics.start_timer("stress_test")

        # åˆ†æ‰¹æ‰§è¡Œä»¥é¿å…è¿‡è½½
        batch_size = 10
        user_batches = [
            user_pool[i : i + batch_size] for i in range(0, len(user_pool), batch_size)
        ]

        for batch_idx, batch in enumerate(user_batches):
            print(
                f"   æ‰§è¡Œæ‰¹æ¬¡ {batch_idx + 1}/{len(user_batches)} ({len(batch)} ç”¨æˆ·)"
            )

            batch_tasks = []
            for user in batch:
                for operation in operations:
                    batch_tasks.append(operation(user))

            results = await asyncio.gather(*batch_tasks, return_exceptions=True)

            for result in results:
                total_operations += 1
                if isinstance(result, bool):
                    successful_operations += 1
                elif isinstance(result, Exception):
                    errors.append(str(result))

        duration = performance_metrics.end_timer("stress_test")
        success_rate = (
            successful_operations / total_operations if total_operations > 0 else 0
        )
        throughput = successful_operations / duration if duration > 0 else 0

        print("\nğŸ“Š å‹åŠ›æµ‹è¯•ç»“æœ:")
        print(f"   - æ€»æ“ä½œæ•°: {total_operations}")
        print(f"   - æˆåŠŸæ•°: {successful_operations}")
        print(f"   - æˆåŠŸç‡: {success_rate * 100:.1f}%")
        print(f"   - æ€»è€—æ—¶: {duration:.2f}s")
        print(f"   - ååé‡: {throughput:.1f} ops/s")
        print(f"   - é”™è¯¯æ•°: {len(errors)}")

        if errors:
            print(f"\nâš ï¸ é”™è¯¯ç¤ºä¾‹: {errors[:3]}")

        # å‹åŠ›æµ‹è¯•æ–­è¨€
        assert success_rate >= 0.90, f"å‹åŠ›æµ‹è¯•æˆåŠŸç‡è¿‡ä½: {success_rate * 100:.1f}%"
        assert throughput >= 20, f"å‹åŠ›æµ‹è¯•ååé‡è¿‡ä½: {throughput:.1f} ops/s"
