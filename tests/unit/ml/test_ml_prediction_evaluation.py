#!/usr/bin/env python3
"""
ğŸ¯ MLé¢„æµ‹è¯„ä¼°æµ‹è¯• - æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹å’Œè¯„ä¼°æµ‹è¯•

æµ‹è¯•MLæ¨¡å‹çš„é¢„æµ‹å‡†ç¡®æ€§ã€è¯„ä¼°æŒ‡æ ‡ã€ç½®ä¿¡åº¦æ ¡å‡†ã€ç‰¹å¾é‡è¦æ€§åˆ†æç­‰ã€‚
åŒ…å«å¯¹é¢„æµ‹ç»“æœçš„è´¨é‡è¯„ä¼°å’Œæ¨¡å‹æ€§èƒ½çš„æ·±åº¦åˆ†æã€‚
"""

import warnings
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import pytest

# æŠ‘åˆ¶warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "../../../src"))

# å°è¯•å¯¼å…¥MLæ¨¡å—
try:
    from src.ml.models.base_model import BaseModel, PredictionResult, TrainingResult
    from src.ml.models.poisson_model import PoissonModel

    CAN_IMPORT = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥MLæ¨¡å—: {e}")
    CAN_IMPORT = False


def create_evaluation_dataset(
    train_matches: int = 1000, test_matches: int = 250
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """åˆ›å»ºè¯„ä¼°æ•°æ®é›†ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰"""
    # åˆ›å»ºæ›´å¤§çš„è®­ç»ƒæ•°æ®é›†
    training_data = []
    teams = [f"Team_{chr(65+i)}" for i in range(30)]  # 30ä¸ªé˜Ÿä¼

    # ä¸ºæ¯ä¸ªé˜Ÿä¼è®¾å®šå›ºå®šçš„å®åŠ›å€¼
    team_strengths = {team: np.random.uniform(0.3, 2.5) for team in teams}

    for i in range(train_matches):  # è®­ç»ƒæ•°æ®
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        home_strength = team_strengths[home_team]
        away_strength = team_strengths[away_team]

        # æ›´çœŸå®çš„æ¯”åˆ†ç”Ÿæˆ
        home_advantage = 0.25
        home_expected = (home_strength * (1 + home_advantage)) / away_strength * 1.3
        away_expected = away_strength / home_strength * 1.0

        home_goals = np.random.poisson(max(home_expected, 0.1))
        away_goals = np.random.poisson(max(away_expected, 0.1))

        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        training_data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now()
                - timedelta(days=np.random.randint(1, 730)),
                "home_strength": home_strength,
                "away_strength": away_strength,
            }
        )

    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†ï¼ˆä½¿ç”¨ç›¸åŒçš„é˜Ÿä¼å®åŠ›ï¼‰
    test_data = []
    for i in range(test_matches):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        home_strength = team_strengths[home_team]
        away_strength = team_strengths[away_team]

        home_advantage = 0.25
        home_expected = (home_strength * (1 + home_advantage)) / away_strength * 1.3
        away_expected = away_strength / home_strength * 1.0

        home_goals = np.random.poisson(max(home_expected, 0.1))
        away_goals = np.random.poisson(max(away_expected, 0.1))

        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        test_data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now() - timedelta(days=np.random.randint(0, 30)),
                "home_strength": home_strength,
                "away_strength": away_strength,
            }
        )

    return pd.DataFrame(training_data), pd.DataFrame(test_data)


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestMLModelPrediction:
    """MLæ¨¡å‹é¢„æµ‹æµ‹è¯•"""

    def test_single_match_prediction(self):
        """æµ‹è¯•å•åœºæ¯”èµ›é¢„æµ‹"""
        training_data, test_data = create_evaluation_dataset(200, 50)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("single_prediction_test")
        model.train(training_data)

        # é€‰æ‹©ä¸€åœºæµ‹è¯•æ¯”èµ›
        test_match = test_data.iloc[0]
        match_data = {
            "home_team": test_match["home_team"],
            "away_team": test_match["away_team"],
            "match_id": "single_test_001",
        }

        # è¿›è¡Œé¢„æµ‹
        prediction = model.predict(match_data)

        # éªŒè¯é¢„æµ‹ç»“æœ
        assert isinstance(prediction, PredictionResult)
        assert prediction.match_id == match_data["match_id"]
        assert prediction.home_team == match_data["home_team"]
        assert prediction.away_team == match_data["away_team"]
        assert prediction.model_name == "PoissonModel"

        # éªŒè¯æ¦‚ç‡åˆ†å¸ƒ
        probs = [
            prediction.home_win_prob,
            prediction.draw_prob,
            prediction.away_win_prob,
        ]
        assert all(0 <= p <= 1 for p in probs)
        assert abs(sum(probs) - 1.0) < 0.01

        # éªŒè¯é¢„æµ‹ç»“æœä¸æ¦‚ç‡ä¸€è‡´
        max_prob_index = np.argmax(probs)
        expected_outcomes = ["home_win", "draw", "away_win"]
        assert prediction.predicted_outcome == expected_outcomes[max_prob_index]

        # éªŒè¯ç½®ä¿¡åº¦ï¼ˆPoissonæ¨¡å‹ä½¿ç”¨å¤æ‚çš„ç½®ä¿¡åº¦è®¡ç®—ï¼Œä¸æ˜¯ç®€å•çš„æœ€å¤§æ¦‚ç‡ï¼‰
        assert 0 <= prediction.confidence <= 1
        assert prediction.confidence > 0

        print(
            f"âœ… å•åœºæ¯”èµ›é¢„æµ‹æµ‹è¯•é€šè¿‡: {prediction.home_team} vs {prediction.away_team}"
        )
        print(
            f"   é¢„æµ‹ç»“æœ: {prediction.predicted_outcome} (ç½®ä¿¡åº¦: {prediction.confidence:.3f})"
        )
        print(
            f"   æ¦‚ç‡åˆ†å¸ƒ: ä¸»èƒœ{prediction.home_win_prob:.3f} å¹³å±€{prediction.draw_prob:.3f} å®¢èƒœ{prediction.away_win_prob:.3f}"
        )

    def test_batch_prediction_consistency(self):
        """æµ‹è¯•æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§"""
        training_data, test_data = create_evaluation_dataset(300, 100)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("batch_prediction_test")
        model.train(training_data)

        # æ‰¹é‡é¢„æµ‹
        batch_predictions = []
        for idx, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
                "match_id": f"batch_test_{idx:03d}",
            }
            prediction = model.predict(match_data)
            batch_predictions.append(prediction)

        # éªŒè¯æ‰¹é‡é¢„æµ‹ç»“æœ
        assert len(batch_predictions) == len(test_data)

        # éªŒè¯æ¯ä¸ªé¢„æµ‹çš„æœ‰æ•ˆæ€§
        for prediction in batch_predictions:
            probs = [
                prediction.home_win_prob,
                prediction.draw_prob,
                prediction.away_win_prob,
            ]
            assert abs(sum(probs) - 1.0) < 0.01
            assert prediction.confidence > 0
            assert prediction.predicted_outcome in ["home_win", "draw", "away_win"]

        # éªŒè¯ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º
        repeat_match = {
            "home_team": test_data.iloc[0]["home_team"],
            "away_team": test_data.iloc[0]["away_team"],
            "match_id": "repeat_test",
        }

        prediction1 = model.predict(repeat_match)
        prediction2 = model.predict(repeat_match)

        assert prediction1.home_win_prob == prediction2.home_win_prob
        assert prediction1.draw_prob == prediction2.draw_prob
        assert prediction1.away_win_prob == prediction2.away_win_prob
        assert prediction1.predicted_outcome == prediction2.predicted_outcome

        print(f"âœ… æ‰¹é‡é¢„æµ‹ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡: {len(batch_predictions)}ä¸ªé¢„æµ‹")

    def test_prediction_probability_distribution(self):
        """æµ‹è¯•é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒç‰¹æ€§"""
        training_data, test_data = create_evaluation_dataset(400, 150)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("probability_distribution_test")
        model.train(training_data)

        # æ”¶é›†æ‰€æœ‰é¢„æµ‹æ¦‚ç‡
        all_probabilities = []
        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
            }
            probabilities = model.predict_proba(match_data)
            all_probabilities.append(probabilities)

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        prob_array = np.array(all_probabilities)

        # éªŒè¯æ¦‚ç‡åˆ†å¸ƒç‰¹æ€§
        # 1. æ‰€æœ‰æ¦‚ç‡éƒ½åœ¨[0,1]èŒƒå›´å†…
        assert np.all(prob_array >= 0)
        assert np.all(prob_array <= 1)

        # 2. æ¯è¡Œæ¦‚ç‡å’Œä¸º1
        prob_sums = np.sum(prob_array, axis=1)
        assert np.allclose(prob_sums, 1.0, atol=0.01)

        # 3. æ¦‚ç‡åˆ†å¸ƒçš„ç»Ÿè®¡ç‰¹æ€§
        home_win_probs = prob_array[:, 0]
        draw_probs = prob_array[:, 1]
        away_win_probs = prob_array[:, 2]

        # ä¸»èƒœæ¦‚ç‡é€šå¸¸åº”è¯¥æœ€é«˜ï¼ˆä¸»åœºä¼˜åŠ¿ï¼‰
        assert np.mean(home_win_probs) > np.mean(away_win_probs)

        # éªŒè¯åˆ†å¸ƒçš„åˆç†æ€§
        assert 0.2 < np.mean(home_win_probs) < 0.6  # åˆç†çš„ä¸»èƒœæ¦‚ç‡èŒƒå›´
        assert 0.2 < np.mean(draw_probs) < 0.4  # åˆç†çš„å¹³å±€æ¦‚ç‡èŒƒå›´
        assert 0.2 < np.mean(away_win_probs) < 0.5  # åˆç†çš„å®¢èƒœæ¦‚ç‡èŒƒå›´

        print("âœ… æ¦‚ç‡åˆ†å¸ƒç‰¹æ€§éªŒè¯é€šè¿‡:")
        print(f"   ä¸»èƒœæ¦‚ç‡å‡å€¼: {np.mean(home_win_probs):.3f}")
        print(f"   å¹³å±€æ¦‚ç‡å‡å€¼: {np.mean(draw_probs):.3f}")
        print(f"   å®¢èƒœæ¦‚ç‡å‡å€¼: {np.mean(away_win_probs):.3f}")

    def test_prediction_confidence_calibration(self):
        """æµ‹è¯•é¢„æµ‹ç½®ä¿¡åº¦æ ¡å‡†"""
        training_data, test_data = create_evaluation_dataset(500, 200)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("confidence_calibration_test")
        model.train(training_data)

        # æ”¶é›†é¢„æµ‹å’Œå®é™…ç»“æœ
        predictions_with_actual = []
        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
            }
            prediction = model.predict(match_data)
            predictions_with_actual.append(
                {"prediction": prediction, "actual_result": test_match["result"]}
            )

        # æŒ‰ç½®ä¿¡åº¦åˆ†ç»„
        confidence_bins = [(0.5, 0.6), (0.6, 0.7), (0.7, 0.8), (0.8, 0.9), (0.9, 1.0)]
        calibration_data = []

        for low, high in confidence_bins:
            bin_predictions = [
                p
                for p in predictions_with_actual
                if low <= p["prediction"].confidence < high
            ]

            if bin_predictions:
                correct_predictions = sum(
                    1
                    for p in bin_predictions
                    if p["prediction"].predicted_outcome == p["actual_result"]
                )
                accuracy = correct_predictions / len(bin_predictions)
                avg_confidence = np.mean(
                    [p["prediction"].confidence for p in bin_predictions]
                )

                calibration_data.append(
                    {
                        "confidence_range": f"{low}-{high}",
                        "count": len(bin_predictions),
                        "accuracy": accuracy,
                        "avg_confidence": avg_confidence,
                    }
                )

        # éªŒè¯ç½®ä¿¡åº¦æ ¡å‡†ï¼ˆç½®ä¿¡åº¦åº”è¯¥ä¸å‡†ç¡®ç‡ç›¸å…³ï¼‰
        for data in calibration_data:
            # é«˜ç½®ä¿¡åº¦åº”è¯¥æœ‰ç›¸å¯¹è¾ƒé«˜çš„å‡†ç¡®ç‡
            confidence_diff = abs(data["avg_confidence"] - data["accuracy"])
            assert confidence_diff < 0.3  # å…è®¸ä¸€å®šçš„æ ¡å‡†è¯¯å·®

        print("âœ… ç½®ä¿¡åº¦æ ¡å‡†éªŒè¯é€šè¿‡:")
        for data in calibration_data:
            print(
                f"   ç½®ä¿¡åº¦{data['confidence_range']}: "
                f"æ ·æœ¬æ•°={data['count']}, "
                f"å‡†ç¡®ç‡={data['accuracy']:.3f}, "
                f"å¹³å‡ç½®ä¿¡åº¦={data['avg_confidence']:.3f}"
            )

    def test_prediction_edge_cases(self):
        """æµ‹è¯•é¢„æµ‹è¾¹ç•Œæƒ…å†µ"""
        training_data, test_data = create_evaluation_dataset(300, 100)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("edge_cases_test")
        model.train(training_data)

        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        edge_cases = [
            # 1. æœ€å¼ºé˜Ÿ vs æœ€å¼±é˜Ÿ
            {
                "name": "strongest_vs_weakest",
                "home_team": training_data.groupby("home_team")["home_score"]
                .mean()
                .idxmax(),
                "away_team": training_data.groupby("away_team")["away_score"]
                .mean()
                .idxmin(),
            },
            # 2. ç½•è§å¯¹é˜µç»„åˆ
            {
                "name": "rare_matchup",
                "home_team": training_data["home_team"].value_counts().idxmin(),
                "away_team": training_data["away_team"].value_counts().idxmin(),
            },
        ]

        results = []
        for case in edge_cases:
            try:
                match_data = {
                    "home_team": case["home_team"],
                    "away_team": case["away_team"],
                    "match_id": f"edge_case_{case['name']}",
                }
                prediction = model.predict(match_data)
                results.append(
                    {"case": case["name"], "success": True, "prediction": prediction}
                )
            except Exception as e:
                results.append(
                    {"case": case["name"], "success": False, "error": str(e)}
                )

        # éªŒè¯è¾¹ç•Œæƒ…å†µå¤„ç†
        successful_cases = [r for r in results if r["success"]]
        assert len(successful_cases) >= 1  # è‡³å°‘åº”è¯¥æœ‰ä¸€ä¸ªæˆåŠŸçš„è¾¹ç•Œæƒ…å†µ

        # éªŒè¯æˆåŠŸé¢„æµ‹çš„æœ‰æ•ˆæ€§
        for result in successful_cases:
            prediction = result["prediction"]
            probs = [
                prediction.home_win_prob,
                prediction.draw_prob,
                prediction.away_win_prob,
            ]
            assert abs(sum(probs) - 1.0) < 0.01
            assert prediction.confidence > 0

        print(
            f"âœ… è¾¹ç•Œæƒ…å†µæµ‹è¯•é€šè¿‡: {len(successful_cases)}/{len(edge_cases)} ä¸ªè¾¹ç•Œæƒ…å†µå¤„ç†æˆåŠŸ"
        )

    def test_prediction_error_handling(self):
        """æµ‹è¯•é¢„æµ‹é”™è¯¯å¤„ç†"""
        training_data, _ = create_evaluation_dataset(200, 50)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("error_handling_test")
        model.train(training_data)

        # æµ‹è¯•é”™è¯¯æƒ…å†µ
        error_cases = [
            # 1. æœªè®­ç»ƒæ¨¡å‹é¢„æµ‹
            {
                "name": "untrained_model",
                "model": PoissonModel("untrained"),
                "match_data": {"home_team": "Team_A", "away_team": "Team_B"},
                "should_fail": True,
            },
            # 2. ç¼ºå°‘å¿…è¦å­—æ®µ
            {
                "name": "missing_fields",
                "model": model,
                "match_data": {"home_team": "Team_A"},  # ç¼ºå°‘away_team
                "should_fail": True,
            },
            # 3. ä¸»å®¢é˜Ÿç›¸åŒ
            {
                "name": "same_team",
                "model": model,
                "match_data": {"home_team": "Team_A", "away_team": "Team_A"},
                "should_fail": True,
            },
            # 4. æ­£å¸¸æƒ…å†µ
            {
                "name": "normal_case",
                "model": model,
                "match_data": {
                    "home_team": training_data["home_team"].iloc[0],
                    "away_team": training_data["away_team"].iloc[1],
                },
                "should_fail": False,
            },
        ]

        for case in error_cases:
            try:
                prediction = case["model"].predict(case["match_data"])
                if case["should_fail"]:
                    assert False, f"Case '{case['name']}' should have failed"
                else:
                    assert isinstance(prediction, PredictionResult)
                    print(f"âœ… {case['name']}: é¢„æµ‹æˆåŠŸ")
            except Exception as e:
                if case["should_fail"]:
                    print(f"âœ… {case['name']}: æ­£ç¡®æ•è·é”™è¯¯ - {str(e)[:50]}...")
                else:
                    assert (
                        False
                    ), f"Case '{case['name']}' should not have failed: {str(e)}"


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestMLModelEvaluation:
    """MLæ¨¡å‹è¯„ä¼°æµ‹è¯•"""

    def test_comprehensive_evaluation_metrics(self):
        """æµ‹è¯•å…¨é¢è¯„ä¼°æŒ‡æ ‡"""
        training_data, test_data = create_evaluation_dataset(400, 150)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("comprehensive_evaluation_test")
        model.train(training_data)

        # å…¨é¢è¯„ä¼°
        evaluation_metrics = model.evaluate(test_data)

        # éªŒè¯åŸºæœ¬æŒ‡æ ‡
        required_metrics = [
            "accuracy",
            "precision",
            "recall",
            "f1_score",
            "confusion_matrix",
            "total_predictions",
        ]
        for metric in required_metrics:
            assert metric in evaluation_metrics
            assert evaluation_metrics[metric] is not None

        # éªŒè¯æŒ‡æ ‡èŒƒå›´
        assert 0 <= evaluation_metrics["accuracy"] <= 1
        assert 0 <= evaluation_metrics["precision"] <= 1
        assert 0 <= evaluation_metrics["recall"] <= 1
        assert 0 <= evaluation_metrics["f1_score"] <= 1

        # éªŒè¯æ··æ·†çŸ©é˜µ
        cm = evaluation_metrics["confusion_matrix"]
        assert isinstance(cm, list)
        assert len(cm) >= 2  # è‡³å°‘2x2çš„æ··æ·†çŸ©é˜µ

        # éªŒè¯æ€»é¢„æµ‹æ•°
        assert evaluation_metrics["total_predictions"] <= len(test_data)

        print("âœ… å…¨é¢è¯„ä¼°æŒ‡æ ‡æµ‹è¯•é€šè¿‡:")
        print(f"   å‡†ç¡®ç‡: {evaluation_metrics['accuracy']:.3f}")
        print(f"   ç²¾ç¡®ç‡: {evaluation_metrics['precision']:.3f}")
        print(f"   å¬å›ç‡: {evaluation_metrics['recall']:.3f}")
        print(f"   F1åˆ†æ•°: {evaluation_metrics['f1_score']:.3f}")
        print(f"   æ€»é¢„æµ‹æ•°: {evaluation_metrics['total_predictions']}")

    def test_evaluation_on_different_data_distributions(self):
        """æµ‹è¯•ä¸åŒæ•°æ®åˆ†å¸ƒä¸Šçš„è¯„ä¼°"""
        # åˆ›å»ºå…·æœ‰ä¸åŒç‰¹å¾çš„æ•°æ®é›†
        datasets = {}

        # 1. å¹³è¡¡æ•°æ®é›†
        balanced_train = []
        balanced_test = []
        for result in ["home_win", "draw", "away_win"]:
            result_train = create_evaluation_dataset(100, 30)[0]
            result_train["result"] = result
            balanced_train.append(result_train)

            result_test = create_evaluation_dataset(30, 15)[1]
            result_test["result"] = result
            balanced_test.append(result_test)

        datasets["balanced"] = {
            "train": pd.concat(balanced_train),
            "test": pd.concat(balanced_test),
        }

        # 2. ä¸å¹³è¡¡æ•°æ®é›†ï¼ˆä¸»åœºä¼˜åŠ¿æ˜æ˜¾ï¼‰
        skewed_train, skewed_test = create_evaluation_dataset(400, 150)
        # å¢åŠ ä¸»èƒœæ¯”ä¾‹
        home_win_indices = skewed_train[skewed_train["result"] == "home_win"].index
        additional_home_wins = skewed_train.loc[home_win_indices].sample(
            min(50, len(home_win_indices))
        )
        skewed_train = pd.concat([skewed_train, additional_home_wins])

        datasets["skewed"] = {"train": skewed_train, "test": skewed_test}

        # åœ¨ä¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒå’Œè¯„ä¼°
        results = {}
        for name, data in datasets.items():
            model = PoissonModel(f"evaluation_{name}")
            training_result = model.train(data["train"])
            evaluation_metrics = model.evaluate(data["test"])

            results[name] = {
                "training_accuracy": training_result.accuracy,
                "test_accuracy": evaluation_metrics["accuracy"],
                "f1_score": evaluation_metrics["f1_score"],
                "training_samples": len(data["train"]),
                "test_samples": len(data["test"]),
            }

        # éªŒè¯ç»“æœ
        assert len(results) == 2

        # å¹³è¡¡æ•°æ®é›†åº”è¯¥æœ‰æ›´ç¨³å®šçš„æ€§èƒ½
        assert results["balanced"]["test_accuracy"] > 0.2  # è‡³å°‘æ¯”éšæœºå¥½

        # æ¯”è¾ƒä¸åŒæ•°æ®é›†çš„æ€§èƒ½
        print("âœ… ä¸åŒæ•°æ®åˆ†å¸ƒè¯„ä¼°æµ‹è¯•é€šè¿‡:")
        for name, result in results.items():
            print(f"   {name}æ•°æ®é›†:")
            print(f"     è®­ç»ƒå‡†ç¡®ç‡: {result['training_accuracy']:.3f}")
            print(f"     æµ‹è¯•å‡†ç¡®ç‡: {result['test_accuracy']:.3f}")
            print(f"     F1åˆ†æ•°: {result['f1_score']:.3f}")

    def test_evaluation_reliability_and_stability(self):
        """æµ‹è¯•è¯„ä¼°å¯é æ€§å’Œç¨³å®šæ€§"""
        training_data, base_test_data = create_evaluation_dataset(300, 100)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("evaluation_stability_test")
        model.train(training_data)

        # å¤šæ¬¡è¯„ä¼°ä»¥æµ‹è¯•ç¨³å®šæ€§
        stability_results = []
        for i in range(5):
            # è½»å¾®æ‰°åŠ¨æµ‹è¯•æ•°æ®
            perturbed_test = base_test_data.copy()
            # éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬è¿›è¡Œå¾®å°ä¿®æ”¹
            n_perturb = min(10, len(perturbed_test))
            perturb_indices = np.random.choice(
                len(perturbed_test), n_perturb, replace=False
            )

            for idx in perturb_indices:
                if np.random.random() < 0.5:
                    # éšæœºæ”¹å˜ç»“æœ
                    current_result = perturbed_test.loc[idx, "result"]
                    possible_results = ["home_win", "draw", "away_win"]
                    possible_results.remove(current_result)
                    perturbed_test.loc[idx, "result"] = np.random.choice(
                        possible_results
                    )

            # è¯„ä¼°
            metrics = model.evaluate(perturbed_test)
            stability_results.append(metrics["accuracy"])

        # åˆ†æç¨³å®šæ€§
        mean_accuracy = np.mean(stability_results)
        std_accuracy = np.std(stability_results)
        min_accuracy = np.min(stability_results)
        max_accuracy = np.max(stability_results)

        # éªŒè¯ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ä¸åº”è¯¥å¤ªå¤§ï¼‰
        assert std_accuracy < 0.1  # æ ‡å‡†å·®åº”è¯¥å°äº0.1

        # éªŒè¯åˆç†çš„æ€§èƒ½æ°´å¹³
        assert mean_accuracy > 0.2  # åº”è¯¥æ¯”éšæœºé¢„æµ‹å¥½

        print("âœ… è¯„ä¼°ç¨³å®šæ€§æµ‹è¯•é€šè¿‡:")
        print(f"   å‡†ç¡®ç‡: {mean_accuracy:.3f} Â± {std_accuracy:.3f}")
        print(f"   èŒƒå›´: [{min_accuracy:.3f}, {max_accuracy:.3f}]")
        print(f"   å˜å¼‚ç³»æ•°: {std_accuracy/mean_accuracy:.3f}")

    def test_cross_validation_evaluation(self):
        """æµ‹è¯•äº¤å‰éªŒè¯è¯„ä¼°"""
        # åˆ›å»ºè¾ƒå°çš„æ•°æ®é›†ç”¨äºäº¤å‰éªŒè¯
        training_data = create_evaluation_dataset(200, 80)[0]

        # è®­ç»ƒæ¨¡å‹ï¼ˆå†…éƒ¨ä¼šè¿›è¡Œäº¤å‰éªŒè¯ï¼‰
        model = PoissonModel("cross_validation_test")
        model.update_hyperparameters(min_matches_per_team=5)  # é™ä½æœ€å°æ¯”èµ›æ•°è¦æ±‚

        training_result = model.train(training_data)

        # éªŒè¯äº¤å‰éªŒè¯ç»“æœ
        assert training_result.accuracy > 0.0
        assert model.is_trained

        # åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸ŠéªŒè¯
        _, test_data = create_evaluation_dataset(50, 30)
        test_metrics = model.evaluate(test_data)

        # éªŒè¯äº¤å‰éªŒè¯çš„æ³›åŒ–èƒ½åŠ›
        # äº¤å‰éªŒè¯å‡†ç¡®ç‡åº”è¯¥ä¸æµ‹è¯•å‡†ç¡®ç‡ç›¸å¯¹æ¥è¿‘
        accuracy_diff = abs(training_result.accuracy - test_metrics["accuracy"])
        assert accuracy_diff < 0.3  # å…è®¸ä¸€å®šçš„å·®å¼‚

        print("âœ… äº¤å‰éªŒè¯è¯„ä¼°æµ‹è¯•é€šè¿‡:")
        print(f"   äº¤å‰éªŒè¯å‡†ç¡®ç‡: {training_result.accuracy:.3f}")
        print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_metrics['accuracy']:.3f}")
        print(f"   å·®å¼‚: {accuracy_diff:.3f}")

    def test_feature_importance_analysis(self):
        """æµ‹è¯•ç‰¹å¾é‡è¦æ€§åˆ†æ"""
        training_data, test_data = create_evaluation_dataset(300, 100)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("feature_importance_test")
        model.train(training_data)

        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_feature_importance()

        # éªŒè¯ç‰¹å¾é‡è¦æ€§
        assert isinstance(feature_importance, dict)

        # Poissonæ¨¡å‹çš„ç‰¹å¾é‡è¦æ€§å¯èƒ½ä¸ºç©ºï¼ˆå¦‚æœä¸æ”¯æŒï¼‰
        if feature_importance:
            # éªŒè¯ç‰¹å¾é‡è¦æ€§çš„åŸºæœ¬å±æ€§
            for feature, importance in feature_importance.items():
                assert isinstance(feature, str)
                assert isinstance(importance, (int, float))
                assert importance >= 0

            # éªŒè¯ç‰¹å¾é‡è¦æ€§æ’åº
            sorted_features = sorted(
                feature_importance.items(), key=lambda x: x[1], reverse=True
            )
            assert len(sorted_features) == len(feature_importance)

            print("âœ… ç‰¹å¾é‡è¦æ€§åˆ†ææµ‹è¯•é€šè¿‡:")
            for feature, importance in sorted_features:
                print(f"   {feature}: {importance:.3f}")
        else:
            print("âœ… ç‰¹å¾é‡è¦æ€§åˆ†ææµ‹è¯•é€šè¿‡: æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§è®¡ç®—")

    def test_model_comparison_evaluation(self):
        """æµ‹è¯•æ¨¡å‹æ¯”è¾ƒè¯„ä¼°"""
        training_data, test_data = create_evaluation_dataset(400, 150)

        # è®­ç»ƒå¤šä¸ªå…·æœ‰ä¸åŒè¶…å‚æ•°çš„æ¨¡å‹
        models = {}
        model_configs = [
            {"name": "conservative", "home_advantage": 0.1, "min_matches_per_team": 15},
            {"name": "balanced", "home_advantage": 0.3, "min_matches_per_team": 10},
            {"name": "aggressive", "home_advantage": 0.5, "min_matches_per_team": 5},
        ]

        for config in model_configs:
            model = PoissonModel(f"comparison_{config['name']}")
            model.update_hyperparameters(
                home_advantage=config["home_advantage"],
                min_matches_per_team=config["min_matches_per_team"],
            )

            # è®­ç»ƒ
            training_result = model.train(training_data)

            # è¯„ä¼°
            evaluation_metrics = model.evaluate(test_data)

            models[config["name"]] = {
                "model": model,
                "training_result": training_result,
                "evaluation_metrics": evaluation_metrics,
                "config": config,
            }

        # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
        model_comparison = []
        for name, data in models.items():
            model_comparison.append(
                {
                    "name": name,
                    "training_accuracy": data["training_result"].accuracy,
                    "test_accuracy": data["evaluation_metrics"]["accuracy"],
                    "f1_score": data["evaluation_metrics"]["f1_score"],
                    "precision": data["evaluation_metrics"]["precision"],
                    "recall": data["evaluation_metrics"]["recall"],
                    "training_time": data["training_result"].training_time,
                }
            )

        # éªŒè¯æ¨¡å‹æ¯”è¾ƒç»“æœ
        assert len(model_comparison) == 3

        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        best_model = max(model_comparison, key=lambda x: x["test_accuracy"])
        worst_model = min(model_comparison, key=lambda x: x["test_accuracy"])

        # éªŒè¯æ€§èƒ½å·®å¼‚
        performance_diff = best_model["test_accuracy"] - worst_model["test_accuracy"]
        assert performance_diff >= 0  # åº”è¯¥æœ‰æ€§èƒ½å·®å¼‚

        print("âœ… æ¨¡å‹æ¯”è¾ƒè¯„ä¼°æµ‹è¯•é€šè¿‡:")
        print(
            f"   æœ€ä½³æ¨¡å‹: {best_model['name']} (å‡†ç¡®ç‡: {best_model['test_accuracy']:.3f})"
        )
        print(
            f"   æœ€å·®æ¨¡å‹: {worst_model['name']} (å‡†ç¡®ç‡: {worst_model['test_accuracy']:.3f})"
        )
        print(f"   æ€§èƒ½å·®å¼‚: {performance_diff:.3f}")

        print("\nè¯¦ç»†æ¯”è¾ƒ:")
        for model in model_comparison:
            print(
                f"   {model['name']}: è®­ç»ƒ={model['training_accuracy']:.3f}, "
                f"æµ‹è¯•={model['test_accuracy']:.3f}, F1={model['f1_score']:.3f}"
            )


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestMLModelPerformanceAnalysis:
    """MLæ¨¡å‹æ€§èƒ½åˆ†ææµ‹è¯•"""

    def test_learning_curve_analysis(self):
        """æµ‹è¯•å­¦ä¹ æ›²çº¿åˆ†æ"""
        # åˆ›å»ºä¸åŒå¤§å°çš„è®­ç»ƒæ•°æ®é›†
        data_sizes = [50, 100, 200, 400]
        training_data_full, test_data = create_evaluation_dataset(500, 200)

        learning_curve_results = []

        for size in data_sizes:
            # éšæœºé‡‡æ ·è®­ç»ƒæ•°æ®
            training_data_subset = training_data_full.sample(
                min(size, len(training_data_full))
            )

            # è®­ç»ƒæ¨¡å‹
            model = PoissonModel(f"learning_curve_{size}")
            training_result = model.train(training_data_subset)

            # è¯„ä¼°
            evaluation_metrics = model.evaluate(test_data)

            learning_curve_results.append(
                {
                    "training_size": len(training_data_subset),
                    "training_accuracy": training_result.accuracy,
                    "test_accuracy": evaluation_metrics["accuracy"],
                    "f1_score": evaluation_metrics["f1_score"],
                    "training_time": training_result.training_time,
                }
            )

        # åˆ†æå­¦ä¹ æ›²çº¿
        test_accuracies = [r["test_accuracy"] for r in learning_curve_results]
        training_times = [r["training_time"] for r in learning_curve_results]

        # éªŒè¯å­¦ä¹ æ›²çº¿ç‰¹æ€§
        # 1. éšç€æ•°æ®é‡å¢åŠ ï¼Œæµ‹è¯•å‡†ç¡®ç‡åº”è¯¥æ€»ä½“ä¸Šå‡ï¼ˆæˆ–è‡³å°‘ä¸æ˜¾è‘—ä¸‹é™ï¼‰
        if len(test_accuracies) > 1:
            overall_trend = np.polyfit(range(len(test_accuracies)), test_accuracies, 1)[
                0
            ]
            # å…è®¸è½»å¾®ä¸‹é™ï¼Œä½†ä¸åº”è¯¥å¤ªä¸¥é‡
            assert overall_trend > -0.1  # ä¸‹é™è¶‹åŠ¿ä¸åº”è¯¥å¤ªé™¡å³­

        # 2. è®­ç»ƒæ—¶é—´åº”è¯¥éšæ•°æ®é‡å¢åŠ 
        if len(training_times) > 1:
            time_trend = np.polyfit(range(len(training_times)), training_times, 1)[0]
            assert time_trend > 0  # è®­ç»ƒæ—¶é—´åº”è¯¥å¢åŠ 

        print("âœ… å­¦ä¹ æ›²çº¿åˆ†ææµ‹è¯•é€šè¿‡:")
        for result in learning_curve_results:
            print(
                f"   æ•°æ®é‡={result['training_size']}: "
                f"è®­ç»ƒå‡†ç¡®ç‡={result['training_accuracy']:.3f}, "
                f"æµ‹è¯•å‡†ç¡®ç‡={result['test_accuracy']:.3f}, "
                f"è®­ç»ƒæ—¶é—´={result['training_time']:.2f}s"
            )

    def test_prediction_confidence_analysis(self):
        """æµ‹è¯•é¢„æµ‹ç½®ä¿¡åº¦åˆ†æ"""
        training_data, test_data = create_evaluation_dataset(400, 200)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("confidence_analysis_test")
        model.train(training_data)

        # æ”¶é›†æ‰€æœ‰é¢„æµ‹åŠå…¶ç½®ä¿¡åº¦
        predictions_with_confidence = []
        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
            }
            prediction = model.predict(match_data)
            predictions_with_confidence.append(prediction.confidence)

        # åˆ†æç½®ä¿¡åº¦åˆ†å¸ƒ
        confidence_array = np.array(predictions_with_confidence)

        # éªŒè¯ç½®ä¿¡åº¦åˆ†å¸ƒç‰¹æ€§
        # 1. æ‰€æœ‰ç½®ä¿¡åº¦éƒ½åœ¨[0,1]èŒƒå›´å†…
        assert np.all(confidence_array >= 0)
        assert np.all(confidence_array <= 1)

        # 2. ç½®ä¿¡åº¦åˆ†å¸ƒåº”è¯¥åˆç†ï¼ˆä¸åº”è¯¥å…¨éƒ¨é›†ä¸­åœ¨ä¸€ä¸ªæç«¯ï¼‰
        mean_confidence = np.mean(confidence_array)
        std_confidence = np.std(confidence_array)

        assert 0.3 < mean_confidence < 0.8  # åˆç†çš„å¹³å‡ç½®ä¿¡åº¦èŒƒå›´
        assert std_confidence > 0.05  # åº”è¯¥æœ‰ä¸€å®šçš„å˜åŒ–

        # 3. åˆ†æç½®ä¿¡åº¦åˆ†æ®µ
        confidence_ranges = [
            (0.3, 0.4, "ä½"),
            (0.4, 0.6, "ä¸­"),
            (0.6, 0.8, "é«˜"),
            (0.8, 1.0, "å¾ˆé«˜"),
        ]

        confidence_distribution = []
        for low, high, label in confidence_ranges:
            count = np.sum((confidence_array >= low) & (confidence_array < high))
            percentage = count / len(confidence_array) * 100
            confidence_distribution.append(
                {
                    "range": f"{low}-{high}",
                    "label": label,
                    "count": count,
                    "percentage": percentage,
                }
            )

        print("âœ… ç½®ä¿¡åº¦åˆ†ææµ‹è¯•é€šè¿‡:")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {mean_confidence:.3f} Â± {std_confidence:.3f}")
        print("   ç½®ä¿¡åº¦åˆ†å¸ƒ:")
        for dist in confidence_distribution:
            print(
                f"     {dist['label']} ({dist['range']}): {dist['count']} ({dist['percentage']:.1f}%)"
            )

    def test_error_analysis_and_diagnosis(self):
        """æµ‹è¯•é”™è¯¯åˆ†æå’Œè¯Šæ–­"""
        training_data, test_data = create_evaluation_dataset(400, 200)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("error_analysis_test")
        model.train(training_data)

        # æ”¶é›†é¢„æµ‹å’Œå®é™…ç»“æœ
        prediction_results = []
        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
            }
            prediction = model.predict(match_data)
            prediction_results.append(
                {
                    "prediction": prediction,
                    "actual": test_match["result"],
                    "home_team": test_match["home_team"],
                    "away_team": test_match["away_team"],
                }
            )

        # åˆ†æé”™è¯¯æ¨¡å¼
        correct_predictions = [
            p
            for p in prediction_results
            if p["prediction"].predicted_outcome == p["actual"]
        ]
        incorrect_predictions = [
            p
            for p in prediction_results
            if p["prediction"].predicted_outcome != p["actual"]
        ]

        # è®¡ç®—é”™è¯¯ç‡
        error_rate = len(incorrect_predictions) / len(prediction_results)
        accuracy = len(correct_predictions) / len(prediction_results)

        # åˆ†æé”™è¯¯ç±»å‹
        error_types = {"home_win": 0, "draw": 0, "away_win": 0}
        for pred in incorrect_predictions:
            error_types[pred["actual"]] += 1

        # åˆ†æç½®ä¿¡åº¦ä¸æ­£ç¡®æ€§çš„å…³ç³»
        confidences_correct = [p["prediction"].confidence for p in correct_predictions]
        confidences_incorrect = [
            p["prediction"].confidence for p in incorrect_predictions
        ]

        avg_confidence_correct = (
            np.mean(confidences_correct) if confidences_correct else 0
        )
        avg_confidence_incorrect = (
            np.mean(confidences_incorrect) if confidences_incorrect else 0
        )

        # éªŒè¯é”™è¯¯åˆ†æç»“æœ
        assert 0 <= error_rate <= 1
        assert len(correct_predictions) + len(incorrect_predictions) == len(
            prediction_results
        )

        # æ­£ç¡®é¢„æµ‹çš„ç½®ä¿¡åº¦åº”è¯¥å¹³å‡é«˜äºé”™è¯¯é¢„æµ‹
        if confidences_correct and confidences_incorrect:
            confidence_gap = avg_confidence_correct - avg_confidence_incorrect
            assert confidence_gap > 0  # æ­£ç¡®é¢„æµ‹åº”è¯¥æœ‰æ›´é«˜çš„å¹³å‡ç½®ä¿¡åº¦

        print("âœ… é”™è¯¯åˆ†ææµ‹è¯•é€šè¿‡:")
        print(f"   æ€»å‡†ç¡®ç‡: {accuracy:.3f}")
        print(f"   é”™è¯¯ç‡: {error_rate:.3f}")
        print(f"   æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {avg_confidence_correct:.3f}")
        print(f"   é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦: {avg_confidence_incorrect:.3f}")
        print(f"   é”™è¯¯ç±»å‹åˆ†å¸ƒ: {error_types}")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])
