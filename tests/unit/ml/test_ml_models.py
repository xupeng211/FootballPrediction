#!/usr/bin/env python3
"""
ğŸ¤– MLæ¨¡å‹æµ‹è¯• - æœºå™¨å­¦ä¹ æ¨¡å‹å…¨é¢æµ‹è¯•

æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€é¢„æµ‹ã€è¯„ä¼°ã€ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½
åŒ…æ‹¬åŸºç¡€æ¨¡å‹ã€æ³Šæ¾æ¨¡å‹ã€è®­ç»ƒå™¨ç­‰æ ¸å¿ƒç»„ä»¶
"""

import asyncio
import numpy as np
import pandas as pd
import pytest
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict
from unittest.mock import MagicMock, patch
import tempfile
import os
import pickle

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../src'))

# å°è¯•å¯¼å…¥MLæ¨¡å—
try:
    from src.ml.models.base_model import BaseModel, PredictionResult, TrainingResult
    from src.ml.models.poisson_model import PoissonModel
    from src.ml.model_training import ModelTrainer, TrainingConfig, TrainingStatus, ModelType
    CAN_IMPORT = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥MLæ¨¡å—: {e}")
    CAN_IMPORT = False


# åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
def create_mock_training_data(num_samples: int = 1000) -> pd.DataFrame:
    """åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®"""
    teams = ["Team_A", "Team_B", "Team_C", "Team_D", "Team_E", "Team_F", "Team_G", "Team_H"]
    data = []

    for i in range(num_samples):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        # æ¨¡æ‹Ÿæ¯”åˆ†ï¼ŒåŸºäºéšæœºæ¦‚ç‡
        home_goals = np.random.poisson(1.5)
        away_goals = np.random.poisson(1.1)

        # ç¡®å®šæ¯”èµ›ç»“æœ
        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        data.append({
            "home_team": home_team,
            "away_team": away_team,
            "home_score": home_goals,
            "away_score": away_goals,
            "result": result,
            "match_date": datetime.now() - timedelta(days=np.random.randint(0, 365))
        })

    return pd.DataFrame(data)


def create_mock_prediction_data() -> Dict[str, Any]:
    """åˆ›å»ºæ¨¡æ‹Ÿé¢„æµ‹æ•°æ®"""
    return {
        "home_team": "Team_A",
        "away_team": "Team_B",
        "match_id": "test_match_001"
    }


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestBaseModel:
    """åŸºç¡€æ¨¡å‹æµ‹è¯•"""

    @pytest.fixture
    def mock_base_model(self):
        """åˆ›å»ºåŸºç¡€æ¨¡å‹çš„æ¨¡æ‹Ÿå®ç°"""
        class MockBaseModel(BaseModel):
            def __init__(self):
                super().__init__("MockModel", "1.0")
                self.mock_prediction = None

            def prepare_features(self, match_data: Dict[str, Any]) -> np.ndarray:
                return np.array([1.0, 2.0, 3.0, 4.0])

            def train(self, training_data: pd.DataFrame, validation_data: pd.DataFrame = None) -> TrainingResult:
                self.is_trained = True
                return TrainingResult(
                    model_name=self.model_name,
                    model_version=self.model_version,
                    accuracy=0.75,
                    precision=0.73,
                    recall=0.71,
                    f1_score=0.72,
                    confusion_matrix=[[50, 10, 5], [8, 45, 12], [6, 11, 53]],
                    training_samples=len(training_data),
                    validation_samples=len(validation_data) if validation_data is not None else 0,
                    training_time=120.5,
                    features_used=["feature1", "feature2", "feature3"],
                    hyperparameters={"param1": "value1"},
                    created_at=datetime.now()
                )

            def predict(self, match_data: Dict[str, Any]) -> PredictionResult:
                if not self.is_trained:
                    raise RuntimeError("Model must be trained before making predictions")

                return PredictionResult(
                    match_id=match_data.get("match_id", "unknown"),
                    home_team=match_data["home_team"],
                    away_team=match_data["away_team"],
                    home_win_prob=0.6,
                    draw_prob=0.25,
                    away_win_prob=0.15,
                    predicted_outcome="home_win",
                    confidence=0.75,
                    model_name=self.model_name,
                    model_version=self.model_version,
                    created_at=datetime.now()
                )

            def predict_proba(self, match_data: Dict[str, Any]) -> tuple[float, float, float]:
                if not self.is_trained:
                    raise RuntimeError("Model must be trained before making predictions")
                return (0.6, 0.25, 0.15)

            def evaluate(self, test_data: pd.DataFrame) -> Dict[str, float]:
                return {
                    "accuracy": 0.75,
                    "precision": 0.73,
                    "recall": 0.71,
                    "f1_score": 0.72
                }

            def save_model(self, file_path: str) -> bool:
                try:
                    with open(file_path, 'wb') as f:
                        pickle.dump({"is_trained": self.is_trained}, f)
                    return True
                except Exception:
                    return False

            def load_model(self, file_path: str) -> bool:
                try:
                    with open(file_path, 'rb') as f:
                        data = pickle.load(f)
                    self.is_trained = data.get("is_trained", False)
                    return True
                except Exception:
                    return False

        return MockBaseModel()

    def test_model_initialization(self, mock_base_model):
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
        assert mock_base_model.model_name == "MockModel"
        assert mock_base_model.model_version == "1.0"
        assert mock_base_model.is_trained is False
        assert mock_base_model.model is None
        assert mock_base_model.feature_names == []

    def test_validate_prediction_input_valid(self, mock_base_model):
        """æµ‹è¯•æœ‰æ•ˆé¢„æµ‹è¾“å…¥éªŒè¯"""
        valid_data = {
            "home_team": "Team_A",
            "away_team": "Team_B"
        }
        assert mock_base_model.validate_prediction_input(valid_data) is True

    def test_validate_prediction_input_missing_field(self, mock_base_model):
        """æµ‹è¯•ç¼ºå°‘å¿…å¡«å­—æ®µçš„é¢„æµ‹è¾“å…¥éªŒè¯"""
        invalid_data = {
            "home_team": "Team_A"
            # ç¼ºå°‘ away_team
        }
        assert mock_base_model.validate_prediction_input(invalid_data) is False

    def test_validate_prediction_input_same_teams(self, mock_base_model):
        """æµ‹è¯•ä¸»å®¢é˜Ÿç›¸åŒçš„é¢„æµ‹è¾“å…¥éªŒè¯"""
        invalid_data = {
            "home_team": "Team_A",
            "away_team": "Team_A"
        }
        assert mock_base_model.validate_prediction_input(invalid_data) is False

    def test_calculate_confidence(self, mock_base_model):
        """æµ‹è¯•ç½®ä¿¡åº¦è®¡ç®—"""
        # æµ‹è¯•ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒ
        probs1 = (0.8, 0.15, 0.05)  # é«˜ç½®ä¿¡åº¦
        confidence1 = mock_base_model.calculate_confidence(probs1)
        assert 0.7 <= confidence1 <= 1.0

        probs2 = (0.4, 0.33, 0.27)  # ä½ç½®ä¿¡åº¦
        confidence2 = mock_base_model.calculate_confidence(probs2)
        assert 0.1 <= confidence2 < 0.7

        probs3 = (0.33, 0.34, 0.33)  # æä½ç½®ä¿¡åº¦
        confidence3 = mock_base_model.calculate_confidence(probs3)
        assert 0.1 <= confidence3 < confidence2

    def test_get_outcome_from_probabilities(self, mock_base_model):
        """æµ‹è¯•ä»æ¦‚ç‡åˆ†å¸ƒè·å–é¢„æµ‹ç»“æœ"""
        probs1 = (0.6, 0.25, 0.15)
        outcome1 = mock_base_model.get_outcome_from_probabilities(probs1)
        assert outcome1 == "home_win"

        probs2 = (0.2, 0.5, 0.3)
        outcome2 = mock_base_model.get_outcome_from_probabilities(probs2)
        assert outcome2 == "draw"

        probs3 = (0.1, 0.3, 0.6)
        outcome3 = mock_base_model.get_outcome_from_probabilities(probs3)
        assert outcome3 == "away_win"

    def test_validate_training_data_valid(self, mock_base_model):
        """æµ‹è¯•æœ‰æ•ˆè®­ç»ƒæ•°æ®éªŒè¯"""
        valid_data = pd.DataFrame({
            "home_team": ["Team_A", "Team_B"],
            "away_team": ["Team_B", "Team_C"],
            "result": ["home_win", "draw"]
        })
        assert mock_base_model.validate_training_data(valid_data) is True

    def test_validate_training_data_empty(self, mock_base_model):
        """æµ‹è¯•ç©ºè®­ç»ƒæ•°æ®éªŒè¯"""
        empty_data = pd.DataFrame()
        assert mock_base_model.validate_training_data(empty_data) is False

    def test_validate_training_data_missing_columns(self, mock_base_model):
        """æµ‹è¯•ç¼ºå°‘å¿…è¦åˆ—çš„è®­ç»ƒæ•°æ®éªŒè¯"""
        invalid_data = pd.DataFrame({
            "home_team": ["Team_A", "Team_B"],
            "away_team": ["Team_B", "Team_C"]
            # ç¼ºå°‘ result åˆ—
        })
        assert mock_base_model.validate_training_data(invalid_data) is False

    def test_training_workflow(self, mock_base_model):
        """æµ‹è¯•å®Œæ•´è®­ç»ƒå·¥ä½œæµ"""
        training_data = create_mock_training_data()

        # è®­ç»ƒå‰æ£€æŸ¥
        assert mock_base_model.is_trained is False

        # æ‰§è¡Œè®­ç»ƒ
        result = mock_base_model.train(training_data)

        # è®­ç»ƒåæ£€æŸ¥
        assert mock_base_model.is_trained is True
        assert isinstance(result, TrainingResult)
        assert result.model_name == "MockModel"
        assert result.accuracy > 0
        assert result.training_time > 0

    def test_prediction_workflow(self, mock_base_model):
        """æµ‹è¯•å®Œæ•´é¢„æµ‹å·¥ä½œæµ"""
        # å…ˆè®­ç»ƒæ¨¡å‹
        training_data = create_mock_training_data()
        mock_base_model.train(training_data)

        # æ‰§è¡Œé¢„æµ‹
        prediction_data = create_mock_prediction_data()
        result = mock_base_model.predict(prediction_data)

        # éªŒè¯é¢„æµ‹ç»“æœ
        assert isinstance(result, PredictionResult)
        assert result.home_team == "Team_A"
        assert result.away_team == "Team_B"
        assert result.home_win_prob + result.draw_prob + result.away_win_prob == pytest.approx(1.0)
        assert result.predicted_outcome in ["home_win", "draw", "away_win"]
        assert 0 <= result.confidence <= 1.0

    def test_prediction_without_training(self, mock_base_model):
        """æµ‹è¯•æœªè®­ç»ƒæ¨¡å‹çš„é¢„æµ‹"""
        prediction_data = create_mock_prediction_data()

        with pytest.raises(RuntimeError, match="Model must be trained"):
            mock_base_model.predict(prediction_data)

    def test_model_save_and_load(self, mock_base_model):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            file_path = tmp_file.name

        try:
            # è®­ç»ƒæ¨¡å‹
            training_data = create_mock_training_data()
            mock_base_model.train(training_data)

            # ä¿å­˜æ¨¡å‹
            save_success = mock_base_model.save_model(file_path)
            assert save_success is True

            # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹å¹¶åŠ è½½
            new_model = type(mock_base_model)()
            load_success = new_model.load_model(file_path)
            assert load_success is True
            assert new_model.is_trained is True

        finally:
            if os.path.exists(file_path):
                os.unlink(file_path)

    def test_get_model_info(self, mock_base_model):
        """æµ‹è¯•è·å–æ¨¡å‹ä¿¡æ¯"""
        info = mock_base_model.get_model_info()

        assert info["model_name"] == "MockModel"
        assert info["model_version"] == "1.0"
        assert info["is_trained"] is False
        assert info["feature_count"] == 0
        assert isinstance(info["hyperparameters"], dict)

    def test_log_training_step(self, mock_base_model):
        """æµ‹è¯•è®­ç»ƒæ­¥éª¤è®°å½•"""
        metrics = {"accuracy": 0.7, "loss": 0.5}
        mock_base_model.log_training_step(1, metrics)

        assert len(mock_base_model.training_history) == 1
        assert mock_base_model.training_history[0]["step"] == 1
        assert mock_base_model.training_history[0]["metrics"] == metrics

    def test_get_training_curve(self, mock_base_model):
        """æµ‹è¯•è·å–è®­ç»ƒæ›²çº¿"""
        # æ·»åŠ ä¸€äº›è®­ç»ƒå†å²
        for i in range(3):
            mock_base_model.log_training_step(i, {"accuracy": 0.6 + i * 0.1, "loss": 1.0 - i * 0.2})

        curves = mock_base_model.get_training_curve()
        assert "accuracy" in curves
        assert "loss" in curves
        assert len(curves["accuracy"]) == 3
        assert curves["accuracy"] == [0.6, 0.7, 0.8]

    def test_update_hyperparameters(self, mock_base_model):
        """æµ‹è¯•æ›´æ–°è¶…å‚æ•°"""
        original_params = mock_base_model.hyperparameters.copy()
        mock_base_model.update_hyperparameters(new_param="new_value", learning_rate=0.001)

        assert mock_base_model.hyperparameters != original_params
        assert "new_param" in mock_base_model.hyperparameters
        assert mock_base_model.hyperparameters["new_param"] == "new_value"

    def test_reset_model(self, mock_base_model):
        """æµ‹è¯•é‡ç½®æ¨¡å‹"""
        # è®­ç»ƒæ¨¡å‹å¹¶æ·»åŠ å†å²
        training_data = create_mock_training_data()
        mock_base_model.train(training_data)
        mock_base_model.log_training_step(1, {"accuracy": 0.7})

        assert mock_base_model.is_trained is True
        assert len(mock_base_model.training_history) > 0

        # é‡ç½®æ¨¡å‹
        mock_base_model.reset_model()

        assert mock_base_model.is_trained is False
        assert mock_base_model.model is None
        assert len(mock_base_model.training_history) == 0
        assert mock_base_model.last_training_time is None


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestPoissonModel:
    """æ³Šæ¾æ¨¡å‹æµ‹è¯•"""

    @pytest.fixture
    def poisson_model(self):
        """åˆ›å»ºæ³Šæ¾æ¨¡å‹å®ä¾‹"""
        return PoissonModel("1.0")

    @pytest.fixture
    def training_data(self):
        """åˆ›å»ºè®­ç»ƒæ•°æ®"""
        return create_mock_training_data(500)

    def test_poisson_model_initialization(self, poisson_model):
        """æµ‹è¯•æ³Šæ¾æ¨¡å‹åˆå§‹åŒ–"""
        assert poisson_model.model_name == "PoissonModel"
        assert poisson_model.model_version == "1.0"
        assert poisson_model.is_trained is False
        assert poisson_model.home_advantage == 0.3
        assert len(poisson_model.team_attack_strength) == 0

    def test_prepare_features(self, poisson_model):
        """æµ‹è¯•ç‰¹å¾å‡†å¤‡"""
        match_data = create_mock_prediction_data()

        # æœªè®­ç»ƒæ—¶çš„ç‰¹å¾å‡†å¤‡
        features = poisson_model.prepare_features(match_data)
        assert len(features) == 4
        assert all(f == 1.0 for f in features)  # é»˜è®¤å€¼

    def test_team_strengths_calculation(self, poisson_model, training_data):
        """æµ‹è¯•çƒé˜Ÿå¼ºåº¦è®¡ç®—"""
        poisson_model.train(training_data)

        # æ£€æŸ¥æ˜¯å¦è®¡ç®—äº†çƒé˜Ÿå¼ºåº¦
        assert len(poisson_model.team_attack_strength) > 0
        assert len(poisson_model.team_defense_strength) > 0
        assert poisson_model.total_matches == len(training_data)

    def test_poisson_training(self, poisson_model, training_data):
        """æµ‹è¯•æ³Šæ¾æ¨¡å‹è®­ç»ƒ"""
        result = poisson_model.train(training_data)

        assert isinstance(result, TrainingResult)
        assert result.model_name == "PoissonModel"
        assert poisson_model.is_trained is True
        assert result.training_samples == len(training_data)
        assert result.training_time > 0
        assert len(result.features_used) > 0

    def test_poisson_prediction(self, poisson_model, training_data):
        """æµ‹è¯•æ³Šæ¾æ¨¡å‹é¢„æµ‹"""
        # è®­ç»ƒæ¨¡å‹
        poisson_model.train(training_data)

        # è¿›è¡Œé¢„æµ‹
        match_data = create_mock_prediction_data()
        result = poisson_model.predict(match_data)

        assert isinstance(result, PredictionResult)
        assert result.home_team == "Team_A"
        assert result.away_team == "Team_B"
        assert abs(result.home_win_prob + result.draw_prob + result.away_win_prob - 1.0) < 0.01
        assert result.predicted_outcome in ["home_win", "draw", "away_win"]
        assert 0 <= result.confidence <= 1.0

    def test_poisson_predict_proba(self, poisson_model, training_data):
        """æµ‹è¯•æ¦‚ç‡é¢„æµ‹"""
        # è®­ç»ƒæ¨¡å‹
        poisson_model.train(training_data)

        # è¿›è¡Œæ¦‚ç‡é¢„æµ‹
        match_data = create_mock_prediction_data()
        probabilities = poisson_model.predict_proba(match_data)

        assert len(probabilities) == 3
        assert all(0 <= p <= 1 for p in probabilities)
        assert abs(sum(probabilities) - 1.0) < 0.01

    def test_expected_goals_calculation(self, poisson_model, training_data):
        """æµ‹è¯•æœŸæœ›è¿›çƒè®¡ç®—"""
        poisson_model.train(training_data)

        # è®¡ç®—æœŸæœ›è¿›çƒ
        home_expected = poisson_model._calculate_expected_goals("Team_A", "Team_B", is_home=True)
        away_expected = poisson_model._calculate_expected_goals("Team_B", "Team_A", is_home=False)

        assert home_expected > 0
        assert away_expected > 0
        # ä¸»é˜Ÿé€šå¸¸æœ‰ä¸»åœºä¼˜åŠ¿ï¼ŒæœŸæœ›è¿›çƒå¯èƒ½æ›´é«˜
        assert home_expected >= away_expected * 0.8  # å…è®¸ä¸€å®šè¯¯å·®

    def test_match_probabilities_calculation(self, poisson_model):
        """æµ‹è¯•æ¯”èµ›æ¦‚ç‡è®¡ç®—"""
        home_expected = 1.5
        away_expected = 1.1

        home_win, draw, away_win = poisson_model._calculate_match_probabilities(home_expected, away_expected)

        assert all(0 <= p <= 1 for p in [home_win, draw, away_win])
        assert abs(home_win + draw + away_win - 1.0) < 0.01

    def test_model_evaluation(self, poisson_model, training_data):
        """æµ‹è¯•æ¨¡å‹è¯„ä¼°"""
        # è®­ç»ƒæ¨¡å‹
        poisson_model.train(training_data)

        # è¯„ä¼°æ¨¡å‹
        test_data = create_mock_training_data(100)
        metrics = poisson_model.evaluate(test_data)

        assert isinstance(metrics, dict)
        assert "accuracy" in metrics
        assert "precision" in metrics
        assert "recall" in metrics
        assert "f1_score" in metrics
        assert "total_predictions" in metrics
        assert 0 <= metrics["accuracy"] <= 1

    def test_cross_validation(self, poisson_model, training_data):
        """æµ‹è¯•äº¤å‰éªŒè¯"""
        metrics = poisson_model._cross_validate(training_data, folds=3)

        assert isinstance(metrics, dict)
        assert "accuracy" in metrics
        assert "accuracy_std" in metrics
        assert "precision" in metrics
        assert "precision_std" in metrics

    def test_model_save_and_load(self, poisson_model, training_data):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            file_path = tmp_file.name

        try:
            # è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹
            poisson_model.train(training_data)
            save_success = poisson_model.save_model(file_path)
            assert save_success is True

            # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½
            new_model = PoissonModel()
            load_success = new_model.load_model(file_path)
            assert load_success is True
            assert new_model.is_trained is True
            assert new_model.model_name == poisson_model.model_name
            assert new_model.total_matches == poisson_model.total_matches

        finally:
            if os.path.exists(file_path):
                os.unlink(file_path)

    def test_prediction_without_training(self, poisson_model):
        """æµ‹è¯•æœªè®­ç»ƒæ¨¡å‹çš„é¢„æµ‹"""
        match_data = create_mock_prediction_data()

        with pytest.raises(RuntimeError, match="Model must be trained"):
            poisson_model.predict(match_data)

    def test_invalid_prediction_input(self, poisson_model, training_data):
        """æµ‹è¯•æ— æ•ˆé¢„æµ‹è¾“å…¥"""
        poisson_model.train(training_data)

        # æµ‹è¯•ç¼ºå°‘å¿…å¡«å­—æ®µ
        invalid_data = {"home_team": "Team_A"}  # ç¼ºå°‘ away_team
        with pytest.raises(ValueError):
            poisson_model.predict(invalid_data)

    def test_hyperparameter_updates(self, poisson_model):
        """æµ‹è¯•è¶…å‚æ•°æ›´æ–°"""
        new_params = {
            "home_advantage": 0.4,
            "min_matches_per_team": 15,
            "max_goals": 12
        }

        poisson_model.update_hyperparameters(**new_params)

        for key, value in new_params.items():
            assert poisson_model.hyperparameters[key] == value


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨æµ‹è¯•"""

    @pytest.fixture
    def training_config(self):
        """åˆ›å»ºè®­ç»ƒé…ç½®"""
        config = TrainingConfig()
        config.model_type = ModelType.RANDOM_FOREST
        config.epochs = 5
        return config

    @pytest.fixture
    def model_trainer(self, training_config):
        """åˆ›å»ºæ¨¡å‹è®­ç»ƒå™¨"""
        return ModelTrainer(training_config)

    @pytest.fixture
    def sample_data(self):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        np.random.seed(42)
        n_samples = 200

        data = pd.DataFrame({
            "feature1": np.random.randn(n_samples),
            "feature2": np.random.randn(n_samples),
            "feature3": np.random.randn(n_samples),
            "target": np.random.choice(["home_win", "draw", "away_win"], n_samples)
        })

        return data

    def test_model_trainer_initialization(self, model_trainer):
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒå™¨åˆå§‹åŒ–"""
        assert model_trainer.config is not None
        assert model_trainer.status == TrainingStatus.PENDING
        assert model_trainer.model is None
        assert len(model_trainer.training_history) == 0

    @pytest.mark.asyncio
    async def test_prepare_data(self, model_trainer, sample_data):
        """æµ‹è¯•æ•°æ®å‡†å¤‡"""
        X_train, X_test, y_train, y_test = await model_trainer.prepare_data(
            sample_data, "target"
        )

        assert len(X_train) > len(X_test)
        assert len(X_train) == len(y_train)
        assert len(X_test) == len(y_test)
        assert X_train.shape[1] == 3  # 3ä¸ªç‰¹å¾
        assert list(X_train.columns) == ["feature1", "feature2", "feature3"]

    @pytest.mark.asyncio
    async def test_train_model(self, model_trainer, sample_data):
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒ"""
        # å‡†å¤‡æ•°æ®
        X_train, X_test, y_train, y_test = await model_trainer.prepare_data(
            sample_data, "target"
        )

        # è®­ç»ƒæ¨¡å‹
        result = await model_trainer.train(X_train, y_train, X_test, y_test)

        assert result["status"] == "completed"
        assert "model_name" in result
        assert "training_time" in result
        assert "metrics" in result
        assert "feature_importance" in result
        assert model_trainer.status == TrainingStatus.COMPLETED
        assert model_trainer.model is not None

    @pytest.mark.asyncio
    async def test_evaluate_model(self, model_trainer, sample_data):
        """æµ‹è¯•æ¨¡å‹è¯„ä¼°"""
        # å…ˆè®­ç»ƒæ¨¡å‹
        X_train, X_test, y_train, y_test = await model_trainer.prepare_data(
            sample_data, "target"
        )
        await model_trainer.train(X_train, y_train, X_test, y_test)

        # è¯„ä¼°æ¨¡å‹
        metrics = await model_trainer.evaluate(X_test, y_test)

        assert isinstance(metrics, dict)
        assert "accuracy" in metrics
        assert "precision" in metrics
        assert "recall" in metrics
        assert "f1_score" in metrics
        assert all(0 <= v <= 1 for v in metrics.values() if isinstance(v, (int, float)))

    @pytest.mark.asyncio
    async def test_save_and_load_model(self, model_trainer, sample_data):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
            file_path = tmp_file.name

        try:
            # è®­ç»ƒæ¨¡å‹
            X_train, X_test, y_train, y_test = await model_trainer.prepare_data(
                sample_data, "target"
            )
            await model_trainer.train(X_train, y_train, X_test, y_test)

            # ä¿å­˜æ¨¡å‹
            save_success = await model_trainer.save_model(file_path)
            assert save_success is True

            # åˆ›å»ºæ–°çš„è®­ç»ƒå™¨å¹¶åŠ è½½æ¨¡å‹
            new_trainer = ModelTrainer()
            load_success = await new_trainer.load_model(file_path)
            assert load_success is True
            assert new_trainer.model is not None

        finally:
            if os.path.exists(file_path):
                os.unlink(file_path)
            # æ¸…ç†å†å²æ–‡ä»¶
            history_path = file_path.replace(".pkl", "_history.json")
            if os.path.exists(history_path):
                os.unlink(history_path)

    def test_get_training_summary(self, model_trainer):
        """æµ‹è¯•è·å–è®­ç»ƒæ‘˜è¦"""
        summary = model_trainer.get_training_summary()

        assert isinstance(summary, dict)
        assert "status" in summary
        assert "model_type" in summary
        assert "training_epochs" in summary
        assert summary["status"] == TrainingStatus.PENDING.value
        assert summary["training_epochs"] == 0


@pytest.mark.unit
@pytest.mark.ml
class TestMLIntegration:
    """MLæ¨¡å—é›†æˆæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_complete_ml_workflow(self):
        """æµ‹è¯•å®Œæ•´çš„MLå·¥ä½œæµ"""
        if not CAN_IMPORT:
            pytest.skip("MLæ¨¡å—å¯¼å…¥å¤±è´¥")

        # 1. å‡†å¤‡æ•°æ®
        training_data = create_mock_training_data(300)

        # 2. åˆ›å»ºå¹¶è®­ç»ƒæ³Šæ¾æ¨¡å‹
        poisson_model = PoissonModel()
        training_result = poisson_model.train(training_data)

        # 3. è¿›è¡Œé¢„æµ‹
        prediction_data = create_mock_prediction_data()
        prediction_result = poisson_model.predict(prediction_data)

        # 4. è¯„ä¼°æ¨¡å‹
        test_data = create_mock_training_data(100)
        evaluation_metrics = poisson_model.evaluate(test_data)

        # 5. éªŒè¯å®Œæ•´å·¥ä½œæµ
        assert poisson_model.is_trained is True
        assert isinstance(training_result, TrainingResult)
        assert isinstance(prediction_result, PredictionResult)
        assert isinstance(evaluation_metrics, dict)
        assert training_result.accuracy > 0
        assert abs(prediction_result.home_win_prob +
                  prediction_result.draw_prob +
                  prediction_result.away_win_prob - 1.0) < 0.01
        assert evaluation_metrics.get("accuracy", 0) > 0

    @pytest.mark.asyncio
    async def test_model_comparison(self):
        """æµ‹è¯•æ¨¡å‹æ¯”è¾ƒ"""
        if not CAN_IMPORT:
            pytest.skip("MLæ¨¡å—å¯¼å…¥å¤±è´¥")

        training_data = create_mock_training_data(200)
        test_data = create_mock_training_data(50)

        models = [PoissonModel() for _ in range(3)]
        results = []

        for i, model in enumerate(models):
            model.model_version = f"comparison_{i+1}"
            training_result = model.train(training_data)
            evaluation_metrics = model.evaluate(test_data)

            results.append({
                "model": model,
                "training": training_result,
                "evaluation": evaluation_metrics
            })

        # æ¯”è¾ƒæ¨¡å‹æ€§èƒ½
        accuracies = [r["evaluation"]["accuracy"] for r in results]
        assert len(accuracies) == 3
        # è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹åº”è¯¥æœ‰ä¸åŒçš„æ€§èƒ½
        assert len(set(round(acc, 3) for acc in accuracies)) > 1 or max(accuracies) > 0.6

    def test_model_factory_pattern(self):
        """æµ‹è¯•æ¨¡å‹å·¥å‚æ¨¡å¼"""
        if not CAN_IMPORT:
            pytest.skip("MLæ¨¡å—å¯¼å…¥å¤±è´¥")

        # æµ‹è¯•åˆ›å»ºä¸åŒç±»å‹çš„æ¨¡å‹
        models = {
            "poisson": PoissonModel("1.0"),
            "poisson_v2": PoissonModel("2.0"),
        }

        for name, model in models.items():
            assert model.model_name == "PoissonModel"
            assert model.is_trained is False
            assert isinstance(model, BaseModel)

    def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        if not CAN_IMPORT:
            pytest.skip("MLæ¨¡å—å¯¼å…¥å¤±è´¥")

        model = PoissonModel()

        # æµ‹è¯•æœªè®­ç»ƒæ¨¡å‹çš„é¢„æµ‹
        with pytest.raises(RuntimeError):
            model.predict({"home_team": "A", "away_team": "B"})

        # æµ‹è¯•æ— æ•ˆæ•°æ®è®­ç»ƒ
        empty_data = pd.DataFrame()
        with pytest.raises(ValueError):
            model.train(empty_data)

        # æµ‹è¯•æ— æ•ˆè¾“å…¥é¢„æµ‹
        training_data = create_mock_training_data(50)
        model.train(training_data)

        with pytest.raises(ValueError):
            model.predict({"home_team": "A"})  # ç¼ºå°‘away_team

        with pytest.raises(ValueError):
            model.predict({"home_team": "A", "away_team": "A"})  # ç›¸åŒé˜Ÿä¼

    def test_data_quality_validation(self):
        """æµ‹è¯•æ•°æ®è´¨é‡éªŒè¯"""
        if not CAN_IMPORT:
            pytest.skip("MLæ¨¡å—å¯¼å…¥å¤±è´¥")

        model = PoissonModel()

        # æµ‹è¯•ç©ºæ•°æ®
        empty_data = pd.DataFrame()
        assert model.validate_training_data(empty_data) is False

        # æµ‹è¯•ç¼ºå°‘åˆ—çš„æ•°æ®
        incomplete_data = pd.DataFrame({
            "home_team": ["A", "B"],
            "away_team": ["C", "D"]
            # ç¼ºå°‘å…¶ä»–å¿…è¦åˆ—
        })
        assert model.validate_training_data(incomplete_data) is False

        # æµ‹è¯•åŒ…å«ç©ºå€¼çš„æ•°æ®
        null_data = pd.DataFrame({
            "home_team": ["A", None, "C"],
            "away_team": ["B", "D", "E"],
            "result": ["home_win", "draw", "away_win"]
        })
        # åº”è¯¥è¿”å›Trueä½†å‘å‡ºè­¦å‘Š
        assert model.validate_training_data(null_data) is True


# æµ‹è¯•è¿è¡Œå™¨
async def run_ml_tests():
    """è¿è¡ŒMLæµ‹è¯•å¥—ä»¶"""
    print("ğŸ¤– å¼€å§‹MLæ¨¡å‹æµ‹è¯•")
    print("=" * 60)

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„MLæµ‹è¯•é€»è¾‘
    print("âœ… MLæ¨¡å‹æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(run_ml_tests())