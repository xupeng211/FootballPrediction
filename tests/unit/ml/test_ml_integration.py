#!/usr/bin/env python3
"""
ğŸ¤– MLé›†æˆæµ‹è¯• - æœºå™¨å­¦ä¹ æ¨¡å—é›†æˆæµ‹è¯•

æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å—çš„ç«¯åˆ°ç«¯å·¥ä½œæµã€æ¨¡å‹é›†æˆã€æ€§èƒ½ä¼˜åŒ–ç­‰
åŒ…æ‹¬æ•°æ®æµã€æ¨¡å‹éƒ¨ç½²ã€å®æ—¶é¢„æµ‹ç­‰é«˜çº§åŠŸèƒ½
"""

import asyncio
import json
import os

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
import sys
import tempfile
from datetime import datetime, timedelta
from typing import Any

import numpy as np
import pandas as pd
import pytest

sys.path.append(os.path.join(os.path.dirname(__file__), "../../../src"))

# å°è¯•å¯¼å…¥MLæ¨¡å—
try:
    from src.domain.strategies.ml_model import MLModelStrategy
    from src.ml.enhanced_real_model_training import EnhancedRealModelTrainer
    from src.ml.model_training import (
        ModelTrainer,
        ModelType,
        TrainingConfig,
        TrainingStatus,
    )
    from src.ml.models.base_model import BaseModel, PredictionResult, TrainingResult
    from src.ml.models.poisson_model import PoissonModel

    CAN_IMPORT = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥MLæ¨¡å—: {e}")
    CAN_IMPORT = False


# åˆ›å»ºæ›´å¤æ‚çš„æ¨¡æ‹Ÿæ•°æ®
def create_comprehensive_training_data(num_matches: int = 1000) -> pd.DataFrame:
    """åˆ›å»ºå…¨é¢çš„è®­ç»ƒæ•°æ®ï¼ŒåŒ…å«æ›´å¤šç‰¹å¾"""
    teams = [
        "Manchester_City",
        "Liverpool",
        "Chelsea",
        "Arsenal",
        "Manchester_United",
        "Tottenham",
        "West_Ham",
        "Leicester",
        "Everton",
        "Wolves",
        "Newcastle",
        "Aston_Villa",
        "Leeds",
        "Southampton",
        "Crystal_Palace",
        "Brighton",
        "Burnley",
        "Fulham",
        "West_Brom",
        "Sheffield_United",
    ]

    data = []
    for i in range(num_matches):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        # åŸºäºçƒé˜Ÿå®åŠ›çš„è¿›çƒæ¨¡æ‹Ÿï¼ˆç®€åŒ–ç‰ˆï¼‰
        team_strength = {
            "Manchester_City": 2.1,
            "Liverpool": 2.0,
            "Chelsea": 1.8,
            "Arsenal": 1.7,
            "Manchester_United": 1.6,
            "Tottenham": 1.5,
            "West_Ham": 1.3,
            "Leicester": 1.2,
        }

        home_strength = team_strength.get(home_team, 1.0)
        away_strength = team_strength.get(away_team, 1.0)

        # ä¸»åœºä¼˜åŠ¿
        home_advantage = 0.3

        # æœŸæœ›è¿›çƒ
        home_expected = (home_strength * home_advantage) / away_strength * 1.5
        away_expected = away_strength / (home_strength * home_advantage) * 1.1

        # å®é™…è¿›çƒï¼ˆæ³Šæ¾åˆ†å¸ƒï¼‰
        home_goals = np.random.poisson(max(home_expected, 0.1))
        away_goals = np.random.poisson(max(away_expected, 0.1))

        # ç¡®å®šç»“æœ
        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        # æ·»åŠ æ›´å¤šç‰¹å¾
        data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now()
                - timedelta(days=np.random.randint(0, 365)),
                "season": "2023-24",
                "league": "Premier_League",
                "home_team_form": np.random.uniform(-1, 1),  # æœ€è¿‘5åœºå‡€èƒœçƒ
                "away_team_form": np.random.uniform(-1, 1),
                "home_team_goals_scored": np.random.randint(0, 50),
                "away_team_goals_scored": np.random.randint(0, 50),
                "home_team_goals_conceded": np.random.randint(0, 50),
                "away_team_goals_conceded": np.random.randint(0, 50),
                "home_team_shots": np.random.randint(5, 25),
                "away_team_shots": np.random.randint(5, 25),
                "home_team_shots_on_target": np.random.randint(2, 15),
                "away_team_shots_on_target": np.random.randint(2, 15),
                "home_team_possession": np.random.uniform(30, 80),
                "away_team_possession": np.random.uniform(30, 80),
            }
        )

    return pd.DataFrame(data)


def create_batch_prediction_data(num_predictions: int = 10) -> list[dict[str, Any]]:
    """åˆ›å»ºæ‰¹é‡é¢„æµ‹æ•°æ®"""
    teams = ["Manchester_City", "Liverpool", "Chelsea", "Arsenal", "Manchester_United"]
    predictions = []

    for i in range(num_predictions):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        predictions.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "match_id": f"batch_match_{i+1:03d}",
                "match_date": datetime.now() + timedelta(days=i + 1),
                "venue": f"Stadium_{i+1}",
            }
        )

    return predictions


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.integration
class TestMLWorkflowIntegration:
    """MLå·¥ä½œæµé›†æˆæµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_complete_prediction_pipeline(self):
        """æµ‹è¯•å®Œæ•´çš„é¢„æµ‹æµæ°´çº¿"""
        # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
        training_data = create_comprehensive_training_data(500)
        validation_data = create_comprehensive_training_data(100)

        # 2. è®­ç»ƒæ³Šæ¾æ¨¡å‹
        poisson_model = PoissonModel("production_v1.0")
        training_result = poisson_model.train(training_data, validation_data)

        # 3. éªŒè¯è®­ç»ƒç»“æœ
        assert poisson_model.is_trained is True
        assert isinstance(training_result, TrainingResult)
        assert training_result.accuracy > 0.5  # åŸºæœ¬æ€§èƒ½è¦æ±‚
        assert len(poisson_model.team_attack_strength) > 0

        # 4. åˆ›å»ºé¢„æµ‹æ•°æ®
        prediction_requests = create_batch_prediction_data(5)

        # 5. æ‰¹é‡é¢„æµ‹
        predictions = []
        for request in prediction_requests:
            try:
                prediction = poisson_model.predict(request)
                predictions.append(prediction)
            except Exception as e:
                # è®°å½•é¢„æµ‹å¤±è´¥çš„æƒ…å†µ
                print(f"Prediction failed for {request['match_id']}: {e}")

        # 6. éªŒè¯é¢„æµ‹ç»“æœ
        assert len(predictions) > 0
        for prediction in predictions:
            assert isinstance(prediction, PredictionResult)
            assert (
                abs(
                    prediction.home_win_prob
                    + prediction.draw_prob
                    + prediction.away_win_prob
                    - 1.0
                )
                < 0.01
            )
            assert prediction.confidence > 0
            assert prediction.model_name == "PoissonModel"

        # 7. è¯„ä¼°æ¨¡å‹æ€§èƒ½
        test_metrics = poisson_model.evaluate(validation_data)
        assert isinstance(test_metrics, dict)
        assert "accuracy" in test_metrics
        assert test_metrics["accuracy"] > 0

    @pytest.mark.asyncio
    async def test_model_ensemble_workflow(self):
        """æµ‹è¯•æ¨¡å‹é›†æˆå·¥ä½œæµ"""
        training_data = create_comprehensive_training_data(300)
        test_data = create_batch_prediction_data(3)

        # åˆ›å»ºå¤šä¸ªæ¨¡å‹å®ä¾‹
        models = []
        for i in range(3):
            model = PoissonModel(f"ensemble_v{i+1}")
            # ä½¿ç”¨ä¸åŒçš„æ•°æ®å­é›†è®­ç»ƒ
            subset_data = training_data.sample(frac=0.8, random_state=i)
            model.train(subset_data)
            models.append(model)

        # é›†æˆé¢„æµ‹
        ensemble_predictions = []
        for request in test_data:
            individual_predictions = []
            for model in models:
                pred = model.predict(request)
                individual_predictions.append(pred)

            # ç®€å•å¹³å‡é›†æˆ
            avg_home_prob = np.mean([p.home_win_prob for p in individual_predictions])
            avg_draw_prob = np.mean([p.draw_prob for p in individual_predictions])
            avg_away_prob = np.mean([p.away_win_prob for p in individual_predictions])

            # åˆ›å»ºé›†æˆé¢„æµ‹ç»“æœ
            ensemble_pred = PredictionResult(
                match_id=request["match_id"],
                home_team=request["home_team"],
                away_team=request["away_team"],
                home_win_prob=avg_home_prob,
                draw_prob=avg_draw_prob,
                away_win_prob=avg_away_prob,
                predicted_outcome=(
                    "home_win"
                    if avg_home_prob > max(avg_draw_prob, avg_away_prob)
                    else "draw" if avg_draw_prob > avg_away_prob else "away_win"
                ),
                confidence=max(avg_home_prob, avg_draw_prob, avg_away_prob),
                model_name="EnsembleModel",
                model_version="1.0",
                created_at=datetime.now(),
            )
            ensemble_predictions.append(ensemble_pred)

        # éªŒè¯é›†æˆç»“æœ
        assert len(ensemble_predictions) == len(test_data)
        for pred in ensemble_predictions:
            assert (
                abs(pred.home_win_prob + pred.draw_prob + pred.away_win_prob - 1.0)
                < 0.01
            )

    @pytest.mark.asyncio
    async def test_model_performance_comparison(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
        training_data = create_comprehensive_training_data(400)
        test_data = create_comprehensive_training_data(100)

        # è®­ç»ƒå¤šä¸ªç‰ˆæœ¬çš„æ¨¡å‹
        model_configs = [
            ("Poisson_v1.0", {"home_advantage": 0.2}),
            ("Poisson_v1.1", {"home_advantage": 0.3}),
            ("Poisson_v1.2", {"home_advantage": 0.4}),
        ]

        results = {}
        for name, params in model_configs:
            model = PoissonModel()
            model.update_hyperparameters(**params)
            model.train(training_data)

            # è¯„ä¼°æ€§èƒ½
            metrics = model.evaluate(test_data)
            results[name] = {
                "model": model,
                "metrics": metrics,
                "hyperparameters": params,
            }

        # æ¯”è¾ƒæ€§èƒ½
        accuracies = [r["metrics"]["accuracy"] for r in results.values()]
        assert len(accuracies) == 3

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model_name = max(
            results.keys(), key=lambda k: results[k]["metrics"]["accuracy"]
        )
        best_model = results[best_model_name]["model"]

        # ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œé¢„æµ‹
        test_prediction = best_model.predict(create_batch_prediction_data(1)[0])
        assert isinstance(test_prediction, PredictionResult)

    def test_model_versioning_and_management(self):
        """æµ‹è¯•æ¨¡å‹ç‰ˆæœ¬ç®¡ç†"""
        training_data = create_comprehensive_training_data(200)

        # åˆ›å»ºä¸åŒç‰ˆæœ¬çš„æ¨¡å‹
        versions = ["1.0", "1.1", "2.0"]
        models = {}

        for version in versions:
            model = PoissonModel(version)
            model.train(training_data)
            models[version] = model

        # éªŒè¯ç‰ˆæœ¬ä¿¡æ¯
        for version, model in models.items():
            assert model.model_version == version
            assert model.is_trained is True

        # æ¯”è¾ƒä¸åŒç‰ˆæœ¬çš„æ€§èƒ½
        test_data = create_comprehensive_training_data(50)
        version_metrics = {}

        for version, model in models.items():
            metrics = model.evaluate(test_data)
            version_metrics[version] = metrics["accuracy"]

        # è‡³å°‘åº”è¯¥æœ‰ä¸€äº›æ€§èƒ½å·®å¼‚
        accuracies = list(version_metrics.values())
        assert (
            len(set(round(acc, 3) for acc in accuracies)) >= 1
        )  # å¯èƒ½ç›¸åŒï¼Œä½†è‡³å°‘è¦æµ‹è¯•

    def test_error_recovery_and_robustness(self):
        """æµ‹è¯•é”™è¯¯æ¢å¤å’Œé²æ£’æ€§"""
        model = PoissonModel()
        training_data = create_comprehensive_training_data(100)

        # æµ‹è¯•å„ç§é”™è¯¯æƒ…å†µ
        test_cases = [
            # æœªè®­ç»ƒæ¨¡å‹é¢„æµ‹
            lambda: model.predict({"home_team": "A", "away_team": "B"}),
            # æ— æ•ˆè¾“å…¥æ•°æ®
            lambda: model.train(pd.DataFrame()),
            # ç¼ºå°‘å¿…è¦å­—æ®µçš„é¢„æµ‹
            lambda: model.predict({"home_team": "A"}) if model.is_trained else None,
            # ç›¸åŒé˜Ÿä¼é¢„æµ‹
            lambda: (
                model.predict({"home_team": "A", "away_team": "A"})
                if model.is_trained
                else None
            ),
        ]

        # è®­ç»ƒæ¨¡å‹ç”¨äºåç»­æµ‹è¯•
        model.train(training_data)

        error_count = 0
        for test_case in test_cases:
            try:
                result = test_case()
                if result is None:
                    continue
            except (ValueError, RuntimeError, KeyError):
                error_count += 1

        # åº”è¯¥æ•è·åˆ°ä¸€äº›é”™è¯¯
        assert error_count >= 2

    @pytest.mark.asyncio
    async def test_concurrent_prediction_handling(self):
        """æµ‹è¯•å¹¶å‘é¢„æµ‹å¤„ç†"""
        model = PoissonModel()
        training_data = create_comprehensive_training_data(200)
        model.train(training_data)

        # åˆ›å»ºæ‰¹é‡é¢„æµ‹ä»»åŠ¡
        batch_data = create_batch_prediction_data(10)

        async def predict_single(data):
            return model.predict(data)

        # å¹¶å‘æ‰§è¡Œé¢„æµ‹
        tasks = [predict_single(data) for data in batch_data]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # éªŒè¯ç»“æœ
        successful_predictions = [r for r in results if isinstance(r, PredictionResult)]
        exceptions = [r for r in results if isinstance(r, Exception)]

        assert len(successful_predictions) > 0
        assert len(successful_predictions) + len(exceptions) == len(batch_data)

        for pred in successful_predictions:
            assert isinstance(pred, PredictionResult)
            assert (
                abs(pred.home_win_prob + pred.draw_prob + pred.away_win_prob - 1.0)
                < 0.01
            )


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.integration
class TestMLModelDeployment:
    """MLæ¨¡å‹éƒ¨ç½²æµ‹è¯•"""

    @pytest.mark.asyncio
    async def test_model_export_import_workflow(self):
        """æµ‹è¯•æ¨¡å‹å¯¼å‡ºå¯¼å…¥å·¥ä½œæµ"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # 1. è®­ç»ƒæ¨¡å‹
            model = PoissonModel("deployment_v1.0")
            training_data = create_comprehensive_training_data(300)
            model.train(training_data)

            # 2. ä¿å­˜æ¨¡å‹
            model_path = os.path.join(temp_dir, "poisson_model.pkl")
            save_success = model.save_model(model_path)
            assert save_success is True
            assert os.path.exists(model_path)

            # 3. ä¿å­˜å…ƒæ•°æ®
            metadata = {
                "model_name": model.model_name,
                "model_version": model.model_version,
                "training_date": datetime.now().isoformat(),
                "training_samples": len(training_data),
                "performance_metrics": model.evaluate(
                    create_comprehensive_training_data(50)
                ),
            }

            metadata_path = os.path.join(temp_dir, "model_metadata.json")
            with open(metadata_path, "w") as f:
                json.dump(metadata, f, indent=2)

            # 4. åŠ è½½æ¨¡å‹
            loaded_model = PoissonModel()
            load_success = loaded_model.load_model(model_path)
            assert load_success is True
            assert loaded_model.is_trained is True
            assert loaded_model.model_version == "deployment_v1.0"

            # 5. éªŒè¯åŠ è½½çš„æ¨¡å‹åŠŸèƒ½
            test_prediction_data = create_batch_prediction_data(1)[0]
            original_prediction = model.predict(test_prediction_data)
            loaded_prediction = loaded_model.predict(test_prediction_data)

            # é¢„æµ‹ç»“æœåº”è¯¥åŸºæœ¬ä¸€è‡´
            assert (
                abs(original_prediction.home_win_prob - loaded_prediction.home_win_prob)
                < 0.01
            )

    @pytest.mark.asyncio
    async def test_model_serving_simulation(self):
        """æµ‹è¯•æ¨¡å‹æœåŠ¡æ¨¡æ‹Ÿ"""

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹æœåŠ¡ç±»
        class ModelService:
            def __init__(self):
                self.models = {}
                self.load_models()

            def load_models(self):
                """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
                training_data = create_comprehensive_training_data(200)

                # åŠ è½½å¤šä¸ªæ¨¡å‹
                models_config = [
                    ("poisson_v1", PoissonModel("v1.0")),
                    ("poisson_v2", PoissonModel("v2.0")),
                ]

                for name, model in models_config:
                    model.train(training_data)
                    self.models[name] = model

            async def predict(self, model_name: str, request_data: dict[str, Any]):
                """å¼‚æ­¥é¢„æµ‹æ¥å£"""
                if model_name not in self.models:
                    raise ValueError(f"Model {model_name} not found")

                model = self.models[model_name]
                prediction = model.predict(request_data)
                return prediction

            def list_models(self):
                """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
                return list(self.models.keys())

        # æµ‹è¯•æ¨¡å‹æœåŠ¡
        service = ModelService()
        assert len(service.list_models()) == 2

        # æµ‹è¯•é¢„æµ‹
        test_request = create_batch_prediction_data(1)[0]
        prediction = await service.predict("poisson_v1", test_request)

        assert isinstance(prediction, PredictionResult)
        assert prediction.model_name == "PoissonModel"

        # æµ‹è¯•é”™è¯¯å¤„ç†
        with pytest.raises(ValueError):
            await service.predict("nonexistent_model", test_request)

    def test_model_monitoring_metrics(self):
        """æµ‹è¯•æ¨¡å‹ç›‘æ§æŒ‡æ ‡"""
        model = PoissonModel()
        training_data = create_comprehensive_training_data(200)
        model.train(training_data)

        # æ¨¡æ‹Ÿé¢„æµ‹ç›‘æ§
        predictions = []
        prediction_times = []

        for i in range(10):
            start_time = datetime.now()
            test_data = create_batch_prediction_data(1)[0]
            prediction = model.predict(test_data)
            end_time = datetime.now()

            predictions.append(prediction)
            prediction_times.append((end_time - start_time).total_seconds())

        # è®¡ç®—ç›‘æ§æŒ‡æ ‡
        avg_prediction_time = np.mean(prediction_times)
        max_prediction_time = np.max(prediction_times)
        confidence_scores = [p.confidence for p in predictions]
        avg_confidence = np.mean(confidence_scores)

        # éªŒè¯ç›‘æ§æŒ‡æ ‡
        assert avg_prediction_time < 1.0  # é¢„æµ‹æ—¶é—´åº”è¯¥å¾ˆå¿«
        assert max_prediction_time < 5.0  # æœ€å¤§é¢„æµ‹æ—¶é—´
        assert 0 < avg_confidence <= 1.0  # ç½®ä¿¡åº¦åœ¨åˆç†èŒƒå›´

        # åˆ›å»ºç›‘æ§æŠ¥å‘Š
        monitoring_report = {
            "model_name": model.model_name,
            "model_version": model.model_version,
            "total_predictions": len(predictions),
            "avg_prediction_time_ms": avg_prediction_time * 1000,
            "max_prediction_time_ms": max_prediction_time * 1000,
            "avg_confidence": avg_confidence,
            "min_confidence": min(confidence_scores),
            "max_confidence": max(confidence_scores),
            "uptime": "100%",  # æ¨¡æ‹ŸæŒ‡æ ‡
            "error_rate": "0%",  # æ¨¡æ‹ŸæŒ‡æ ‡
        }

        assert isinstance(monitoring_report, dict)
        assert monitoring_report["total_predictions"] == 10


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.integration
class TestMLDataPipeline:
    """MLæ•°æ®æµæ°´çº¿æµ‹è¯•"""

    def test_feature_engineering_pipeline(self):
        """æµ‹è¯•ç‰¹å¾å·¥ç¨‹æµæ°´çº¿"""
        # åˆ›å»ºåŸå§‹æ•°æ®
        raw_data = create_comprehensive_training_data(100)

        # æ¨¡æ‹Ÿç‰¹å¾å·¥ç¨‹æ­¥éª¤
        def engineer_features(df):
            """ç‰¹å¾å·¥ç¨‹å‡½æ•°"""
            engineered_df = df.copy()

            # 1. è®¡ç®—è¿›çƒå·®å¼‚
            engineered_df["goal_difference"] = (
                engineered_df["home_score"] - engineered_df["away_score"]
            )

            # 2. è®¡ç®—æ€»è¿›çƒæ•°
            engineered_df["total_goals"] = (
                engineered_df["home_score"] + engineered_df["away_score"]
            )

            # 3. åˆ›å»ºèƒœè´Ÿæ ‡ç­¾ï¼ˆæ•°å€¼åŒ–ï¼‰
            engineered_df["result_numeric"] = engineered_df["result"].map(
                {"home_win": 1, "draw": 0, "away_win": -1}
            )

            # 4. è®¡ç®—ä¸»å®¢é˜Ÿå®åŠ›æ¯”ï¼ˆç®€åŒ–ç‰ˆï¼‰
            engineered_df["strength_ratio"] = engineered_df[
                "home_team_goals_scored"
            ] / (engineered_df["away_team_goals_conceded"] + 1)

            # 5. æ—¶é—´ç‰¹å¾
            engineered_df["month"] = pd.to_datetime(
                engineered_df["match_date"]
            ).dt.month
            engineered_df["day_of_week"] = pd.to_datetime(
                engineered_df["match_date"]
            ).dt.dayofweek

            return engineered_df

        # åº”ç”¨ç‰¹å¾å·¥ç¨‹
        engineered_data = engineer_features(raw_data)

        # éªŒè¯ç‰¹å¾å·¥ç¨‹ç»“æœ
        assert "goal_difference" in engineered_data.columns
        assert "total_goals" in engineered_data.columns
        assert "result_numeric" in engineered_data.columns
        assert "strength_ratio" in engineered_data.columns
        assert "month" in engineered_data.columns
        assert "day_of_week" in engineered_data.columns

        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        assert len(engineered_data) == len(raw_data)
        assert engineered_data["goal_difference"].notna().all()

    def test_data_validation_pipeline(self):
        """æµ‹è¯•æ•°æ®éªŒè¯æµæ°´çº¿"""
        # åˆ›å»ºåŒ…å«å„ç§é—®é¢˜çš„æ•°æ®
        problematic_data = pd.DataFrame(
            {
                "home_team": ["Team_A", "Team_B", None, "Team_D", "Team_E"],
                "away_team": ["Team_B", "Team_A", "Team_C", "Team_E", "Team_D"],
                "home_score": [2, -1, 1, 3, 100],  # åŒ…å«è´Ÿæ•°å’Œå¼‚å¸¸å€¼
                "away_score": [1, 2, None, 0, 99],  # åŒ…å«ç©ºå€¼å’Œå¼‚å¸¸å€¼
                "result": ["home_win", "invalid", "draw", "home_win", "away_win"],
            }
        )

        def validate_data(df):
            """æ•°æ®éªŒè¯å‡½æ•°"""
            validation_results = {
                "total_rows": len(df),
                "issues": [],
                "clean_data": df.copy(),
            }

            # 1. æ£€æŸ¥ç©ºå€¼
            null_counts = df.isnull().sum()
            for col, count in null_counts.items():
                if count > 0:
                    validation_results["issues"].append(
                        f"Column {col} has {count} null values"
                    )

            # 2. æ£€æŸ¥æ•°å€¼èŒƒå›´
            numeric_columns = ["home_score", "away_score"]
            for col in numeric_columns:
                if col in df.columns:
                    invalid_values = df[(df[col] < 0) | (df[col] > 20)]
                    if len(invalid_values) > 0:
                        validation_results["issues"].append(
                            f"Column {col} has {len(invalid_values)} invalid values"
                        )

            # 3. æ£€æŸ¥ç»“æœå€¼
            if "result" in df.columns:
                valid_results = {"home_win", "draw", "away_win"}
                invalid_results = df[~df["result"].isin(valid_results)]
                if len(invalid_results) > 0:
                    validation_results["issues"].append(
                        f"Result column has {len(invalid_results)} invalid values"
                    )

            # 4. æ£€æŸ¥ä¸»å®¢é˜Ÿç›¸åŒ
            same_teams = df[df["home_team"] == df["away_team"]]
            if len(same_teams) > 0:
                validation_results["issues"].append(
                    f"Found {len(same_teams)} matches with same home and away team"
                )

            return validation_results

        # æ‰§è¡Œæ•°æ®éªŒè¯
        validation_results = validate_data(problematic_data)

        # éªŒè¯ç»“æœ
        assert validation_results["total_rows"] == 5
        assert len(validation_results["issues"]) > 0  # åº”è¯¥å‘ç°ä¸€äº›é—®é¢˜

        # æ£€æŸ¥æ˜¯å¦å‘ç°äº†é¢„æœŸçš„é—®é¢˜
        issue_text = " ".join(validation_results["issues"])
        assert "null" in issue_text.lower()
        assert "invalid" in issue_text.lower()

    def test_model_training_pipeline(self):
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒæµæ°´çº¿"""

        # åˆ›å»ºè®­ç»ƒæµæ°´çº¿
        class TrainingPipeline:
            def __init__(self):
                self.model = None
                self.training_history = []

            def preprocess_data(self, data):
                """æ•°æ®é¢„å¤„ç†"""
                # ç§»é™¤ç©ºå€¼
                clean_data = data.dropna()
                # è¿‡æ»¤å¼‚å¸¸å€¼
                clean_data = clean_data[
                    (clean_data["home_score"] >= 0)
                    & (clean_data["home_score"] <= 10)
                    & (clean_data["away_score"] >= 0)
                    & (clean_data["away_score"] <= 10)
                ]
                return clean_data

            def split_data(self, data, train_ratio=0.8):
                """æ•°æ®åˆ†å‰²"""
                n = len(data)
                train_size = int(n * train_ratio)
                train_data = data.iloc[:train_size]
                val_data = data.iloc[train_size:]
                return train_data, val_data

            def train_model(self, train_data, val_data):
                """è®­ç»ƒæ¨¡å‹"""
                self.model = PoissonModel("pipeline_v1.0")
                result = self.model.train(train_data, val_data)
                return result

            def evaluate_model(self, test_data):
                """è¯„ä¼°æ¨¡å‹"""
                if self.model is None:
                    raise ValueError("Model not trained")
                return self.model.evaluate(test_data)

            def run_pipeline(self, raw_data):
                """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
                # 1. æ•°æ®é¢„å¤„ç†
                processed_data = self.preprocess_data(raw_data)
                self.training_history.append(
                    f"Preprocessed {len(processed_data)} samples"
                )

                # 2. æ•°æ®åˆ†å‰²
                train_data, val_data = self.split_data(processed_data)
                self.training_history.append(
                    f"Split data: {len(train_data)} train, {len(val_data)} val"
                )

                # 3. æ¨¡å‹è®­ç»ƒ
                training_result = self.train_model(train_data, val_data)
                self.training_history.append(
                    f"Model trained with accuracy: {training_result.accuracy:.3f}"
                )

                # 4. æ¨¡å‹è¯„ä¼°
                eval_metrics = self.evaluate_model(val_data)
                self.training_history.append(
                    f"Model evaluated: {eval_metrics['accuracy']:.3f} accuracy"
                )

                return {
                    "model": self.model,
                    "training_result": training_result,
                    "evaluation_metrics": eval_metrics,
                    "training_history": self.training_history,
                }

        # è¿è¡Œè®­ç»ƒæµæ°´çº¿
        pipeline = TrainingPipeline()
        raw_data = create_comprehensive_training_data(300)
        results = pipeline.run_pipeline(raw_data)

        # éªŒè¯æµæ°´çº¿ç»“æœ
        assert pipeline.model is not None
        assert pipeline.model.is_trained is True
        assert len(pipeline.training_history) == 4
        assert isinstance(results["training_result"], TrainingResult)
        assert isinstance(results["evaluation_metrics"], dict)
        assert results["evaluation_metrics"]["accuracy"] > 0


# æµ‹è¯•è¿è¡Œå™¨
async def run_ml_integration_tests():
    """è¿è¡ŒMLé›†æˆæµ‹è¯•å¥—ä»¶"""
    print("ğŸ¤– å¼€å§‹MLé›†æˆæµ‹è¯•")
    print("=" * 60)

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„MLé›†æˆæµ‹è¯•é€»è¾‘
    print("âœ… MLé›†æˆæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(run_ml_integration_tests())
