#!/usr/bin/env python3
"""
ğŸ”„ MLç­–ç•¥é›†æˆæµ‹è¯• - æœºå™¨å­¦ä¹ ç­–ç•¥å’Œé›†æˆå·¥ä½œæµæµ‹è¯•

æµ‹è¯•MLç­–ç•¥æ¨¡å¼ã€é›†æˆé¢„æµ‹ã€å·¥ä½œæµç¼–æ’ã€ç­–ç•¥é€‰æ‹©å™¨ç­‰åŠŸèƒ½ã€‚
éªŒè¯ä¸åŒMLç­–ç•¥çš„ç»„åˆä½¿ç”¨å’Œæ•´ä½“ç³»ç»Ÿé›†æˆã€‚
"""

import asyncio
import warnings
from datetime import datetime, timedelta
from enum import Enum
from typing import Any

import numpy as np
import pandas as pd
import pytest

# æŠ‘åˆ¶warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
import os
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "../../../src"))

# å°è¯•å¯¼å…¥MLæ¨¡å—
try:
    from src.ml.models.base_model import BaseModel, PredictionResult, TrainingResult
    from src.ml.models.poisson_model import PoissonModel

    CAN_IMPORT = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥MLæ¨¡å—: {e}")
    CAN_IMPORT = False


# æ¨¡æ‹ŸMLç­–ç•¥ç³»ç»Ÿ
class MLStrategyType(Enum):
    """MLç­–ç•¥ç±»å‹"""

    POISSON = "poisson"
    ENSEMBLE = "ensemble"
    WEIGHTED = "weighted"
    ADAPTIVE = "adaptive"


class MockMLStrategy:
    """æ¨¡æ‹ŸMLç­–ç•¥"""

    def __init__(self, strategy_type: MLStrategyType, name: str, weight: float = 1.0):
        self.strategy_type = strategy_type
        self.name = name
        self.weight = weight
        self.model = None
        self.is_trained = False
        self.performance_history = []

    async def train(self, training_data: pd.DataFrame) -> dict[str, Any]:
        """è®­ç»ƒç­–ç•¥"""
        if self.strategy_type == MLStrategyType.POISSON:
            self.model = PoissonModel(f"{self.name}_poisson")
            training_result = self.model.train(training_data)
            self.is_trained = True
            return {
                "strategy": self.name,
                "accuracy": training_result.accuracy,
                "training_time": training_result.training_time,
                "model_info": self.model.get_model_info(),
            }
        else:
            # æ¨¡æ‹Ÿå…¶ä»–ç­–ç•¥çš„è®­ç»ƒ
            await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶é—´
            self.is_trained = True
            accuracy = np.random.uniform(0.4, 0.8)
            return {
                "strategy": self.name,
                "accuracy": accuracy,
                "training_time": np.random.uniform(0.1, 2.0),
                "model_info": {"model_type": self.strategy_type.value},
            }

    async def predict(self, match_data: dict[str, Any]) -> PredictionResult | None:
        """é¢„æµ‹"""
        if not self.is_trained:
            return None

        if self.strategy_type == MLStrategyType.POISSON and self.model:
            return self.model.predict(match_data)
        else:
            # æ¨¡æ‹Ÿå…¶ä»–ç­–ç•¥çš„é¢„æµ‹
            probabilities = np.random.dirichlet([1, 1, 1])  # ç”Ÿæˆéšæœºæ¦‚ç‡åˆ†å¸ƒ
            max_prob_index = np.argmax(probabilities)
            outcomes = ["home_win", "draw", "away_win"]

            return PredictionResult(
                match_id=match_data.get("match_id", "unknown"),
                home_team=match_data["home_team"],
                away_team=match_data["away_team"],
                home_win_prob=float(probabilities[0]),
                draw_prob=float(probabilities[1]),
                away_win_prob=float(probabilities[2]),
                predicted_outcome=outcomes[max_prob_index],
                confidence=float(max(probabilities)),
                model_name=self.name,
                model_version="1.0",
                created_at=datetime.now(),
            )

    def update_performance(self, actual_result: str, prediction: PredictionResult):
        """æ›´æ–°æ€§èƒ½å†å²"""
        is_correct = prediction.predicted_outcome == actual_result
        self.performance_history.append(
            {
                "timestamp": datetime.now(),
                "is_correct": is_correct,
                "confidence": prediction.confidence,
                "predicted_outcome": prediction.predicted_outcome,
                "actual_outcome": actual_result,
            }
        )

    def get_recent_accuracy(self, window_size: int = 50) -> float:
        """è·å–æœ€è¿‘å‡†ç¡®ç‡"""
        if not self.performance_history:
            return 0.0

        recent_history = self.performance_history[-window_size:]
        correct_predictions = sum(1 for h in recent_history if h["is_correct"])
        return correct_predictions / len(recent_history)


class MockStrategySelector:
    """æ¨¡æ‹Ÿç­–ç•¥é€‰æ‹©å™¨"""

    def __init__(self):
        self.strategies: dict[str, MockMLStrategy] = {}
        self.selection_history = []

    def register_strategy(self, strategy: MockMLStrategy):
        """æ³¨å†Œç­–ç•¥"""
        self.strategies[strategy.name] = strategy

    def select_best_strategy(
        self, match_data: dict[str, Any], selection_method: str = "accuracy"
    ) -> MockMLStrategy | None:
        """é€‰æ‹©æœ€ä½³ç­–ç•¥"""
        if not self.strategies:
            return None

        if selection_method == "accuracy":
            # åŸºäºæœ€è¿‘å‡†ç¡®ç‡é€‰æ‹©
            best_strategy = max(
                self.strategies.values(), key=lambda s: s.get_recent_accuracy()
            )
        elif selection_method == "weighted":
            # åŸºäºæƒé‡é€‰æ‹©
            best_strategy = max(self.strategies.values(), key=lambda s: s.weight)
        else:
            # éšæœºé€‰æ‹©
            best_strategy = np.random.choice(list(self.strategies.values()))

        self.selection_history.append(
            {
                "timestamp": datetime.now(),
                "selected_strategy": best_strategy.name,
                "method": selection_method,
                "match_data": match_data,
            }
        )

        return best_strategy

    def get_strategy_performance_summary(self) -> dict[str, dict[str, float]]:
        """è·å–ç­–ç•¥æ€§èƒ½æ‘˜è¦"""
        summary = {}
        for name, strategy in self.strategies.items():
            summary[name] = {
                "recent_accuracy": strategy.get_recent_accuracy(),
                "weight": strategy.weight,
                "total_predictions": len(strategy.performance_history),
                "is_trained": strategy.is_trained,
            }
        return summary


class MockEnsemblePredictor:
    """æ¨¡æ‹Ÿé›†æˆé¢„æµ‹å™¨"""

    def __init__(self, ensemble_method: str = "weighted_average"):
        self.ensemble_method = ensemble_method
        self.strategies: list[MockMLStrategy] = []
        self.prediction_history = []

    def add_strategy(self, strategy: MockMLStrategy):
        """æ·»åŠ ç­–ç•¥"""
        self.strategies.append(strategy)

    async def predict(self, match_data: dict[str, Any]) -> PredictionResult | None:
        """é›†æˆé¢„æµ‹"""
        if not self.strategies:
            return None

        # è·å–æ‰€æœ‰ç­–ç•¥çš„é¢„æµ‹
        individual_predictions = []
        for strategy in self.strategies:
            if strategy.is_trained:
                prediction = await strategy.predict(match_data)
                if prediction:
                    individual_predictions.append((prediction, strategy))

        if not individual_predictions:
            return None

        # é›†æˆæ–¹æ³•
        if self.ensemble_method == "weighted_average":
            return self._weighted_average_ensemble(individual_predictions)
        elif self.ensemble_method == "majority_vote":
            return self._majority_vote_ensemble(individual_predictions)
        elif self.ensemble_method == "confidence_weighted":
            return self._confidence_weighted_ensemble(individual_predictions)
        else:
            return self._simple_average_ensemble(individual_predictions)

    def _weighted_average_ensemble(
        self, predictions: list[tuple[PredictionResult, MockMLStrategy]]
    ) -> PredictionResult:
        """åŠ æƒå¹³å‡é›†æˆ"""
        total_weight = sum(strategy.weight for _, strategy in predictions)

        home_win_prob = (
            sum(pred.home_win_prob * strategy.weight for pred, strategy in predictions)
            / total_weight
        )
        draw_prob = (
            sum(pred.draw_prob * strategy.weight for pred, strategy in predictions)
            / total_weight
        )
        away_win_prob = (
            sum(pred.away_win_prob * strategy.weight for pred, strategy in predictions)
            / total_weight
        )

        # å½’ä¸€åŒ–æ¦‚ç‡
        total_prob = home_win_prob + draw_prob + away_win_prob
        if total_prob > 0:
            home_win_prob /= total_prob
            draw_prob /= total_prob
            away_win_prob /= total_prob

        # ç¡®å®šé¢„æµ‹ç»“æœ
        probs = [home_win_prob, draw_prob, away_win_prob]
        outcomes = ["home_win", "draw", "away_win"]
        predicted_outcome = outcomes[np.argmax(probs)]
        confidence = max(probs)

        return PredictionResult(
            match_id=predictions[0][0].match_id,
            home_team=predictions[0][0].home_team,
            away_team=predictions[0][0].away_team,
            home_win_prob=home_win_prob,
            draw_prob=draw_prob,
            away_win_prob=away_win_prob,
            predicted_outcome=predicted_outcome,
            confidence=confidence,
            model_name=f"ensemble_{self.ensemble_method}",
            model_version="1.0",
            created_at=datetime.now(),
        )

    def _majority_vote_ensemble(
        self, predictions: list[tuple[PredictionResult, MockMLStrategy]]
    ) -> PredictionResult:
        """å¤šæ•°æŠ•ç¥¨é›†æˆ"""
        outcomes = [pred.predicted_outcome for pred, _ in predictions]
        outcome_counts = {outcome: outcomes.count(outcome) for outcome in set(outcomes)}

        # æ‰¾åˆ°æœ€å¤šç¥¨æ•°çš„ç»“æœ
        predicted_outcome = max(outcome_counts, key=outcome_counts.get)

        # å¹³å‡æ¦‚ç‡
        home_win_prob = np.mean([pred.home_win_prob for pred, _ in predictions])
        draw_prob = np.mean([pred.draw_prob for pred, _ in predictions])
        away_win_prob = np.mean([pred.away_win_prob for pred, _ in predictions])

        # å½’ä¸€åŒ–
        total_prob = home_win_prob + draw_prob + away_win_prob
        if total_prob > 0:
            home_win_prob /= total_prob
            draw_prob /= total_prob
            away_win_prob /= total_prob

        confidence = max(home_win_prob, draw_prob, away_win_prob)

        return PredictionResult(
            match_id=predictions[0][0].match_id,
            home_team=predictions[0][0].home_team,
            away_team=predictions[0][0].away_team,
            home_win_prob=home_win_prob,
            draw_prob=draw_prob,
            away_win_prob=away_win_prob,
            predicted_outcome=predicted_outcome,
            confidence=confidence,
            model_name=f"ensemble_{self.ensemble_method}",
            model_version="1.0",
            created_at=datetime.now(),
        )

    def _confidence_weighted_ensemble(
        self, predictions: list[tuple[PredictionResult, MockMLStrategy]]
    ) -> PredictionResult:
        """ç½®ä¿¡åº¦åŠ æƒé›†æˆ"""
        total_confidence = sum(pred.confidence for pred, _ in predictions)

        if total_confidence == 0:
            return self._simple_average_ensemble(predictions)

        home_win_prob = (
            sum(pred.home_win_prob * pred.confidence for pred, _ in predictions)
            / total_confidence
        )
        draw_prob = (
            sum(pred.draw_prob * pred.confidence for pred, _ in predictions)
            / total_confidence
        )
        away_win_prob = (
            sum(pred.away_win_prob * pred.confidence for pred, _ in predictions)
            / total_confidence
        )

        # å½’ä¸€åŒ–
        total_prob = home_win_prob + draw_prob + away_win_prob
        if total_prob > 0:
            home_win_prob /= total_prob
            draw_prob /= total_prob
            away_win_prob /= total_prob

        probs = [home_win_prob, draw_prob, away_win_prob]
        outcomes = ["home_win", "draw", "away_win"]
        predicted_outcome = outcomes[np.argmax(probs)]
        confidence = max(probs)

        return PredictionResult(
            match_id=predictions[0][0].match_id,
            home_team=predictions[0][0].home_team,
            away_team=predictions[0][0].away_team,
            home_win_prob=home_win_prob,
            draw_prob=draw_prob,
            away_win_prob=away_win_prob,
            predicted_outcome=predicted_outcome,
            confidence=confidence,
            model_name=f"ensemble_{self.ensemble_method}",
            model_version="1.0",
            created_at=datetime.now(),
        )

    def _simple_average_ensemble(
        self, predictions: list[tuple[PredictionResult, MockMLStrategy]]
    ) -> PredictionResult:
        """ç®€å•å¹³å‡é›†æˆ"""
        home_win_prob = np.mean([pred.home_win_prob for pred, _ in predictions])
        draw_prob = np.mean([pred.draw_prob for pred, _ in predictions])
        away_win_prob = np.mean([pred.away_win_prob for pred, _ in predictions])

        # å½’ä¸€åŒ–
        total_prob = home_win_prob + draw_prob + away_win_prob
        if total_prob > 0:
            home_win_prob /= total_prob
            draw_prob /= total_prob
            away_win_prob /= total_prob

        probs = [home_win_prob, draw_prob, away_win_prob]
        outcomes = ["home_win", "draw", "away_win"]
        predicted_outcome = outcomes[np.argmax(probs)]
        confidence = max(probs)

        return PredictionResult(
            match_id=predictions[0][0].match_id,
            home_team=predictions[0][0].home_team,
            away_team=predictions[0][0].away_team,
            home_win_prob=home_win_prob,
            draw_prob=draw_prob,
            away_win_prob=away_win_prob,
            predicted_outcome=predicted_outcome,
            confidence=confidence,
            model_name=f"ensemble_{self.ensemble_method}",
            model_version="1.0",
            created_at=datetime.now(),
        )


def create_test_dataset(num_matches: int = 200) -> tuple[pd.DataFrame, pd.DataFrame]:
    """åˆ›å»ºæµ‹è¯•æ•°æ®é›†"""
    teams = [f"Team_{chr(65+i)}" for i in range(20)]
    training_data = []
    test_data = []

    # è®­ç»ƒæ•°æ®
    for i in range(num_matches):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        home_goals = np.random.poisson(1.5)
        away_goals = np.random.poisson(1.1)

        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        training_data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now()
                - timedelta(days=np.random.randint(1, 365)),
            }
        )

    # æµ‹è¯•æ•°æ®
    for i in range(num_matches // 4):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        home_goals = np.random.poisson(1.5)
        away_goals = np.random.poisson(1.1)

        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        test_data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now() - timedelta(days=np.random.randint(0, 30)),
            }
        )

    return pd.DataFrame(training_data), pd.DataFrame(test_data)


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.asyncio
class TestMLStrategySystem:
    """MLç­–ç•¥ç³»ç»Ÿæµ‹è¯•"""

    async def test_single_strategy_training_and_prediction(self):
        """æµ‹è¯•å•ä¸€ç­–ç•¥è®­ç»ƒå’Œé¢„æµ‹"""
        training_data, test_data = create_test_dataset(150, 50)

        # åˆ›å»ºç­–ç•¥
        strategy = MockMLStrategy(MLStrategyType.POISSON, "single_test", weight=1.0)

        # è®­ç»ƒç­–ç•¥
        training_result = await strategy.train(training_data)

        assert training_result["strategy"] == "single_test"
        assert "accuracy" in training_result
        assert "training_time" in training_result
        assert strategy.is_trained

        # é¢„æµ‹
        test_match = test_data.iloc[0]
        match_data = {
            "home_team": test_match["home_team"],
            "away_team": test_match["away_team"],
            "match_id": "single_strategy_test",
        }

        prediction = await strategy.predict(match_data)

        assert prediction is not None
        assert prediction.home_team == match_data["home_team"]
        assert prediction.away_team == match_data["away_team"]
        assert prediction.match_id == match_data["match_id"]
        assert (
            abs(
                prediction.home_win_prob
                + prediction.draw_prob
                + prediction.away_win_prob
                - 1.0
            )
            < 0.01
        )

        print(
            f"âœ… å•ä¸€ç­–ç•¥æµ‹è¯•é€šè¿‡: {strategy.name}, å‡†ç¡®ç‡={training_result['accuracy']:.3f}"
        )

    async def test_multiple_strategy_training(self):
        """æµ‹è¯•å¤šç­–ç•¥è®­ç»ƒ"""
        training_data, test_data = create_test_dataset(200, 50)

        # åˆ›å»ºå¤šä¸ªç­–ç•¥
        strategies = [
            MockMLStrategy(MLStrategyType.POISSON, "poisson_strategy", weight=1.0),
            MockMLStrategy(MLStrategyType.ENSEMBLE, "ensemble_strategy", weight=0.8),
            MockMLStrategy(MLStrategyType.WEIGHTED, "weighted_strategy", weight=1.2),
            MockMLStrategy(MLStrategyType.ADAPTIVE, "adaptive_strategy", weight=0.9),
        ]

        # å¹¶è¡Œè®­ç»ƒæ‰€æœ‰ç­–ç•¥
        training_tasks = [strategy.train(training_data) for strategy in strategies]
        training_results = await asyncio.gather(*training_tasks)

        # éªŒè¯è®­ç»ƒç»“æœ
        assert len(training_results) == len(strategies)

        for i, (strategy, result) in enumerate(zip(strategies, training_results)):
            assert strategy.is_trained
            assert result["strategy"] == strategy.name
            assert "accuracy" in result
            assert "training_time" in result
            print(
                f"  ç­–ç•¥ {strategy.name}: å‡†ç¡®ç‡={result['accuracy']:.3f}, è®­ç»ƒæ—¶é—´={result['training_time']:.2f}s"
            )

        print(f"âœ… å¤šç­–ç•¥è®­ç»ƒæµ‹è¯•é€šè¿‡: {len(strategies)}ä¸ªç­–ç•¥è®­ç»ƒå®Œæˆ")

    async def test_strategy_selector_functionality(self):
        """æµ‹è¯•ç­–ç•¥é€‰æ‹©å™¨åŠŸèƒ½"""
        training_data, test_data = create_test_dataset(150, 50)

        # åˆ›å»ºç­–ç•¥é€‰æ‹©å™¨
        selector = MockStrategySelector()

        # åˆ›å»ºå¹¶æ³¨å†Œç­–ç•¥
        strategies = [
            MockMLStrategy(MLStrategyType.POISSON, "high_accuracy", weight=1.0),
            MockMLStrategy(MLStrategyType.ENSEMBLE, "medium_accuracy", weight=1.5),
            MockMLStrategy(MLStrategyType.WEIGHTED, "low_accuracy", weight=2.0),
        ]

        for strategy in strategies:
            selector.register_strategy(strategy)
            await strategy.train(training_data)

        # æ¨¡æ‹Ÿä¸åŒçš„æ€§èƒ½å†å²
        strategies[0].performance_history = [
            {"is_correct": True, "confidence": 0.8} for _ in range(20)
        ]  # é«˜å‡†ç¡®ç‡

        strategies[1].performance_history = [
            {"is_correct": True, "confidence": 0.6} for _ in range(15)
        ] + [
            {"is_correct": False, "confidence": 0.4} for _ in range(5)
        ]  # ä¸­ç­‰å‡†ç¡®ç‡

        strategies[2].performance_history = [
            {"is_correct": True, "confidence": 0.7} for _ in range(10)
        ] + [
            {"is_correct": False, "confidence": 0.5} for _ in range(10)
        ]  # ä½å‡†ç¡®ç‡

        # æµ‹è¯•ä¸åŒçš„é€‰æ‹©æ–¹æ³•
        test_match = test_data.iloc[0]
        match_data = {
            "home_team": test_match["home_team"],
            "away_team": test_match["away_team"],
        }

        selection_methods = ["accuracy", "weighted", "random"]
        selected_strategies = []

        for method in selection_methods:
            selected = selector.select_best_strategy(match_data, method)
            selected_strategies.append((method, selected.name if selected else None))

        # éªŒè¯é€‰æ‹©ç»“æœ
        assert len(selected_strategies) == len(selection_methods)

        # åŸºäºå‡†ç¡®ç‡çš„é€‰æ‹©åº”è¯¥é€‰æ‹©æœ€é«˜å‡†ç¡®ç‡çš„ç­–ç•¥
        accuracy_selection = next(s for s in selected_strategies if s[0] == "accuracy")
        assert accuracy_selection[1] == "high_accuracy"

        # åŸºäºæƒé‡çš„é€‰æ‹©åº”è¯¥é€‰æ‹©æœ€é«˜æƒé‡çš„ç­–ç•¥
        weighted_selection = next(s for s in selected_strategies if s[0] == "weighted")
        assert weighted_selection[1] == "low_accuracy"

        print("âœ… ç­–ç•¥é€‰æ‹©å™¨æµ‹è¯•é€šè¿‡:")
        for method, strategy_name in selected_strategies:
            print(f"  {method}æ–¹æ³•: é€‰æ‹©äº†{strategy_name}")

    async def test_ensemble_prediction_methods(self):
        """æµ‹è¯•é›†æˆé¢„æµ‹æ–¹æ³•"""
        training_data, test_data = create_test_dataset(200, 50)

        # åˆ›å»ºç­–ç•¥
        strategies = [
            MockMLStrategy(MLStrategyType.POISSON, "poisson_ensemble", weight=1.0),
            MockMLStrategy(MLStrategyType.ENSEMBLE, "ensemble_ensemble", weight=0.8),
            MockMLStrategy(MLStrategyType.WEIGHTED, "weighted_ensemble", weight=1.2),
        ]

        # è®­ç»ƒç­–ç•¥
        for strategy in strategies:
            await strategy.train(training_data)

        # æµ‹è¯•ä¸åŒçš„é›†æˆæ–¹æ³•
        ensemble_methods = [
            "weighted_average",
            "majority_vote",
            "confidence_weighted",
            "simple_average",
        ]
        ensemble_results = []

        test_match = test_data.iloc[0]
        match_data = {
            "home_team": test_match["home_team"],
            "away_team": test_match["away_team"],
            "match_id": "ensemble_test",
        }

        for method in ensemble_methods:
            ensemble = MockEnsemblePredictor(ensemble_method=method)
            for strategy in strategies:
                ensemble.add_strategy(strategy)

            prediction = await ensemble.predict(match_data)
            ensemble_results.append((method, prediction))

        # éªŒè¯é›†æˆç»“æœ
        assert len(ensemble_results) == len(ensemble_methods)

        for method, prediction in ensemble_results:
            assert prediction is not None
            assert prediction.model_name.startswith("ensemble_")
            assert (
                abs(
                    prediction.home_win_prob
                    + prediction.draw_prob
                    + prediction.away_win_prob
                    - 1.0
                )
                < 0.01
            )
            print(
                f"  {method}: {prediction.predicted_outcome} (ç½®ä¿¡åº¦: {prediction.confidence:.3f})"
            )

        print(f"âœ… é›†æˆé¢„æµ‹æ–¹æ³•æµ‹è¯•é€šè¿‡: {len(ensemble_methods)}ç§æ–¹æ³•")

    async def test_strategy_performance_tracking(self):
        """æµ‹è¯•ç­–ç•¥æ€§èƒ½è·Ÿè¸ª"""
        training_data, test_data = create_test_dataset(150, 50)

        # åˆ›å»ºç­–ç•¥
        strategy = MockMLStrategy(
            MLStrategyType.POISSON, "performance_test", weight=1.0
        )
        await strategy.train(training_data)

        # æ¨¡æ‹Ÿé¢„æµ‹å’Œæ€§èƒ½è·Ÿè¸ª
        performance_results = []

        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
                "match_id": f"perf_test_{len(performance_results)}",
            }

            prediction = await strategy.predict(match_data)
            if prediction:
                # æ›´æ–°æ€§èƒ½
                strategy.update_performance(test_match["result"], prediction)

                # è®°å½•ç»“æœ
                performance_results.append(
                    {
                        "predicted": prediction.predicted_outcome,
                        "actual": test_match["result"],
                        "confidence": prediction.confidence,
                        "correct": prediction.predicted_outcome == test_match["result"],
                    }
                )

        # éªŒè¯æ€§èƒ½è·Ÿè¸ª
        assert len(strategy.performance_history) == len(performance_results)
        assert len(strategy.performance_history) > 0

        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_predictions = len(performance_results)
        correct_predictions = sum(1 for r in performance_results if r["correct"])
        overall_accuracy = correct_predictions / total_predictions

        recent_accuracy = strategy.get_recent_accuracy(window_size=20)

        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        assert 0 <= overall_accuracy <= 1
        assert 0 <= recent_accuracy <= 1
        assert len(strategy.performance_history) == total_predictions

        # éªŒè¯æœ€è¿‘å‡†ç¡®ç‡çš„è®¡ç®—
        recent_history = strategy.performance_history[-20:]
        expected_recent_accuracy = sum(
            1 for h in recent_history if h["is_correct"]
        ) / len(recent_history)
        assert abs(recent_accuracy - expected_recent_accuracy) < 0.001

        print("âœ… ç­–ç•¥æ€§èƒ½è·Ÿè¸ªæµ‹è¯•é€šè¿‡:")
        print(f"  æ€»é¢„æµ‹æ•°: {total_predictions}")
        print(f"  æ€»å‡†ç¡®ç‡: {overall_accuracy:.3f}")
        print(f"  æœ€è¿‘å‡†ç¡®ç‡: {recent_accuracy:.3f}")

    async def test_adaptive_strategy_selection(self):
        """æµ‹è¯•è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©"""
        training_data, test_data = create_test_dataset(200, 80)

        # åˆ›å»ºç­–ç•¥é€‰æ‹©å™¨
        selector = MockStrategySelector()

        # åˆ›å»ºç­–ç•¥
        strategies = [
            MockMLStrategy(MLStrategyType.POISSON, "adaptive_poisson", weight=1.0),
            MockMLStrategy(MLStrategyType.ENSEMBLE, "adaptive_ensemble", weight=1.0),
            MockMLStrategy(MLStrategyType.WEIGHTED, "adaptive_weighted", weight=1.0),
        ]

        # è®­ç»ƒå¹¶æ³¨å†Œç­–ç•¥
        for strategy in strategies:
            await strategy.train(training_data)
            selector.register_strategy(strategy)

        # æ¨¡æ‹Ÿè‡ªé€‚åº”é€‰æ‹©è¿‡ç¨‹
        selection_counts = {strategy.name: 0 for strategy in strategies}

        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
            }

            # æ¨¡æ‹Ÿæ€§èƒ½æ›´æ–°
            for strategy in strategies:
                prediction = await strategy.predict(match_data)
                if prediction:
                    # éšæœºå†³å®šæ˜¯å¦æ­£ç¡®ï¼Œæ¨¡æ‹ŸçœŸå®æ€§èƒ½å˜åŒ–
                    np.random.random() < 0.6  # 60%å‡†ç¡®ç‡
                    strategy.update_performance(test_match["result"], prediction)

            # é€‰æ‹©æœ€ä½³ç­–ç•¥
            best_strategy = selector.select_best_strategy(match_data, "accuracy")
            if best_strategy:
                selection_counts[best_strategy.name] += 1

        # éªŒè¯è‡ªé€‚åº”é€‰æ‹©
        total_selections = sum(selection_counts.values())
        assert total_selections > 0

        # åº”è¯¥æœ‰ä¸åŒçš„ç­–ç•¥è¢«é€‰æ‹©
        selected_strategies = [
            name for name, count in selection_counts.items() if count > 0
        ]
        assert len(selected_strategies) >= 1

        # éªŒè¯é€‰æ‹©å†å²
        assert len(selector.selection_history) == total_selections

        print("âœ… è‡ªé€‚åº”ç­–ç•¥é€‰æ‹©æµ‹è¯•é€šè¿‡:")
        for strategy_name, count in selection_counts.items():
            percentage = count / total_selections * 100
            print(f"  {strategy_name}: è¢«é€‰æ‹©{count}æ¬¡ ({percentage:.1f}%)")


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.asyncio
class TestMLWorkflowIntegration:
    """MLå·¥ä½œæµé›†æˆæµ‹è¯•"""

    async def test_end_to_end_ml_workflow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯MLå·¥ä½œæµ"""
        # 1. æ•°æ®å‡†å¤‡
        training_data, test_data = create_test_dataset(300, 100)

        # 2. ç­–ç•¥ç³»ç»Ÿåˆå§‹åŒ–
        selector = MockStrategySelector()
        ensemble_predictor = MockEnsemblePredictor(
            ensemble_method="confidence_weighted"
        )

        # 3. åˆ›å»ºå’Œè®­ç»ƒç­–ç•¥
        strategies = [
            MockMLStrategy(MLStrategyType.POISSON, "workflow_poisson", weight=1.2),
            MockMLStrategy(MLStrategyType.ENSEMBLE, "workflow_ensemble", weight=0.9),
            MockMLStrategy(MLStrategyType.ADAPTIVE, "workflow_adaptive", weight=1.0),
        ]

        # 4. å¹¶è¡Œè®­ç»ƒ
        training_tasks = [strategy.train(training_data) for strategy in strategies]
        await asyncio.gather(*training_tasks)

        # 5. æ³¨å†Œç­–ç•¥
        for strategy in strategies:
            selector.register_strategy(strategy)
            ensemble_predictor.add_strategy(strategy)

        # 6. æ‰¹é‡é¢„æµ‹å·¥ä½œæµ
        workflow_results = []

        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
                "match_id": f"workflow_{len(workflow_results)}",
            }

            # 6.1 ç­–ç•¥é€‰æ‹©
            best_strategy = selector.select_best_strategy(match_data, "accuracy")

            # 6.2 é›†æˆé¢„æµ‹
            ensemble_prediction = await ensemble_predictor.predict(match_data)

            # 6.3 å•ç­–ç•¥é¢„æµ‹ï¼ˆæœ€ä½³ç­–ç•¥ï¼‰
            single_prediction = None
            if best_strategy:
                single_prediction = await best_strategy.predict(match_data)

            # 6.4 ç»“æœè®°å½•
            workflow_results.append(
                {
                    "match_id": match_data["match_id"],
                    "actual_result": test_match["result"],
                    "ensemble_prediction": ensemble_prediction,
                    "single_prediction": single_prediction,
                    "selected_strategy": best_strategy.name if best_strategy else None,
                }
            )

        # 7. å·¥ä½œæµç»“æœåˆ†æ
        successful_predictions = [
            r
            for r in workflow_results
            if r["ensemble_prediction"] and r["single_prediction"]
        ]

        # é›†æˆé¢„æµ‹å‡†ç¡®ç‡
        ensemble_correct = sum(
            1
            for r in successful_predictions
            if r["ensemble_prediction"].predicted_outcome == r["actual_result"]
        )
        ensemble_accuracy = ensemble_correct / len(successful_predictions)

        # å•ç­–ç•¥é¢„æµ‹å‡†ç¡®ç‡
        single_correct = sum(
            1
            for r in successful_predictions
            if r["single_prediction"].predicted_outcome == r["actual_result"]
        )
        single_accuracy = single_correct / len(successful_predictions)

        # éªŒè¯å·¥ä½œæµç»“æœ
        assert len(workflow_results) == len(test_data)
        assert len(successful_predictions) > 0
        assert 0 <= ensemble_accuracy <= 1
        assert 0 <= single_accuracy <= 1

        print("âœ… ç«¯åˆ°ç«¯MLå·¥ä½œæµæµ‹è¯•é€šè¿‡:")
        print(f"  å¤„ç†æ¯”èµ›æ•°: {len(workflow_results)}")
        print(f"  æˆåŠŸé¢„æµ‹æ•°: {len(successful_predictions)}")
        print(f"  é›†æˆé¢„æµ‹å‡†ç¡®ç‡: {ensemble_accuracy:.3f}")
        print(f"  å•ç­–ç•¥é¢„æµ‹å‡†ç¡®ç‡: {single_accuracy:.3f}")
        print(f"  ç­–ç•¥é€‰æ‹©å†å²: {len(selector.selection_history)}æ¬¡")

    async def test_concurrent_workflow_processing(self):
        """æµ‹è¯•å¹¶å‘å·¥ä½œæµå¤„ç†"""
        training_data, test_data = create_test_dataset(200, 60)

        # åˆ›å»ºå¤šä¸ªå·¥ä½œæµå®ä¾‹
        workflow_count = 3
        workflows = []

        for i in range(workflow_count):
            selector = MockStrategySelector()
            ensemble_predictor = MockEnsemblePredictor(
                ensemble_method="weighted_average"
            )

            # æ¯ä¸ªå·¥ä½œæµä½¿ç”¨ä¸åŒçš„ç­–ç•¥
            strategies = [
                MockMLStrategy(
                    MLStrategyType.POISSON, f"concurrent_poisson_{i}", weight=1.0
                ),
                MockMLStrategy(
                    MLStrategyType.WEIGHTED, f"concurrent_weighted_{i}", weight=1.0
                ),
            ]

            workflows.append(
                {
                    "selector": selector,
                    "ensemble_predictor": ensemble_predictor,
                    "strategies": strategies,
                }
            )

        # å¹¶å‘è®­ç»ƒæ‰€æœ‰å·¥ä½œæµ
        training_tasks = []
        for workflow in workflows:
            for strategy in workflow["strategies"]:
                training_tasks.append(strategy.train(training_data))

        training_results = await asyncio.gather(*training_tasks)
        assert len(training_results) == workflow_count * 2

        # è®¾ç½®å·¥ä½œæµ
        for workflow in workflows:
            for strategy in workflow["strategies"]:
                workflow["selector"].register_strategy(strategy)
                workflow["ensemble_predictor"].add_strategy(strategy)

        # å¹¶å‘é¢„æµ‹å¤„ç†
        async def process_workflow(
            workflow_id: int, workflow_data: dict, test_subset: pd.DataFrame
        ):
            """å¤„ç†å•ä¸ªå·¥ä½œæµ"""
            results = []

            for _, test_match in test_subset.iterrows():
                match_data = {
                    "home_team": test_match["home_team"],
                    "away_team": test_match["away_team"],
                    "match_id": f"concurrent_{workflow_id}_{len(results)}",
                }

                prediction = await workflow_data["ensemble_predictor"].predict(
                    match_data
                )
                if prediction:
                    results.append(
                        {
                            "workflow_id": workflow_id,
                            "match_id": match_data["match_id"],
                            "prediction": prediction,
                            "actual": test_match["result"],
                        }
                    )

            return results

        # åˆ†å‰²æµ‹è¯•æ•°æ®ç»™ä¸åŒå·¥ä½œæµ
        test_subsets = np.array_split(test_data, workflow_count)

        # å¹¶å‘æ‰§è¡Œå·¥ä½œæµ
        workflow_tasks = [
            process_workflow(i, workflow, test_subset)
            for i, (workflow, test_subset) in enumerate(zip(workflows, test_subsets))
        ]

        workflow_results = await asyncio.gather(*workflow_tasks)

        # éªŒè¯å¹¶å‘ç»“æœ
        total_predictions = sum(len(results) for results in workflow_results)
        assert total_predictions == len(test_data)

        # è®¡ç®—æ¯ä¸ªå·¥ä½œæµçš„å‡†ç¡®ç‡
        workflow_accuracies = []
        for i, results in enumerate(workflow_results):
            if results:
                correct = sum(
                    1
                    for r in results
                    if r["prediction"].predicted_outcome == r["actual"]
                )
                accuracy = correct / len(results)
                workflow_accuracies.append(accuracy)
                print(f"  å·¥ä½œæµ{i}: {len(results)}ä¸ªé¢„æµ‹, å‡†ç¡®ç‡={accuracy:.3f}")

        assert len(workflow_accuracies) == workflow_count

        print("âœ… å¹¶å‘å·¥ä½œæµå¤„ç†æµ‹è¯•é€šè¿‡:")
        print(f"  å·¥ä½œæµæ•°é‡: {workflow_count}")
        print(f"  æ€»é¢„æµ‹æ•°: {total_predictions}")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {np.mean(workflow_accuracies):.3f}")

    async def test_workflow_error_handling_and_recovery(self):
        """æµ‹è¯•å·¥ä½œæµé”™è¯¯å¤„ç†å’Œæ¢å¤"""
        training_data, test_data = create_test_dataset(150, 50)

        # åˆ›å»ºå·¥ä½œæµ
        selector = MockStrategySelector()
        ensemble_predictor = MockEnsemblePredictor()

        # åˆ›å»ºæ­£å¸¸ç­–ç•¥å’Œæœ‰é—®é¢˜çš„ç­–ç•¥
        normal_strategy = MockMLStrategy(
            MLStrategyType.POISSON, "normal_strategy", weight=1.0
        )
        await normal_strategy.train(training_data)

        # åˆ›å»ºæœªè®­ç»ƒçš„ç­–ç•¥ï¼ˆä¼šå¤±è´¥ï¼‰
        untrained_strategy = MockMLStrategy(
            MLStrategyType.ENSEMBLE, "untrained_strategy", weight=1.0
        )
        # ä¸è®­ç»ƒï¼Œæ¨¡æ‹Ÿé”™è¯¯æƒ…å†µ

        # æ³¨å†Œç­–ç•¥
        selector.register_strategy(normal_strategy)
        selector.register_strategy(untrained_strategy)
        ensemble_predictor.add_strategy(normal_strategy)
        ensemble_predictor.add_strategy(untrained_strategy)

        # æµ‹è¯•é”™è¯¯å¤„ç†
        error_handling_results = []

        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
                "match_id": f"error_test_{len(error_handling_results)}",
            }

            try:
                # 1. ç­–ç•¥é€‰æ‹©ï¼ˆåº”è¯¥èƒ½å¤„ç†æœªè®­ç»ƒç­–ç•¥ï¼‰
                best_strategy = selector.select_best_strategy(match_data, "accuracy")

                # 2. é›†æˆé¢„æµ‹ï¼ˆåº”è¯¥èƒ½å¤„ç†éƒ¨åˆ†å¤±è´¥ï¼‰
                ensemble_prediction = await ensemble_predictor.predict(match_data)

                # 3. å•ç­–ç•¥é¢„æµ‹ï¼ˆå¯èƒ½å¤±è´¥ï¼‰
                single_prediction = None
                if best_strategy and best_strategy.is_trained:
                    single_prediction = await best_strategy.predict(match_data)

                error_handling_results.append(
                    {
                        "match_id": match_data["match_id"],
                        "success": True,
                        "ensemble_success": ensemble_prediction is not None,
                        "single_success": single_prediction is not None,
                        "selected_strategy": (
                            best_strategy.name if best_strategy else None
                        ),
                    }
                )

            except Exception as e:
                error_handling_results.append(
                    {
                        "match_id": match_data["match_id"],
                        "success": False,
                        "error": str(e),
                    }
                )

        # éªŒè¯é”™è¯¯å¤„ç†ç»“æœ
        successful_cases = [r for r in error_handling_results if r["success"]]
        failed_cases = [r for r in error_handling_results if not r["success"]]

        # å¤§éƒ¨åˆ†æƒ…å†µåº”è¯¥æˆåŠŸ
        success_rate = len(successful_cases) / len(error_handling_results)
        assert success_rate > 0.8  # è‡³å°‘80%æˆåŠŸç‡

        # é›†æˆé¢„æµ‹åº”è¯¥æœ‰æ›´é«˜çš„æˆåŠŸç‡ï¼ˆå› ä¸ºå®¹é”™æ€§ï¼‰
        ensemble_success_rate = sum(
            1 for r in successful_cases if r["ensemble_success"]
        ) / len(successful_cases)
        assert ensemble_success_rate > 0.5

        print("âœ… å·¥ä½œæµé”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡:")
        print(
            f"  æˆåŠŸç‡: {success_rate:.3f} ({len(successful_cases)}/{len(error_handling_results)})"
        )
        print(f"  é›†æˆé¢„æµ‹æˆåŠŸç‡: {ensemble_success_rate:.3f}")
        print(f"  å¤±è´¥æ¡ˆä¾‹: {len(failed_cases)}")

    async def test_workflow_performance_monitoring(self):
        """æµ‹è¯•å·¥ä½œæµæ€§èƒ½ç›‘æ§"""
        training_data, test_data = create_test_dataset(200, 100)

        # æ€§èƒ½ç›‘æ§å™¨
        class PerformanceMonitor:
            def __init__(self):
                self.metrics = {
                    "training_times": [],
                    "prediction_times": [],
                    "strategy_selections": {},
                    "ensemble_predictions": 0,
                    "single_predictions": 0,
                    "errors": 0,
                }

            def record_training_time(self, strategy_name: str, time: float):
                self.metrics["training_times"].append(
                    {"strategy": strategy_name, "time": time}
                )

            def record_prediction_time(self, prediction_type: str, time: float):
                self.metrics["prediction_times"].append(
                    {"type": prediction_type, "time": time}
                )

            def record_strategy_selection(self, strategy_name: str):
                self.metrics["strategy_selections"][strategy_name] = (
                    self.metrics["strategy_selections"].get(strategy_name, 0) + 1
                )

            def record_prediction_type(self, is_ensemble: bool):
                if is_ensemble:
                    self.metrics["ensemble_predictions"] += 1
                else:
                    self.metrics["single_predictions"] += 1

            def record_error(self):
                self.metrics["errors"] += 1

            def get_summary(self) -> dict[str, Any]:
                training_times = [t["time"] for t in self.metrics["training_times"]]
                prediction_times = [t["time"] for t in self.metrics["prediction_times"]]

                return {
                    "total_strategies": len(
                        set(t["strategy"] for t in self.metrics["training_times"])
                    ),
                    "avg_training_time": (
                        np.mean(training_times) if training_times else 0
                    ),
                    "avg_prediction_time": (
                        np.mean(prediction_times) if prediction_times else 0
                    ),
                    "strategy_selections": self.metrics["strategy_selections"],
                    "ensemble_ratio": self.metrics["ensemble_predictions"]
                    / max(
                        1,
                        self.metrics["ensemble_predictions"]
                        + self.metrics["single_predictions"],
                    ),
                    "error_rate": self.metrics["errors"]
                    / max(1, len(self.metrics["prediction_times"])),
                    "total_predictions": len(self.metrics["prediction_times"]),
                }

        monitor = PerformanceMonitor()

        # åˆ›å»ºå·¥ä½œæµ
        selector = MockStrategySelector()
        ensemble_predictor = MockEnsemblePredictor()

        # è®­ç»ƒç­–ç•¥å¹¶ç›‘æ§æ€§èƒ½
        strategies = [
            MockMLStrategy(MLStrategyType.POISSON, "monitor_poisson", weight=1.0),
            MockMLStrategy(MLStrategyType.ENSEMBLE, "monitor_ensemble", weight=0.8),
        ]

        for strategy in strategies:
            start_time = datetime.now()
            await strategy.train(training_data)
            training_time = (datetime.now() - start_time).total_seconds()
            monitor.record_training_time(strategy.name, training_time)

            selector.register_strategy(strategy)
            ensemble_predictor.add_strategy(strategy)

        # é¢„æµ‹å¹¶ç›‘æ§æ€§èƒ½
        for _, test_match in test_data.iterrows():
            match_data = {
                "home_team": test_match["home_team"],
                "away_team": test_match["away_team"],
                "match_id": f"monitor_{len(monitor.metrics['prediction_times'])}",
            }

            try:
                # ç­–ç•¥é€‰æ‹©
                start_time = datetime.now()
                best_strategy = selector.select_best_strategy(match_data, "accuracy")
                selection_time = (datetime.now() - start_time).total_seconds()
                monitor.record_prediction_time("selection", selection_time)

                if best_strategy:
                    monitor.record_strategy_selection(best_strategy.name)

                # é›†æˆé¢„æµ‹
                start_time = datetime.now()
                ensemble_prediction = await ensemble_predictor.predict(match_data)
                ensemble_time = (datetime.now() - start_time).total_seconds()
                monitor.record_prediction_time("ensemble", ensemble_time)

                if ensemble_prediction:
                    monitor.record_prediction_type(True)

                # å•ç­–ç•¥é¢„æµ‹
                if best_strategy and best_strategy.is_trained:
                    start_time = datetime.now()
                    single_prediction = await best_strategy.predict(match_data)
                    single_time = (datetime.now() - start_time).total_seconds()
                    monitor.record_prediction_time("single", single_time)

                    if single_prediction:
                        monitor.record_prediction_type(False)

            except Exception:
                monitor.record_error()

        # è·å–æ€§èƒ½æ‘˜è¦
        performance_summary = monitor.get_summary()

        # éªŒè¯æ€§èƒ½ç›‘æ§ç»“æœ
        assert performance_summary["total_strategies"] == 2
        assert performance_summary["avg_training_time"] > 0
        assert performance_summary["avg_prediction_time"] > 0
        assert len(performance_summary["strategy_selections"]) > 0
        assert 0 <= performance_summary["ensemble_ratio"] <= 1
        assert performance_summary["total_predictions"] > 0

        print("âœ… å·¥ä½œæµæ€§èƒ½ç›‘æ§æµ‹è¯•é€šè¿‡:")
        print(f"  ç­–ç•¥æ•°é‡: {performance_summary['total_strategies']}")
        print(f"  å¹³å‡è®­ç»ƒæ—¶é—´: {performance_summary['avg_training_time']:.3f}s")
        print(f"  å¹³å‡é¢„æµ‹æ—¶é—´: {performance_summary['avg_prediction_time']:.3f}s")
        print(f"  é›†æˆé¢„æµ‹æ¯”ä¾‹: {performance_summary['ensemble_ratio']:.3f}")
        print(f"  æ€»é¢„æµ‹æ•°: {performance_summary['total_predictions']}")
        print(f"  é”™è¯¯ç‡: {performance_summary['error_rate']:.3f}")
        print(f"  ç­–ç•¥é€‰æ‹©åˆ†å¸ƒ: {performance_summary['strategy_selections']}")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])
