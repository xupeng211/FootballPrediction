#!/usr/bin/env python3
"""
ğŸ¤– MLæ¨¡å‹è®­ç»ƒæµ‹è¯• - æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæµç¨‹æµ‹è¯•

æµ‹è¯•MLæ¨¡å‹çš„å®Œæ•´è®­ç»ƒæµç¨‹ï¼ŒåŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€éªŒè¯ã€
ä¿å­˜å’ŒåŠ è½½ç­‰åŠŸèƒ½ã€‚è¦†ç›–PoissonModelå’Œå…¶ä»–MLæ¨¡å‹çš„è®­ç»ƒåœºæ™¯ã€‚
"""

import asyncio
import os
import tempfile
import warnings
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import pytest

# æŠ‘åˆ¶warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
import sys

sys.path.append(os.path.join(os.path.dirname(__file__), "../../../src"))

# å°è¯•å¯¼å…¥MLæ¨¡å—
try:
    from src.ml.model_training import (ModelTrainer, ModelType, TrainingConfig,
                                       TrainingStatus)
    from src.ml.models.base_model import (BaseModel, PredictionResult,
                                          TrainingResult)
    from src.ml.models.poisson_model import PoissonModel

    CAN_IMPORT = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥MLæ¨¡å—: {e}")
    CAN_IMPORT = False


def create_training_data(num_matches: int = 500) -> pd.DataFrame:
    """åˆ›å»ºè®­ç»ƒæ•°æ®"""
    teams = [f"Team_{chr(65+i)}" for i in range(20)]  # Team_A åˆ° Team_T
    data = []

    for i in range(num_matches):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        # åŸºäºçƒé˜Ÿå¼ºåº¦ç”Ÿæˆæ¯”åˆ†
        team_strengths = {team: np.random.uniform(0.5, 2.0) for team in teams}
        home_strength = team_strengths[home_team]
        away_strength = team_strengths[away_team]

        # ä¸»åœºä¼˜åŠ¿
        home_advantage = 0.3
        home_expected = (home_strength * (1 + home_advantage)) / away_strength * 1.5
        away_expected = away_strength / home_strength * 1.1

        home_goals = np.random.poisson(max(home_expected, 0.1))
        away_goals = np.random.poisson(max(away_expected, 0.1))

        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now()
                - timedelta(days=np.random.randint(0, 365)),
                "home_team_strength": home_strength,
                "away_team_strength": away_strength,
            }
        )

    return pd.DataFrame(data)


def create_test_data(num_matches: int = 100) -> pd.DataFrame:
    """åˆ›å»ºæµ‹è¯•æ•°æ®"""
    return create_training_data(num_matches)


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestMLModelTraining:
    """MLæ¨¡å‹è®­ç»ƒæµ‹è¯•"""

    def test_poisson_model_training_workflow(self):
        """æµ‹è¯•Poissonæ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹"""
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(300)
        test_data = create_test_data(100)

        # åˆå§‹åŒ–æ¨¡å‹
        model = PoissonModel("test_poisson")

        # éªŒè¯åˆå§‹çŠ¶æ€
        assert not model.is_trained
        assert model.model is None
        assert len(model.team_attack_strength) == 0

        # è®­ç»ƒæ¨¡å‹
        training_result = model.train(training_data)

        # éªŒè¯è®­ç»ƒç»“æœ
        assert model.is_trained
        assert isinstance(training_result, TrainingResult)
        assert training_result.model_name == "PoissonModel"
        assert training_result.training_samples == len(training_data)
        assert training_result.accuracy > 0.0
        assert training_result.training_time > 0
        assert len(model.team_attack_strength) > 0
        assert len(model.team_defense_strength) > 0

        # éªŒè¯æ¨¡å‹é¢„æµ‹åŠŸèƒ½
        test_match = {
            "home_team": training_data["home_team"].iloc[0],
            "away_team": training_data["away_team"].iloc[1],
            "match_id": "test_match_001",
        }

        prediction = model.predict(test_match)
        assert isinstance(prediction, PredictionResult)
        assert prediction.home_team == test_match["home_team"]
        assert prediction.away_team == test_match["away_team"]
        assert (
            abs(
                prediction.home_win_prob
                + prediction.draw_prob
                + prediction.away_win_prob
                - 1.0
            )
            < 0.01
        )
        assert prediction.confidence > 0.0
        assert prediction.model_name == "PoissonModel"

        print(
            f"âœ… Poissonæ¨¡å‹è®­ç»ƒå®Œæˆ: å‡†ç¡®ç‡={training_result.accuracy:.3f}, è®­ç»ƒæ—¶é—´={training_result.training_time:.2f}s"
        )

    def test_model_training_with_validation_data(self):
        """æµ‹è¯•å¸¦éªŒè¯æ•°æ®çš„æ¨¡å‹è®­ç»ƒ"""
        # åˆ›å»ºæ•°æ®
        training_data = create_training_data(200)
        validation_data = create_test_data(50)

        # åˆå§‹åŒ–æ¨¡å‹
        model = PoissonModel("validation_test")

        # è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦éªŒè¯æ•°æ®ï¼‰
        training_result = model.train(training_data, validation_data)

        # éªŒè¯è®­ç»ƒç»“æœåŒ…å«éªŒè¯ä¿¡æ¯
        assert training_result.validation_samples == len(validation_data)
        assert training_result.accuracy > 0.0

        # åœ¨éªŒè¯æ•°æ®ä¸Šè¯„ä¼°
        validation_metrics = model.evaluate(validation_data)
        assert "accuracy" in validation_metrics
        assert "precision" in validation_metrics
        assert "recall" in validation_metrics
        assert "f1_score" in validation_metrics

        print(
            f"âœ… éªŒè¯è®­ç»ƒå®Œæˆ: è®­ç»ƒå‡†ç¡®ç‡={training_result.accuracy:.3f}, éªŒè¯å‡†ç¡®ç‡={validation_metrics['accuracy']:.3f}"
        )

    def test_model_cross_validation(self):
        """æµ‹è¯•æ¨¡å‹äº¤å‰éªŒè¯"""
        # åˆ›å»ºè¾ƒå°æ•°æ®é›†ç”¨äºäº¤å‰éªŒè¯
        training_data = create_training_data(100)

        # åˆå§‹åŒ–æ¨¡å‹
        model = PoissonModel("cv_test")

        # è®¾ç½®è¶…å‚æ•°
        model.update_hyperparameters(min_matches_per_team=5)

        # è®­ç»ƒæ¨¡å‹ï¼ˆä¼šä½¿ç”¨äº¤å‰éªŒè¯ï¼‰
        training_result = model.train(training_data)

        # éªŒè¯äº¤å‰éªŒè¯ç»“æœ
        assert training_result.accuracy > 0.0
        assert model.is_trained
        assert len(model.team_attack_strength) > 0

        # éªŒè¯æ¨¡å‹å¯ä»¥æ­£å¸¸é¢„æµ‹
        test_match = {
            "home_team": training_data["home_team"].iloc[0],
            "away_team": training_data["away_team"].iloc[1],
        }

        probabilities = model.predict_proba(test_match)
        assert len(probabilities) == 3
        assert all(0 <= p <= 1 for p in probabilities)
        assert abs(sum(probabilities) - 1.0) < 0.01

        print(f"âœ… äº¤å‰éªŒè¯å®Œæˆ: å‡†ç¡®ç‡={training_result.accuracy:.3f}")

    def test_model_hyperparameter_optimization(self):
        """æµ‹è¯•æ¨¡å‹è¶…å‚æ•°ä¼˜åŒ–"""
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(200)

        # æµ‹è¯•ä¸åŒçš„è¶…å‚æ•°é…ç½®
        hyperparameter_configs = [
            {"home_advantage": 0.1, "min_matches_per_team": 5},
            {"home_advantage": 0.3, "min_matches_per_team": 10},
            {"home_advantage": 0.5, "min_matches_per_team": 15},
        ]

        results = []

        for config in hyperparameter_configs:
            model = PoissonModel(f"hyperparam_test_{len(results)}")
            model.update_hyperparameters(**config)

            training_result = model.train(training_data)
            results.append(
                {
                    "config": config,
                    "accuracy": training_result.accuracy,
                    "training_time": training_result.training_time,
                }
            )

        # éªŒè¯ä¸åŒé…ç½®äº§ç”Ÿä¸åŒç»“æœ
        accuracies = [r["accuracy"] for r in results]
        assert len(set(acc for acc in accuracies)) > 1  # è‡³å°‘æœ‰ä¸¤ä¸ªä¸åŒçš„å‡†ç¡®ç‡

        # æ‰¾åˆ°æœ€ä½³é…ç½®
        best_result = max(results, key=lambda x: x["accuracy"])
        print(
            f"âœ… è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ: æœ€ä½³é…ç½®={best_result['config']}, å‡†ç¡®ç‡={best_result['accuracy']:.3f}"
        )

    def test_model_save_and_load_workflow(self):
        """æµ‹è¯•æ¨¡å‹ä¿å­˜å’ŒåŠ è½½å·¥ä½œæµ"""
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(150)

        # è®­ç»ƒæ¨¡å‹
        original_model = PoissonModel("save_test_original")
        training_result = original_model.train(training_data)

        # è·å–è®­ç»ƒåçš„é¢„æµ‹ç»“æœä½œä¸ºåŸºå‡†
        test_match = {
            "home_team": training_data["home_team"].iloc[0],
            "away_team": training_data["away_team"].iloc[1],
            "match_id": "save_load_test",
        }

        original_prediction = original_model.predict(test_match)
        original_team_strengths = original_model.team_attack_strength.copy()

        # ä¿å­˜æ¨¡å‹åˆ°ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pkl") as tmp_file:
            model_path = tmp_file.name

        try:
            # ä¿å­˜æ¨¡å‹
            save_success = original_model.save_model(model_path)
            assert save_success
            assert os.path.exists(model_path)

            # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½
            loaded_model = PoissonModel("save_test_loaded")
            load_success = loaded_model.load_model(model_path)
            assert load_success

            # éªŒè¯åŠ è½½çš„æ¨¡å‹çŠ¶æ€
            assert loaded_model.is_trained == original_model.is_trained
            assert loaded_model.model_name == original_model.model_name
            assert loaded_model.model_version == original_model.model_version
            assert (
                loaded_model.team_attack_strength == original_model.team_attack_strength
            )
            assert (
                loaded_model.team_defense_strength
                == original_model.team_defense_strength
            )

            # éªŒè¯åŠ è½½æ¨¡å‹çš„é¢„æµ‹ç»“æœä¸€è‡´æ€§
            loaded_prediction = loaded_model.predict(test_match)

            assert loaded_prediction.home_team == original_prediction.home_team
            assert loaded_prediction.away_team == original_prediction.away_team
            assert (
                abs(loaded_prediction.home_win_prob - original_prediction.home_win_prob)
                < 0.001
            )
            assert (
                abs(loaded_prediction.draw_prob - original_prediction.draw_prob) < 0.001
            )
            assert (
                abs(loaded_prediction.away_win_prob - original_prediction.away_win_prob)
                < 0.001
            )

            print("âœ… æ¨¡å‹ä¿å­˜åŠ è½½æµ‹è¯•é€šè¿‡: é¢„æµ‹ç»“æœä¸€è‡´")

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(model_path):
                os.unlink(model_path)

    def test_model_training_with_different_data_sizes(self):
        """æµ‹è¯•ä¸åŒæ•°æ®å¤§å°çš„æ¨¡å‹è®­ç»ƒ"""
        data_sizes = [50, 100, 200, 500]
        training_results = []

        for size in data_sizes:
            training_data = create_training_data(size)
            model = PoissonModel(f"size_test_{size}")

            start_time = datetime.now()
            training_result = model.train(training_data)
            end_time = datetime.now()

            actual_training_time = (end_time - start_time).total_seconds()

            training_results.append(
                {
                    "data_size": size,
                    "training_time": training_result.training_time,
                    "actual_time": actual_training_time,
                    "accuracy": training_result.accuracy,
                    "team_count": len(model.team_attack_strength),
                }
            )

            # éªŒè¯åŸºæœ¬è®­ç»ƒç»“æœ
            assert model.is_trained
            assert training_result.training_samples == size
            assert training_result.accuracy > 0.0

        # åˆ†æè®­ç»ƒæ—¶é—´éšæ•°æ®å¤§å°çš„å˜åŒ–
        print("âœ… ä¸åŒæ•°æ®å¤§å°è®­ç»ƒæµ‹è¯•å®Œæˆ:")
        for result in training_results:
            print(
                f"  æ•°æ®é‡={result['data_size']}, è®­ç»ƒæ—¶é—´={result['training_time']:.2f}s, "
                f"å®é™…æ—¶é—´={result['actual_time']:.2f}s, å‡†ç¡®ç‡={result['accuracy']:.3f}"
            )

    def test_model_training_error_handling(self):
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒé”™è¯¯å¤„ç†"""
        model = PoissonModel("error_test")

        # æµ‹è¯•ç©ºæ•°æ®
        empty_data = pd.DataFrame()
        with pytest.raises(ValueError, match="Invalid training data"):
            model.train(empty_data)

        # æµ‹è¯•ç¼ºå°‘å¿…è¦åˆ—çš„æ•°æ®
        invalid_data = pd.DataFrame(
            {
                "home_team": ["Team_A", "Team_B"],
                "away_team": ["Team_C", "Team_D"],
                # ç¼ºå°‘ score å’Œ result åˆ—
            }
        )
        with pytest.raises(ValueError, match="Invalid training data"):
            model.train(invalid_data)

        # æµ‹è¯•æ•°æ®é‡ä¸è¶³
        small_data = pd.DataFrame(
            {
                "home_team": ["Team_A"],
                "away_team": ["Team_B"],
                "home_score": [1],
                "away_score": [0],
                "result": ["home_win"],
            }
        )
        # åº”è¯¥èƒ½è®­ç»ƒä½†ä¼šæœ‰è­¦å‘Š
        training_result = model.train(small_data)
        assert training_result.training_samples == 1

        print("âœ… é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡")

    def test_model_training_progress_tracking(self):
        """æµ‹è¯•æ¨¡å‹è®­ç»ƒè¿›åº¦è·Ÿè¸ª"""
        training_data = create_training_data(100)
        model = PoissonModel("progress_test")

        # éªŒè¯åˆå§‹çŠ¶æ€
        assert len(model.training_history) == 0
        assert model.last_training_time is None

        # è®­ç»ƒæ¨¡å‹
        training_result = model.train(training_data)

        # éªŒè¯è®­ç»ƒçŠ¶æ€æ›´æ–°
        assert model.is_trained
        assert model.last_training_time is not None
        assert isinstance(model.last_training_time, datetime)

        # éªŒè¯æ¨¡å‹ä¿¡æ¯
        model_info = model.get_model_info()
        assert model_info["model_name"] == "PoissonModel"
        assert model_info["is_trained"] is True
        assert (
            model_info["feature_count"] == 4
        )  # home_attack, home_defense, away_attack, away_defense
        assert "hyperparameters" in model_info

        print("âœ… è®­ç»ƒè¿›åº¦è·Ÿè¸ªæµ‹è¯•é€šè¿‡")


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.asyncio
class TestAsyncModelTraining:
    """å¼‚æ­¥æ¨¡å‹è®­ç»ƒæµ‹è¯•"""

    async def test_async_model_trainer_workflow(self):
        """æµ‹è¯•å¼‚æ­¥æ¨¡å‹è®­ç»ƒå™¨å·¥ä½œæµ"""
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(200)
        target_column = "result"
        feature_columns = ["home_team", "away_team", "home_score", "away_score"]

        # åˆ›å»ºè®­ç»ƒå™¨
        config = TrainingConfig()
        config.model_type = ModelType.RANDOM_FOREST
        config.epochs = 5  # å‡å°‘epochä»¥åŠ å¿«æµ‹è¯•

        trainer = ModelTrainer(config)

        # å‡†å¤‡æ•°æ®
        X_train, X_test, y_train, y_test = await trainer.prepare_data(
            training_data, target_column, feature_columns
        )

        assert len(X_train) > 0
        assert len(X_test) > 0
        assert len(y_train) == len(X_train)
        assert len(y_test) == len(X_test)

        # è®­ç»ƒæ¨¡å‹
        training_result = await trainer.train(X_train, y_train, X_test, y_test)

        assert training_result["status"] == "completed"
        assert "model_name" in training_result
        assert "training_time" in training_result
        assert "metrics" in training_result

        # è¯„ä¼°æ¨¡å‹
        evaluation_metrics = await trainer.evaluate(X_test, y_test)

        assert "accuracy" in evaluation_metrics
        assert "precision" in evaluation_metrics
        assert "recall" in evaluation_metrics
        assert "f1_score" in evaluation_metrics

        # è·å–è®­ç»ƒæ‘˜è¦
        summary = trainer.get_training_summary()
        assert summary["status"] == "completed"
        assert summary["training_epochs"] > 0
        assert summary["model_name"] is not None

        print(f"âœ… å¼‚æ­¥è®­ç»ƒå™¨æµ‹è¯•å®Œæˆ: å‡†ç¡®ç‡={evaluation_metrics['accuracy']:.3f}")

    async def test_async_model_save_load(self):
        """æµ‹è¯•å¼‚æ­¥æ¨¡å‹ä¿å­˜å’ŒåŠ è½½"""
        # åˆ›å»ºæ•°æ®
        training_data = create_training_data(100)
        target_column = "result"
        feature_columns = ["home_team", "away_team", "home_score", "away_score"]

        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ModelTrainer()
        X_train, X_test, y_train, y_test = await trainer.prepare_data(
            training_data, target_column, feature_columns
        )

        # è®­ç»ƒæ¨¡å‹
        await trainer.train(X_train, y_train)

        # ä¿å­˜æ¨¡å‹
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pkl") as tmp_file:
            model_path = tmp_file.name

        try:
            save_success = await trainer.save_model(model_path)
            assert save_success
            assert os.path.exists(model_path)

            # åˆ›å»ºæ–°çš„è®­ç»ƒå™¨å¹¶åŠ è½½æ¨¡å‹
            new_trainer = ModelTrainer()
            load_success = await new_trainer.load_model(model_path)
            assert load_success
            assert new_trainer.model is not None

            print("âœ… å¼‚æ­¥æ¨¡å‹ä¿å­˜åŠ è½½æµ‹è¯•é€šè¿‡")

        finally:
            if os.path.exists(model_path):
                os.unlink(model_path)

    async def test_concurrent_model_training(self):
        """æµ‹è¯•å¹¶å‘æ¨¡å‹è®­ç»ƒ"""
        # åˆ›å»ºæ•°æ®
        training_data = create_training_data(150)
        target_column = "result"
        feature_columns = ["home_team", "away_team", "home_score", "away_score"]

        async def train_model(model_id: int):
            config = TrainingConfig()
            config.model_type = ModelType.RANDOM_FOREST
            config.epochs = 3  # å‡å°‘epochä»¥åŠ å¿«æµ‹è¯•

            trainer = ModelTrainer(config)
            X_train, X_test, y_train, y_test = await trainer.prepare_data(
                training_data, target_column, feature_columns
            )

            result = await trainer.train(X_train, y_train, X_test, y_test)
            return model_id, result

        # å¹¶å‘è®­ç»ƒå¤šä¸ªæ¨¡å‹
        tasks = [train_model(i) for i in range(3)]
        results = await asyncio.gather(*tasks)

        # éªŒè¯ç»“æœ
        assert len(results) == 3
        for model_id, result in results:
            assert result["status"] == "completed"
            assert "model_name" in result

        print(f"âœ… å¹¶å‘æ¨¡å‹è®­ç»ƒæµ‹è¯•é€šè¿‡: è®­ç»ƒäº†{len(results)}ä¸ªæ¨¡å‹")


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestMLModelIntegration:
    """MLæ¨¡å‹é›†æˆæµ‹è¯•"""

    def test_end_to_end_prediction_pipeline(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯é¢„æµ‹æµæ°´çº¿"""
        # 1. åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(300)
        test_matches = [
            {
                "home_team": training_data["home_team"].iloc[0],
                "away_team": training_data["away_team"].iloc[1],
                "match_id": "pipeline_test_001",
            },
            {
                "home_team": training_data["home_team"].iloc[2],
                "away_team": training_data["away_team"].iloc[3],
                "match_id": "pipeline_test_002",
            },
        ]

        # 2. è®­ç»ƒæ¨¡å‹
        model = PoissonModel("pipeline_test")
        training_result = model.train(training_data)

        # 3. æ‰¹é‡é¢„æµ‹
        predictions = []
        for match in test_matches:
            prediction = model.predict(match)
            predictions.append(prediction)

        # 4. éªŒè¯é¢„æµ‹ç»“æœ
        assert len(predictions) == len(test_matches)
        for i, prediction in enumerate(predictions):
            assert prediction.match_id == test_matches[i]["match_id"]
            assert (
                abs(
                    prediction.home_win_prob
                    + prediction.draw_prob
                    + prediction.away_win_prob
                    - 1.0
                )
                < 0.01
            )
            assert prediction.confidence > 0.0

        # 5. è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„è¡¨ç°
        test_data = create_test_data(50)
        evaluation_metrics = model.evaluate(test_data)

        assert evaluation_metrics["accuracy"] > 0.0
        assert evaluation_metrics["f1_score"] > 0.0

        print(
            f"âœ… ç«¯åˆ°ç«¯æµæ°´çº¿æµ‹è¯•å®Œæˆ: è®­ç»ƒå‡†ç¡®ç‡={training_result.accuracy:.3f}, "
            f"æµ‹è¯•å‡†ç¡®ç‡={evaluation_metrics['accuracy']:.3f}"
        )

    def test_model_ensemble_prediction(self):
        """æµ‹è¯•æ¨¡å‹é›†æˆé¢„æµ‹"""
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(200)

        # è®­ç»ƒå¤šä¸ªå…·æœ‰ä¸åŒè¶…å‚æ•°çš„æ¨¡å‹
        models = []
        hyperparams = [
            {"home_advantage": 0.1},
            {"home_advantage": 0.3},
            {"home_advantage": 0.5},
        ]

        for i, params in enumerate(hyperparams):
            model = PoissonModel(f"ensemble_model_{i}")
            model.update_hyperparameters(**params)
            model.train(training_data)
            models.append(model)

        # æµ‹è¯•æ¯”èµ›
        test_match = {
            "home_team": training_data["home_team"].iloc[0],
            "away_team": training_data["away_team"].iloc[1],
            "match_id": "ensemble_test",
        }

        # é›†æˆé¢„æµ‹ï¼ˆç®€å•å¹³å‡ï¼‰
        ensemble_probs = [0.0, 0.0, 0.0]  # [home_win, draw, away_win]
        individual_predictions = []

        for model in models:
            prediction = model.predict(test_match)
            individual_predictions.append(prediction)
            ensemble_probs[0] += prediction.home_win_prob
            ensemble_probs[1] += prediction.draw_prob
            ensemble_probs[2] += prediction.away_win_prob

        # å¹³å‡æ¦‚ç‡
        ensemble_probs = [p / len(models) for p in ensemble_probs]

        # éªŒè¯é›†æˆé¢„æµ‹
        assert abs(sum(ensemble_probs) - 1.0) < 0.01
        assert all(0 <= p <= 1 for p in ensemble_probs)

        # è®¡ç®—é›†æˆç½®ä¿¡åº¦
        max_prob = max(ensemble_probs)
        ensemble_confidence = max_prob

        print(
            f"âœ… é›†æˆé¢„æµ‹æµ‹è¯•å®Œæˆ: é›†æˆç½®ä¿¡åº¦={ensemble_confidence:.3f}, "
            f"æ¨¡å‹æ•°é‡={len(models)}"
        )

    def test_model_performance_comparison(self):
        """æµ‹è¯•æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ"""
        # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        training_data = create_training_data(300)
        test_data = create_test_data(100)

        # ä¸åŒé…ç½®çš„æ¨¡å‹
        model_configs = [
            {"name": "conservative", "home_advantage": 0.1, "min_matches_per_team": 15},
            {"name": "balanced", "home_advantage": 0.3, "min_matches_per_team": 10},
            {"name": "aggressive", "home_advantage": 0.5, "min_matches_per_team": 5},
        ]

        results = []

        for config in model_configs:
            model = PoissonModel(f"comparison_{config['name']}")
            model.update_hyperparameters(
                home_advantage=config["home_advantage"],
                min_matches_per_team=config["min_matches_per_team"],
            )

            # è®­ç»ƒ
            training_result = model.train(training_data)

            # è¯„ä¼°
            evaluation_metrics = model.evaluate(test_data)

            results.append(
                {
                    "config": config["name"],
                    "training_accuracy": training_result.accuracy,
                    "test_accuracy": evaluation_metrics["accuracy"],
                    "precision": evaluation_metrics["precision"],
                    "recall": evaluation_metrics["recall"],
                    "f1_score": evaluation_metrics["f1_score"],
                    "training_time": training_result.training_time,
                }
            )

        # éªŒè¯ç»“æœå·®å¼‚
        test_accuracies = [r["test_accuracy"] for r in results]
        assert len(set(acc for acc in test_accuracies)) > 1  # è‡³å°‘æœ‰ä¸¤ä¸ªä¸åŒçš„å‡†ç¡®ç‡

        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        best_model = max(results, key=lambda x: x["test_accuracy"])

        print("âœ… æ¨¡å‹æ¯”è¾ƒæµ‹è¯•å®Œæˆ:")
        for result in results:
            print(
                f"  {result['config']}: æµ‹è¯•å‡†ç¡®ç‡={result['test_accuracy']:.3f}, "
                f"F1åˆ†æ•°={result['f1_score']:.3f}"
            )
        print(f"æœ€ä½³æ¨¡å‹: {best_model['config']}")

    def test_model_robustness_testing(self):
        """æµ‹è¯•æ¨¡å‹é²æ£’æ€§"""
        # åˆ›å»ºè®­ç»ƒæ•°æ®
        training_data = create_training_data(200)
        model = PoissonModel("robustness_test")
        model.train(training_data)

        # æµ‹è¯•è¾¹ç•Œæƒ…å†µ
        edge_cases = [
            # å¼ºé˜Ÿ vs å¼±é˜Ÿ
            {
                "home_team": training_data["home_team"].iloc[0],
                "away_team": training_data["away_team"].iloc[-1],
                "match_id": "strong_vs_weak",
            },
            # ç›¸åŒé˜Ÿä¼ï¼ˆåº”è¯¥å¤±è´¥ï¼‰
            {"home_team": "Team_A", "away_team": "Team_A", "match_id": "same_team"},
            # æœªçŸ¥é˜Ÿä¼
            {
                "home_team": "Unknown_Team_X",
                "away_team": "Unknown_Team_Y",
                "match_id": "unknown_teams",
            },
        ]

        results = []

        for case in edge_cases:
            try:
                prediction = model.predict(case)
                results.append(
                    {
                        "case": case["match_id"],
                        "success": True,
                        "prediction": prediction,
                    }
                )
            except Exception as e:
                results.append(
                    {"case": case["match_id"], "success": False, "error": str(e)}
                )

        # éªŒè¯ç»“æœ
        successful_predictions = [r for r in results if r["success"]]
        failed_predictions = [r for r in results if not r["success"]]

        # è‡³å°‘åº”è¯¥æœ‰ä¸€äº›æˆåŠŸçš„é¢„æµ‹
        assert len(successful_predictions) > 0

        # ç›¸åŒé˜Ÿä¼åº”è¯¥å¤±è´¥
        same_team_result = next(r for r in results if r["case"] == "same_team")
        assert not same_team_result["success"]

        print(
            f"âœ… é²æ£’æ€§æµ‹è¯•å®Œæˆ: æˆåŠŸé¢„æµ‹={len(successful_predictions)}, "
            f"å¤±è´¥é¢„æµ‹={len(failed_predictions)}"
        )


# æµ‹è¯•å·¥å…·å‡½æ•°
def create_mock_training_data_with_noise(
    num_matches: int = 200, noise_level: float = 0.1
) -> pd.DataFrame:
    """åˆ›å»ºå¸¦å™ªå£°çš„æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®"""
    data = create_training_data(num_matches)

    # æ·»åŠ å™ªå£°
    if noise_level > 0:
        for col in ["home_score", "away_score"]:
            noise = np.random.normal(0, noise_level, len(data))
            data[col] = np.clip(data[col] + noise, 0, None).astype(int)

    return data


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
class TestMLModelPerformanceMetrics:
    """MLæ¨¡å‹æ€§èƒ½æŒ‡æ ‡æµ‹è¯•"""

    def test_comprehensive_model_evaluation(self):
        """æµ‹è¯•å…¨é¢æ¨¡å‹è¯„ä¼°"""
        # åˆ›å»ºé«˜è´¨é‡è®­ç»ƒæ•°æ®
        training_data = create_mock_training_data_with_noise(300, noise_level=0.05)
        test_data = create_mock_training_data_with_noise(100, noise_level=0.05)

        # è®­ç»ƒæ¨¡å‹
        model = PoissonModel("evaluation_test")
        training_result = model.train(training_data)

        # å…¨é¢è¯„ä¼°
        evaluation_metrics = model.evaluate(test_data)

        # éªŒè¯æ‰€æœ‰å¿…è¦æŒ‡æ ‡
        required_metrics = [
            "accuracy",
            "precision",
            "recall",
            "f1_score",
            "confusion_matrix",
        ]
        for metric in required_metrics:
            assert metric in evaluation_metrics
            assert evaluation_metrics[metric] is not None

        # éªŒè¯æŒ‡æ ‡èŒƒå›´
        assert 0 <= evaluation_metrics["accuracy"] <= 1
        assert 0 <= evaluation_metrics["precision"] <= 1
        assert 0 <= evaluation_metrics["recall"] <= 1
        assert 0 <= evaluation_metrics["f1_score"] <= 1

        # éªŒè¯æ··æ·†çŸ©é˜µ
        cm = evaluation_metrics["confusion_matrix"]
        assert isinstance(cm, list)
        assert len(cm) > 0

        print(
            f"âœ… å…¨é¢æ¨¡å‹è¯„ä¼°å®Œæˆ: å‡†ç¡®ç‡={evaluation_metrics['accuracy']:.3f}, "
            f"F1åˆ†æ•°={evaluation_metrics['f1_score']:.3f}"
        )

    def test_training_stability_analysis(self):
        """æµ‹è¯•è®­ç»ƒç¨³å®šæ€§åˆ†æ"""
        # åˆ›å»ºå¤šä¸ªç›¸ä¼¼çš„è®­ç»ƒæ•°æ®é›†
        base_data = create_training_data(200)
        stability_results = []

        for i in range(5):
            # è½»å¾®æ‰°åŠ¨æ•°æ®
            perturbed_data = base_data.copy()
            perturbed_data["home_score"] = np.clip(
                perturbed_data["home_score"]
                + np.random.choice([-1, 0, 1], len(perturbed_data)),
                0,
                None,
            )
            perturbed_data["away_score"] = np.clip(
                perturbed_data["away_score"]
                + np.random.choice([-1, 0, 1], len(perturbed_data)),
                0,
                None,
            )

            # é‡æ–°è®¡ç®—ç»“æœ
            for idx, row in perturbed_data.iterrows():
                if row["home_score"] > row["away_score"]:
                    perturbed_data.loc[idx, "result"] = "home_win"
                elif row["home_score"] < row["away_score"]:
                    perturbed_data.loc[idx, "result"] = "away_win"
                else:
                    perturbed_data.loc[idx, "result"] = "draw"

            # è®­ç»ƒæ¨¡å‹
            model = PoissonModel(f"stability_test_{i}")
            training_result = model.train(perturbed_data)
            stability_results.append(training_result.accuracy)

        # åˆ†æç¨³å®šæ€§
        mean_accuracy = np.mean(stability_results)
        std_accuracy = np.std(stability_results)
        min_accuracy = np.min(stability_results)
        max_accuracy = np.max(stability_results)

        # éªŒè¯ç¨³å®šæ€§ï¼ˆæ ‡å‡†å·®ä¸åº”å¤ªå¤§ï¼‰
        assert std_accuracy < 0.2  # æ ‡å‡†å·®åº”è¯¥å°äº0.2

        print("âœ… è®­ç»ƒç¨³å®šæ€§åˆ†æå®Œæˆ:")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {mean_accuracy:.3f} Â± {std_accuracy:.3f}")
        print(f"  å‡†ç¡®ç‡èŒƒå›´: [{min_accuracy:.3f}, {max_accuracy:.3f}]")
        print(f"  å˜å¼‚ç³»æ•°: {std_accuracy/mean_accuracy:.3f}")


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    pytest.main([__file__, "-v", "--tb=short"])
