#!/usr/bin/env python3
"""
ğŸš€ MLæ€§èƒ½æµ‹è¯• - æœºå™¨å­¦ä¹ æ¨¡å‹æ€§èƒ½æµ‹è¯•

æµ‹è¯•æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ã€ä¼˜åŒ–å’Œæ‰©å±•æ€§
åŒ…æ‹¬è®­ç»ƒé€Ÿåº¦ã€é¢„æµ‹é€Ÿåº¦ã€å†…å­˜ä½¿ç”¨ã€å¹¶å‘æ€§èƒ½ç­‰
"""

import asyncio
import os

# æ¨¡æ‹Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–é—®é¢˜
import sys
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta

import numpy as np
import pandas as pd
import psutil
import pytest

sys.path.append(os.path.join(os.path.dirname(__file__), "../../../src"))

# å°è¯•å¯¼å…¥MLæ¨¡å—
try:
    from src.ml.model_training import (
        ModelTrainer,
        ModelType,
        TrainingConfig,
        TrainingStatus,
    )
    from src.ml.models.base_model import BaseModel, PredictionResult, TrainingResult
    from src.ml.models.poisson_model import PoissonModel

    CAN_IMPORT = True
except ImportError as e:
    print(f"Warning: æ— æ³•å¯¼å…¥MLæ¨¡å—: {e}")
    CAN_IMPORT = False


# æ€§èƒ½æµ‹è¯•å·¥å…·ç±»
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self):
        self.start_time = None
        self.end_time = None
        self.memory_start = None
        self.memory_end = None

    def start(self):
        """å¼€å§‹ç›‘æ§"""
        self.start_time = time.time()
        process = psutil.Process(os.getpid())
        self.memory_start = process.memory_info().rss / 1024 / 1024  # MB

    def stop(self):
        """åœæ­¢ç›‘æ§"""
        self.end_time = time.time()
        process = psutil.Process(os.getpid())
        self.memory_end = process.memory_info().rss / 1024 / 1024  # MB

    def get_execution_time(self) -> float:
        """è·å–æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰"""
        if self.start_time and self.end_time:
            return self.end_time - self.start_time
        return 0

    def get_memory_usage(self) -> dict[str, float]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        return {
            "start_mb": self.memory_start or 0,
            "end_mb": self.memory_end or 0,
            "delta_mb": (self.memory_end or 0) - (self.memory_start or 0),
        }


def create_performance_test_data(num_samples: int = 1000) -> pd.DataFrame:
    """åˆ›å»ºæ€§èƒ½æµ‹è¯•æ•°æ®"""
    teams = [f"Team_{chr(65+i)}" for i in range(20)]  # Team_A åˆ° Team_T
    data = []

    for i in range(num_samples):
        home_team = np.random.choice(teams)
        away_team = np.random.choice([t for t in teams if t != home_team])

        # ä½¿ç”¨æ›´å¤æ‚çš„æ•°æ®ç”Ÿæˆä»¥å¢åŠ è®¡ç®—é‡
        team_strength = np.random.uniform(0.5, 2.5, len(teams))
        home_strength = team_strength[teams.index(home_team)]
        away_strength = team_strength[teams.index(away_team)]

        # è€ƒè™‘æ›´å¤šå› ç´ çš„è¿›çƒæ¨¡æ‹Ÿ
        home_advantage = 0.3
        weather_factor = np.random.uniform(0.8, 1.2)
        fatigue_factor = np.random.uniform(0.9, 1.1)

        home_expected = (
            (home_strength * home_advantage * weather_factor) / away_strength * 1.5
        )
        away_expected = (
            away_strength / (home_strength * home_advantage) * 1.1 * fatigue_factor
        )

        home_goals = np.random.poisson(max(home_expected, 0.1))
        away_goals = np.random.poisson(max(away_expected, 0.1))

        if home_goals > away_goals:
            result = "home_win"
        elif home_goals < away_goals:
            result = "away_win"
        else:
            result = "draw"

        data.append(
            {
                "home_team": home_team,
                "away_team": away_team,
                "home_score": home_goals,
                "away_score": away_goals,
                "result": result,
                "match_date": datetime.now()
                - timedelta(days=np.random.randint(0, 365)),
                "home_team_strength": home_strength,
                "away_team_strength": away_strength,
                "weather_factor": weather_factor,
                "fatigue_factor": fatigue_factor,
            }
        )

    return pd.DataFrame(data)


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.performance
class TestMLModelPerformance:
    """MLæ¨¡å‹æ€§èƒ½æµ‹è¯•"""

    def test_training_performance_small_dataset(self):
        """æµ‹è¯•å°æ•°æ®é›†è®­ç»ƒæ€§èƒ½"""
        model = PoissonModel()
        training_data = create_performance_test_data(100)

        monitor = PerformanceMonitor()
        monitor.start()

        result = model.train(training_data)

        monitor.stop()

        # æ€§èƒ½æ–­è¨€
        execution_time = monitor.get_execution_time()
        memory_usage = monitor.get_memory_usage()

        assert execution_time < 5.0  # å°æ•°æ®é›†è®­ç»ƒåº”åœ¨5ç§’å†…å®Œæˆ
        assert result.accuracy > 0.3  # åŸºæœ¬æ€§èƒ½è¦æ±‚
        assert model.is_trained is True

        print(
            f"å°æ•°æ®é›†è®­ç»ƒæ€§èƒ½: {execution_time:.3f}s, å†…å­˜å˜åŒ–: {memory_usage['delta_mb']:.2f}MB"
        )

    def test_training_performance_medium_dataset(self):
        """æµ‹è¯•ä¸­ç­‰æ•°æ®é›†è®­ç»ƒæ€§èƒ½"""
        model = PoissonModel()
        training_data = create_performance_test_data(1000)

        monitor = PerformanceMonitor()
        monitor.start()

        result = model.train(training_data)

        monitor.stop()

        # æ€§èƒ½æ–­è¨€
        execution_time = monitor.get_execution_time()
        memory_usage = monitor.get_memory_usage()

        assert execution_time < 15.0  # ä¸­ç­‰æ•°æ®é›†è®­ç»ƒåº”åœ¨15ç§’å†…å®Œæˆ
        assert result.accuracy > 0.3
        assert len(model.team_attack_strength) > 0

        print(
            f"ä¸­ç­‰æ•°æ®é›†è®­ç»ƒæ€§èƒ½: {execution_time:.3f}s, å†…å­˜å˜åŒ–: {memory_usage['delta_mb']:.2f}MB"
        )

    def test_prediction_performance_single(self):
        """æµ‹è¯•å•ä¸ªé¢„æµ‹æ€§èƒ½"""
        model = PoissonModel()
        training_data = create_performance_test_data(200)
        model.train(training_data)

        test_data = {
            "home_team": "Team_A",
            "away_team": "Team_B",
            "match_id": "perf_test_001",
        }

        monitor = PerformanceMonitor()
        monitor.start()

        result = model.predict(test_data)

        monitor.stop()

        # æ€§èƒ½æ–­è¨€
        execution_time = monitor.get_execution_time()
        memory_usage = monitor.get_memory_usage()

        assert execution_time < 0.1  # å•ä¸ªé¢„æµ‹åº”åœ¨100mså†…å®Œæˆ
        assert isinstance(result, PredictionResult)
        assert (
            abs(result.home_win_prob + result.draw_prob + result.away_win_prob - 1.0)
            < 0.01
        )

        print(
            f"å•ä¸ªé¢„æµ‹æ€§èƒ½: {execution_time:.6f}s, å†…å­˜å˜åŒ–: {memory_usage['delta_mb']:.2f}MB"
        )

    def test_prediction_performance_batch(self):
        """æµ‹è¯•æ‰¹é‡é¢„æµ‹æ€§èƒ½"""
        model = PoissonModel()
        training_data = create_performance_test_data(200)
        model.train(training_data)

        # åˆ›å»ºæ‰¹é‡é¢„æµ‹æ•°æ®
        batch_size = 100
        batch_data = []
        for i in range(batch_size):
            batch_data.append(
                {
                    "home_team": f"Team_{chr(65 + i % 20)}",
                    "away_team": f"Team_{chr(65 + (i + 1) % 20)}",
                    "match_id": f"batch_test_{i:03d}",
                }
            )

        monitor = PerformanceMonitor()
        monitor.start()

        predictions = []
        for data in batch_data:
            result = model.predict(data)
            predictions.append(result)

        monitor.stop()

        # æ€§èƒ½æ–­è¨€
        execution_time = monitor.get_execution_time()
        monitor.get_memory_usage()
        avg_time_per_prediction = execution_time / batch_size

        assert execution_time < 5.0  # æ‰¹é‡é¢„æµ‹åº”åœ¨5ç§’å†…å®Œæˆ
        assert avg_time_per_prediction < 0.05  # å¹³å‡æ¯ä¸ªé¢„æµ‹åœ¨50mså†…
        assert len(predictions) == batch_size

        print(
            f"æ‰¹é‡é¢„æµ‹æ€§èƒ½: {execution_time:.3f}s, å¹³å‡æ¯ä¸ª: {avg_time_per_prediction:.6f}s"
        )

    def test_model_memory_usage(self):
        """æµ‹è¯•æ¨¡å‹å†…å­˜ä½¿ç”¨"""
        # è·å–åˆå§‹å†…å­˜ä½¿ç”¨
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # åˆ›å»ºå¹¶è®­ç»ƒå¤šä¸ªæ¨¡å‹
        models = []
        training_data = create_performance_test_data(300)

        for i in range(5):
            model = PoissonModel(f"memory_test_v{i}")
            model.train(training_data)
            models.append(model)

        # è·å–è®­ç»ƒåå†…å­˜ä½¿ç”¨
        after_training_memory = process.memory_info().rss / 1024 / 1024
        memory_increase = after_training_memory - initial_memory

        # è¿›è¡Œé¢„æµ‹ä»¥æµ‹è¯•æ¨ç†å†…å­˜ä½¿ç”¨
        test_data = {"home_team": "Team_A", "away_team": "Team_B"}
        for model in models:
            model.predict(test_data)

        # è·å–é¢„æµ‹åå†…å­˜ä½¿ç”¨
        after_prediction_memory = process.memory_info().rss / 1024 / 1024

        # å†…å­˜ä½¿ç”¨æ–­è¨€
        assert memory_increase < 100  # 5ä¸ªæ¨¡å‹å†…å­˜å¢é•¿åº”å°äº100MB
        assert (
            after_prediction_memory - after_training_memory < 10
        )  # é¢„æµ‹å†…å­˜å¢é•¿åº”å°äº10MB

        print(
            f"å†…å­˜ä½¿ç”¨: åˆå§‹={initial_memory:.2f}MB, è®­ç»ƒå={after_training_memory:.2f}MB, "
            f"é¢„æµ‹å={after_prediction_memory:.2f}MB, å¢é•¿={memory_increase:.2f}MB"
        )

    def test_concurrent_prediction_performance(self):
        """æµ‹è¯•å¹¶å‘é¢„æµ‹æ€§èƒ½"""
        model = PoissonModel()
        training_data = create_performance_test_data(200)
        model.train(training_data)

        def predict_task(task_id):
            """å•ä¸ªé¢„æµ‹ä»»åŠ¡"""
            test_data = {
                "home_team": f"Team_{chr(65 + task_id % 20)}",
                "away_team": f"Team_{chr(65 + (task_id + 1) % 20)}",
                "match_id": f"concurrent_test_{task_id:03d}",
            }
            start_time = time.time()
            result = model.predict(test_data)
            end_time = time.time()
            return {
                "task_id": task_id,
                "execution_time": end_time - start_time,
                "result": result,
            }

        # å¹¶å‘æµ‹è¯•
        num_tasks = 50
        max_workers = 4

        monitor = PerformanceMonitor()
        monitor.start()

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(predict_task, i) for i in range(num_tasks)]
            results = [future.result() for future in as_completed(futures)]

        monitor.stop()

        # åˆ†æç»“æœ
        execution_times = [r["execution_time"] for r in results]
        successful_tasks = len([r for r in results if r["result"] is not None])

        total_execution_time = monitor.get_execution_time()
        avg_task_time = np.mean(execution_times)
        max_task_time = np.max(execution_times)

        # æ€§èƒ½æ–­è¨€
        assert successful_tasks == num_tasks
        assert total_execution_time < 10.0  # æ€»æ‰§è¡Œæ—¶é—´åº”åœ¨10ç§’å†…
        assert avg_task_time < 0.5  # å¹³å‡ä»»åŠ¡æ—¶é—´åº”åœ¨500mså†…

        print(
            f"å¹¶å‘é¢„æµ‹æ€§èƒ½: ä»»åŠ¡æ•°={num_tasks}, å¹¶å‘æ•°={max_workers}, "
            f"æ€»æ—¶é—´={total_execution_time:.3f}s, å¹³å‡ä»»åŠ¡æ—¶é—´={avg_task_time:.6f}s, "
            f"æœ€å¤§ä»»åŠ¡æ—¶é—´={max_task_time:.6f}s"
        )

    def test_model_scaling_performance(self):
        """æµ‹è¯•æ¨¡å‹æ‰©å±•æ€§èƒ½"""
        data_sizes = [100, 500, 1000, 2000]
        performance_results = []

        for size in data_sizes:
            model = PoissonModel(f"scale_test_{size}")
            training_data = create_performance_test_data(size)

            monitor = PerformanceMonitor()
            monitor.start()

            result = model.train(training_data)

            monitor.stop()

            performance_results.append(
                {
                    "data_size": size,
                    "training_time": monitor.get_execution_time(),
                    "memory_usage": monitor.get_memory_usage()["delta_mb"],
                    "accuracy": result.accuracy,
                }
            )

        # åˆ†ææ‰©å±•æ€§
        training_times = [r["training_time"] for r in performance_results]
        [r["memory_usage"] for r in performance_results]

        # è®¡ç®—æ—¶é—´å¤æ‚åº¦ï¼ˆåº”è¯¥æ˜¯è¿‘ä¼¼çº¿æ€§çš„ï¼‰
        size_ratio = data_sizes[-1] / data_sizes[0]
        time_ratio = training_times[-1] / training_times[0]

        # æ‰©å±•æ€§æ–­è¨€
        assert time_ratio < size_ratio * 1.5  # æ—¶é—´å¢é•¿ä¸åº”è¶…è¿‡æ•°æ®å¢é•¿çš„1.5å€
        assert all(t < 30 for t in training_times)  # æ‰€æœ‰è®­ç»ƒæ—¶é—´éƒ½åº”åœ¨30ç§’å†…

        print("æ‰©å±•æ€§æµ‹è¯•ç»“æœ:")
        for result in performance_results:
            print(
                f"  æ•°æ®é‡={result['data_size']}, è®­ç»ƒæ—¶é—´={result['training_time']:.3f}s, "
                f"å†…å­˜å¢é•¿={result['memory_usage']:.2f}MB, å‡†ç¡®ç‡={result['accuracy']:.3f}"
            )

        print(f"æ•°æ®å¢é•¿{size_ratio:.1f}å€ï¼Œæ—¶é—´å¢é•¿{time_ratio:.1f}å€")


@pytest.mark.skipif(not CAN_IMPORT, reason="MLæ¨¡å—å¯¼å…¥å¤±è´¥")
@pytest.mark.unit
@pytest.mark.ml
@pytest.mark.performance
class TestMLModelOptimization:
    """MLæ¨¡å‹ä¼˜åŒ–æµ‹è¯•"""

    def test_hyperparameter_optimization_impact(self):
        """æµ‹è¯•è¶…å‚æ•°ä¼˜åŒ–çš„å½±å“"""
        base_training_data = create_performance_test_data(500)

        # ä¸åŒçš„è¶…å‚æ•°é…ç½®
        hyperparameter_configs = [
            {"home_advantage": 0.1, "min_matches_per_team": 5},
            {"home_advantage": 0.3, "min_matches_per_team": 10},
            {"home_advantage": 0.5, "min_matches_per_team": 15},
            {"home_advantage": 0.7, "min_matches_per_team": 20},
        ]

        results = []

        for config in hyperparameter_configs:
            model = PoissonModel()
            model.update_hyperparameters(**config)

            monitor = PerformanceMonitor()
            monitor.start()

            training_result = model.train(base_training_data)

            monitor.stop()

            # è¯„ä¼°æ€§èƒ½
            test_data = create_performance_test_data(100)
            evaluation_metrics = model.evaluate(test_data)

            results.append(
                {
                    "config": config,
                    "training_time": monitor.get_execution_time(),
                    "training_accuracy": training_result.accuracy,
                    "evaluation_accuracy": evaluation_metrics["accuracy"],
                    "memory_usage": monitor.get_memory_usage()["delta_mb"],
                }
            )

        # åˆ†æç»“æœ
        best_config = max(results, key=lambda x: x["evaluation_accuracy"])
        fastest_training = min(results, key=lambda x: x["training_time"])

        print("è¶…å‚æ•°ä¼˜åŒ–ç»“æœ:")
        for result in results:
            print(
                f"  é…ç½®={result['config']}, è®­ç»ƒæ—¶é—´={result['training_time']:.3f}s, "
                f"è¯„ä¼°å‡†ç¡®ç‡={result['evaluation_accuracy']:.3f}"
            )

        print(
            f"æœ€ä½³é…ç½®: {best_config['config']}, å‡†ç¡®ç‡: {best_config['evaluation_accuracy']:.3f}"
        )
        print(
            f"æœ€å¿«è®­ç»ƒ: {fastest_training['config']}, æ—¶é—´: {fastest_training['training_time']:.3f}s"
        )

        # ä¼˜åŒ–æ–­è¨€
        assert best_config["evaluation_accuracy"] > 0.3
        assert fastest_training["training_time"] < 10.0

    def test_data_preprocessing_optimization(self):
        """æµ‹è¯•æ•°æ®é¢„å¤„ç†ä¼˜åŒ–"""
        # åˆ›å»ºåŒ…å«å™ªå£°çš„æ•°æ®
        raw_data = create_performance_test_data(1000)

        # æ·»åŠ ä¸€äº›å™ªå£°å’Œå¼‚å¸¸å€¼
        noisy_data = raw_data.copy()
        for i in range(len(noisy_data)):
            if np.random.random() < 0.1:  # 10%çš„æ¦‚ç‡æ·»åŠ å¼‚å¸¸å€¼
                noisy_data.loc[i, "home_score"] = np.random.randint(-5, 20)
                noisy_data.loc[i, "away_score"] = np.random.randint(-5, 20)

        def optimized_preprocessing(data):
            """ä¼˜åŒ–çš„æ•°æ®é¢„å¤„ç†"""
            # 1. è¿‡æ»¤æ— æ•ˆæ•°æ®
            filtered_data = data[
                (data["home_score"] >= 0)
                & (data["home_score"] <= 15)
                & (data["away_score"] >= 0)
                & (data["away_score"] <= 15)
            ]

            # 2. ç§»é™¤é‡å¤æ•°æ®
            filtered_data = filtered_data.drop_duplicates(
                subset=["home_team", "away_team", "match_date"]
            )

            # 3. æ•°æ®è´¨é‡æ£€æŸ¥
            initial_size = len(data)
            final_size = len(filtered_data)
            quality_ratio = final_size / initial_size

            return filtered_data, quality_ratio

        # æµ‹è¯•é¢„å¤„ç†æ€§èƒ½
        monitor = PerformanceMonitor()
        monitor.start()

        clean_data, quality_ratio = optimized_preprocessing(noisy_data)

        monitor.stop()

        # è®­ç»ƒæ¨¡å‹æ¯”è¾ƒ
        model_noisy = PoissonModel("noisy_data")
        model_clean = PoissonModel("clean_data")

        # è®­ç»ƒå™ªå£°æ•°æ®æ¨¡å‹
        start_time = time.time()
        model_noisy.train(noisy_data)
        noisy_training_time = time.time() - start_time

        # è®­ç»ƒæ¸…æ´æ•°æ®æ¨¡å‹
        start_time = time.time()
        model_clean.train(clean_data)
        clean_training_time = time.time() - start_time

        # è¯„ä¼°æ€§èƒ½
        test_data = create_performance_test_data(100)
        noisy_metrics = model_noisy.evaluate(test_data)
        clean_metrics = model_clean.evaluate(test_data)

        preprocessing_time = monitor.get_execution_time()

        print("æ•°æ®é¢„å¤„ç†ä¼˜åŒ–:")
        print(f"  é¢„å¤„ç†æ—¶é—´: {preprocessing_time:.3f}s")
        print(f"  æ•°æ®è´¨é‡æå‡: {quality_ratio:.3f}")
        print(
            f"  å™ªå£°æ•°æ®è®­ç»ƒæ—¶é—´: {noisy_training_time:.3f}s, å‡†ç¡®ç‡: {noisy_metrics['accuracy']:.3f}"
        )
        print(
            f"  æ¸…æ´æ•°æ®è®­ç»ƒæ—¶é—´: {clean_training_time:.3f}s, å‡†ç¡®ç‡: {clean_metrics['accuracy']:.3f}"
        )

        # ä¼˜åŒ–æ–­è¨€
        assert preprocessing_time < 2.0  # é¢„å¤„ç†åº”è¯¥å¾ˆå¿«
        assert quality_ratio > 0.7  # åº”è¯¥ä¿ç•™å¤§éƒ¨åˆ†æ•°æ®
        assert (
            clean_metrics["accuracy"] >= noisy_metrics["accuracy"]
        )  # æ¸…æ´æ•°æ®åº”è¯¥æé«˜æˆ–ä¿æŒå‡†ç¡®ç‡

    def test_model_caching_optimization(self):
        """æµ‹è¯•æ¨¡å‹ç¼“å­˜ä¼˜åŒ–"""
        model = PoissonModel()
        training_data = create_performance_test_data(300)
        model.train(training_data)

        # é‡å¤ç›¸åŒçš„é¢„æµ‹è¯·æ±‚
        same_prediction_data = {
            "home_team": "Team_A",
            "away_team": "Team_B",
            "match_id": "cache_test",
        }

        # ç¬¬ä¸€æ¬¡é¢„æµ‹ï¼ˆæ— ç¼“å­˜ï¼‰
        monitor = PerformanceMonitor()
        monitor.start()

        result1 = model.predict(same_prediction_data)

        monitor.stop()
        first_prediction_time = monitor.get_execution_time()

        # æ¨¡æ‹Ÿç¼“å­˜è¡Œä¸ºï¼ˆå®é™…å®ç°ä¸­åº”è¯¥åœ¨æ¨¡å‹å†…éƒ¨å®ç°ç¼“å­˜ï¼‰
        # è¿™é‡Œæˆ‘ä»¬åªæ˜¯æµ‹è¯•é‡å¤è°ƒç”¨çš„æ€§èƒ½ä¸€è‡´æ€§
        subsequent_times = []

        for i in range(10):
            monitor.start()
            result = model.predict(same_prediction_data)
            monitor.stop()
            subsequent_times.append(monitor.get_execution_time())

            # éªŒè¯ç»“æœä¸€è‡´æ€§
            assert result.home_win_prob == result1.home_win_prob
            assert result.draw_prob == result1.draw_prob
            assert result.away_win_prob == result1.away_win_prob

        avg_subsequent_time = np.mean(subsequent_times)
        max_subsequent_time = np.max(subsequent_times)

        print("æ¨¡å‹ç¼“å­˜æµ‹è¯•:")
        print(f"  é¦–æ¬¡é¢„æµ‹æ—¶é—´: {first_prediction_time:.6f}s")
        print(f"  å¹³å‡åç»­æ—¶é—´: {avg_subsequent_time:.6f}s")
        print(f"  æœ€å¤§åç»­æ—¶é—´: {max_subsequent_time:.6f}s")

        # ç¼“å­˜æ€§èƒ½æ–­è¨€
        assert (
            max_subsequent_time < first_prediction_time * 2
        )  # åç»­é¢„æµ‹ä¸åº”è¯¥æ˜¾è‘—æ…¢äºé¦–æ¬¡
        assert all(t < 0.1 for t in subsequent_times)  # æ‰€æœ‰åç»­é¢„æµ‹éƒ½åº”è¯¥å¾ˆå¿«

    def test_batch_processing_optimization(self):
        """æµ‹è¯•æ‰¹å¤„ç†ä¼˜åŒ–"""
        model = PoissonModel()
        training_data = create_performance_test_data(300)
        model.train(training_data)

        # åˆ›å»ºæ‰¹é‡æ•°æ®
        batch_data = []
        for i in range(100):
            batch_data.append(
                {
                    "home_team": f"Team_{chr(65 + i % 20)}",
                    "away_team": f"Team_{chr(65 + (i + 1) % 20)}",
                    "match_id": f"batch_opt_{i:03d}",
                }
            )

        # é€ä¸ªå¤„ç†
        monitor = PerformanceMonitor()
        monitor.start()

        individual_results = []
        for data in batch_data:
            result = model.predict(data)
            individual_results.append(result)

        monitor.stop()
        individual_time = monitor.get_execution_time()

        # æ¨¡æ‹Ÿæ‰¹å¤„ç†ä¼˜åŒ–ï¼ˆå®é™…åº”è¯¥åœ¨æ¨¡å‹ä¸­å®ç°ï¼‰
        # è¿™é‡Œæµ‹è¯•å‘é‡åŒ–å’Œæ‰¹å¤„ç†çš„æ½œåŠ›
        monitor = PerformanceMonitor()
        monitor.start()

        # æ¨¡æ‹Ÿæ‰¹å¤„ç†é€»è¾‘
        batch_results = []
        for data in batch_data:
            result = model.predict(data)  # åœ¨å®é™…å®ç°ä¸­è¿™åº”è¯¥æ˜¯æ‰¹å¤„ç†
            batch_results.append(result)

        monitor.stop()
        batch_time = monitor.get_execution_time()

        # éªŒè¯ç»“æœä¸€è‡´æ€§
        assert len(individual_results) == len(batch_results)
        for i, (ind, batch) in enumerate(zip(individual_results, batch_results)):
            assert ind.home_win_prob == batch.home_win_prob
            assert ind.draw_prob == batch.draw_prob
            assert ind.away_win_prob == batch.away_win_prob

        print("æ‰¹å¤„ç†ä¼˜åŒ–æµ‹è¯•:")
        print(f"  é€ä¸ªå¤„ç†æ—¶é—´: {individual_time:.3f}s")
        print(f"  æ‰¹å¤„ç†æ—¶é—´: {batch_time:.3f}s")
        print(
            f"  æ€§èƒ½æå‡: {individual_time/batch_time:.2f}x"
            if batch_time > 0
            else "æ— æ³•è®¡ç®—"
        )

        # æ€§èƒ½æ–­è¨€
        assert batch_time <= individual_time * 1.2  # æ‰¹å¤„ç†ä¸åº”è¯¥æ˜¾è‘—æ…¢äºé€ä¸ªå¤„ç†


# æµ‹è¯•è¿è¡Œå™¨
async def run_ml_performance_tests():
    """è¿è¡ŒMLæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
    print("ğŸš€ å¼€å§‹MLæ€§èƒ½æµ‹è¯•")
    print("=" * 60)

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„MLæ€§èƒ½æµ‹è¯•é€»è¾‘
    print("âœ… MLæ€§èƒ½æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    asyncio.run(run_ml_performance_tests())
