# æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆ

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜è¶³çƒé¢„æµ‹ç³»ç»Ÿçš„æ€§èƒ½æµ‹è¯•ç­–ç•¥ï¼ŒåŒ…æ‹¬Locustæ€§èƒ½æµ‹è¯•ã€APIåŸºå‡†æµ‹è¯•ã€æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–ç­‰ã€‚

## ğŸ“‹ ç›®å½•

- [æ€§èƒ½æµ‹è¯•æ¦‚è¿°](#æ€§èƒ½æµ‹è¯•æ¦‚è¿°)
  - [æµ‹è¯•ç›®æ ‡](#æµ‹è¯•ç›®æ ‡)
  - [æµ‹è¯•ç¯å¢ƒ](#æµ‹è¯•ç¯å¢ƒ)
  - [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
- [Locustæ€§èƒ½æµ‹è¯•](#locustæ€§èƒ½æµ‹è¯•)
  - [åŸºç¡€é…ç½®](#åŸºç¡€é…ç½®)
  - [æµ‹è¯•åœºæ™¯](#æµ‹è¯•åœºæ™¯)
  - [æµ‹è¯•æ‰§è¡Œ](#æµ‹è¯•æ‰§è¡Œ)
- [APIåŸºå‡†æµ‹è¯•](#apiåŸºå‡†æµ‹è¯•)
  - [HTTPæ€§èƒ½æµ‹è¯•](#httpæ€§èƒ½æµ‹è¯•)
  - [WebSocketæ€§èƒ½æµ‹è¯•](#websocketæ€§èƒ½æµ‹è¯•)
  - [GraphQLæ€§èƒ½æµ‹è¯•](#graphqlæ€§èƒ½æµ‹è¯•)
- [æ•°æ®åº“æ€§èƒ½æµ‹è¯•](#æ•°æ®åº“æ€§èƒ½æµ‹è¯•)
  - [æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–](#æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ–)
  - [è¿æ¥æ± ä¼˜åŒ–](#è¿æ¥æ± ä¼˜åŒ–)
  - [ç´¢å¼•ä¼˜åŒ–](#ç´¢å¼•ä¼˜åŒ–)
- [åº”ç”¨æ€§èƒ½æµ‹è¯•](#åº”ç”¨æ€§èƒ½æµ‹è¯•)
  - [å†…å­˜ä½¿ç”¨æµ‹è¯•](#å†…å­˜ä½¿ç”¨æµ‹è¯•)
  - [CPUä½¿ç”¨æµ‹è¯•](#cpuä½¿ç”¨æµ‹è¯•)
  - [å“åº”æ—¶é—´æµ‹è¯•](#å“åº”æ—¶é—´æµ‹è¯•)
- [ç›‘æ§ä¸åˆ†æ](#ç›‘æ§ä¸åˆ†æ)
  - [æ€§èƒ½ç›‘æ§](#æ€§èƒ½ç›‘æ§)
  - [ç“¶é¢ˆåˆ†æ](#ç“¶é¢ˆåˆ†æ)
  - [ä¼˜åŒ–å»ºè®®](#ä¼˜åŒ–å»ºè®®)

---

## æ€§èƒ½æµ‹è¯•æ¦‚è¿°

### æµ‹è¯•ç›®æ ‡

è¶³çƒé¢„æµ‹ç³»ç»Ÿæ€§èƒ½æµ‹è¯•çš„ä¸»è¦ç›®æ ‡åŒ…æ‹¬ï¼š

1. **å“åº”æ—¶é—´ç›®æ ‡**
   - APIå¹³å‡å“åº”æ—¶é—´ < 200ms
   - 95%è¯·æ±‚å“åº”æ—¶é—´ < 500ms
   - 99%è¯·æ±‚å“åº”æ—¶é—´ < 1000ms

2. **ååé‡ç›®æ ‡**
   - æ”¯æŒ1000å¹¶å‘ç”¨æˆ·
   - å¤„ç†10000 RPS (Requests Per Second)
   - æ•°æ®åº“æŸ¥è¯¢ < 100ms

3. **èµ„æºåˆ©ç”¨ç›®æ ‡**
   - CPUä½¿ç”¨ç‡ < 70%
   - å†…å­˜ä½¿ç”¨ç‡ < 80%
   - æ•°æ®åº“è¿æ¥æ± ä½¿ç”¨ç‡ < 80%

4. **ç¨³å®šæ€§ç›®æ ‡**
   - 24å°æ—¶æŒç»­è¿è¡Œæ— å†…å­˜æ³„æ¼
   - é”™è¯¯ç‡ < 0.1%
   - ç³»ç»Ÿå¯ç”¨æ€§ > 99.9%

### æµ‹è¯•ç¯å¢ƒ

**ç”Ÿäº§ç¯å¢ƒè§„æ ¼**:
- **åº”ç”¨æœåŠ¡å™¨**: 4æ ¸8GBå†…å­˜
- **æ•°æ®åº“æœåŠ¡å™¨**: 8æ ¸16GBå†…å­˜ï¼ŒSSDå­˜å‚¨
- **ç¼“å­˜æœåŠ¡å™¨**: 4æ ¸8GBå†…å­˜Redis
- **è´Ÿè½½å‡è¡¡å™¨**: Nginx
- **ç›‘æ§**: Prometheus + Grafana

**æµ‹è¯•ç¯å¢ƒè§„æ ¼**:
- **åº”ç”¨æœåŠ¡å™¨**: 2æ ¸4GBå†…å­˜
- **æ•°æ®åº“æœåŠ¡å™¨**: 4æ ¸8GBå†…å­˜
- **ç¼“å­˜æœåŠ¡å™¨**: 2æ ¸4GBå†…å­˜Redis

### æ€§èƒ½æŒ‡æ ‡

**å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)**:
- **å“åº”æ—¶é—´**: å¹³å‡ã€ä¸­ä½æ•°ã€95ç™¾åˆ†ä½ã€99ç™¾åˆ†ä½
- **ååé‡**: RPS (æ¯ç§’è¯·æ±‚æ•°)ã€å¹¶å‘ç”¨æˆ·æ•°
- **é”™è¯¯ç‡**: HTTPé”™è¯¯ç‡ã€ä¸šåŠ¡é”™è¯¯ç‡
- **èµ„æºåˆ©ç”¨ç‡**: CPUã€å†…å­˜ã€ç£ç›˜I/Oã€ç½‘ç»œI/O
- **æ•°æ®åº“æ€§èƒ½**: æŸ¥è¯¢æ—¶é—´ã€è¿æ¥æ•°ã€é”ç­‰å¾…æ—¶é—´

**ç›‘æ§æŒ‡æ ‡**:
- åº”ç”¨æŒ‡æ ‡: è¯·æ±‚è®¡æ•°ã€å“åº”æ—¶é—´ã€é”™è¯¯ç‡
- ç³»ç»ŸæŒ‡æ ‡: CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
- æ•°æ®åº“æŒ‡æ ‡: æŸ¥è¯¢æ€§èƒ½ã€è¿æ¥æ± çŠ¶æ€
- ç¼“å­˜æŒ‡æ ‡: å‘½ä¸­ç‡ã€å†…å­˜ä½¿ç”¨ã€è¿æ¥æ•°

---

## Locustæ€§èƒ½æµ‹è¯•

### åŸºç¡€é…ç½®

```python
# locustfile.py
import random
import json
import time
from locust import HttpUser, task, between, events
from locust.env import Environment
from locust.stats import stats_history, stats_printer
from locust.log import setup_logging
import gevent

class FootballPredictionUser(HttpUser):
    """è¶³çƒé¢„æµ‹ç³»ç»Ÿç”¨æˆ·æ¨¡æ‹Ÿ"""

    wait_time = between(1, 3)  # è¯·æ±‚é—´éš”1-3ç§’
    weight = 1  # ç”¨æˆ·æƒé‡

    def on_start(self):
        """ç”¨æˆ·å¼€å§‹å‰çš„åˆå§‹åŒ–æ“ä½œ"""
        self.headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Locust-Performance-Test'
        }
        self.test_matches = [1, 2, 3, 4, 5]  # æµ‹è¯•æ¯”èµ›ID
        self.test_teams = [101, 102, 103, 104, 105]  # æµ‹è¯•é˜Ÿä¼ID

    @task(20)
    def health_check(self):
        """å¥åº·æ£€æŸ¥ (20%æ¦‚ç‡)"""
        with self.client.get("/health", headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Health check failed: {response.status_code}")
            else:
                response.success()

    @task(30)
    def get_matches(self):
        """è·å–æ¯”èµ›åˆ—è¡¨ (30%æ¦‚ç‡)"""
        params = {
            'page': random.randint(1, 10),
            'per_page': random.choice([10, 20, 50]),
            'status': random.choice(['SCHEDULED', 'FINISHED', 'IN_PLAY'])
        }

        with self.client.get("/api/v1/data/matches", params=params, headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Get matches failed: {response.status_code}")
            else:
                response.success()

    @task(25)
    def get_match_details(self):
        """è·å–æ¯”èµ›è¯¦æƒ… (25%æ¦‚ç‡)"""
        match_id = random.choice(self.test_matches)

        with self.client.get(f"/api/v1/data/matches/{match_id}", headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Get match details failed: {response.status_code}")
            else:
                response.success()

    @task(15)
    def get_predictions(self):
        """è·å–é¢„æµ‹ç»“æœ (15%æ¦‚ç‡)"""
        match_id = random.choice(self.test_matches)

        with self.client.get(f"/api/v1/predictions/match/{match_id}", headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Get predictions failed: {response.status_code}")
            else:
                response.success()

    @task(10)
    def submit_prediction_request(self):
        """æäº¤é¢„æµ‹è¯·æ±‚ (10%æ¦‚ç‡)"""
        match_id = random.choice(self.test_matches)
        payload = {
            "match_id": match_id,
            "features": {
                "home_form": random.uniform(0.5, 1.0),
                "away_form": random.uniform(0.5, 1.0),
                "head_to_head": random.uniform(0.3, 0.7),
                "team_strength": random.uniform(0.4, 0.9)
            }
        }

        with self.client.post("/api/v1/predictions", json=payload, headers=self.headers, catch_response=True) as response:
            if response.status_code not in [200, 201]:
                response.failure(f"Submit prediction failed: {response.status_code}")
            else:
                response.success()

class HeavyUser(HttpUser):
    """é‡è´Ÿè½½ç”¨æˆ·æ¨¡æ‹Ÿ"""

    wait_time = between(0.1, 0.5)  # çŸ­é—´éš”ï¼Œé«˜é¢‘ç‡è¯·æ±‚
    weight = 3  # æ›´å¤šçš„æ­¤ç±»ç”¨æˆ·

    def on_start(self):
        """åˆå§‹åŒ–"""
        self.headers = {
            'Content-Type': 'application/json',
            'User-Agent': 'Locust-Heavy-User'
        }

    @task(40)
    def intensive_prediction(self):
        """å¯†é›†é¢„æµ‹è¯·æ±‚"""
        payload = {
            "match_id": random.randint(1, 100),
            "features": {
                "home_form": random.uniform(0, 1),
                "away_form": random.uniform(0, 1),
                "head_to_head": random.uniform(0, 1),
                "team_strength": random.uniform(0, 1),
                "historical_data": [random.uniform(0, 1) for _ in range(10)]
            }
        }

        with self.client.post("/api/v1/predictions/batch", json=payload, headers=self.headers, catch_response=True) as response:
            if response.status_code not in [200, 201]:
                response.failure(f"Batch prediction failed: {response.status_code}")
            else:
                response.success()

    @task(30)
    def data_analysis(self):
        """æ•°æ®åˆ†æè¯·æ±‚"""
        with self.client.get("/api/v1/analytics/performance", headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Data analysis failed: {response.status_code}")
            else:
                response.success()

    @task(20)
    def real_time_updates(self):
        """å®æ—¶æ›´æ–°è¯·æ±‚"""
        match_id = random.randint(1, 100)
        with self.client.get(f"/api/v1/streaming/match/{match_id}", headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Real-time updates failed: {response.status_code}")
            else:
                response.success()

    @task(10)
    def complex_query(self):
        """å¤æ‚æŸ¥è¯¢è¯·æ±‚"""
        params = {
            'start_date': '2023-01-01',
            'end_date': '2023-12-31',
            'competition': random.choice(['Premier League', 'Championship', 'League One']),
            'team_performance': True,
            'include_predictions': True
        }

        with self.client.get("/api/v1/analytics/comprehensive", params=params, headers=self.headers, catch_response=True) as response:
            if response.status_code != 200:
                response.failure(f"Complex query failed: {response.status_code}")
            else:
                response.success()

# è‡ªå®šä¹‰äº‹ä»¶ç›‘å¬å™¨
@events.init_command_line_parser.add_listener
def _(parser):
    parser.add_argument("--test-duration", type=int, default=60, help="Test duration in seconds")
    parser.add_argument("--ramp-up", type=int, default=10, help="Ramp up time in seconds")
    parser.add_argument("--users", type=int, default=100, help="Number of users to simulate")

@events.test_start.add_listener
def on_test_start(environment, **kwargs):
    """æµ‹è¯•å¼€å§‹æ—¶çš„å¤„ç†"""
    print("=== Performance Test Started ===")
    print(f"Target: {environment.host}")
    print(f"Users: {environment.parsed_options.users}")
    print(f"Duration: {environment.parsed_options.test_duration}s")
    print(f"Ramp up: {environment.parsed_options.ramp_up}s")

@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    """æµ‹è¯•ç»“æŸæ—¶çš„å¤„ç†"""
    print("=== Performance Test Completed ===")

    # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
    stats = environment.stats
    print(f"Total requests: {stats.total.num_requests}")
    print(f"Total failures: {stats.total.num_failures}")
    print(f"Failure rate: {(stats.total.num_failures / stats.total.num_requests * 100):.2f}%")
    print(f"Average response time: {stats.total.avg_response_time:.2f}ms")
    print(f"Median response time: {stats.total.median_response_time:.2f}ms")
    print(f"95th percentile: {stats.total.get_response_time_percentile(0.95):.2f}ms")
    print(f"99th percentile: {stats.total.get_response_time_percentile(0.99):.2f}ms")

@events.request.add_listener
def on_request(request_type, name, response_time, response_length, response, context, exception, **kwargs):
    """è¯·æ±‚äº‹ä»¶ç›‘å¬å™¨"""
    if exception:
        print(f"Request failed: {name} - {exception}")
    else:
        if response_time > 1000:  # è®°å½•æ…¢è¯·æ±‚
            print(f"Slow request: {name} - {response_time:.2f}ms")

if __name__ == "__main__":
    # è®¾ç½®æ—¥å¿—
    setup_logging("INFO", None)

    # åˆ›å»ºç¯å¢ƒ
    env = Environment(user_classes=[FootballPredictionUser, HeavyUser])

    # å¯åŠ¨æ€§èƒ½æµ‹è¯•
    env.create_local_runner()

    # å¯åŠ¨Webç•Œé¢
    env.create_web_ui("127.0.0.1", 8089)

    # å¯åŠ¨ç»Ÿè®¡æ‰“å°
    gevent.spawn(stats_printer(env.stats))

    # ç­‰å¾…æµ‹è¯•å®Œæˆ
    gevent.spawn_later(env.parsed_options.test_duration, lambda: env.quit())

    # è¿è¡Œæµ‹è¯•
    env.runner.greenlet.join()
```

### æµ‹è¯•åœºæ™¯

```python
# test_scenarios.py
import json
import random
from datetime import datetime, timedelta
from locust import HttpUser, task, between, events

class NormalLoadScenario(HttpUser):
    """æ­£å¸¸è´Ÿè½½åœºæ™¯"""

    wait_time = between(2, 5)
    weight = 1

    def on_start(self):
        """åˆå§‹åŒ–ç”¨æˆ·æ•°æ®"""
        self.user_id = random.randint(1000, 9999)
        self.session_id = f"session_{self.user_id}_{int(time.time())}"
        self.headers = {
            'Content-Type': 'application/json',
            'X-User-ID': str(self.user_id),
            'X-Session-ID': self.session_id
        }

    @task(25)
    def browse_matches(self):
        """æµè§ˆæ¯”èµ›"""
        with self.client.get("/api/v1/data/matches", headers=self.headers) as response:
            if response.status_code == 200:
                data = response.json()
                # ç¼“å­˜ä¸€äº›æ¯”èµ›IDä¾›åç»­ä½¿ç”¨
                if 'matches' in data:
                    self.recent_matches = [match['id'] for match in data['matches'][:5]]

    @task(20)
    def view_team_stats(self):
        """æŸ¥çœ‹é˜Ÿä¼ç»Ÿè®¡"""
        team_id = random.randint(1, 100)
        with self.client.get(f"/api/v1/data/teams/{team_id}/stats", headers=self.headers) as response:
            pass

    @task(15)
    def get_prediction(self):
        """è·å–é¢„æµ‹"""
        if hasattr(self, 'recent_matches'):
            match_id = random.choice(self.recent_matches)
            with self.client.get(f"/api/v1/predictions/match/{match_id}", headers=self.headers) as response:
                pass

    @task(10)
    def check_league_table(self):
        """æŸ¥çœ‹è”èµ›ç§¯åˆ†æ¦œ"""
        competition_id = random.choice([39, 40, 41])  # Premier League, Championship, League One
        with self.client.get(f"/api/v1/data/competitions/{competition_id}/standings", headers=self.headers) as response:
            pass

    @task(5)
    def submit_feedback(self):
        """æäº¤åé¦ˆ"""
        feedback_data = {
            "rating": random.randint(1, 5),
            "comment": "Performance test feedback",
            "timestamp": datetime.now().isoformat()
        }
        with self.client.post("/api/v1/feedback", json=feedback_data, headers=self.headers) as response:
            pass

class PeakLoadScenario(HttpUser):
    """å³°å€¼è´Ÿè½½åœºæ™¯"""

    wait_time = between(0.5, 2)
    weight = 2

    def on_start(self):
        """åˆå§‹åŒ–"""
        self.user_id = random.randint(1000, 9999)
        self.headers = {
            'Content-Type': 'application/json',
            'X-User-ID': str(self.user_id)
        }

    @task(40)
    def rapid_predictions(self):
        """å¿«é€Ÿé¢„æµ‹è¯·æ±‚"""
        matches = list(range(1, 50))
        random.shuffle(matches)

        for match_id in matches[:10]:  # æ‰¹é‡è¯·æ±‚10ä¸ªæ¯”èµ›
            payload = {
                "match_id": match_id,
                "features": {
                    "home_form": random.uniform(0, 1),
                    "away_form": random.uniform(0, 1),
                    "head_to_head": random.uniform(0, 1)
                }
            }

            with self.client.post("/api/v1/predictions", json=payload, headers=self.headers) as response:
                pass

    @task(30)
    def batch_predictions(self):
        """æ‰¹é‡é¢„æµ‹"""
        payload = {
            "matches": [
                {
                    "match_id": i,
                    "features": {
                        "home_form": random.uniform(0, 1),
                        "away_form": random.uniform(0, 1)
                    }
                }
                for i in range(1, 21)  # 20åœºæ¯”èµ›
            ]
        }

        with self.client.post("/api/v1/predictions/batch", json=payload, headers=self.headers) as response:
            pass

    @task(20)
    def real_time_updates(self):
        """å®æ—¶æ›´æ–°"""
        match_id = random.randint(1, 50)
        with self.client.get(f"/api/v1/streaming/match/{match_id}/live", headers=self.headers) as response:
            pass

    @task(10)
    def historical_analysis(self):
        """å†å²åˆ†æ"""
        params = {
            "start_date": (datetime.now() - timedelta(days=30)).isoformat(),
            "end_date": datetime.now().isoformat(),
            "metrics": ["accuracy", "profit", "win_rate"]
        }

        with self.client.get("/api/v1/analytics/historical", params=params, headers=self.headers) as response:
            pass

class StressTestScenario(HttpUser):
    """å‹åŠ›æµ‹è¯•åœºæ™¯"""

    wait_time = between(0.1, 1)
    weight = 1

    def on_start(self):
        """åˆå§‹åŒ–"""
        self.user_id = random.randint(1000, 9999)
        self.headers = {
            'Content-Type': 'application/json',
            'X-User-ID': str(self.user_id)
        }

    @task(50)
    def intensive_computations(self):
        """å¯†é›†è®¡ç®—è¯·æ±‚"""
        payload = {
            "model_type": random.choice(["xgboost", "neural_network", "ensemble"]),
            "features": {
                f"feature_{i}": random.uniform(0, 1) for i in range(100)  # 100ä¸ªç‰¹å¾
            },
            "iterations": random.randint(100, 1000)
        }

        with self.client.post("/api/v1/models/compute", json=payload, headers=self.headers) as response:
            pass

    @task(30)
    def large_data_requests(self):
        """å¤§æ•°æ®è¯·æ±‚"""
        with self.client.get("/api/v1/data/export", params={
            "format": "json",
            "include_history": True,
            "include_predictions": True,
            "include_statistics": True
        }, headers=self.headers) as response:
            pass

    @task(20)
    def concurrent_operations(self):
        """å¹¶å‘æ“ä½œ"""
        import gevent

        def make_request():
            match_id = random.randint(1, 100)
            with self.client.get(f"/api/v1/data/matches/{match_id}", headers=self.headers) as response:
                return response

        # å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
        requests = [gevent.spawn(make_request) for _ in range(5)]
        gevent.joinall(requests, timeout=10)
```

### æµ‹è¯•æ‰§è¡Œ

```python
# run_performance_tests.py
import os
import sys
import time
import json
import subprocess
from datetime import datetime
import argparse
from locust.main import main as locust_main

class PerformanceTestRunner:
    """æ€§èƒ½æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, config_file="performance_config.json"):
        self.config = self.load_config(config_file)
        self.results_dir = "performance_results"
        self.ensure_results_dir()

    def load_config(self, config_file):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"Config file {config_file} not found, using defaults")
            return self.get_default_config()

    def get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "tests": [
                {
                    "name": "baseline_test",
                    "users": 50,
                    "spawn_rate": 5,
                    "duration": 300,
                    "scenario": "NormalLoadScenario"
                },
                {
                    "name": "peak_load_test",
                    "users": 500,
                    "spawn_rate": 50,
                    "duration": 600,
                    "scenario": "PeakLoadScenario"
                },
                {
                    "name": "stress_test",
                    "users": 1000,
                    "spawn_rate": 100,
                    "duration": 300,
                    "scenario": "StressTestScenario"
                }
            ],
            "target_host": "http://localhost:8000",
            "results_dir": "performance_results"
        }

    def ensure_results_dir(self):
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        os.makedirs(self.results_dir, exist_ok=True)

    def run_single_test(self, test_config):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        test_name = test_config['name']
        users = test_config['users']
        spawn_rate = test_config['spawn_rate']
        duration = test_config['duration']
        scenario = test_config['scenario']

        print(f"Running test: {test_name}")
        print(f"Users: {users}")
        print(f"Spawn rate: {spawn_rate}")
        print(f"Duration: {duration}s")
        print(f"Scenario: {scenario}")

        # å‡†å¤‡æµ‹è¯•ç»“æœæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = os.path.join(self.results_dir, f"{test_name}_{timestamp}.json")
        html_file = os.path.join(self.results_dir, f"{test_name}_{timestamp}.html")

        # æ„å»ºLocustå‘½ä»¤
        cmd = [
            "locust",
            "-f", "locustfile.py",
            "--host", self.config['target_host'],
            "--users", str(users),
            "--spawn-rate", str(spawn_rate),
            "--run-time", f"{duration}s",
            "--csv", os.path.join(self.results_dir, f"{test_name}_{timestamp}"),
            "--html", html_file,
            "--class", scenario
        ]

        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['LOCUST_TEST_NAME'] = test_name
        env['LOCUST_RESULT_FILE'] = result_file

        try:
            # è¿è¡Œæµ‹è¯•
            print(f"Starting test: {test_name}")
            start_time = time.time()

            process = subprocess.Popen(cmd, env=env, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdout, stderr = process.communicate()

            end_time = time.time()
            duration_actual = end_time - start_time

            # è®°å½•ç»“æœ
            result = {
                "test_name": test_name,
                "config": test_config,
                "start_time": datetime.fromtimestamp(start_time).isoformat(),
                "end_time": datetime.fromtimestamp(end_time).isoformat(),
                "duration": duration_actual,
                "status": "completed" if process.returncode == 0 else "failed",
                "stdout": stdout.decode('utf-8'),
                "stderr": stderr.decode('utf-8')
            }

            with open(result_file, 'w') as f:
                json.dump(result, f, indent=2)

            print(f"Test completed: {test_name}")
            print(f"Duration: {duration_actual:.2f}s")
            print(f"Status: {result['status']}")

            return result

        except Exception as e:
            print(f"Error running test {test_name}: {e}")
            return None

    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("Starting performance test suite...")
        results = []

        for test_config in self.config['tests']:
            result = self.run_single_test(test_config)
            if result:
                results.append(result)

        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        self.generate_summary_report(results)

        return results

    def generate_summary_report(self, results):
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        summary_file = os.path.join(self.results_dir, f"summary_{timestamp}.md")

        with open(summary_file, 'w') as f:
            f.write("# Performance Test Summary\n\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n\n")

            f.write("## Test Results Overview\n\n")
            f.write("| Test Name | Users | Duration | Status | Avg Response Time | Error Rate |\n")
            f.write("|-----------|-------|----------|--------|------------------|------------|\n")

            for result in results:
                test_name = result['test_name']
                users = result['config']['users']
                duration = result['duration']
                status = result['status']

                # ä»CSVæ–‡ä»¶ä¸­è¯»å–ç»Ÿè®¡ä¿¡æ¯
                csv_file = os.path.join(self.results_dir, f"{test_name}_{datetime.fromisoformat(result['start_time']).strftime('%Y%m%d_%H%M%S')}_stats.csv")
                avg_response_time = "N/A"
                error_rate = "N/A"

                if os.path.exists(csv_file):
                    try:
                        import pandas as pd
                        df = pd.read_csv(csv_file)
                        if not df.empty:
                            avg_response_time = f"{df['Average Response Time'].iloc[-1]:.2f}ms"
                            error_rate = f"{df['Failure Count'].iloc[-1] / df['Request Count'].iloc[-1] * 100:.2f}%"
                    except Exception as e:
                        print(f"Error reading CSV file {csv_file}: {e}")

                f.write(f"| {test_name} | {users} | {duration:.2f}s | {status} | {avg_response_time} | {error_rate} |\n")

            f.write("\n## Recommendations\n\n")

            # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
            failed_tests = [r for r in results if r['status'] == 'failed']
            if failed_tests:
                f.write("### Failed Tests\n")
                for test in failed_tests:
                    f.write(f"- {test['test_name']}: Check logs for details\n")

            # æ€§èƒ½å»ºè®®
            f.write("\n### Performance Optimization Suggestions\n")
            f.write("- Monitor response times for high-latency endpoints\n")
            f.write("- Consider implementing caching for frequently accessed data\n")
            f.write("- Optimize database queries for slow operations\n")
            f.write("- Consider load balancing for high-traffic scenarios\n")

        print(f"Summary report generated: {summary_file}")

    def run_comparison_test(self, base_config, new_config):
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print("Running comparison test...")

        # è¿è¡ŒåŸºçº¿æµ‹è¯•
        print("Running baseline test...")
        base_results = self.run_single_test(base_config)

        # ç­‰å¾…ç³»ç»Ÿæ¢å¤
        time.sleep(60)

        # è¿è¡Œæ–°é…ç½®æµ‹è¯•
        print("Running new configuration test...")
        new_results = self.run_single_test(new_config)

        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.generate_comparison_report(base_results, new_results)

        return base_results, new_results

    def generate_comparison_report(self, base_results, new_results):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        comparison_file = os.path.join(self.results_dir, f"comparison_{timestamp}.md")

        with open(comparison_file, 'w') as f:
            f.write("# Performance Comparison Report\n\n")
            f.write(f"Generated: {datetime.now().isoformat()}\n\n")

            f.write("## Test Configuration Comparison\n\n")
            f.write("| Metric | Baseline | New Configuration | Change |\n")
            f.write("|--------|----------|-------------------|--------|\n")
            f.write(f"| Users | {base_results['config']['users']} | {new_results['config']['users']} | - |\n")
            f.write(f"| Duration | {base_results['duration']:.2f}s | {new_results['duration']:.2f}s | {(new_results['duration'] - base_results['duration']):+.2f}s |\n")

            # è¯»å–è¯¦ç»†çš„æ€§èƒ½æ•°æ®è¿›è¡Œå¯¹æ¯”
            f.write("\n## Performance Metrics Comparison\n\n")

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„æ€§èƒ½å¯¹æ¯”é€»è¾‘
            f.write("Detailed performance comparison would be implemented here...\n")

        print(f"Comparison report generated: {comparison_file}")

def main():
    parser = argparse.ArgumentParser(description='Run performance tests')
    parser.add_argument('--config', default='performance_config.json', help='Configuration file path')
    parser.add_argument('--test', help='Specific test to run')
    parser.add_argument('--comparison', action='store_true', help='Run comparison test')
    args = parser.parse_args()

    runner = PerformanceTestRunner(args.config)

    if args.comparison:
        # è¿è¡Œå¯¹æ¯”æµ‹è¯•
        base_config = runner.config['tests'][0]  # ç¬¬ä¸€ä¸ªæµ‹è¯•ä½œä¸ºåŸºçº¿
        new_config = runner.config['tests'][1]   # ç¬¬äºŒä¸ªæµ‹è¯•ä½œä¸ºæ–°é…ç½®
        runner.run_comparison_test(base_config, new_config)
    elif args.test:
        # è¿è¡Œç‰¹å®šæµ‹è¯•
        test_config = next((t for t in runner.config['tests'] if t['name'] == args.test), None)
        if test_config:
            runner.run_single_test(test_config)
        else:
            print(f"Test '{args.test}' not found in configuration")
    else:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        runner.run_all_tests()

if __name__ == "__main__":
    main()
```

---

## APIåŸºå‡†æµ‹è¯•

### HTTPæ€§èƒ½æµ‹è¯•

```python
# api_benchmark.py
import asyncio
import aiohttp
import time
import statistics
import json
from datetime import datetime
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import matplotlib.pyplot as plt
import pandas as pd

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    endpoint: str
    method: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    median_response_time: float
    min_response_time: float
    max_response_time: float
    p95_response_time: float
    p99_response_time: float
    requests_per_second: float
    error_rate: float
    throughput_mbps: float

class APIBenchmark:
    """APIåŸºå‡†æµ‹è¯•å·¥å…·"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        self.results = []

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        if self.session:
            await self.session.close()

    async def benchmark_endpoint(self, endpoint: str, method: str = "GET",
                              payload: Optional[Dict] = None,
                              headers: Optional[Dict] = None,
                              concurrent_users: int = 10,
                              total_requests: int = 100) -> BenchmarkResult:
        """å¯¹å•ä¸ªç«¯ç‚¹è¿›è¡ŒåŸºå‡†æµ‹è¯•"""

        url = f"{self.base_url}{endpoint}"
        request_times = []
        success_count = 0
        error_count = 0
        total_bytes = 0

        print(f"Benchmarking {method} {url}")
        print(f"Concurrent users: {concurrent_users}")
        print(f"Total requests: {total_requests}")

        async def make_request():
            """å‘é€å•ä¸ªè¯·æ±‚"""
            nonlocal success_count, error_count, total_bytes

            try:
                start_time = time.time()

                if method.upper() == "GET":
                    async with self.session.get(url, headers=headers) as response:
                        content = await response.read()
                        response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                        total_bytes += len(content)

                        if response.status < 400:
                            success_count += 1
                        else:
                            error_count += 1

                        return response_time, response.status

                elif method.upper() == "POST":
                    async with self.session.post(url, json=payload, headers=headers) as response:
                        content = await response.read()
                        response_time = (time.time() - start_time) * 1000
                        total_bytes += len(content)

                        if response.status < 400:
                            success_count += 1
                        else:
                            error_count += 1

                        return response_time, response.status

            except Exception as e:
                error_count += 1
                return None, None

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        requests_per_user = total_requests // concurrent_users

        for _ in range(concurrent_users):
            for _ in range(requests_per_user):
                tasks.append(make_request())

        # æ‰§è¡Œæ‰€æœ‰è¯·æ±‚
        start_time = time.time()
        results = await asyncio.gather(*tasks, return_exceptions=True)
        end_time = time.time()

        # å¤„ç†ç»“æœ
        for result in results:
            if isinstance(result, tuple) and result[0] is not None:
                request_times.append(result[0])

        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if request_times:
            avg_time = statistics.mean(request_times)
            median_time = statistics.median(request_times)
            min_time = min(request_times)
            max_time = max(request_times)
            p95_time = statistics.quantiles(request_times, n=20)[18]  # 95th percentile
            p99_time = statistics.quantiles(request_times, n=100)[98]  # 99th percentile
        else:
            avg_time = median_time = min_time = max_time = p95_time = p99_time = 0

        total_time = end_time - start_time
        rps = total_requests / total_time if total_time > 0 else 0
        error_rate = (error_count / total_requests * 100) if total_requests > 0 else 0
        throughput_mbps = (total_bytes * 8) / (total_time * 1000000) if total_time > 0 else 0

        benchmark_result = BenchmarkResult(
            endpoint=endpoint,
            method=method,
            total_requests=total_requests,
            successful_requests=success_count,
            failed_requests=error_count,
            average_response_time=avg_time,
            median_response_time=median_time,
            min_response_time=min_time,
            max_response_time=max_time,
            p95_response_time=p95_time,
            p99_response_time=p99_time,
            requests_per_second=rps,
            error_rate=error_rate,
            throughput_mbps=throughput_mbps
        )

        self.results.append(benchmark_result)
        return benchmark_result

    async def run_comprehensive_benchmark(self):
        """è¿è¡Œå…¨é¢çš„åŸºå‡†æµ‹è¯•"""

        print("Starting comprehensive API benchmark...")

        # æµ‹è¯•ç«¯ç‚¹é…ç½®
        endpoints = [
            ("GET", "/health", None, None),
            ("GET", "/api/v1/data/matches", None, {"Accept": "application/json"}),
            ("GET", "/api/v1/data/matches/1", None, {"Accept": "application/json"}),
            ("GET", "/api/v1/predictions/match/1", None, {"Accept": "application/json"}),
            ("POST", "/api/v1/predictions", {
                "match_id": 1,
                "features": {
                    "home_form": 0.7,
                    "away_form": 0.5,
                    "head_to_head": 0.6
                }
            }, {"Content-Type": "application/json"}),
            ("GET", "/api/v1/analytics/performance", None, {"Accept": "application/json"}),
            ("GET", "/api/v1/monitoring/metrics", None, {"Accept": "application/json"}),
        ]

        results = []

        for method, endpoint, payload, headers in endpoints:
            result = await self.benchmark_endpoint(
                endpoint=endpoint,
                method=method,
                payload=payload,
                headers=headers,
                concurrent_users=50,
                total_requests=1000
            )
            results.append(result)

            # è¾“å‡ºç»“æœ
            print(f"\n{method} {endpoint} Results:")
            print(f"  Average Response Time: {result.average_response_time:.2f}ms")
            print(f"  Requests Per Second: {result.requests_per_second:.2f}")
            print(f"  Error Rate: {result.error_rate:.2f}%")
            print(f"  P95 Response Time: {result.p95_response_time:.2f}ms")
            print("-" * 50)

        return results

    def generate_report(self, output_file: str = "benchmark_report.md"):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        if not self.results:
            print("No benchmark results to report")
            return

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        with open(output_file, 'w') as f:
            f.write("# API Benchmark Report\n\n")
            f.write(f"Generated: {timestamp}\n\n")

            f.write("## Summary\n\n")
            f.write("| Endpoint | Method | Avg Response Time | RPS | Error Rate | P95 |\n")
            f.write("|----------|--------|-------------------|-----|------------|-----|\n")

            for result in self.results:
                f.write(f"| {result.endpoint} | {result.method} | {result.average_response_time:.2f}ms | {result.requests_per_second:.2f} | {result.error_rate:.2f}% | {result.p95_response_time:.2f}ms |\n")

            f.write("\n## Detailed Results\n\n")

            for result in self.results:
                f.write(f"### {result.method} {result.endpoint}\n\n")
                f.write(f"- **Total Requests**: {result.total_requests}\n")
                f.write(f"- **Successful Requests**: {result.successful_requests}\n")
                f.write(f"- **Failed Requests**: {result.failed_requests}\n")
                f.write(f"- **Average Response Time**: {result.average_response_time:.2f}ms\n")
                f.write(f"- **Median Response Time**: {result.median_response_time:.2f}ms\n")
                f.write(f"- **Min Response Time**: {result.min_response_time:.2f}ms\n")
                f.write(f"- **Max Response Time**: {result.max_response_time:.2f}ms\n")
                f.write(f"- **95th Percentile**: {result.p95_response_time:.2f}ms\n")
                f.write(f"- **99th Percentile**: {result.p99_response_time:.2f}ms\n")
                f.write(f"- **Requests Per Second**: {result.requests_per_second:.2f}\n")
                f.write(f"- **Error Rate**: {result.error_rate:.2f}%\n")
                f.write(f"- **Throughput**: {result.throughput_mbps:.2f} Mbps\n\n")

        print(f"Benchmark report generated: {output_file}")

    def generate_visualization(self, output_file: str = "benchmark_visualization.png"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if not self.results:
            print("No benchmark results to visualize")
            return

        # å‡†å¤‡æ•°æ®
        endpoints = [f"{r.method} {r.endpoint}" for r in self.results]
        avg_times = [r.average_response_time for r in self.results]
        p95_times = [r.p95_response_time for r in self.results]
        rps_values = [r.requests_per_second for r in self.results]

        # åˆ›å»ºå›¾è¡¨
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # å“åº”æ—¶é—´å¯¹æ¯”
        ax1.bar(range(len(endpoints)), avg_times, alpha=0.7, label='Average')
        ax1.bar(range(len(endpoints)), p95_times, alpha=0.7, label='P95')
        ax1.set_xlabel('Endpoint')
        ax1.set_ylabel('Response Time (ms)')
        ax1.set_title('Response Time Comparison')
        ax1.set_xticks(range(len(endpoints)))
        ax1.set_xticklabels([e.split()[-1] for e in endpoints], rotation=45)
        ax1.legend()

        # RPSå¯¹æ¯”
        ax2.bar(range(len(endpoints)), rps_values, color='green', alpha=0.7)
        ax2.set_xlabel('Endpoint')
        ax2.set_ylabel('Requests Per Second')
        ax2.set_title('Throughput Comparison')
        ax2.set_xticks(range(len(endpoints)))
        ax2.set_xticklabels([e.split()[-1] for e in endpoints], rotation=45)

        # é”™è¯¯ç‡å¯¹æ¯”
        error_rates = [r.error_rate for r in self.results]
        ax3.bar(range(len(endpoints)), error_rates, color='red', alpha=0.7)
        ax3.set_xlabel('Endpoint')
        ax3.set_ylabel('Error Rate (%)')
        ax3.set_title('Error Rate Comparison')
        ax3.set_xticks(range(len(endpoints)))
        ax3.set_xticklabels([e.split()[-1] for e in endpoints], rotation=45)

        # ç»¼åˆæ€§èƒ½é›·è¾¾å›¾
        categories = ['Speed', 'Throughput', 'Reliability', 'Consistency']
        N = len(categories)

        # è®¡ç®—æ ‡å‡†åŒ–æŒ‡æ ‡
        speed_scores = [1 - (min(avg_times) / t) for t in avg_times]
        throughput_scores = [t / max(rps_values) for t in rps_values]
        reliability_scores = [1 - (e / 100) for e in error_rates]
        consistency_scores = [1 - ((p95 - a) / a) for a, p95 in zip(avg_times, p95_times)]

        angles = [n / float(N) * 2 * 3.14159 for n in range(N)]
        angles += angles[:1]

        ax4 = plt.subplot(2, 2, 4, projection='polar')
        for i, endpoint in enumerate(endpoints):
            values = [
                speed_scores[i],
                throughput_scores[i],
                reliability_scores[i],
                consistency_scores[i]
            ]
            values += values[:1]

            ax4.plot(angles, values, 'o-', linewidth=2, label=endpoint.split()[-1])
            ax4.fill(angles, values, alpha=0.25)

        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(categories)
        ax4.set_title('Performance Radar Chart')
        ax4.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"Visualization generated: {output_file}")

async def main():
    """ä¸»å‡½æ•°"""
    async with APIBenchmark() as benchmark:
        # è¿è¡Œå…¨é¢åŸºå‡†æµ‹è¯•
        results = await benchmark.run_comprehensive_benchmark()

        # ç”ŸæˆæŠ¥å‘Šå’Œå¯è§†åŒ–
        benchmark.generate_report()
        benchmark.generate_visualization()

        # è¾“å‡ºæ€»ç»“
        print("\n" + "="*50)
        print("BENCHMARK SUMMARY")
        print("="*50)

        total_avg_time = sum(r.average_response_time for r in results) / len(results)
        total_rps = sum(r.requests_per_second for r in results) / len(results)
        total_error_rate = sum(r.error_rate for r in results) / len(results)

        print(f"Overall Average Response Time: {total_avg_time:.2f}ms")
        print(f"Overall Average RPS: {total_rps:.2f}")
        print(f"Overall Average Error Rate: {total_error_rate:.2f}%")

        # æ€§èƒ½è¯„ä¼°
        if total_avg_time < 200:
            print("âœ… Response Time: EXCELLENT")
        elif total_avg_time < 500:
            print("âš ï¸ Response Time: GOOD")
        else:
            print("âŒ Response Time: NEEDS IMPROVEMENT")

        if total_rps > 100:
            print("âœ… Throughput: EXCELLENT")
        elif total_rps > 50:
            print("âš ï¸ Throughput: GOOD")
        else:
            print("âŒ Throughput: NEEDS IMPROVEMENT")

        if total_error_rate < 1:
            print("âœ… Error Rate: EXCELLENT")
        elif total_error_rate < 5:
            print("âš ï¸ Error Rate: GOOD")
        else:
            print("âŒ Error Rate: NEEDS IMPROVEMENT")

if __name__ == "__main__":
    asyncio.run(main())
```

---

## ç›‘æ§ä¸åˆ†æ

### æ€§èƒ½ç›‘æ§

```python
# performance_monitor.py
import time
import psutil
import asyncio
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import matplotlib.pyplot as plt
import pandas as pd
from dataclasses import dataclass, asdict

@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    disk_usage: float
    network_sent: float
    network_recv: float
    load_avg: float

@dataclass
class ApplicationMetrics:
    """åº”ç”¨æŒ‡æ ‡"""
    timestamp: str
    active_connections: int
    request_count: int
    response_time_avg: float
    error_rate: float
    throughput: float

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""

    def __init__(self, db_path: str = "performance_monitoring.db"):
        self.db_path = db_path
        self.is_running = False
        self.monitoring_task = None
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS system_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                cpu_percent REAL,
                memory_percent REAL,
                disk_usage REAL,
                network_sent REAL,
                network_recv REAL,
                load_avg REAL
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS application_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                active_connections INTEGER,
                request_count INTEGER,
                response_time_avg REAL,
                error_rate REAL,
                throughput REAL
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS performance_alerts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                alert_type TEXT,
                severity TEXT,
                message TEXT,
                metric_name TEXT,
                metric_value REAL,
                threshold_value REAL,
                resolved BOOLEAN DEFAULT FALSE
            )
        ''')

        conn.commit()
        conn.close()

    async def collect_system_metrics(self) -> SystemMetrics:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        timestamp = datetime.now().isoformat()

        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)

        # å†…å­˜ä½¿ç”¨ç‡
        memory = psutil.virtual_memory()
        memory_percent = memory.percent

        # ç£ç›˜ä½¿ç”¨ç‡
        disk = psutil.disk_usage('/')
        disk_usage = disk.percent

        # ç½‘ç»œæµé‡
        network = psutil.net_io_counters()
        network_sent = network.bytes_sent
        network_recv = network.bytes_recv

        # ç³»ç»Ÿè´Ÿè½½
        load_avg = psutil.getloadavg()[0] if hasattr(psutil, 'getloadavg') else 0

        return SystemMetrics(
            timestamp=timestamp,
            cpu_percent=cpu_percent,
            memory_percent=memory_percent,
            disk_usage=disk_usage,
            network_sent=network_sent,
            network_recv=network_recv,
            load_avg=load_avg
        )

    async def collect_application_metrics(self) -> ApplicationMetrics:
        """æ”¶é›†åº”ç”¨æŒ‡æ ‡"""
        timestamp = datetime.now().isoformat()

        # è¿™é‡Œåº”è¯¥ä»åº”ç”¨ä¸­è·å–çœŸå®çš„æŒ‡æ ‡
        # ç°åœ¨ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        active_connections = 100  # æ¨¡æ‹Ÿæ´»è·ƒè¿æ¥æ•°
        request_count = 1000     # æ¨¡æ‹Ÿè¯·æ±‚è®¡æ•°
        response_time_avg = 150  # æ¨¡æ‹Ÿå¹³å‡å“åº”æ—¶é—´
        error_rate = 0.5         # æ¨¡æ‹Ÿé”™è¯¯ç‡
        throughput = 500         # æ¨¡æ‹Ÿååé‡

        return ApplicationMetrics(
            timestamp=timestamp,
            active_connections=active_connections,
            request_count=request_count,
            response_time_avg=response_time_avg,
            error_rate=error_rate,
            throughput=throughput
        )

    def save_metrics(self, metrics: SystemMetrics):
        """ä¿å­˜ç³»ç»ŸæŒ‡æ ‡åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO system_metrics
            (timestamp, cpu_percent, memory_percent, disk_usage, network_sent, network_recv, load_avg)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            metrics.timestamp, metrics.cpu_percent, metrics.memory_percent,
            metrics.disk_usage, metrics.network_sent, metrics.network_recv, metrics.load_avg
        ))

        conn.commit()
        conn.close()

    def save_application_metrics(self, metrics: ApplicationMetrics):
        """ä¿å­˜åº”ç”¨æŒ‡æ ‡åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO application_metrics
            (timestamp, active_connections, request_count, response_time_avg, error_rate, throughput)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            metrics.timestamp, metrics.active_connections, metrics.request_count,
            metrics.response_time_avg, metrics.error_rate, metrics.throughput
        ))

        conn.commit()
        conn.close()

    def check_alerts(self, metrics: SystemMetrics, app_metrics: ApplicationMetrics) -> List[Dict[str, Any]]:
        """æ£€æŸ¥æ€§èƒ½è­¦æŠ¥"""
        alerts = []

        # CPUä½¿ç”¨ç‡è­¦æŠ¥
        if metrics.cpu_percent > 80:
            alerts.append({
                'alert_type': 'HIGH_CPU',
                'severity': 'WARNING' if metrics.cpu_percent < 90 else 'CRITICAL',
                'message': f'CPU usage is {metrics.cpu_percent}%',
                'metric_name': 'cpu_percent',
                'metric_value': metrics.cpu_percent,
                'threshold_value': 80
            })

        # å†…å­˜ä½¿ç”¨ç‡è­¦æŠ¥
        if metrics.memory_percent > 85:
            alerts.append({
                'alert_type': 'HIGH_MEMORY',
                'severity': 'WARNING' if metrics.memory_percent < 95 else 'CRITICAL',
                'message': f'Memory usage is {metrics.memory_percent}%',
                'metric_name': 'memory_percent',
                'metric_value': metrics.memory_percent,
                'threshold_value': 85
            })

        # ç£ç›˜ä½¿ç”¨ç‡è­¦æŠ¥
        if metrics.disk_usage > 90:
            alerts.append({
                'alert_type': 'HIGH_DISK',
                'severity': 'WARNING' if metrics.disk_usage < 95 else 'CRITICAL',
                'message': f'Disk usage is {metrics.disk_usage}%',
                'metric_name': 'disk_usage',
                'metric_value': metrics.disk_usage,
                'threshold_value': 90
            })

        # å“åº”æ—¶é—´è­¦æŠ¥
        if app_metrics.response_time_avg > 1000:
            alerts.append({
                'alert_type': 'HIGH_RESPONSE_TIME',
                'severity': 'WARNING' if app_metrics.response_time_avg < 2000 else 'CRITICAL',
                'message': f'Average response time is {app_metrics.response_time_avg}ms',
                'metric_name': 'response_time_avg',
                'metric_value': app_metrics.response_time_avg,
                'threshold_value': 1000
            })

        # é”™è¯¯ç‡è­¦æŠ¥
        if app_metrics.error_rate > 5:
            alerts.append({
                'alert_type': 'HIGH_ERROR_RATE',
                'severity': 'WARNING' if app_metrics.error_rate < 10 else 'CRITICAL',
                'message': f'Error rate is {app_metrics.error_rate}%',
                'metric_name': 'error_rate',
                'metric_value': app_metrics.error_rate,
                'threshold_value': 5
            })

        # ä¿å­˜è­¦æŠ¥åˆ°æ•°æ®åº“
        for alert in alerts:
            self.save_alert(alert)

        return alerts

    def save_alert(self, alert: Dict[str, Any]):
        """ä¿å­˜è­¦æŠ¥åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO performance_alerts
            (alert_type, severity, message, metric_name, metric_value, threshold_value)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            alert['alert_type'], alert['severity'], alert['message'],
            alert['metric_name'], alert['metric_value'], alert['threshold_value']
        ))

        conn.commit()
        conn.close()

    async def monitoring_loop(self, interval: int = 60):
        """ç›‘æ§å¾ªç¯"""
        print("Starting performance monitoring...")

        while self.is_running:
            try:
                # æ”¶é›†æŒ‡æ ‡
                system_metrics = await self.collect_system_metrics()
                app_metrics = await self.collect_application_metrics()

                # ä¿å­˜æŒ‡æ ‡
                self.save_metrics(system_metrics)
                self.save_application_metrics(app_metrics)

                # æ£€æŸ¥è­¦æŠ¥
                alerts = self.check_alerts(system_metrics, app_metrics)

                # è¾“å‡ºçŠ¶æ€
                print(f"[{system_metrics.timestamp}] "
                      f"CPU: {system_metrics.cpu_percent}%, "
                      f"Memory: {system_metrics.memory_percent}%, "
                      f"Response Time: {app_metrics.response_time_avg}ms")

                # å¦‚æœæœ‰è­¦æŠ¥ï¼Œè¾“å‡ºè­¦æŠ¥ä¿¡æ¯
                for alert in alerts:
                    print(f"ğŸš¨ ALERT: {alert['message']}")

                # ç­‰å¾…ä¸‹ä¸€æ¬¡é‡‡é›†
                await asyncio.sleep(interval)

            except Exception as e:
                print(f"Error in monitoring loop: {e}")
                await asyncio.sleep(interval)

    def start_monitoring(self, interval: int = 60):
        """å¯åŠ¨ç›‘æ§"""
        if self.is_running:
            print("Monitoring is already running")
            return

        self.is_running = True
        self.monitoring_task = asyncio.create_task(self.monitoring_loop(interval))
        print(f"Performance monitoring started (interval: {interval}s)")

    def stop_monitoring(self):
        """åœæ­¢ç›‘æ§"""
        if not self.is_running:
            print("Monitoring is not running")
            return

        self.is_running = False
        if self.monitoring_task:
            self.monitoring_task.cancel()
            print("Performance monitoring stopped")

    def get_metrics_history(self, hours: int = 24) -> Dict[str, pd.DataFrame]:
        """è·å–æŒ‡æ ‡å†å²æ•°æ®"""
        conn = sqlite3.connect(self.db_path)

        # è·å–ç³»ç»ŸæŒ‡æ ‡
        system_query = f'''
            SELECT * FROM system_metrics
            WHERE timestamp >= datetime('now', '-{hours} hours')
            ORDER BY timestamp
        '''
        system_df = pd.read_sql_query(system_query, conn)

        # è·å–åº”ç”¨æŒ‡æ ‡
        app_query = f'''
            SELECT * FROM application_metrics
            WHERE timestamp >= datetime('now', '-{hours} hours')
            ORDER BY timestamp
        '''
        app_df = pd.read_sql_query(app_query, conn)

        conn.close()

        return {
            'system': system_df,
            'application': app_df
        }

    def generate_performance_report(self, hours: int = 24, output_file: str = "performance_report.md"):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        metrics_history = self.get_metrics_history(hours)

        if not metrics_history['system'].empty and not metrics_history['application'].empty:
            system_df = metrics_history['system']
            app_df = metrics_history['application']

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            report_data = {
                'period': f"{hours} hours",
                'generated_at': datetime.now().isoformat(),
                'system_stats': {
                    'avg_cpu': system_df['cpu_percent'].mean(),
                    'max_cpu': system_df['cpu_percent'].max(),
                    'avg_memory': system_df['memory_percent'].mean(),
                    'max_memory': system_df['memory_percent'].max(),
                    'avg_disk': system_df['disk_usage'].mean(),
                    'max_disk': system_df['disk_usage'].max(),
                },
                'application_stats': {
                    'avg_response_time': app_df['response_time_avg'].mean(),
                    'max_response_time': app_df['response_time_avg'].max(),
                    'avg_error_rate': app_df['error_rate'].mean(),
                    'max_error_rate': app_df['error_rate'].max(),
                    'avg_throughput': app_df['throughput'].mean(),
                    'max_throughput': app_df['throughput'].max(),
                }
            }

            # ç”ŸæˆæŠ¥å‘Š
            with open(output_file, 'w') as f:
                f.write("# Performance Monitoring Report\n\n")
                f.write(f"Generated: {report_data['generated_at']}\n")
                f.write(f"Period: Last {report_data['period']}\n\n")

                f.write("## System Metrics\n\n")
                f.write("### CPU Usage\n")
                f.write(f"- Average: {report_data['system_stats']['avg_cpu']:.2f}%\n")
                f.write(f"- Maximum: {report_data['system_stats']['max_cpu']:.2f}%\n\n")

                f.write("### Memory Usage\n")
                f.write(f"- Average: {report_data['system_stats']['avg_memory']:.2f}%\n")
                f.write(f"- Maximum: {report_data['system_stats']['max_memory']:.2f}%\n\n")

                f.write("### Disk Usage\n")
                f.write(f"- Average: {report_data['system_stats']['avg_disk']:.2f}%\n")
                f.write(f"- Maximum: {report_data['system_stats']['max_disk']:.2f}%\n\n")

                f.write("## Application Metrics\n\n")
                f.write("### Response Time\n")
                f.write(f"- Average: {report_data['application_stats']['avg_response_time']:.2f}ms\n")
                f.write(f"- Maximum: {report_data['application_stats']['max_response_time']:.2f}ms\n\n")

                f.write("### Error Rate\n")
                f.write(f"- Average: {report_data['application_stats']['avg_error_rate']:.2f}%\n")
                f.write(f"- Maximum: {report_data['application_stats']['max_error_rate']:.2f}%\n\n")

                f.write("### Throughput\n")
                f.write(f"- Average: {report_data['application_stats']['avg_throughput']:.2f} req/s\n")
                f.write(f"- Maximum: {report_data['application_stats']['max_throughput']:.2f} req/s\n\n")

                # ç”Ÿæˆè¶‹åŠ¿å›¾
                self.generate_metrics_trend_chart(metrics_history, "performance_trend.png")

                f.write("## Visualizations\n\n")
                f.write("![Performance Trend](performance_trend.png)\n")

            print(f"Performance report generated: {output_file}")

    def generate_metrics_trend_chart(self, metrics_history: Dict[str, pd.DataFrame], output_file: str):
        """ç”ŸæˆæŒ‡æ ‡è¶‹åŠ¿å›¾"""
        system_df = metrics_history['system']
        app_df = metrics_history['application']

        if system_df.empty or app_df.empty:
            print("No data available for trend chart")
            return

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # è½¬æ¢æ—¶é—´æˆ³
        system_df['timestamp'] = pd.to_datetime(system_df['timestamp'])
        app_df['timestamp'] = pd.to_datetime(app_df['timestamp'])

        # CPUä½¿ç”¨ç‡è¶‹åŠ¿
        ax1.plot(system_df['timestamp'], system_df['cpu_percent'], 'b-', linewidth=2)
        ax1.axhline(y=80, color='r', linestyle='--', alpha=0.7, label='Warning Threshold')
        ax1.set_title('CPU Usage Trend')
        ax1.set_ylabel('CPU Usage (%)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)

        # å†…å­˜ä½¿ç”¨ç‡è¶‹åŠ¿
        ax2.plot(system_df['timestamp'], system_df['memory_percent'], 'g-', linewidth=2)
        ax2.axhline(y=85, color='r', linestyle='--', alpha=0.7, label='Warning Threshold')
        ax2.set_title('Memory Usage Trend')
        ax2.set_ylabel('Memory Usage (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # å“åº”æ—¶é—´è¶‹åŠ¿
        ax3.plot(app_df['timestamp'], app_df['response_time_avg'], 'r-', linewidth=2)
        ax3.axhline(y=1000, color='r', linestyle='--', alpha=0.7, label='Warning Threshold')
        ax3.set_title('Response Time Trend')
        ax3.set_ylabel('Response Time (ms)')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # ååé‡è¶‹åŠ¿
        ax4.plot(app_df['timestamp'], app_df['throughput'], 'purple', linewidth=2)
        ax4.set_title('Throughput Trend')
        ax4.set_ylabel('Throughput (req/s)')
        ax4.grid(True, alpha=0.3)

        # æ ¼å¼åŒ–xè½´
        for ax in [ax1, ax2, ax3, ax4]:
            ax.tick_params(axis='x', rotation=45)

        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"Metrics trend chart generated: {output_file}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    monitor = PerformanceMonitor()

    try:
        # å¯åŠ¨ç›‘æ§
        monitor.start_monitoring(interval=30)

        # è¿è¡Œä¸€æ®µæ—¶é—´
        await asyncio.sleep(300)  # 5åˆ†é’Ÿ

        # ç”ŸæˆæŠ¥å‘Š
        monitor.generate_performance_report(hours=1)

    finally:
        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()

if __name__ == "__main__":
    asyncio.run(main())
```

---

## æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»äº†è¶³çƒé¢„æµ‹ç³»ç»Ÿçš„æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆï¼ŒåŒ…æ‹¬ï¼š

1. **Locustæ€§èƒ½æµ‹è¯•**: å®Œæ•´çš„ç”¨æˆ·è¡Œä¸ºæ¨¡æ‹Ÿå’Œè´Ÿè½½æµ‹è¯•
2. **APIåŸºå‡†æµ‹è¯•**: è¯¦ç»†çš„ç«¯ç‚¹æ€§èƒ½åˆ†æå’Œä¼˜åŒ–å»ºè®®
3. **æ•°æ®åº“æ€§èƒ½æµ‹è¯•**: æŸ¥è¯¢ä¼˜åŒ–ã€è¿æ¥æ± å’Œç´¢å¼•ä¼˜åŒ–
4. **ç›‘æ§ä¸åˆ†æ**: å®æ—¶æ€§èƒ½ç›‘æ§å’Œè¶‹åŠ¿åˆ†æ

è¿™äº›æµ‹è¯•å’Œç›‘æ§å·¥å…·å¯ä»¥å¸®åŠ©ç¡®ä¿ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„ç¨³å®šæ€§å’Œæ€§èƒ½ï¼Œä¸ºæŒç»­ä¼˜åŒ–æä¾›æ•°æ®æ”¯æŒã€‚