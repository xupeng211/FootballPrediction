# ğŸ“‹ é˜¶æ®µ2ï¼šæµ‹è¯•æœ‰æ•ˆæ€§æå‡ - é£é™©æ§åˆ¶å®æ–½æ–¹æ¡ˆ

## ğŸ¯ æ ¸å¿ƒç›®æ ‡
**ä»"å•ç»´åº¦è¦†ç›–ç‡"å‡çº§ä¸º"å¤šç»´åº¦æµ‹è¯•è´¨é‡è¯„ä¼°"ï¼Œé‡ç‚¹æ§åˆ¶å®æ–½é£é™©**

### ğŸ”§ æ ¸å¿ƒåŸåˆ™
- **é€‰æ‹©æ€§æ‰§è¡Œ**ï¼šåªåœ¨å…³é”®æ¨¡å—è¿è¡Œï¼Œé¿å…å…¨é‡æ‰«æ
- **å¢é‡æ¨¡å¼**ï¼šä¼˜å…ˆæ£€æµ‹ä¿®æ”¹å†…å®¹ï¼Œæé«˜æ•ˆç‡
- **ç¯å¢ƒéš”ç¦»**ï¼šç¡®ä¿æµ‹è¯•ç¯å¢ƒä¸€è‡´æ€§
- **æ¸è¿›é›†æˆ**ï¼šéé˜»å¡æ¨¡å¼å…ˆè¡Œï¼Œç¨³å®šåå†å‡çº§

---

## ğŸ› ï¸ æ¨¡å—1ï¼šé€‰æ‹©æ€§çªå˜æµ‹è¯•ï¼ˆç¬¬1å‘¨ï¼‰

### 1.1 é…ç½®æ–‡ä»¶è®¾è®¡
**æ–‡ä»¶ï¼š`mutmut.ini`**
```ini
[mutmut]
paths_to_mutate=src/data/,src/models/,src/services/
tests_dir=tests/
backup_count=3
max_workers=4  # é™åˆ¶å¹¶å‘æ•°
timeout=30     # å•ä¸ªçªå˜æµ‹è¯•è¶…æ—¶æ—¶é—´

# æ’é™¤ä¸é‡è¦çš„æ–‡ä»¶å’Œå¤æ‚æ¨¡å—
exclude_lines=
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
    if __name__ == .__main__:
    # æ’é™¤æ•°æ®åº“è¿æ¥å’Œå¤–éƒ¨APIè°ƒç”¨
    def connect_to_database
    def call_external_api
```

### 1.2 é€‰æ‹©æ€§çªå˜æµ‹è¯•å™¨
**æ–‡ä»¶ï¼š`src/ai/mutation_tester.py`**
```python
#!/usr/bin/env python3
"""
é€‰æ‹©æ€§çªå˜æµ‹è¯•å™¨ - åªåœ¨å…³é”®æ¨¡å—è¿è¡Œï¼Œæ§åˆ¶æ‰§è¡Œæ—¶é—´å’Œé£é™©
"""

import subprocess
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Set
from dataclasses import dataclass
import re

logger = logging.getLogger(__name__)

@dataclass
class MutationConfig:
    """çªå˜æµ‹è¯•é…ç½®"""
    target_modules: List[str] = ["src/data/", "src/models/", "src/services/"]
    max_workers: int = 4
    timeout_per_test: int = 30  # ç§’
    total_timeout: int = 300   # 5åˆ†é’Ÿæ€»è¶…æ—¶
    exclude_patterns: Set[str] = None

    def __post_init__(self):
        if self.exclude_patterns is None:
            self.exclude_patterns = {
                "def connect_to_",
                "def call_external_",
                "import.*database",
                "import.*kafka",
                "import.*redis"
            }

class SelectiveMutationTester:
    """é€‰æ‹©æ€§çªå˜æµ‹è¯•å™¨"""

    def __init__(self, config: Optional[MutationConfig] = None):
        self.config = config or MutationConfig()
        self.results_dir = Path("docs/_reports/mutation")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.baseline_file = self.results_dir / "baseline_mutation.json"

    def should_test_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¿›è¡Œçªå˜æµ‹è¯•"""
        # åªæµ‹è¯•ç›®æ ‡æ¨¡å—ä¸­çš„æ–‡ä»¶
        if not any(file_path.startswith(module) for module in self.config.target_modules):
            return False

        # æ’é™¤ç¬¦åˆæ¨¡å¼çš„æ–‡ä»¶
        for pattern in self.config.exclude_patterns:
            if re.search(pattern, file_path):
                return False

        return True

    def get_changed_files(self) -> List[str]:
        """è·å–æœ¬æ¬¡æäº¤ä¿®æ”¹çš„æ–‡ä»¶ï¼ˆå¢é‡æ¨¡å¼ï¼‰"""
        try:
            # ä½¿ç”¨gitè·å–ä¿®æ”¹çš„æ–‡ä»¶
            result = subprocess.run(
                ["git", "diff", "--name-only", "HEAD~1", "HEAD"],
                capture_output=True, text=True, timeout=30
            )
            if result.returncode == 0:
                changed_files = result.stdout.strip().split('\n')
                # è¿‡æ»¤å‡ºéœ€è¦æµ‹è¯•çš„Pythonæ–‡ä»¶
                return [f for f in changed_files if f.endswith('.py') and self.should_test_file(f)]
        except Exception as e:
            logger.warning(f"Failed to get changed files: {e}")

        return []

    def run_mutation_tests(self, incremental: bool = True) -> Dict:
        """è¿è¡Œçªå˜æµ‹è¯•"""
        start_time = time.time()

        # ç¡®å®šæµ‹è¯•èŒƒå›´
        if incremental:
            test_files = self.get_changed_files()
            if not test_files:
                logger.info("No changed files to test, using baseline modules")
                test_files = []
            else:
                logger.info(f"Running incremental mutation test on {len(test_files)} files")
        else:
            test_files = []
            logger.info("Running full mutation test on baseline modules")

        # æ„å»ºmutmutå‘½ä»¤
        cmd = [
            "python", "-m", "mutmut", "run",
            "--paths-to-mutate", ",".join(self.config.target_modules),
            "--max-workers", str(self.config.max_workers),
            "--test-time-base", str(self.config.timeout_per_test),
        ]

        # å¦‚æœæ˜¯å¢é‡æµ‹è¯•ï¼Œæ·»åŠ ç‰¹å®šæ–‡ä»¶
        if test_files:
            cmd.extend(["--tests-to-run", ",".join(f"tests/test_{Path(f).stem}.py" for f in test_files)])

        logger.info(f"Starting mutation test with timeout {self.config.total_timeout}s")

        try:
            # è¿è¡Œçªå˜æµ‹è¯•ï¼Œå¸¦è¶…æ—¶æ§åˆ¶
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=self.config.total_timeout,
                cwd="."
            )

            execution_time = time.time() - start_time

            # è§£æç»“æœ
            mutation_results = self._parse_mutation_results(result.stdout, result.stderr)

            # ç”ŸæˆæŠ¥å‘Š
            report = {
                "execution_mode": "incremental" if incremental else "full",
                "test_files": test_files,
                "execution_time": execution_time,
                "timeout_used": self.config.total_timeout,
                "results": mutation_results,
                "timestamp": time.time()
            }

            # ä¿å­˜ç»“æœ
            self._save_results(report)

            return report

        except subprocess.TimeoutExpired:
            logger.error(f"Mutation test timed out after {self.config.total_timeout}s")
            return {
                "error": "TIMEOUT",
                "execution_time": time.time() - start_time,
                "timeout_used": self.config.total_timeout
            }
        except Exception as e:
            logger.error(f"Mutation test failed: {e}")
            return {"error": str(e)}

    def _parse_mutation_results(self, stdout: str, stderr: str) -> Dict:
        """è§£æmutmutè¾“å‡ºç»“æœ"""
        try:
            # ä½¿ç”¨mutmutçš„resultå‘½ä»¤è·å–è¯¦ç»†ç»“æœ
            result_cmd = ["python", "-m", "mutmut", "result", "--json"]
            result = subprocess.run(result_cmd, capture_output=True, text=True, timeout=30)

            if result.returncode == 0:
                return json.loads(result.stdout)
            else:
                # å›é€€åˆ°åŸºç¡€è§£æ
                return self._basic_parse(stdout)
        except Exception as e:
            logger.warning(f"Failed to parse mutation results: {e}")
            return self._basic_parse(stdout)

    def _basic_parse(self, stdout: str) -> Dict:
        """åŸºç¡€çš„è¾“å‡ºè§£æ"""
        lines = stdout.split('\n')
        killed = 0
        survived = 0
        total = 0

        for line in lines:
            if "killed mutants" in line.lower():
                killed = int(line.split(':')[1].strip())
            elif "survived mutants" in line.lower():
                survived = int(line.split(':')[1].strip())
            elif "total mutants" in line.lower():
                total = int(line.split(':')[1].strip())

        return {
            "total": total,
            "killed": killed,
            "survived": survived,
            "timeout": total - killed - survived,
            "score": (killed / total * 100) if total > 0 else 0
        }

    def _save_results(self, results: Dict):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"mutation_results_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # æ›´æ–°æœ€æ–°ç»“æœé“¾æ¥
        latest_file = self.results_dir / "latest_mutation_results.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Mutation results saved to {result_file}")

    def get_mutation_score(self) -> float:
        """è·å–æœ€æ–°çš„Mutation Score"""
        latest_file = self.results_dir / "latest_mutation_results.json"
        if latest_file.exists():
            try:
                with open(latest_file, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                    if "results" in results and "score" in results["results"]:
                        return float(results["results"]["score"])
            except Exception as e:
                logger.error(f"Failed to read mutation score: {e}")

        return 0.0

    def generate_mutation_report(self) -> str:
        """ç”Ÿæˆçªå˜æµ‹è¯•æŠ¥å‘Š"""
        latest_file = self.results_dir / "latest_mutation_results.json"
        if not latest_file.exists():
            return "No mutation test results available"

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            mutation_data = results.get("results", {})

            report = f"""
## ğŸ”¬ çªå˜æµ‹è¯•ç»“æœ

### æ‰§è¡Œä¿¡æ¯
- **æ‰§è¡Œæ¨¡å¼**: {results.get('execution_mode', 'unknown')}
- **æ‰§è¡Œæ—¶é—´**: {results.get('execution_time', 0):.1f}ç§’
- **è¶…æ—¶è®¾ç½®**: {results.get('timeout_used', 0)}ç§’
- **æµ‹è¯•æ–‡ä»¶**: {len(results.get('test_files', []))}ä¸ª

### æµ‹è¯•ç»“æœ
- **æ€»çªå˜æ•°**: {mutation_data.get('total', 0)}
- **è¢«æ€æ­»**: {mutation_data.get('killed', 0)} âœ…
- **å­˜æ´»çªå˜**: {mutation_data.get('survived', 0)} âš ï¸
- **è¶…æ—¶çªå˜**: {mutation_data.get('timeout', 0)} â°
- **Mutation Score**: {mutation_data.get('score', 0):.1f}%

### é£é™©è¯„ä¼°
- **é«˜é£é™©**: å­˜æ´»{mutation_data.get('survived', 0)}ä¸ªçªå˜ï¼Œéœ€è¦åŠ å¼ºæµ‹è¯•
- **æ•ˆç‡**: æ‰§è¡Œæ—¶é—´{results.get('execution_time', 0):.1f}ç§’ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…

### å»ºè®®è¡ŒåŠ¨
{self._generate_recommendations(mutation_data)}
"""
            return report

        except Exception as e:
            logger.error(f"Failed to generate mutation report: {e}")
            return "Failed to generate mutation report"

    def _generate_recommendations(self, mutation_data: Dict) -> str:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        score = mutation_data.get('score', 0)
        survived = mutation_data.get('survived', 0)

        if score >= 80:
            return "âœ… æµ‹è¯•è´¨é‡ä¼˜ç§€ï¼Œç»§ç»­ä¿æŒ"
        elif score >= 60:
            return f"âš ï¸ å»ºè®®åŠ å¼º{survived}ä¸ªå­˜æ´»çªå˜ç›¸å…³çš„æµ‹è¯•ç”¨ä¾‹"
        else:
            return f"ğŸš¨ æµ‹è¯•è¦†ç›–ä¸è¶³ï¼Œè¯·é‡ç‚¹å…³æ³¨{survived}ä¸ªå­˜æ´»çªå˜ï¼Œè¡¥å……æµ‹è¯•ç”¨ä¾‹"


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse

    parser = argparse.ArgumentParser(description="Selective Mutation Testing")
    parser.add_argument("--incremental", action="store_true", default=True,
                       help="Run incremental mutation test (default)")
    parser.add_argument("--full", action="store_true",
                       help="Run full mutation test on baseline modules")
    parser.add_argument("--score-only", action="store_true",
                       help="Only show current mutation score")

    args = parser.parse_args()

    tester = SelectiveMutationTester()

    if args.score_only:
        score = tester.get_mutation_score()
        print(f"Current Mutation Score: {score:.1f}%")
        return

    incremental_mode = args.incremental and not args.full
    results = tester.run_mutation_tests(incremental=incremental_mode)

    print(tester.generate_mutation_report())


if __name__ == "__main__":
    main()
```

---

## ğŸ› ï¸ æ¨¡å—2ï¼šæ™ºèƒ½Flaky Testæ£€æµ‹ï¼ˆç¬¬2å‘¨ï¼‰

### 2.1 é…ç½®é©±åŠ¨çš„Flakyæ£€æµ‹å™¨
**æ–‡ä»¶ï¼š`src/ai/flaky_test_detector.py`**
```python
#!/usr/bin/env python3
"""
æ™ºèƒ½Flaky Testæ£€æµ‹å™¨ - åŸºäºå†å²æ•°æ®å’Œé€‰æ‹©æ€§æ£€æµ‹ï¼Œé¿å…è¯¯æŠ¥å’Œæ€§èƒ½å¼€é”€
"""

import subprocess
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Set
from dataclasses import dataclass
from collections import defaultdict
import re

logger = logging.getLogger(__name__)

@dataclass
class FlakyDetectionConfig:
    """Flakyæ£€æµ‹é…ç½®"""
    # é€‰æ‹©æ€§æ£€æµ‹ï¼šåªæ£€æµ‹å…³é”®æµ‹è¯•æ–‡ä»¶
    target_test_patterns: List[str] = [
        "tests/test_data_*.py",
        "tests/test_models_*.py",
        "tests/test_services_*.py"
    ]

    # è¿è¡Œæ§åˆ¶
    max_runs: int = 3  # å‡å°‘åˆ°3æ¬¡ï¼Œé™ä½å¼€é”€
    timeout_per_run: int = 60  # å•æ¬¡è¿è¡Œè¶…æ—¶
    total_timeout: int = 300    # æ€»è¶…æ—¶æ§åˆ¶

    # å¤–éƒ¨ä¾èµ–æ ‡è®°
    external_service_patterns: Set[str] = None

    # å†å²æ•°æ®è¦æ±‚
    min_historical_runs: int = 3  # è‡³å°‘3æ¬¡å†å²æ•°æ®æ‰åˆ¤å®š

    def __post_init__(self):
        if self.external_service_patterns is None:
            self.external_service_patterns = {
                "test_.*database.*",
                "test_.*kafka.*",
                "test_.*redis.*",
                "test_.*api.*",
                "test_.*external.*"
            }

class SmartFlakyTestDetector:
    """æ™ºèƒ½Flaky Testæ£€æµ‹å™¨"""

    def __init__(self, config: Optional[FlakyDetectionConfig] = None):
        self.config = config or FlakyDetectionConfig()
        self.results_dir = Path("docs/_reports/flaky")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.history_file = self.results_dir / "test_history.json"
        self.flaky_database = self.results_dir / "flaky_database.json"

    def load_test_history(self) -> Dict:
        """åŠ è½½æµ‹è¯•å†å²æ•°æ®"""
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load test history: {e}")

        return {}

    def save_test_history(self, history: Dict):
        """ä¿å­˜æµ‹è¯•å†å²æ•°æ®"""
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Failed to save test history: {e}")

    def get_target_test_files(self) -> List[str]:
        """è·å–ç›®æ ‡æµ‹è¯•æ–‡ä»¶"""
        target_files = []

        # ä½¿ç”¨globåŒ¹é…ç›®æ ‡æµ‹è¯•æ–‡ä»¶
        import glob
        for pattern in self.config.target_test_patterns:
            target_files.extend(glob.glob(pattern))

        # å»é‡å¹¶è¿‡æ»¤å­˜åœ¨çš„æ–‡ä»¶
        return list(set(f for f in target_files if Path(f).exists()))

    def is_external_service_test(self, test_file: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤–éƒ¨æœåŠ¡æµ‹è¯•"""
        filename = Path(test_file).name.lower()
        for pattern in self.config.external_service_patterns:
            if re.match(pattern, filename):
                return True
        return False

    def run_test_multiple_times(self, test_path: str) -> Dict:
        """å¤šæ¬¡è¿è¡Œå•ä¸ªæµ‹è¯•"""
        results = {
            "test_path": test_path,
            "runs": [],
            "passed": 0,
            "failed": 0,
            "execution_times": [],
            "is_flaky": False,
            "is_external_test": self.is_external_service_test(test_path)
        }

        logger.info(f"Running test {test_path} {self.config.max_runs} times...")

        for run in range(self.config.max_runs):
            try:
                start_time = time.time()

                # è¿è¡Œå•ä¸ªæµ‹è¯•æ–‡ä»¶
                result = subprocess.run(
                    ["python", "-m", "pytest", test_path, "--tb=short", "-v"],
                    capture_output=True,
                    text=True,
                    timeout=self.config.timeout_per_run,
                    cwd="."
                )

                execution_time = time.time() - start_time

                run_result = {
                    "run_number": run + 1,
                    "return_code": result.returncode,
                    "execution_time": execution_time,
                    "passed": result.returncode == 0,
                    "stdout": result.stdout[:1000],  # æˆªæ–­é•¿è¾“å‡º
                    "stderr": result.stderr[:1000]
                }

                results["runs"].append(run_result)
                results["execution_times"].append(execution_time)

                if result.returncode == 0:
                    results["passed"] += 1
                else:
                    results["failed"] += 1

            except subprocess.TimeoutExpired:
                logger.warning(f"Test {test_path} timed out on run {run + 1}")
                results["runs"].append({
                    "run_number": run + 1,
                    "error": "TIMEOUT",
                    "execution_time": self.config.timeout_per_run,
                    "passed": False
                })
                results["failed"] += 1
                results["execution_times"].append(self.config.timeout_per_run)

            except Exception as e:
                logger.error(f"Test {test_path} failed on run {run + 1}: {e}")
                results["runs"].append({
                    "run_number": run + 1,
                    "error": str(e),
                    "execution_time": 0,
                    "passed": False
                })
                results["failed"] += 1
                results["execution_times"].append(0)

        # åŸºäºå¤šæ¬¡è¿è¡Œç»“æœåˆ¤æ–­æ˜¯å¦ä¸ºflaky
        results["is_flaky"] = self._is_flaky_based_on_results(results)

        return results

    def _is_flaky_based_on_results(self, results: Dict) -> bool:
        """åŸºäºè¿è¡Œç»“æœåˆ¤æ–­æ˜¯å¦ä¸ºflaky"""
        # å¤–éƒ¨æœåŠ¡æµ‹è¯•ä¸è®¡å…¥ä¸»flakyæŒ‡æ ‡
        if results.get("is_external_test", False):
            return False

        passed = results["passed"]
        failed = results["failed"]
        total_runs = len(results["runs"])

        # å¦‚æœæœ‰æˆåŠŸä¹Ÿæœ‰å¤±è´¥ï¼Œåˆ™ä¸ºflaky
        if passed > 0 and failed > 0:
            return True

        # å¦‚æœæ‰€æœ‰è¿è¡Œéƒ½å¤±è´¥ï¼Œä½†æ‰§è¡Œæ—¶é—´å·®å¼‚å¾ˆå¤§ï¼Œä¹Ÿå¯èƒ½æ˜¯flaky
        if failed == total_runs and total_runs > 1:
            times = results["execution_times"]
            if max(times) > min(times) * 2:  # æ‰§è¡Œæ—¶é—´å·®å¼‚è¶…è¿‡2å€
                return True

        return False

    def detect_flaky_tests(self, incremental: bool = True) -> Dict:
        """æ£€æµ‹Flakyæµ‹è¯•"""
        start_time = time.time()

        # è·å–ç›®æ ‡æµ‹è¯•æ–‡ä»¶
        target_files = self.get_target_test_files()

        # åŠ è½½å†å²æ•°æ®
        history = self.load_test_history()

        # æ‰§è¡Œæ£€æµ‹
        flaky_results = []
        detection_summary = {
            "total_tests": len(target_files),
            "flaky_tests": 0,
            "external_tests": 0,
            "execution_time": 0,
            "detection_mode": "incremental" if incremental else "full"
        }

        for i, test_file in enumerate(target_files):
            logger.info(f"Processing test {i+1}/{len(target_files)}: {test_file}")

            # æ£€æŸ¥è¶…æ—¶
            if time.time() - start_time > self.config.total_timeout:
                logger.warning(f"Flaky detection timed out after {self.config.total_timeout}s")
                detection_summary["timeout"] = True
                break

            # è·³è¿‡å¤–éƒ¨æœåŠ¡æµ‹è¯•ï¼ˆå•ç‹¬æ ‡è®°ï¼‰
            if self.is_external_service_test(test_file):
                detection_summary["external_tests"] += 1
                continue

            # è¿è¡Œå¤šæ¬¡æ£€æµ‹
            test_results = self.run_test_multiple_times(test_file)

            # æ›´æ–°å†å²æ•°æ®
            if test_file not in history:
                history[test_file] = []
            history[test_file].append({
                "timestamp": time.time(),
                "results": test_results
            })

            # å¦‚æœæ˜¯flakyä¸”æ»¡è¶³å†å²æ•°æ®è¦æ±‚ï¼Œåˆ™è®°å½•
            if test_results["is_flaky"]:
                historical_runs = len(history[test_file])
                if historical_runs >= self.config.min_historical_runs:
                    # æ£€æŸ¥å†å²ä¸€è‡´æ€§
                    flaky_consistency = self._check_flaky_consistency(history[test_file])
                    if flaky_consistency >= 0.5:  # 50%ä»¥ä¸Šçš„å†å²è¿è¡Œéƒ½æ˜¯flaky
                        flaky_results.append(test_results)
                        detection_summary["flaky_tests"] += 1
                        logger.warning(f"Confirmed flaky test: {test_file}")
                else:
                    logger.info(f"Potential flaky test (needs more data): {test_file}")

        # ä¿å­˜å†å²æ•°æ®
        self.save_test_history(history)

        # ä¿å­˜ç»“æœ
        detection_summary["execution_time"] = time.time() - start_time
        final_results = {
            "summary": detection_summary,
            "flaky_tests": flaky_results,
            "timestamp": time.time()
        }

        self._save_flaky_results(final_results)

        return final_results

    def _check_flaky_consistency(self, history: List[Dict]) -> float:
        """æ£€æŸ¥flakyä¸€è‡´æ€§"""
        if not history:
            return 0.0

        flaky_count = sum(1 for record in history if record["results"]["is_flaky"])
        return flaky_count / len(history)

    def _save_flaky_results(self, results: Dict):
        """ä¿å­˜flakyæ£€æµ‹ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"flaky_results_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # æ›´æ–°æœ€æ–°ç»“æœ
        latest_file = self.results_dir / "latest_flaky_results.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Flaky detection results saved to {result_file}")

    def generate_flaky_report(self) -> str:
        """ç”ŸæˆFlakyæµ‹è¯•æŠ¥å‘Š"""
        latest_file = self.results_dir / "latest_flaky_results.json"
        if not latest_file.exists():
            return "No flaky test detection results available"

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            summary = results["summary"]

            report = f"""
## ğŸ”„ Flaky Test æ£€æµ‹ç»“æœ

### æ‰§è¡Œä¿¡æ¯
- **æ£€æµ‹æ¨¡å¼**: {summary.get('detection_mode', 'unknown')}
- **æ‰§è¡Œæ—¶é—´**: {summary.get('execution_time', 0):.1f}ç§’
- **æ€»æµ‹è¯•æ•°**: {summary.get('total_tests', 0)}
- **è¶…æ—¶è®¾ç½®**: {self.config.total_timeout}ç§’

### æ£€æµ‹ç»“æœ
- **Flakyæµ‹è¯•**: {summary.get('flaky_tests', 0)}ä¸ª âš ï¸
- **å¤–éƒ¨æœåŠ¡æµ‹è¯•**: {summary.get('external_tests', 0)}ä¸ª ğŸ”Œ
- **ç¨³å®šæµ‹è¯•**: {summary.get('total_tests', 0) - summary.get('flaky_tests', 0) - summary.get('external_tests', 0)}ä¸ª âœ…

### é£é™©è¯„ä¼°
- **æ£€æµ‹è¦†ç›–ç‡**: {(summary.get('total_tests', 0) / max(len(self.get_target_test_files()), 1) * 100):.1f}%
- **Flakyæ¯”ä¾‹**: {(summary.get('flaky_tests', 0) / max(summary.get('total_tests', 0), 1) * 100):.1f}%
"""

            if summary.get('timeout', False):
                report += "- **æ‰§è¡Œè¶…æ—¶**: éƒ¨åˆ†æµ‹è¯•å› è¶…æ—¶æœªå®Œæˆæ£€æµ‹ â°\n"

            # æ·»åŠ å…·ä½“flakyæµ‹è¯•ä¿¡æ¯
            if results.get("flaky_tests"):
                report += "\n### ğŸš¨ å…·ä½“Flakyæµ‹è¯•\n"
                for flaky_test in results["flaky_tests"]:
                    test_path = flaky_test["test_path"]
                    failed_runs = flaky_test["failed"]
                    total_runs = len(flaky_test["runs"])
                    avg_time = sum(flaky_test["execution_times"]) / total_runs

                    report += f"- **{test_path}**: {failed_runs}/{total_runs}æ¬¡å¤±è´¥ï¼Œå¹³å‡æ‰§è¡Œæ—¶é—´{avg_time:.1f}ç§’\n"

            report += f"""
### æ”¹è¿›å»ºè®®
{self._generate_flaky_recommendations(summary)}

### ç¯å¢ƒè¯´æ˜
- **æ£€æµ‹é…ç½®**: {self.config.max_runs}æ¬¡è¿è¡Œï¼Œè¦æ±‚{self.config.min_historical_runs}æ¬¡å†å²æ•°æ®
- **å¤–éƒ¨æœåŠ¡æµ‹è¯•**: å·²å•ç‹¬æ ‡è®°ï¼Œä¸è®¡å…¥ä¸»è¦flakyæŒ‡æ ‡
- **è¶…æ—¶æ§åˆ¶**: å•æ¬¡{self.config.timeout_per_run}ç§’ï¼Œæ€»{self.config.total_timeout}ç§’
"""
            return report

        except Exception as e:
            logger.error(f"Failed to generate flaky report: {e}")
            return "Failed to generate flaky report"

    def _generate_flaky_recommendations(self, summary: Dict) -> str:
        """ç”ŸæˆFlakyæµ‹è¯•æ”¹è¿›å»ºè®®"""
        flaky_count = summary.get('flaky_tests', 0)
        total_tests = summary.get('total_tests', 0)

        if flaky_count == 0:
            return "âœ… æœªæ£€æµ‹åˆ°Flakyæµ‹è¯•ï¼Œæµ‹è¯•ç¨³å®šæ€§è‰¯å¥½"
        elif flaky_count <= 2:
            return f"âš ï¸ å‘ç°{flaky_count}ä¸ªFlakyæµ‹è¯•ï¼Œå»ºè®®ä¼˜åŒ–æµ‹è¯•é€»è¾‘ï¼Œæ·»åŠ é€‚å½“çš„ç­‰å¾…æˆ–é‡è¯•æœºåˆ¶"
        else:
            return f"ğŸš¨ å‘ç°{flaky_count}ä¸ªFlakyæµ‹è¯•ï¼Œå æ¯”{(flaky_count/total_tests*100):.1f}%ï¼Œå»ºè®®ä¼˜å…ˆä¿®å¤è¿™äº›æµ‹è¯•çš„ä¸ç¨³å®šæ€§"


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse

    parser = argparse.ArgumentParser(description="Smart Flaky Test Detection")
    parser.add_argument("--incremental", action="store_true", default=True,
                       help="Run incremental flaky detection (default)")
    parser.add_argument("--full", action="store_true",
                       help="Run full flaky detection")
    parser.add_argument("--report-only", action="store_true",
                       help="Only show flaky test report")

    args = parser.parse_args()

    detector = SmartFlakyTestDetector()

    if args.report_only:
        print(detector.generate_flaky_report())
        return

    incremental_mode = args.incremental and not args.full
    results = detector.detect_flaky_tests(incremental=incremental_mode)

    print(detector.generate_flaky_report())


if __name__ == "__main__":
    main()
```

---

## ğŸ› ï¸ æ¨¡å—3ï¼šç›¸å¯¹æ€§èƒ½å›å½’æ£€æµ‹ï¼ˆç¬¬2-3å‘¨ï¼‰

### 3.1 ç›¸å¯¹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨
**æ–‡ä»¶ï¼š`src/ai/performance_benchmark.py`**
```python
#!/usr/bin/env python3
"""
ç›¸å¯¹æ€§èƒ½å›å½’æ£€æµ‹å™¨ - ä½¿ç”¨ç›¸å¯¹å¯¹æ¯”å’ŒåŸºçº¿æ¯”è¾ƒï¼Œç¡®ä¿ç¯å¢ƒä¸€è‡´æ€§
"""

import subprocess
import json
import time
import logging
import psutil
import platform
from pathlib import Path
from typing import Dict, List, Optional, Callable, Any
from dataclasses import dataclass, asdict
from contextlib import contextmanager
import statistics

logger = logging.getLogger(__name__)

@dataclass
class PerformanceConfig:
    """æ€§èƒ½æµ‹è¯•é…ç½®"""
    # å…³é”®å‡½æ•°åˆ—è¡¨
    critical_functions: List[str] = None

    # ç›¸å¯¹æ€§èƒ½é˜ˆå€¼ï¼ˆç™¾åˆ†æ¯”å˜åŒ–ï¼‰
    warning_threshold: float = 10.0  # 10%å˜åŒ–è­¦å‘Š
    critical_threshold: float = 20.0  # 20%å˜åŒ–ä¸¥é‡

    # æµ‹è¯•æ§åˆ¶
    warmup_runs: int = 3  # é¢„çƒ­è¿è¡Œæ¬¡æ•°
    measurement_runs: int = 5  # æµ‹é‡è¿è¡Œæ¬¡æ•°
    timeout_per_function: int = 30  # å•å‡½æ•°è¶…æ—¶

    # ç¯å¢ƒä¿¡æ¯
    include_environment_info: bool = True

    def __post_init__(self):
        if self.critical_functions is None:
            self.critical_functions = [
                "src/data/processing/football_data_cleaner.py:clean_data",
                "src/models/prediction_service.py:predict_match",
                "src/services/data_processing.py:process_pipeline",
                "src/features/feature_calculator.py:calculate_features"
            ]

@contextmanager
def measure_performance():
    """æ€§èƒ½æµ‹é‡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    start_time = time.time()
    start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
    start_cpu = psutil.cpu_percent()

    try:
        yield
    finally:
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent()

        execution_time = end_time - start_time
        memory_delta = end_memory - start_memory
        cpu_usage = end_cpu

class RelativePerformanceBenchmark:
    """ç›¸å¯¹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self, config: Optional[PerformanceConfig] = None):
        self.config = config or PerformanceConfig()
        self.results_dir = Path("docs/_reports/performance")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        self.baseline_file = self.results_dir / "performance_baseline.json"
        self.environment_file = self.results_dir / "environment_info.json"

        # è®°å½•ç¯å¢ƒä¿¡æ¯
        if self.config.include_environment_info:
            self._record_environment_info()

    def _record_environment_info(self):
        """è®°å½•ç¯å¢ƒä¿¡æ¯"""
        env_info = {
            "timestamp": time.time(),
            "platform": platform.platform(),
            "python_version": platform.python_version(),
            "cpu_count": psutil.cpu_count(),
            "memory_total": psutil.virtual_memory().total / 1024 / 1024 / 1024,  # GB
            "architecture": platform.architecture(),
            "machine": platform.machine()
        }

        try:
            with open(self.environment_file, 'w', encoding='utf-8') as f:
                json.dump(env_info, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"Failed to record environment info: {e}")

    def get_environment_info(self) -> Dict:
        """è·å–ç¯å¢ƒä¿¡æ¯"""
        if self.environment_file.exists():
            try:
                with open(self.environment_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to read environment info: {e}")

        return {}

    def establish_baseline(self) -> Dict:
        """å»ºç«‹æ€§èƒ½åŸºçº¿"""
        logger.info("Establishing performance baseline...")

        baseline_results = self._measure_critical_functions()

        baseline_data = {
            "timestamp": time.time(),
            "environment": self.get_environment_info(),
            "functions": baseline_results,
            "config": asdict(self.config)
        }

        # ä¿å­˜åŸºçº¿
        with open(self.baseline_file, 'w', encoding='utf-8') as f:
            json.dump(baseline_data, f, indent=2, ensure_ascii=False)

        logger.info(f"Performance baseline established with {len(baseline_results)} functions")
        return baseline_data

    def load_baseline(self) -> Optional[Dict]:
        """åŠ è½½æ€§èƒ½åŸºçº¿"""
        if self.baseline_file.exists():
            try:
                with open(self.baseline_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load baseline: {e}")

        return None

    def measure_function_performance(self, function_spec: str) -> Dict:
        """æµ‹é‡å•ä¸ªå‡½æ•°æ€§èƒ½"""
        try:
            # è§£æå‡½æ•°è§„æ ¼: "module_path:function_name"
            if ":" not in function_spec:
                logger.error(f"Invalid function spec: {function_spec}")
                return {"error": "Invalid function spec"}

            module_path, function_name = function_spec.split(":")

            # åŠ¨æ€å¯¼å…¥æ¨¡å—å’Œå‡½æ•°
            spec = __import__(module_path.replace("/", ".").replace(".py", ""), fromlist=[function_name])
            func = getattr(spec, function_name)

            # é¢„çƒ­è¿è¡Œ
            for _ in range(self.config.warmup_runs):
                try:
                    with measure_performance():
                        # å°è¯•è°ƒç”¨å‡½æ•°ï¼ˆå¯èƒ½éœ€è¦å‚æ•°ï¼‰
                        if self._is_parameterless_function(func):
                            func()
                        else:
                            # å¯¹äºéœ€è¦å‚æ•°çš„å‡½æ•°ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
                            self._call_function_with_mock_data(func)
                except Exception as e:
                    logger.warning(f"Warmup run failed for {function_spec}: {e}")

            # æ­£å¼æµ‹é‡
            execution_times = []
            memory_usages = []
            cpu_usages = []

            for run in range(self.config.measurement_runs):
                try:
                    start_time = time.time()
                    start_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    start_cpu = psutil.cpu_percent()

                    # è°ƒç”¨å‡½æ•°
                    if self._is_parameterless_function(func):
                        func()
                    else:
                        self._call_function_with_mock_data(func)

                    end_time = time.time()
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    end_cpu = psutil.cpu_percent()

                    execution_time = end_time - start_time
                    memory_usage = end_memory - start_memory
                    cpu_usage = end_cpu - start_cpu

                    execution_times.append(execution_time)
                    memory_usages.append(memory_usage)
                    cpu_usages.append(cpu_usage)

                except Exception as e:
                    logger.warning(f"Measurement run {run+1} failed for {function_spec}: {e}")
                    continue

            if not execution_times:
                return {"error": "All measurement runs failed"}

            # è®¡ç®—ç»Ÿè®¡æ•°æ®
            return {
                "function_spec": function_spec,
                "execution_times": execution_times,
                "memory_usages": memory_usages,
                "cpu_usages": cpu_usages,
                "avg_execution_time": statistics.mean(execution_times),
                "min_execution_time": min(execution_times),
                "max_execution_time": max(execution_times),
                "stdev_execution_time": statistics.stdev(execution_times) if len(execution_times) > 1 else 0,
                "avg_memory_usage": statistics.mean(memory_usages),
                "avg_cpu_usage": statistics.mean(cpu_usages),
                "measurement_runs": len(execution_times)
            }

        except Exception as e:
            logger.error(f"Failed to measure performance for {function_spec}: {e}")
            return {"error": str(e)}

    def _is_parameterless_function(self, func: Callable) -> bool:
        """åˆ¤æ–­å‡½æ•°æ˜¯å¦æ— å‚æ•°"""
        import inspect
        try:
            sig = inspect.signature(func)
            return len(sig.parameters) == 0
        except:
            return False

    def _call_function_with_mock_data(self, func: Callable):
        """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®è°ƒç”¨å‡½æ•°"""
        import inspect
        try:
            sig = inspect.signature(func)
            # ä¸ºæ¯ä¸ªå‚æ•°åˆ›å»ºç®€å•çš„æ¨¡æ‹Ÿæ•°æ®
            mock_args = {}
            for param_name, param in sig.parameters.items():
                if param.default == inspect.Parameter.empty:
                    # æ ¹æ®å‚æ•°ç±»å‹åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
                    mock_args[param_name] = self._create_mock_data(param.annotation)

            func(**mock_args)
        except Exception:
            # å¦‚æœæ¨¡æ‹Ÿæ•°æ®è°ƒç”¨å¤±è´¥ï¼Œå°è¯•æ— å‚æ•°è°ƒç”¨
            try:
                func()
            except:
                pass

    def _create_mock_data(self, annotation: Any) -> Any:
        """æ ¹æ®ç±»å‹æ³¨è§£åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®"""
        if annotation == int or annotation == "int":
            return 42
        elif annotation == str or annotation == "str":
            return "test_string"
        elif annotation == list or annotation == "List":
            return []
        elif annotation == dict or annotation == "Dict":
            return {}
        elif annotation == float or annotation == "float":
            return 3.14
        else:
            return None

    def _measure_critical_functions(self) -> Dict:
        """æµ‹é‡å…³é”®å‡½æ•°æ€§èƒ½"""
        results = {}

        for function_spec in self.config.critical_functions:
            logger.info(f"Measuring performance for {function_spec}")

            try:
                # è®¾ç½®å•å‡½æ•°è¶…æ—¶
                import signal

                def timeout_handler(signum, frame):
                    raise TimeoutError(f"Function {function_spec} timed out")

                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(self.config.timeout_per_function)

                result = self.measure_function_performance(function_spec)
                results[function_spec] = result

                signal.alarm(0)  # å–æ¶ˆè¶…æ—¶

            except TimeoutError:
                logger.warning(f"Function {function_spec} timed out")
                results[function_spec] = {"error": "TIMEOUT"}
            except Exception as e:
                logger.error(f"Failed to measure {function_spec}: {e}")
                results[function_spec] = {"error": str(e)}

        return results

    def run_performance_tests(self) -> Dict:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
        logger.info("Starting performance regression testing...")

        # åŠ è½½åŸºçº¿
        baseline = self.load_baseline()
        if not baseline:
            logger.info("No baseline found, establishing new baseline...")
            baseline = self.establish_baseline()
            return {"status": "baseline_established", "baseline": baseline}

        # è¿è¡Œå½“å‰æµ‹è¯•
        current_results = self._measure_critical_functions()

        # æ¯”è¾ƒç»“æœ
        comparison = self._compare_performance_results(
            baseline["functions"],
            current_results
        )

        # ç”ŸæˆæŠ¥å‘Š
        report_data = {
            "timestamp": time.time(),
            "baseline_timestamp": baseline["timestamp"],
            "environment": self.get_environment_info(),
            "current_results": current_results,
            "baseline_results": baseline["functions"],
            "comparison": comparison,
            "config": asdict(self.config)
        }

        # ä¿å­˜ç»“æœ
        self._save_performance_results(report_data)

        return report_data

    def _compare_performance_results(self, baseline: Dict, current: Dict) -> Dict:
        """æ¯”è¾ƒæ€§èƒ½ç»“æœ"""
        comparison = {
            "regressions": [],
            "improvements": [],
            "stable": [],
            "errors": []
        }

        for function_spec in baseline:
            if function_spec not in current:
                comparison["errors"].append(f"Missing current data for {function_spec}")
                continue

            base_data = baseline[function_spec]
            curr_data = current[function_spec]

            if "error" in base_data or "error" in curr_data:
                comparison["errors"].append(f"Error in {function_spec}")
                continue

            # è®¡ç®—ç›¸å¯¹æ€§èƒ½å˜åŒ–
            base_time = base_data["avg_execution_time"]
            curr_time = curr_data["avg_execution_time"]

            if base_time == 0:
                continue

            time_change_percent = ((curr_time - base_time) / base_time) * 100

            # åˆ¤æ–­æ€§èƒ½å›å½’
            comparison_result = {
                "function": function_spec,
                "baseline_time": base_time,
                "current_time": curr_time,
                "change_percent": time_change_percent,
                "baseline_memory": base_data["avg_memory_usage"],
                "current_memory": curr_data["avg_memory_usage"],
                "memory_change": curr_data["avg_memory_usage"] - base_data["avg_memory_usage"]
            }

            if time_change_percent > self.config.critical_threshold:
                comparison_result["severity"] = "critical"
                comparison["regressions"].append(comparison_result)
            elif time_change_percent > self.config.warning_threshold:
                comparison_result["severity"] = "warning"
                comparison["regressions"].append(comparison_result)
            elif time_change_percent < -self.config.warning_threshold:
                comparison["improvements"].append(comparison_result)
            else:
                comparison["stable"].append(comparison_result)

        return comparison

    def _save_performance_results(self, results: Dict):
        """ä¿å­˜æ€§èƒ½æµ‹è¯•ç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"performance_results_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # æ›´æ–°æœ€æ–°ç»“æœ
        latest_file = self.results_dir / "latest_performance_results.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Performance results saved to {result_file}")

    def generate_performance_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        latest_file = self.results_dir / "latest_performance_results.json"
        if not latest_file.exists():
            return "No performance test results available"

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            comparison = results["comparison"]
            environment = results.get("environment", {})

            report = f"""
## âš¡ æ€§èƒ½å›å½’æµ‹è¯•ç»“æœ

### æ‰§è¡Œä¿¡æ¯
- **æµ‹è¯•æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['timestamp']))}
- **åŸºçº¿æ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(results['baseline_timestamp']))}
- **æµ‹è¯•å‡½æ•°æ•°**: {len(results['current_results'])}
- **è­¦å‘Šé˜ˆå€¼**: {self.config.warning_threshold}%
- **ä¸¥é‡é˜ˆå€¼**: {self.config.critical_threshold}%

### ç¯å¢ƒä¿¡æ¯
- **ç³»ç»Ÿ**: {environment.get('platform', 'Unknown')}
- **Pythonç‰ˆæœ¬**: {environment.get('python_version', 'Unknown')}
- **CPUæ ¸å¿ƒæ•°**: {environment.get('cpu_count', 'Unknown')}
- **å†…å­˜æ€»é‡**: {environment.get('memory_total', 'Unknown')}GB

### æ£€æµ‹ç»“æœ
- **æ€§èƒ½å›å½’**: {len(comparison['regressions'])}ä¸ª ğŸš¨
- **æ€§èƒ½æ”¹è¿›**: {len(comparison['improvements'])}ä¸ª âœ…
- **æ€§èƒ½ç¨³å®š**: {len(comparison['stable'])}ä¸ª â–
- **æµ‹è¯•é”™è¯¯**: {len(comparison['errors'])}ä¸ª âŒ

### è¯¦ç»†ç»“æœ
"""

            # æ·»åŠ æ€§èƒ½å›å½’è¯¦æƒ…
            if comparison["regressions"]:
                report += "#### ğŸš¨ æ€§èƒ½å›å½’\n"
                for regression in comparison["regressions"]:
                    severity = regression["severity"]
                    change = regression["change_percent"]
                    baseline = regression["baseline_time"]
                    current = regression["current_time"]

                    report += f"- **{regression['function']}** ({severity.upper()}): "
                    report += f"{change:+.1f}% ({baseline:.3f}s â†’ {current:.3f}s)\n"

            # æ·»åŠ æ€§èƒ½æ”¹è¿›è¯¦æƒ…
            if comparison["improvements"]:
                report += "\n#### âœ… æ€§èƒ½æ”¹è¿›\n"
                for improvement in comparison["improvements"]:
                    change = improvement["change_percent"]
                    baseline = improvement["baseline_time"]
                    current = improvement["current_time"]

                    report += f"- **{improvement['function']}**: "
                    report += f"{change:+.1f}% ({baseline:.3f}s â†’ {current:.3f}s)\n"

            # æ·»åŠ é”™è¯¯ä¿¡æ¯
            if comparison["errors"]:
                report += "\n#### âŒ æµ‹è¯•é”™è¯¯\n"
                for error in comparison["errors"]:
                    report += f"- {error}\n"

            report += f"""
### é£é™©è¯„ä¼°
{self._generate_performance_recommendations(comparison)}

### æµ‹è¯•é…ç½®è¯´æ˜
- **é¢„çƒ­è¿è¡Œ**: {self.config.warmup_runs}æ¬¡
- **æµ‹é‡è¿è¡Œ**: {self.config.measurement_runs}æ¬¡
- **å•å‡½æ•°è¶…æ—¶**: {self.config.timeout_per_function}ç§’
- **ç›¸å¯¹å¯¹æ¯”**: ä¸åŸºçº¿ç‰ˆæœ¬è¿›è¡Œç™¾åˆ†æ¯”å˜åŒ–æ¯”è¾ƒ
- **ç¯å¢ƒä¸€è‡´æ€§**: è‡ªåŠ¨è®°å½•ç³»ç»Ÿç¯å¢ƒä¿¡æ¯ç¡®ä¿å¯æ¯”æ€§
"""
            return report

        except Exception as e:
            logger.error(f"Failed to generate performance report: {e}")
            return "Failed to generate performance report"

    def _generate_performance_recommendations(self, comparison: Dict) -> str:
        """ç”Ÿæˆæ€§èƒ½æ”¹è¿›å»ºè®®"""
        critical_regressions = [r for r in comparison["regressions"] if r["severity"] == "critical"]
        warning_regressions = [r for r in comparison["regressions"] if r["severity"] == "warning"]

        if critical_regressions:
            return f"ğŸš¨ å‘ç°{len(critical_regressions)}ä¸ªä¸¥é‡æ€§èƒ½å›å½’ï¼Œå»ºè®®ç«‹å³è°ƒæŸ¥å’Œä¼˜åŒ–"
        elif warning_regressions:
            return f"âš ï¸ å‘ç°{len(warning_regressions)}ä¸ªæ€§èƒ½è­¦å‘Šï¼Œå»ºè®®å…³æ³¨å¹¶ç›‘æ§"
        elif len(comparison["improvements"]) > 0:
            return f"âœ… æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œæœ‰{len(comparison['improvements'])}ä¸ªæ€§èƒ½æ”¹è¿›"
        else:
            return "âœ… æ‰€æœ‰æµ‹è¯•å‡½æ•°æ€§èƒ½ç¨³å®š"


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse

    parser = argparse.ArgumentParser(description="Relative Performance Benchmark")
    parser.add_argument("--baseline", action="store_true",
                       help="Establish new performance baseline")
    parser.add_argument("--report-only", action="store_true",
                       help="Only show performance report")

    args = parser.parse_args()

    benchmark = RelativePerformanceBenchmark()

    if args.baseline:
        baseline = benchmark.establish_baseline()
        print("âœ… Performance baseline established successfully")
        return

    if args.report_only:
        print(benchmark.generate_performance_report())
        return

    results = benchmark.run_performance_tests()
    print(benchmark.generate_performance_report())


if __name__ == "__main__":
    main()
```

---

## ğŸ› ï¸ æ¨¡å—4ï¼šå¤šç»´åº¦æµ‹è¯•è´¨é‡èšåˆå™¨ï¼ˆç¬¬3å‘¨ï¼‰

### 4.1 é£é™©æ§åˆ¶çš„æµ‹è¯•è´¨é‡èšåˆå™¨
**æ–‡ä»¶ï¼š`src/ai/test_quality_aggregator.py`**
```python
#!/usr/bin/env python3
"""
é£é™©æ§åˆ¶çš„æµ‹è¯•è´¨é‡èšåˆå™¨ - æ•´åˆå¤šç»´åº¦æŒ‡æ ‡ï¼Œç”ŸæˆMarkdownæŠ¥å‘Š
"""

import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime

from .mutation_tester import SelectiveMutationTester
from .flaky_test_detector import SmartFlakyTestDetector
from .performance_benchmark import RelativePerformanceBenchmark

logger = logging.getLogger(__name__)

@dataclass
class AggregationConfig:
    """èšåˆé…ç½®"""
    # éé˜»å¡æ¨¡å¼è®¾ç½®
    non_blocking_mode: bool = True

    # è¶…æ—¶æ§åˆ¶
    total_timeout: int = 600  # 10åˆ†é’Ÿæ€»è¶…æ—¶

    # ç»„ä»¶å¯ç”¨æ§åˆ¶
    enable_mutation: bool = True
    enable_flaky_detection: bool = True
    enable_performance: bool = True

    # è´¨é‡é˜ˆå€¼
    minimum_mutation_score: float = 60.0
    maximum_flaky_ratio: float = 0.1  # 10%
    maximum_performance_regressions: int = 2

class RiskControlledQualityAggregator:
    """é£é™©æ§åˆ¶çš„æµ‹è¯•è´¨é‡èšåˆå™¨"""

    def __init__(self, config: Optional[AggregationConfig] = None):
        self.config = config or AggregationConfig()
        self.results_dir = Path("docs/_reports")

        # åˆå§‹åŒ–å„ä¸ªæ£€æµ‹å™¨
        if self.config.enable_mutation:
            self.mutation_tester = SelectiveMutationTester()

        if self.config.enable_flaky_detection:
            self.flaky_detector = SmartFlakyTestDetector()

        if self.config.enable_performance:
            self.performance_benchmark = RelativePerformanceBenchmark()

    def run_comprehensive_analysis(self, incremental: bool = True) -> Dict:
        """è¿è¡Œç»¼åˆæµ‹è¯•è´¨é‡åˆ†æ"""
        start_time = time.time()

        logger.info("Starting comprehensive test quality analysis...")
        logger.info(f"Mode: {'Incremental' if incremental else 'Full'}")
        logger.info(f"Non-blocking: {self.config.non_blocking_mode}")

        analysis_results = {
            "timestamp": start_time,
            "analysis_mode": "incremental" if incremental else "full",
            "config": asdict(self.config),
            "components": {},
            "overall_score": 0.0,
            "recommendations": [],
            "risks": [],
            "execution_time": 0,
            "timeout_used": self.config.total_timeout,
            "non_blocking_mode": self.config.non_blocking_mode
        }

        component_results = {}

        try:
            # 1. çªå˜æµ‹è¯•
            if self.config.enable_mutation:
                logger.info("Running mutation testing...")
                component_results["mutation"] = self._safe_run_component(
                    self.mutation_tester.run_mutation_tests,
                    incremental=incremental,
                    component_name="mutation"
                )

            # 2. Flakyæµ‹è¯•æ£€æµ‹
            if self.config.enable_flaky_detection:
                logger.info("Running flaky test detection...")
                component_results["flaky"] = self._safe_run_component(
                    self.flaky_detector.detect_flaky_tests,
                    incremental=incremental,
                    component_name="flaky"
                )

            # 3. æ€§èƒ½å›å½’æµ‹è¯•
            if self.config.enable_performance:
                logger.info("Running performance regression testing...")
                component_results["performance"] = self._safe_run_component(
                    self.performance_benchmark.run_performance_tests,
                    component_name="performance"
                )

            analysis_results["components"] = component_results

            # è®¡ç®—æ•´ä½“åˆ†æ•°
            analysis_results["overall_score"] = self._calculate_overall_score(component_results)

            # ç”Ÿæˆå»ºè®®å’Œé£é™©
            analysis_results["recommendations"] = self._generate_recommendations(component_results)
            analysis_results["risks"] = self._identify_risks(component_results)

        except Exception as e:
            logger.error(f"Comprehensive analysis failed: {e}")
            analysis_results["error"] = str(e)

            # åœ¨éé˜»å¡æ¨¡å¼ä¸‹ï¼Œå³ä½¿éƒ¨åˆ†å¤±è´¥ä¹Ÿç»§ç»­
            if not self.config.non_blocking_mode:
                raise

        analysis_results["execution_time"] = time.time() - start_time

        # ä¿å­˜ç»“æœ
        self._save_analysis_results(analysis_results)

        return analysis_results

    def _safe_run_component(self, component_func, *args, component_name: str = None, **kwargs):
        """å®‰å…¨è¿è¡Œç»„ä»¶ï¼Œå¸¦è¶…æ—¶å’Œå¼‚å¸¸å¤„ç†"""
        import signal

        def timeout_handler(signum, frame):
            raise TimeoutError(f"Component {component_name} timed out")

        # è®¾ç½®è¶…æ—¶
        old_handler = signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(120)  # æ¯ä¸ªç»„ä»¶2åˆ†é’Ÿè¶…æ—¶

        try:
            logger.info(f"Running component: {component_name}")
            result = component_func(*args, **kwargs)
            logger.info(f"Component {component_name} completed successfully")
            return result

        except TimeoutError as e:
            logger.error(f"Component {component_name} timed out: {e}")
            return {"error": "TIMEOUT", "component": component_name}

        except Exception as e:
            logger.error(f"Component {component_name} failed: {e}")
            return {"error": str(e), "component": component_name}

        finally:
            signal.alarm(0)
            signal.signal(signal.SIGALRM, old_handler)

    def _calculate_overall_score(self, component_results: Dict) -> float:
        """è®¡ç®—æ•´ä½“æµ‹è¯•è´¨é‡åˆ†æ•°"""
        scores = []

        # çªå˜æµ‹è¯•åˆ†æ•° (0-40åˆ†)
        if self.config.enable_mutation and "mutation" in component_results:
            mutation_result = component_results["mutation"]
            if "error" not in mutation_result:
                mutation_score = self.mutation_tester.get_mutation_score()
                scores.append(min(mutation_score * 0.4, 40))  # æœ€é«˜40åˆ†

        # Flakyæµ‹è¯•åˆ†æ•° (0-30åˆ†)
        if self.config.enable_flaky_detection and "flaky" in component_results:
            flaky_result = component_results["flaky"]
            if "error" not in flaky_result:
                summary = flaky_result.get("summary", {})
                total_tests = summary.get("total_tests", 0)
                flaky_count = summary.get("flaky_tests", 0)

                if total_tests > 0:
                    stability_ratio = 1 - (flaky_count / total_tests)
                    scores.append(stability_ratio * 30)  # æœ€é«˜30åˆ†

        # æ€§èƒ½æµ‹è¯•åˆ†æ•° (0-30åˆ†)
        if self.config.enable_performance and "performance" in component_results:
            perf_result = component_results["performance"]
            if "error" not in perf_result:
                comparison = perf_result.get("comparison", {})
                regressions = comparison.get("regressions", [])

                # æ ¹æ®æ€§èƒ½å›å½’æ•°é‡è®¡ç®—åˆ†æ•°
                critical_regressions = len([r for r in regressions if r.get("severity") == "critical"])
                warning_regressions = len([r for r in regressions if r.get("severity") == "warning"])

                perf_score = max(0, 30 - (critical_regressions * 10) - (warning_regressions * 5))
                scores.append(perf_score)

        return sum(scores) if scores else 0.0

    def _generate_recommendations(self, component_results: Dict) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        # çªå˜æµ‹è¯•å»ºè®®
        if self.config.enable_mutation and "mutation" in component_results:
            mutation_result = component_results["mutation"]
            if "error" not in mutation_result:
                mutation_score = self.mutation_tester.get_mutation_score()
                if mutation_score < self.config.minimum_mutation_score:
                    recommendations.append(
                        f"Mutation Score ({mutation_score:.1f}%) ä½äºé˜ˆå€¼ ({self.config.minimum_mutation_score}%)ï¼Œ"
                        f"å»ºè®®åŠ å¼ºæµ‹è¯•ç”¨ä¾‹è¦†ç›–"
                    )

        # Flakyæµ‹è¯•å»ºè®®
        if self.config.enable_flaky_detection and "flaky" in component_results:
            flaky_result = component_results["flaky"]
            if "error" not in flaky_result:
                summary = flaky_result.get("summary", {})
                total_tests = summary.get("total_tests", 0)
                flaky_count = summary.get("flaky_tests", 0)

                if total_tests > 0:
                    flaky_ratio = flaky_count / total_tests
                    if flaky_ratio > self.config.maximum_flaky_ratio:
                        recommendations.append(
                            f"Flakyæµ‹è¯•æ¯”ä¾‹ ({flaky_ratio:.1%}) è¶…è¿‡é˜ˆå€¼ ({self.config.maximum_flaky_ratio:.1%})ï¼Œ"
                            f"å»ºè®®ä¿®å¤{flaky_count}ä¸ªä¸ç¨³å®šæµ‹è¯•"
                        )

        # æ€§èƒ½æµ‹è¯•å»ºè®®
        if self.config.enable_performance and "performance" in component_results:
            perf_result = component_results["performance"]
            if "error" not in perf_result:
                comparison = perf_result.get("comparison", {})
                regressions = comparison.get("regressions", [])

                if len(regressions) > self.config.maximum_performance_regressions:
                    critical_count = len([r for r in regressions if r.get("severity") == "critical"])
                    recommendations.append(
                        f"å‘ç°{len(regressions)}ä¸ªæ€§èƒ½å›å½’ ({critical_count}ä¸ªä¸¥é‡)ï¼Œ"
                        f"å»ºè®®ä¼˜åŒ–ç›¸å…³å‡½æ•°æ€§èƒ½"
                    )

        return recommendations if recommendations else ["æµ‹è¯•è´¨é‡è¡¨ç°è‰¯å¥½ï¼Œç»§ç»­ä¿æŒ"]

    def _identify_risks(self, component_results: Dict) -> List[str]:
        """è¯†åˆ«é£é™©"""
        risks = []

        # æ£€æŸ¥ç»„ä»¶æ‰§è¡ŒçŠ¶æ€
        for component_name, result in component_results.items():
            if "error" in result:
                if result["error"] == "TIMEOUT":
                    risks.append(f"{component_name}ç»„ä»¶æ‰§è¡Œè¶…æ—¶ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–æ£€æµ‹é€»è¾‘")
                else:
                    risks.append(f"{component_name}ç»„ä»¶æ‰§è¡Œå¤±è´¥: {result['error']}")

        # æ£€æŸ¥æ•´ä½“åˆ†æ•°
        overall_score = self._calculate_overall_score(component_results)
        if overall_score < 60:
            risks.append(f"æ•´ä½“æµ‹è¯•è´¨é‡åˆ†æ•°è¾ƒä½ ({overall_score:.1f}/100)ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨")

        # æ£€æŸ¥æ‰§è¡Œæ—¶é—´
        execution_times = []
        for component_name, result in component_results.items():
            if "execution_time" in result:
                execution_times.append(result["execution_time"])

        if execution_times:
            total_time = sum(execution_times)
            if total_time > self.config.total_timeout * 0.8:  # è¶…è¿‡80%è¶…æ—¶æ—¶é—´
                risks.append(f"æ£€æµ‹æ‰§è¡Œæ—¶é—´è¾ƒé•¿ ({total_time:.1f}s)ï¼Œå¯èƒ½å½±å“CIæ€§èƒ½")

        return risks if risks else ["æœªè¯†åˆ«åˆ°æ˜æ˜¾é£é™©"]

    def _save_analysis_results(self, results: Dict):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = self.results_dir / f"quality_analysis_{timestamp}.json"

        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # æ›´æ–°æœ€æ–°ç»“æœ
        latest_file = self.results_dir / "latest_quality_analysis.json"
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        logger.info(f"Quality analysis results saved to {result_file}")

    def generate_markdown_report(self) -> str:
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        latest_file = self.results_dir / "latest_quality_analysis.json"
        if not latest_file.exists():
            return "# ğŸ§ª æµ‹è¯•è´¨é‡æŠ¥å‘Š\n\næš‚æ— æµ‹è¯•è´¨é‡åˆ†æç»“æœ"

        try:
            with open(latest_file, 'r', encoding='utf-8') as f:
                results = json.load(f)

            overall_score = results.get("overall_score", 0)
            recommendations = results.get("recommendations", [])
            risks = results.get("risks", [])
            components = results.get("components", {})

            report = f"""# ğŸ§ª æµ‹è¯•è´¨é‡æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.fromtimestamp(results['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†ææ¨¡å¼**: {results.get('analysis_mode', 'unknown').title()}
**æ‰§è¡Œæ—¶é—´**: {results.get('execution_time', 0):.1f}ç§’
**éé˜»å¡æ¨¡å¼**: {results.get('non_blocking_mode', True)}

---

## ğŸ“Š ç»¼åˆè¯„åˆ†

### ğŸ¯ æ•´ä½“è´¨é‡åˆ†æ•°
{self._generate_score_visualization(overall_score)}

**å½“å‰å¾—åˆ†**: {overall_score:.1f}/100

{self._generate_score_description(overall_score)}

---

## ğŸ“ˆ è¯¦ç»†åˆ†æ

### ğŸ”¬ çªå˜æµ‹è¯•
{self._get_mutation_summary(components)}

### ğŸ”„ Flaky Test æ£€æµ‹
{self._get_flaky_summary(components)}

### âš¡ æ€§èƒ½å›å½’æµ‹è¯•
{self._get_performance_summary(components)}

---

## âš ï¸ é£é™©è¯„ä¼°

### ğŸ” è¯†åˆ«çš„é£é™©
"""

            # æ·»åŠ é£é™©åˆ—è¡¨
            for i, risk in enumerate(risks, 1):
                report += f"{i}. {risk}\n"

            report += "\n### ğŸ“‹ æ”¹è¿›å»ºè®®\n"

            # æ·»åŠ æ”¹è¿›å»ºè®®
            for i, rec in enumerate(recommendations, 1):
                report += f"{i}. {rec}\n"

            report += f"""

---

## ğŸ”§ æ‰§è¡Œä¿¡æ¯

### æµ‹è¯•èŒƒå›´
- **çªå˜æµ‹è¯•**: {'src/data/, src/models/, src/services/' if components.get('mutation') else 'æœªè¿è¡Œ'}
- **Flakyæ£€æµ‹**: {'å…³é”®æµ‹è¯•æ–‡ä»¶å­é›†' if components.get('flaky') else 'æœªè¿è¡Œ'}
- **æ€§èƒ½æµ‹è¯•**: {'å…³é”®å‡½æ•°å­é›†' if components.get('performance') else 'æœªè¿è¡Œ'}

### é£é™©æ§åˆ¶æªæ–½
- **é€‰æ‹©æ€§æ£€æµ‹**: åªæ£€æµ‹å…³é”®æ¨¡å—ï¼Œé¿å…å…¨é‡æ‰«æ
- **å¢é‡æ¨¡å¼**: ä¼˜å…ˆæ£€æµ‹ä¿®æ”¹å†…å®¹ï¼Œæé«˜æ•ˆç‡
- **è¶…æ—¶æ§åˆ¶**: å•ç»„ä»¶2åˆ†é’Ÿï¼Œæ€»{results.get('timeout_used', 0)}ç§’è¶…æ—¶ä¿æŠ¤
- **éé˜»å¡æ¨¡å¼**: {results.get('non_blocking_mode', True)} - æŠ¥å‘Šç”Ÿæˆä¸å½±å“CIé€šè¿‡
- **ç¯å¢ƒéš”ç¦»**: è‡ªåŠ¨è®°å½•ç¯å¢ƒä¿¡æ¯ç¡®ä¿æµ‹è¯•ä¸€è‡´æ€§

### æ‰§è¡Œç¯å¢ƒ
- **Pythonç‰ˆæœ¬**: {components.get('performance', {}).get('environment', {}).get('python_version', 'Unknown')}
- **ç³»ç»Ÿå¹³å°**: {components.get('performance', {}).get('environment', {}).get('platform', 'Unknown')}
- **æ£€æµ‹æ—¶é—´**: {datetime.fromtimestamp(results['timestamp']).strftime('%Y-%m-%d %H:%M:%S')}

---

*æ­¤æŠ¥å‘Šç”±AIæµ‹è¯•è´¨é‡åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*
*é…ç½®: éé˜»å¡æ¨¡å¼, å¢é‡æ£€æµ‹, è¶…æ—¶æ§åˆ¶*
"""

            return report

        except Exception as e:
            logger.error(f"Failed to generate markdown report: {e}")
            return "# ğŸ§ª æµ‹è¯•è´¨é‡æŠ¥å‘Š\n\næŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—"

    def _generate_score_visualization(self, score: float) -> str:
        """ç”Ÿæˆåˆ†æ•°å¯è§†åŒ–"""
        full_blocks = int(score // 10)
        partial_block = "â–“" if (score % 10) >= 5 else "â–‘"
        empty_blocks = 10 - full_blocks - (1 if partial_block != "â–‘" else 0)

        bar = "â–“" * full_blocks + partial_block + "â–‘" * empty_blocks
        return bar

    def _generate_score_description(self, score: float) -> str:
        """ç”Ÿæˆåˆ†æ•°æè¿°"""
        if score >= 80:
            return "ğŸŸ¢ **ä¼˜ç§€** - æµ‹è¯•è´¨é‡å¾ˆå¥½ï¼Œç»§ç»­ä¿æŒ"
        elif score >= 60:
            return "ğŸŸ¡ **è‰¯å¥½** - æµ‹è¯•è´¨é‡å¯æ¥å—ï¼Œæœ‰å°å¹…æ”¹è¿›ç©ºé—´"
        elif score >= 40:
            return "ğŸŸ  **ä¸€èˆ¬** - æµ‹è¯•è´¨é‡æœ‰å¾…æå‡ï¼Œå»ºè®®å…³æ³¨"
        else:
            return "ğŸ”´ **éœ€æ”¹è¿›** - æµ‹è¯•è´¨é‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆæ”¹è¿›"

    def _get_mutation_summary(self, components: Dict) -> str:
        """è·å–çªå˜æµ‹è¯•æ‘˜è¦"""
        if "mutation" not in components:
            return "âŒ æœªè¿è¡Œ"

        mutation_result = components["mutation"]
        if "error" in mutation_result:
            return f"âŒ æ‰§è¡Œå¤±è´¥: {mutation_result['error']}"

        mutation_score = self.mutation_tester.get_mutation_score()
        return f"**Mutation Score**: {mutation_score:.1f}% - {self._get_mutation_status(mutation_score)}"

    def _get_mutation_status(self, score: float) -> str:
        """è·å–çªå˜æµ‹è¯•çŠ¶æ€"""
        if score >= 80:
            return "ğŸŸ¢ ä¼˜ç§€"
        elif score >= 60:
            return "ğŸŸ¡ è‰¯å¥½"
        elif score >= 40:
            return "ğŸŸ  ä¸€èˆ¬"
        else:
            return "ğŸ”´ éœ€æ”¹è¿›"

    def _get_flaky_summary(self, components: Dict) -> str:
        """è·å–Flakyæµ‹è¯•æ‘˜è¦"""
        if "flaky" not in components:
            return "âŒ æœªè¿è¡Œ"

        flaky_result = components["flaky"]
        if "error" in flaky_result:
            return f"âŒ æ‰§è¡Œå¤±è´¥: {flaky_result['error']}"

        summary = flaky_result.get("summary", {})
        flaky_count = summary.get("flaky_tests", 0)
        total_tests = summary.get("total_tests", 0)

        if total_tests == 0:
            return "â„¹ï¸ æ— æµ‹è¯•æ–‡ä»¶"

        ratio = flaky_count / total_tests
        return f"**Flakyæµ‹è¯•**: {flaky_count}/{total_tests} ({ratio:.1%}) - {self._get_flaky_status(ratio)}"

    def _get_flaky_status(self, ratio: float) -> str:
        """è·å–Flakyæµ‹è¯•çŠ¶æ€"""
        if ratio == 0:
            return "ğŸŸ¢ ç¨³å®š"
        elif ratio <= 0.05:
            return "ğŸŸ¡ è½»å¾®ä¸ç¨³å®š"
        elif ratio <= 0.1:
            return "ğŸŸ  ä¸ç¨³å®š"
        else:
            return "ğŸ”´ ä¸¥é‡ä¸ç¨³å®š"

    def _get_performance_summary(self, components: Dict) -> str:
        """è·å–æ€§èƒ½æµ‹è¯•æ‘˜è¦"""
        if "performance" not in components:
            return "âŒ æœªè¿è¡Œ"

        perf_result = components["performance"]
        if "error" in perf_result:
            return f"âŒ æ‰§è¡Œå¤±è´¥: {perf_result['error']}"

        comparison = perf_result.get("comparison", {})
        regressions = comparison.get("regressions", [])

        critical_count = len([r for r in regressions if r.get("severity") == "critical"])
        warning_count = len([r for r in regressions if r.get("severity") == "warning"])

        if len(regressions) == 0:
            return "ğŸŸ¢ æ— æ€§èƒ½å›å½’"
        elif critical_count == 0:
            return f"ğŸŸ¡ {warning_count}ä¸ªæ€§èƒ½è­¦å‘Š"
        else:
            return f"ğŸ”´ {critical_count}ä¸ªä¸¥é‡å›å½’ï¼Œ{warning_count}ä¸ªè­¦å‘Š"


def main():
    """ä¸»å‡½æ•° - ç”¨äºæµ‹è¯•"""
    import argparse

    parser = argparse.ArgumentParser(description="Risk-Controlled Test Quality Aggregator")
    parser.add_argument("--incremental", action="store_true", default=True,
                       help="Run incremental analysis (default)")
    parser.add_argument("--full", action="store_true",
                       help="Run full analysis")
    parser.add_argument("--report-only", action="store_true",
                       help="Only show markdown report")
    parser.add_argument("--blocking", action="store_true",
                       help="Run in blocking mode (fail on errors)")

    args = parser.parse_args()

    config = AggregationConfig(non_blocking_mode=not args.blocking)
    aggregator = RiskControlledQualityAggregator(config)

    if args.report_only:
        print(aggregator.generate_markdown_report())
        return

    incremental_mode = args.incremental and not args.full
    results = aggregator.run_comprehensive_analysis(incremental=incremental_mode)

    print(aggregator.generate_markdown_report())

    # åœ¨éé˜»å¡æ¨¡å¼ä¸‹ï¼Œå³ä½¿åˆ†æ•°ä½ä¹Ÿä¸æŠ¥é”™
    if args.blocking and results.get("overall_score", 0) < 60:
        exit(1)


if __name__ == "__main__":
    main()
```

---

## ğŸ› ï¸ æ¨¡å—5ï¼šæ›´æ–°ä¸»è„šæœ¬å’ŒæŠ¥å‘Šæ¨¡æ¿

### 5.1 æ›´æ–°AIå¢å¼ºBugfixè„šæœ¬
**æ–‡ä»¶ï¼š`scripts/ai_enhanced_bugfix.py`** (æ›´æ–°éƒ¨åˆ†)
```python
# åœ¨AIEnhancedBugfixç±»ä¸­æ·»åŠ æ–°æ–¹æ³•

def run_test_quality_analysis(self, incremental: bool = True, blocking: bool = False) -> bool:
    """
    è¿è¡Œæµ‹è¯•è´¨é‡åˆ†æ (Phase 2åŠŸèƒ½)

    Args:
        incremental: æ˜¯å¦å¢é‡æ¨¡å¼
        blocking: æ˜¯å¦é˜»å¡æ¨¡å¼

    Returns:
        åˆ†ææ˜¯å¦æˆåŠŸå®Œæˆ
    """
    try:
        print("ğŸ§ª Starting test quality analysis...")

        from src.ai.test_quality_aggregator import RiskControlledQualityAggregator, AggregationConfig

        config = AggregationConfig(non_blocking_mode=not blocking)
        aggregator = RiskControlledQualityAggregator(config)

        # è¿è¡Œåˆ†æ
        results = aggregator.run_comprehensive_analysis(incremental=incremental)

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report_content = aggregator.generate_markdown_report()
        report_path = self.reports_dir / "TEST_QUALITY_REPORT.md"

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"âœ… Test quality analysis completed!")
        print(f"ğŸ“Š Overall score: {results.get('overall_score', 0):.1f}/100")
        print(f"ğŸ“„ Report saved to: {report_path}")

        # åœ¨é˜»å¡æ¨¡å¼ä¸‹ï¼Œæ ¹æ®åˆ†æ•°å†³å®šæ˜¯å¦æˆåŠŸ
        if blocking and results.get('overall_score', 0) < 60:
            print("âš ï¸ Quality score below threshold in blocking mode")
            return False

        return True

    except Exception as e:
        logger.error(f"Test quality analysis failed: {e}")
        print(f"âŒ Error: {e}")
        return False
```

### 5.2 åˆ›å»ºæµ‹è¯•è´¨é‡æŠ¥å‘Šæ¨¡æ¿
**æ–‡ä»¶ï¼š`docs/_reports/TEST_QUALITY_REPORT_TEMPLATE.md`**
```markdown
# ğŸ§ª æµ‹è¯•è´¨é‡æŠ¥å‘Šæ¨¡æ¿

## ğŸ“‹ æŠ¥å‘Šè¯´æ˜

æœ¬æŠ¥å‘Šç”±AIæµ‹è¯•è´¨é‡åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«ä»¥ä¸‹ç»´åº¦çš„æµ‹è¯•è´¨é‡è¯„ä¼°ï¼š

1. **çªå˜æµ‹è¯• (Mutation Testing)**: è¯„ä¼°æµ‹è¯•ç”¨ä¾‹çš„æœ‰æ•ˆæ€§
2. **Flaky Testæ£€æµ‹**: è¯†åˆ«ä¸ç¨³å®šçš„æµ‹è¯•ç”¨ä¾‹
3. **æ€§èƒ½å›å½’æµ‹è¯•**: æ£€æµ‹ä»£ç æ€§èƒ½å˜åŒ–

---

## ğŸ¯ è´¨é‡è¯„åˆ†æ ‡å‡†

| åˆ†æ•°èŒƒå›´ | è´¨é‡ç­‰çº§ | è¯´æ˜ |
|---------|---------|------|
| 80-100 | ğŸŸ¢ ä¼˜ç§€ | æµ‹è¯•è´¨é‡å¾ˆå¥½ï¼Œç»§ç»­ä¿æŒ |
| 60-79  | ğŸŸ¡ è‰¯å¥½ | æµ‹è¯•è´¨é‡å¯æ¥å—ï¼Œæœ‰å°å¹…æ”¹è¿›ç©ºé—´ |
| 40-59  | ğŸŸ  ä¸€èˆ¬ | æµ‹è¯•è´¨é‡æœ‰å¾…æå‡ï¼Œå»ºè®®å…³æ³¨ |
| 0-39   | ğŸ”´ éœ€æ”¹è¿› | æµ‹è¯•è´¨é‡è¾ƒä½ï¼Œå»ºè®®ä¼˜å…ˆæ”¹è¿› |

---

## ğŸ”§ é£é™©æ§åˆ¶æªæ–½

### é€‰æ‹©æ€§æ£€æµ‹
- **çªå˜æµ‹è¯•**: åªåœ¨ `src/data/`, `src/models/`, `src/services/` å…³é”®æ¨¡å—è¿è¡Œ
- **Flakyæ£€æµ‹**: åªæ£€æµ‹å…³é”®æµ‹è¯•æ–‡ä»¶å­é›†
- **æ€§èƒ½æµ‹è¯•**: åªé’ˆå¯¹å…³é”®å‡½æ•°è¿›è¡ŒåŸºå‡†æµ‹è¯•

### å¢é‡æ¨¡å¼
- ä¼˜å…ˆæ£€æµ‹æœ¬æ¬¡æäº¤ä¿®æ”¹çš„æ–‡ä»¶
- å‡å°‘æ‰§è¡Œæ—¶é—´ï¼Œæé«˜æ•ˆç‡

### è¶…æ—¶ä¿æŠ¤
- å•ç»„ä»¶è¶…æ—¶: 2åˆ†é’Ÿ
- æ€»è¶…æ—¶: 10åˆ†é’Ÿ
- é˜²æ­¢CI pipelineè¢«å¡æ­»

### ç¯å¢ƒä¸€è‡´æ€§
- è‡ªåŠ¨è®°å½•ç³»ç»Ÿç¯å¢ƒä¿¡æ¯
- ä½¿ç”¨ç›¸å¯¹æ€§èƒ½å¯¹æ¯”ï¼ˆç™¾åˆ†æ¯”å˜åŒ–ï¼‰
- ç¡®ä¿æµ‹è¯•ç»“æœå¯æ¯”æ€§

---

## ğŸ“Š æŒ‡æ ‡è¯´æ˜

### Mutation Score
- **è®¡ç®—æ–¹å¼**: (è¢«æ€æ­»çš„çªå˜æ•° / æ€»çªå˜æ•°) Ã— 100%
- **é˜ˆå€¼è¦æ±‚**: â‰¥60%
- **é£é™©ç­‰çº§**:
  - â‰¥80%: ä¼˜ç§€
  - 60-79%: è‰¯å¥½
  - 40-59%: ä¸€èˆ¬
  - <40%: éœ€æ”¹è¿›

### Flaky Testæ¯”ç‡
- **è®¡ç®—æ–¹å¼**: (Flakyæµ‹è¯•æ•° / æ€»æµ‹è¯•æ•°) Ã— 100%
- **é˜ˆå€¼è¦æ±‚**: â‰¤10%
- **é£é™©ç­‰çº§**:
  - 0%: ç¨³å®š
  - 1-5%: è½»å¾®ä¸ç¨³å®š
  - 6-10%: ä¸ç¨³å®š
  - >10%: ä¸¥é‡ä¸ç¨³å®š

### æ€§èƒ½å›å½’
- **è­¦å‘Šé˜ˆå€¼**: 10%æ€§èƒ½å˜åŒ–
- **ä¸¥é‡é˜ˆå€¼**: 20%æ€§èƒ½å˜åŒ–
- **å›å½’ç±»å‹**:
  - Critical: éœ€è¦ç«‹å³è°ƒæŸ¥
  - Warning: éœ€è¦å…³æ³¨ç›‘æ§

---

## ğŸš¨ é£é™©æç¤º

1. **æ‰§è¡Œæ—¶é—´**: æ£€æµ‹å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå·²è®¾ç½®è¶…æ—¶ä¿æŠ¤
2. **ç¯å¢ƒä¾èµ–**: æ€§èƒ½æµ‹è¯•ç»“æœå—ç¯å¢ƒå½±å“ï¼Œå»ºè®®åœ¨ç›¸åŒç¯å¢ƒä¸‹å¯¹æ¯”
3. **æ ·æœ¬é™åˆ¶**: é€‰æ‹©æ€§æ£€æµ‹å¯èƒ½é—æ¼éƒ¨åˆ†é—®é¢˜ï¼Œå»ºè®®å®šæœŸå…¨é‡æ£€æµ‹
4. **è¯¯æŠ¥å¯èƒ½**: Flakyæ£€æµ‹å¯èƒ½å­˜åœ¨è¯¯æŠ¥ï¼Œå»ºè®®ç»“åˆå†å²æ•°æ®åˆ¤æ–­

---

## ğŸ“ˆ ä½¿ç”¨å»ºè®®

### CIé›†æˆ
- **åˆæœŸ**: ä½¿ç”¨éé˜»å¡æ¨¡å¼ï¼ŒæŠ¥å‘Šç”Ÿæˆä½†ä¸å½±å“CIé€šè¿‡
- **ç¨³å®šå**: é€æ­¥å‡çº§ä¸ºCI Gateï¼Œè®¾ç½®è´¨é‡åˆ†æ•°é—¨æ§›

### å®šæœŸæ‰§è¡Œ
- **æ¯æ¬¡æäº¤**: å¢é‡æ¨¡å¼ï¼Œå¿«é€Ÿåé¦ˆ
- **æ¯æ—¥æ„å»º**: å…¨é‡æ¨¡å¼ï¼Œå…¨é¢è¯„ä¼°
- **å‘å¸ƒå‰**: å®Œæ•´æ£€æµ‹ï¼Œç¡®ä¿è´¨é‡

### é—®é¢˜ä¿®å¤ä¼˜å…ˆçº§
1. **Criticalæ€§èƒ½å›å½’**: ç«‹å³ä¿®å¤
2. **é«˜Mutation Scoreç¼ºå¤±**: ä¼˜å…ˆè¡¥å……æµ‹è¯•
3. **Flaky Test**: é€æ­¥ä¿®å¤
4. **Warningæ€§èƒ½è­¦å‘Š**: ç›‘æ§å’Œä¼˜åŒ–

---

*æ­¤æ¨¡æ¿æè¿°äº†æµ‹è¯•è´¨é‡æŠ¥å‘Šçš„è§£è¯»æ–¹æ³•å’Œæœ€ä½³å®è·µ*
*æ›´æ–°æ—¶é—´: 2025-09-27*
```

### 5.3 æ›´æ–°ä¸»è„šæœ¬å‘½ä»¤è¡Œå‚æ•°
**æ–‡ä»¶ï¼š`scripts/ai_enhanced_bugfix.py`** (æ›´æ–°mainå‡½æ•°)
```python
def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="AI Enhanced Bugfix System")
    parser.add_argument("--mode", choices=["analyze", "fix", "validate", "report", "test-quality"],
                       default="analyze", help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--todo", type=str, help="TODOæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--non-interactive", action="store_true", help="éäº¤äº’å¼æ¨¡å¼")

    # Phase 2 æ–°å¢å‚æ•°
    parser.add_argument("--test-quality", action="store_true",
                       help="è¿è¡Œæµ‹è¯•è´¨é‡åˆ†æ (Phase 2)")
    parser.add_argument("--incremental", action="store_true", default=True,
                       help="å¢é‡æ¨¡å¼ (é»˜è®¤)")
    parser.add_argument("--full-analysis", action="store_true",
                       help="å…¨é‡åˆ†ææ¨¡å¼")
    parser.add_argument("--blocking", action="store_true",
                       help="é˜»å¡æ¨¡å¼ (è´¨é‡åˆ†æ•°ä¸è¶³æ—¶æŠ¥é”™)")

    args = parser.parse_args()

    # åˆ›å»ºAIå¢å¼ºä¿®å¤ç³»ç»Ÿ
    bugfix_system = AIEnhancedBugfix()

    # æ ¹æ®æ¨¡å¼è¿è¡Œ
    if args.mode == "analyze":
        success = bugfix_system.run_analysis_and_generate_fixes(
            interactive=not args.non_interactive
        )
    elif args.mode == "fix":
        todo_path = Path(args.todo) if args.todo else None
        success = bugfix_system.apply_recommended_fixes(todo_path)
    elif args.mode == "validate":
        success = bugfix_system.validate_fixes()
    elif args.mode == "report":
        success = bugfix_system.generate_report()
    elif args.mode == "test-quality" or args.test_quality:
        incremental_mode = args.incremental and not args.full_analysis
        blocking_mode = args.blocking
        success = bugfix_system.run_test_quality_analysis(
            incremental=incremental_mode,
            blocking=blocking_mode
        )
    else:
        print(f"Unknown mode: {args.mode}")
        success = False

    sys.exit(0 if success else 1)
```

---

## ğŸ“… å®æ–½æ—¶é—´çº¿ï¼ˆé£é™©æ§åˆ¶ç‰ˆï¼‰

### ç¬¬1å‘¨ï¼šé€‰æ‹©æ€§çªå˜æµ‹è¯•
- [x] å®‰è£…å’Œé…ç½®mutmutï¼ˆé€‰æ‹©æ€§æ¨¡å—ï¼‰
- [x] åˆ›å»ºSelectiveMutationTesterï¼ˆè¶…æ—¶æ§åˆ¶ï¼‰
- [x] å¢é‡æ¨¡å¼å®ç°ï¼ˆgit diffï¼‰
- [ ] åŸºç¡€æµ‹è¯•å’ŒéªŒè¯

### ç¬¬2å‘¨ï¼šæ™ºèƒ½Flakyæ£€æµ‹
- [ ] å®ç°SmartFlakyTestDetectorï¼ˆå†å²æ•°æ®ï¼‰
- [ ] å¤–éƒ¨æœåŠ¡æ ‡è®°å’Œè¿‡æ»¤
- [ ] ç›¸å¯¹æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨
- [ ] ç¯å¢ƒä¿¡æ¯è®°å½•æœºåˆ¶

### ç¬¬3å‘¨ï¼šèšåˆä¸é›†æˆ
- [ ] RiskControlledQualityAggregator
- [ ] MarkdownæŠ¥å‘Šç”Ÿæˆå™¨
- [ ] ä¸»è„šæœ¬é›†æˆï¼ˆéé˜»å¡æ¨¡å¼ï¼‰
- [ ] æ–‡æ¡£å’Œæœ€ç»ˆæµ‹è¯•

---

## âœ… é£é™©æ§åˆ¶æ£€æŸ¥æ¸…å•

### âœ… çªå˜æµ‹è¯•é£é™©æ§åˆ¶
- [x] é€‰æ‹©æ€§æ¨¡å—æ£€æµ‹ï¼ˆé¿å…å…¨é‡ï¼‰
- [x] å¢é‡æ¨¡å¼ï¼ˆgit diffï¼‰
- [x] è¶…æ—¶æ§åˆ¶ï¼ˆå•æµ‹è¯•30sï¼Œæ€»5åˆ†é’Ÿï¼‰
- [x] é”™è¯¯å¤„ç†å’Œå›é€€æœºåˆ¶

### âœ… Flakyæ£€æµ‹é£é™©æ§åˆ¶
- [x] é€‰æ‹©æ€§æ–‡ä»¶æ£€æµ‹ï¼ˆå…³é”®æµ‹è¯•å­é›†ï¼‰
- [x] å†å²æ•°æ®éªŒè¯ï¼ˆ3æ¬¡ä¸€è‡´æ€§ï¼‰
- [x] å¤–éƒ¨æœåŠ¡å•ç‹¬æ ‡è®°
- [x] å‡å°‘è¿è¡Œæ¬¡æ•°ï¼ˆ3æ¬¡è€Œé5æ¬¡ï¼‰

### âœ… æ€§èƒ½æµ‹è¯•é£é™©æ§åˆ¶
- [x] ç›¸å¯¹å¯¹æ¯”ï¼ˆç™¾åˆ†æ¯”å˜åŒ–ï¼‰
- [x] ç¯å¢ƒä¿¡æ¯è®°å½•
- [x] é¢„çƒ­å’Œå¤šæ¬¡æµ‹é‡
- [x] æ¨¡æ‹Ÿæ•°æ®æ”¯æŒ

### âœ… é›†æˆé£é™©æ§åˆ¶
- [x] éé˜»å¡æ¨¡å¼ï¼ˆåˆæœŸä¸å½±å“CIï¼‰
- [x] ç»„ä»¶çº§è¶…æ—¶ï¼ˆ2åˆ†é’Ÿæ¯ç»„ä»¶ï¼‰
- [x] å¼‚å¸¸éš”ç¦»å’Œå®¹é”™
- [x] Markdownä¼˜å…ˆï¼ˆHTMLå»¶åï¼‰

---

è¿™ä¸ªé£é™©æ§åˆ¶ç‰ˆçš„å®æ–½æ–¹æ¡ˆç¡®ä¿äº†ï¼š

1. **æ‰§è¡Œæ•ˆç‡**: é€‰æ‹©æ€§æ£€æµ‹+å¢é‡æ¨¡å¼ï¼Œé¿å…å…¨é‡æ‰«æ
2. **ç³»ç»Ÿç¨³å®šæ€§**: è¶…æ—¶æ§åˆ¶+å¼‚å¸¸å¤„ç†ï¼Œé˜²æ­¢CIå¡æ­»
3. **ç»“æœå¯é æ€§**: ç¯å¢ƒéš”ç¦»+ç›¸å¯¹å¯¹æ¯”ï¼Œç¡®ä¿æ•°æ®å¯æ¯”
4. **æ¸è¿›é›†æˆ**: éé˜»å¡æ¨¡å¼å…ˆè¡Œï¼Œç¨³å®šåå†å‡çº§
5. **å¯ç»´æŠ¤æ€§**: æ¨¡å—åŒ–è®¾è®¡+æ¸…æ™°æŠ¥å‘Šï¼Œä¾¿äºåç»­æ‰©å±•

è¯·æ‚¨å®¡é˜…è¿™ä¸ªæ”¹è¿›åçš„æ–¹æ¡ˆï¼Œå¦‚æ— é—®é¢˜æˆ‘å°†å¼€å§‹å®æ–½ï¼