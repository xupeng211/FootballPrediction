# éƒ¨ç½²å’ŒCI/CDæŒ‡å—

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»FootballPredictioné¡¹ç›®çš„éƒ¨ç½²ç­–ç•¥ã€CI/CDæµæ°´çº¿å’Œè¿ç»´æœ€ä½³å®è·µã€‚

---

## ğŸ“‹ ç›®å½•

- [ğŸš€ éƒ¨ç½²æ¶æ„æ¦‚è§ˆ](#-éƒ¨ç½²æ¶æ„æ¦‚è§ˆ)
- [ğŸ³ Dockerå®¹å™¨åŒ–éƒ¨ç½²](#-dockerå®¹å™¨åŒ–éƒ¨ç½²)
- [âš™ï¸ ç¯å¢ƒé…ç½®ç®¡ç†](#ï¸-ç¯å¢ƒé…ç½®ç®¡ç†)
- [ğŸ”„ CI/CDæµæ°´çº¿](#-cicdæµæ°´çº¿)
- [ğŸŒ å¤šç¯å¢ƒéƒ¨ç½²](#-å¤šç¯å¢ƒéƒ¨ç½²)
- [ğŸ“Š ç›‘æ§å’Œæ—¥å¿—](#-ç›‘æ§å’Œæ—¥å¿—)
- [ğŸ”§ DevOpså·¥å…·é“¾](#-devopså·¥å…·é“¾)
- [ğŸ›¡ï¸ å®‰å…¨å’Œå¤‡ä»½](#ï¸-å®‰å…¨å’Œå¤‡ä»½)
- [ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [ğŸ” æ•…éšœæ’é™¤å’Œæ¢å¤](#-æ•…éšœæ’é™¤å’Œæ¢å¤)

---

## ğŸš€ éƒ¨ç½²æ¶æ„æ¦‚è§ˆ

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Load Balancer                        â”‚
â”‚                      (Nginx/HAProxy)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Application Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   App Pod   â”‚  â”‚   App Pod   â”‚  â”‚      App Pod       â”‚  â”‚
â”‚  â”‚   (FastAPI) â”‚  â”‚   (FastAPI) â”‚  â”‚     (FastAPI)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Layer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ PostgreSQL  â”‚  â”‚    Redis    â”‚  â”‚   File Storage     â”‚  â”‚
â”‚  â”‚  (Primary)  â”‚  â”‚   Cluster   â”‚  â”‚    (NFS/S3)        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ PostgreSQL  â”‚                                           â”‚
â”‚  â”‚ (Replica)   â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### éƒ¨ç½²ç»„ä»¶è¯´æ˜

| ç»„ä»¶ | æŠ€æœ¯æ ˆ | èŒè´£ | é«˜å¯ç”¨é…ç½® |
|------|--------|------|------------|
| **è´Ÿè½½å‡è¡¡å™¨** | Nginx/HAProxy | è¯·æ±‚åˆ†å‘ã€SSLç»ˆæ­¢ | å¤šå®ä¾‹ã€å¥åº·æ£€æŸ¥ |
| **åº”ç”¨æœåŠ¡** | FastAPI + Uvicorn | ä¸šåŠ¡é€»è¾‘å¤„ç† | æ°´å¹³æ‰©å±•ã€è‡ªåŠ¨é‡å¯ |
| **æ•°æ®åº“** | PostgreSQL 13+ | æ•°æ®æŒä¹…åŒ– | ä¸»ä»å¤åˆ¶ã€è‡ªåŠ¨æ•…éšœè½¬ç§» |
| **ç¼“å­˜** | Redis 6+ | ç¼“å­˜ã€ä¼šè¯å­˜å‚¨ | Redis Cluster |
| **æ–‡ä»¶å­˜å‚¨** | NFS/S3 | é™æ€æ–‡ä»¶å­˜å‚¨ | åˆ†å¸ƒå¼å­˜å‚¨ |
| **ç›‘æ§** | Prometheus + Grafana | æ€§èƒ½ç›‘æ§ | å¤šå‰¯æœ¬ã€æ•°æ®å¤‡ä»½ |

---

## ğŸ³ Dockerå®¹å™¨åŒ–éƒ¨ç½²

### Dockerfile

```dockerfile
# Dockerfile
FROM python:3.11-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    postgresql-client \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
COPY requirements-dev.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY src/ ./src/
COPY tests/ ./tests/
COPY pyproject.toml .
COPY pytest.ini .
COPY alembic.ini .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd --create-home --shell /bin/bash app \
    && chown -R app:app /app
USER app

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### docker-compose.yml

```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/football_prediction
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=development
      - LOG_LEVEL=INFO
    depends_on:
      - db
      - redis
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  db:
    image: postgres:13
    environment:
      - POSTGRES_DB=football_prediction
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    ports:
      - "5432:5432"
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:6-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - app
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### ç”Ÿäº§ç¯å¢ƒ docker-compose

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  app:
    image: football-prediction:latest
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    environment:
      - DATABASE_URL=postgresql://postgres:${DB_PASSWORD}@db:5432/football_prediction
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=production
      - LOG_LEVEL=WARNING
      - SECRET_KEY=${SECRET_KEY}
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 30s

  db:
    image: postgres:13
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 2G
    environment:
      - POSTGRES_DB=football_prediction
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./backups:/backups
    restart_policy:
      condition: on-failure

  redis:
    image: redis:6-alpine
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
```

---

## âš™ï¸ ç¯å¢ƒé…ç½®ç®¡ç†

### ç¯å¢ƒå˜é‡é…ç½®

#### .env.example
```bash
# .env.example - ç¯å¢ƒå˜é‡æ¨¡æ¿

# åº”ç”¨é…ç½®
APP_NAME=football-prediction
ENVIRONMENT=development
DEBUG=true
SECRET_KEY=your-secret-key-here
LOG_LEVEL=INFO

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://user:password@localhost:5432/football_prediction
DB_POOL_SIZE=10
DB_MAX_OVERFLOW=20
DB_POOL_TIMEOUT=30

# Redisé…ç½®
REDIS_URL=redis://localhost:6379/0
REDIS_MAX_CONNECTIONS=10
REDIS_TIMEOUT=5

# APIé…ç½®
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4

# å¤–éƒ¨æœåŠ¡é…ç½®
FOOTBALL_API_KEY=your-football-api-key
FOOTBALL_API_BASE_URL=https://api.football-data.org/v4

# ç›‘æ§é…ç½®
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000

# æ–‡ä»¶å­˜å‚¨
UPLOAD_PATH=/app/uploads
MAX_UPLOAD_SIZE=10485760

# é‚®ä»¶é…ç½®
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your-email@gmail.com
SMTP_PASSWORD=your-app-password

# å®‰å…¨é…ç½®
JWT_SECRET_KEY=your-jwt-secret
JWT_ALGORITHM=HS256
JWT_EXPIRE_MINUTES=30

# ç¼“å­˜é…ç½®
CACHE_TTL=3600
CACHE_PREFIX=fp:
```

#### .env.ci (CIç¯å¢ƒ)
```bash
# .env.ci
ENVIRONMENT=ci
DEBUG=false
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/test_fp
REDIS_URL=redis://localhost:6379/1
LOG_LEVEL=ERROR
SECRET_KEY=test-secret-key
API_WORKERS=1
```

#### .env.production (ç”Ÿäº§ç¯å¢ƒ)
```bash
# .env.production
ENVIRONMENT=production
DEBUG=false
DATABASE_URL=postgresql://postgres:${DB_PASSWORD}@db:5432/football_prediction
REDIS_URL=redis://redis:6379/0
LOG_LEVEL=WARNING
SECRET_KEY=${SECRET_KEY}
API_WORKERS=8
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40
```

### é…ç½®ç®¡ç†è„šæœ¬

**ç¯å¢ƒæ£€æŸ¥è„šæœ¬**
```bash
#!/bin/bash
# scripts/check_env.sh

echo "ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®..."

# æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
required_vars=(
    "DATABASE_URL"
    "REDIS_URL"
    "SECRET_KEY"
    "ENVIRONMENT"
)

missing_vars=()
for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
        missing_vars+=("$var")
    fi
done

if [ ${#missing_vars[@]} -gt 0 ]; then
    echo "âŒ ç¼ºå°‘å¿…éœ€çš„ç¯å¢ƒå˜é‡:"
    printf '  %s\n' "${missing_vars[@]}"
    exit 1
fi

echo "âœ… æ‰€æœ‰å¿…éœ€çš„ç¯å¢ƒå˜é‡å·²è®¾ç½®"

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "ğŸ” æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
python3 -c "
import asyncio
import os
from sqlalchemy.ext.asyncio import create_async_engine

async def check_db():
    engine = create_async_engine(os.getenv('DATABASE_URL'))
    async with engine.begin() as conn:
        await conn.execute('SELECT 1')
    print('âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸')

asyncio.run(check_db())
"

# æ£€æŸ¥Redisè¿æ¥
echo "ğŸ” æ£€æŸ¥Redisè¿æ¥..."
python3 -c "
import asyncio
import aioredis
import os

async def check_redis():
    redis = aioredis.from_url(os.getenv('REDIS_URL'))
    await redis.ping()
    print('âœ… Redisè¿æ¥æ­£å¸¸')

asyncio.run(check_redis())
"

echo "âœ… ç¯å¢ƒæ£€æŸ¥å®Œæˆ"
```

**ç¯å¢ƒåˆ›å»ºè„šæœ¬**
```bash
#!/bin/bash
# scripts/create_env.sh

ENV_TYPE=${1:-development}

echo "ğŸ”§ åˆ›å»º ${ENV_TYPE} ç¯å¢ƒé…ç½®..."

# å¤åˆ¶æ¨¡æ¿æ–‡ä»¶
if [ ! -f ".env" ]; then
    cp .env.example .env
    echo "âœ… å·²ä» .env.example åˆ›å»º .env æ–‡ä»¶"
else
    echo "âš ï¸ .env æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º"
fi

# æ ¹æ®ç¯å¢ƒç±»å‹è®¾ç½®ç‰¹å®šé…ç½®
case $ENV_TYPE in
    "development")
        echo "ğŸ”§ è®¾ç½®å¼€å‘ç¯å¢ƒé…ç½®..."
        sed -i 's/ENVIRONMENT=.*/ENVIRONMENT=development/' .env
        sed -i 's/DEBUG=.*/DEBUG=true/' .env
        sed -i 's/LOG_LEVEL=.*/LOG_LEVEL=INFO/' .env
        ;;
    "ci")
        echo "ğŸ”§ è®¾ç½®CIç¯å¢ƒé…ç½®..."
        sed -i 's/ENVIRONMENT=.*/ENVIRONMENT=ci/' .env
        sed -i 's/DEBUG=.*/DEBUG=false/' .env
        sed -i 's/LOG_LEVEL=.*/LOG_LEVEL=ERROR/' .env
        ;;
    "production")
        echo "ğŸ”§ è®¾ç½®ç”Ÿäº§ç¯å¢ƒé…ç½®..."
        sed -i 's/ENVIRONMENT=.*/ENVIRONMENT=production/' .env
        sed -i 's/DEBUG=.*/DEBUG=false/' .env
        sed -i 's/LOG_LEVEL=.*/LOG_LEVEL=WARNING/' .env
        ;;
esac

echo "âœ… ç¯å¢ƒé…ç½®åˆ›å»ºå®Œæˆ"
echo "ğŸ’¡ è¯·ç¼–è¾‘ .env æ–‡ä»¶è®¾ç½®å…·ä½“çš„ç¯å¢ƒå˜é‡å€¼"
```

---

## ğŸ”„ CI/CDæµæ°´çº¿

### GitHub Actions å·¥ä½œæµ

#### .github/workflows/ci.yml
```yaml
name: CI Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.11, 3.12]

    services:
      postgres:
        image: postgres:13
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_fp
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:6
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Lint with Ruff
      run: |
        ruff check src/ tests/
        ruff format --check src/ tests/

    - name: Type check with MyPy
      run: mypy src/ --ignore-missing-imports

    - name: Run Smart Tests
      run: |
        cp .env.ci .env
        make test.smart
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_fp
        REDIS_URL: redis://localhost:6379/1

    - name: Run full unit tests
      run: |
        make test.unit
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_fp
        REDIS_URL: redis://localhost:6379/1

    - name: Run integration tests
      run: |
        make test.integration
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_fp
        REDIS_URL: redis://localhost:6379/1

    - name: Generate coverage report
      run: |
        make coverage
        make cov.html

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Run security audit
      run: |
        pip-audit
        bandit -r src/ -f json -o bandit-report.json

    - name: Upload security report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-report
        path: bandit-report.json

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Login to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKER_USERNAME }}
        password: ${{ secrets.DOCKER_PASSWORD }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_USERNAME }}/football-prediction:latest
          ${{ secrets.DOCKER_USERNAME }}/football-prediction:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
```

#### .github/workflows/deploy.yml
```yaml
name: Deploy to Production

on:
  push:
    tags:
      - 'v*'

jobs:
  deploy:
    runs-on: ubuntu-latest
    environment: production

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Deploy to server
      uses: appleboy/ssh-action@v1.0.0
      with:
        host: ${{ secrets.HOST }}
        username: ${{ secrets.USERNAME }}
        key: ${{ secrets.SSH_KEY }}
        script: |
          cd /opt/football-prediction
          docker-compose -f docker-compose.prod.yml pull
          docker-compose -f docker-compose.prod.yml up -d
          docker system prune -f

    - name: Run health check
      run: |
        sleep 30
        curl -f ${{ secrets.PROD_URL }}/health

    - name: Notify Slack
      uses: 8398a7/action-slack@v3
      if: always()
      with:
        status: ${{ job.status }}
        channel: '#deployments'
        webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

### éƒ¨ç½²è„šæœ¬

#### scripts/deploy.sh
```bash
#!/bin/bash
# scripts/deploy.sh

set -e

ENVIRONMENT=${1:-production}
VERSION=${2:-latest}

echo "ğŸš€ å¼€å§‹éƒ¨ç½² $ENVIRONMENT ç¯å¢ƒï¼Œç‰ˆæœ¬: $VERSION"

# æ£€æŸ¥å¿…è¦å·¥å…·
command -v docker >/dev/null 2>&1 || { echo "âŒ Docker æœªå®‰è£…"; exit 1; }
command -v docker-compose >/dev/null 2>&1 || { echo "âŒ Docker Compose æœªå®‰è£…"; exit 1; }

# å¤‡ä»½å½“å‰ç‰ˆæœ¬
echo "ğŸ“¦ å¤‡ä»½å½“å‰ç‰ˆæœ¬..."
mkdir -p backups
docker-compose -f docker-compose.prod.yml exec -T db pg_dump -U postgres football_prediction > backups/backup_$(date +%Y%m%d_%H%M%S).sql

# æ‹‰å–æœ€æ–°é•œåƒ
echo "ğŸ“¥ æ‹‰å–æœ€æ–°é•œåƒ..."
docker-compose -f docker-compose.prod.yml pull

# åœæ­¢æ—§å®¹å™¨
echo "â¹ï¸ åœæ­¢æ—§å®¹å™¨..."
docker-compose -f docker-compose.prod.yml down

# å¯åŠ¨æ–°å®¹å™¨
echo "â–¶ï¸ å¯åŠ¨æ–°å®¹å™¨..."
docker-compose -f docker-compose.prod.yml up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
MAX_RETRIES=30
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    if curl -f http://localhost:8000/health; then
        echo "âœ… å¥åº·æ£€æŸ¥é€šè¿‡"
        break
    fi

    RETRY_COUNT=$((RETRY_COUNT + 1))
    echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨... ($RETRY_COUNT/$MAX_RETRIES)"
    sleep 5
done

if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
    echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
    echo "ğŸ”„ å›æ»šåˆ°ä¸Šä¸€ç‰ˆæœ¬..."
    docker-compose -f docker-compose.prod.yml down
    # è¿™é‡Œå¯ä»¥æ·»åŠ å›æ»šé€»è¾‘
    exit 1
fi

# æ¸…ç†æ—§é•œåƒ
echo "ğŸ§¹ æ¸…ç†æ—§é•œåƒ..."
docker image prune -f

echo "âœ… éƒ¨ç½²å®Œæˆ!"
```

#### scripts/rollback.sh
```bash
#!/bin/bash
# scripts/rollback.sh

set -e

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "âŒ è¯·æŒ‡å®šå¤‡ä»½æ–‡ä»¶"
    echo "ç”¨æ³•: $0 <backup_file.sql>"
    exit 1
fi

echo "ğŸ”„ å¼€å§‹å›æ»šåˆ°å¤‡ä»½: $BACKUP_FILE"

# æ¢å¤æ•°æ®åº“
echo "ğŸ“Š æ¢å¤æ•°æ®åº“..."
docker-compose -f docker-compose.prod.yml exec -T db psql -U postgres -d football_prediction < "$BACKUP_FILE"

# é‡å¯åº”ç”¨
echo "ğŸ”„ é‡å¯åº”ç”¨..."
docker-compose -f docker-compose.prod.yml restart app

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
sleep 10
if curl -f http://localhost:8000/health; then
    echo "âœ… å›æ»šæˆåŠŸ"
else
    echo "âŒ å›æ»šå¤±è´¥"
    exit 1
fi

echo "âœ… å›æ»šå®Œæˆ!"
```

---

## ğŸŒ å¤šç¯å¢ƒéƒ¨ç½²

### ç¯å¢ƒé…ç½®

| ç¯å¢ƒ | ç”¨é€” | åŸŸå | æ•°æ®åº“ | ç¼“å­˜ | ç›‘æ§ |
|------|------|------|--------|------|------|
| **å¼€å‘ç¯å¢ƒ** | æ—¥å¸¸å¼€å‘æµ‹è¯• | dev.fp.local | PostgreSQLå•å®ä¾‹ | Rediså•å®ä¾‹ | åŸºç¡€ç›‘æ§ |
| **æµ‹è¯•ç¯å¢ƒ** | é›†æˆæµ‹è¯• | test.fp.local | PostgreSQLå•å®ä¾‹ | Rediså•å®ä¾‹ | å®Œæ•´ç›‘æ§ |
| **é¢„å‘å¸ƒç¯å¢ƒ** | ä¸Šçº¿å‰éªŒè¯ | staging.fp.local | PostgreSQLä¸»ä» | Redis Cluster | ç”Ÿäº§çº§ç›‘æ§ |
| **ç”Ÿäº§ç¯å¢ƒ** | æ­£å¼æœåŠ¡ | api.fp.com | PostgreSQLä¸»ä» | Redis Cluster | ç”Ÿäº§çº§ç›‘æ§ |

### ç¯å¢ƒéš”ç¦»ç­–ç•¥

#### Kubernetes Namespaceé…ç½®
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: football-prediction-dev
  labels:
    environment: development
    project: football-prediction

---
apiVersion: v1
kind: Namespace
metadata:
  name: football-prediction-prod
  labels:
    environment: production
    project: football-prediction
```

#### ç¯å¢ƒç‰¹å®šé…ç½®
```yaml
# k8s/configmap-dev.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: football-prediction-dev
data:
  ENVIRONMENT: "development"
  DEBUG: "true"
  LOG_LEVEL: "INFO"
  DATABASE_URL: "postgresql://postgres:password@postgres-dev:5432/fp_dev"
  REDIS_URL: "redis://redis-dev:6379/0"
```

```yaml
# k8s/configmap-prod.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: football-prediction-prod
data:
  ENVIRONMENT: "production"
  DEBUG: "false"
  LOG_LEVEL: "WARNING"
  DATABASE_URL: "postgresql://postgres:${DB_PASSWORD}@postgres-prod:5432/fp_prod"
  REDIS_URL: "redis://redis-prod:6379/0"
```

### è“ç»¿éƒ¨ç½²ç­–ç•¥

#### è“ç»¿éƒ¨ç½²è„šæœ¬
```bash
#!/bin/bash
# scripts/blue_green_deploy.sh

set -e

CURRENT_ENV=$1
NEW_VERSION=$2

if [ "$CURRENT_ENV" = "blue" ]; then
    TARGET_ENV="green"
else
    TARGET_ENV="blue"
fi

echo "ğŸ”„ æ‰§è¡Œè“ç»¿éƒ¨ç½²: $CURRENT_ENV â†’ $TARGET_ENV (ç‰ˆæœ¬: $NEW_VERSION)"

# éƒ¨ç½²æ–°ç¯å¢ƒ
echo "ğŸ“¦ éƒ¨ç½² $TARGET_ENV ç¯å¢ƒ..."
kubectl set image deployment/football-prediction-$TARGET_ENV \
  app=football-prediction:$NEW_VERSION \
  -n football-prediction-prod

# ç­‰å¾…éƒ¨ç½²å®Œæˆ
echo "â³ ç­‰å¾… $TARGET_ENV ç¯å¢ƒå°±ç»ª..."
kubectl rollout status deployment/football-prediction-$TARGET_ENV \
  -n football-prediction-prod --timeout=300s

# å¥åº·æ£€æŸ¥
echo "ğŸ” æ‰§è¡Œ $TARGET_ENV ç¯å¢ƒå¥åº·æ£€æŸ¥..."
TARGET_POD=$(kubectl get pods -n football-prediction-prod -l env=$TARGET_ENV -o jsonpath='{.items[0].metadata.name}')
kubectl exec -n football-prediction-prod $TARGET_POD -- curl -f http://localhost:8000/health

# åˆ‡æ¢æµé‡
echo "ğŸ”€ åˆ‡æ¢æµé‡åˆ° $TARGET_ENV ç¯å¢ƒ..."
kubectl patch service football-prediction-service -n football-prediction-prod \
  -p '{"spec":{"selector":{"env":"'$TARGET_ENV'"}}}'

echo "âœ… è“ç»¿éƒ¨ç½²å®Œæˆï¼Œå½“å‰æ´»è·ƒç¯å¢ƒ: $TARGET_ENV"
```

---

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### Prometheusç›‘æ§é…ç½®

#### prometheus.yml
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  - job_name: 'football-prediction'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'nginx'
    static_configs:
      - targets: ['nginx-exporter:9113']
```

#### alert_rules.yml
```yaml
# alert_rules.yml
groups:
  - name: football-prediction-alerts
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors per second"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanizePercentage }}"

      - alert: DatabaseConnectionFailure
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database connection failure"
          description: "Cannot connect to PostgreSQL database"

      - alert: RedisConnectionFailure
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "Redis connection failure"
          description: "Cannot connect to Redis server"
```

### Grafanaä»ªè¡¨æ¿

#### åº”ç”¨ç›‘æ§æŒ‡æ ‡
```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import functools

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'Number of active connections'
)

PREDICTION_COUNT = Counter(
    'predictions_total',
    'Total predictions made',
    ['strategy_type', 'result']
)

def track_requests(func):
    """è¯·æ±‚è¿½è¸ªè£…é¥°å™¨"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            status_code = getattr(result, 'status_code', 200)
            REQUEST_COUNT.labels(
                method='GET',
                endpoint=func.__name__,
                status_code=status_code
            ).inc()
            return result
        except Exception as e:
            REQUEST_COUNT.labels(
                method='GET',
                endpoint=func.__name__,
                status_code=500
            ).inc()
            raise
        finally:
            REQUEST_DURATION.labels(
                method='GET',
                endpoint=func.__name__
            ).observe(time.time() - start_time)

    return wrapper

def start_metrics_server():
    """å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨"""
    start_http_server(9090)
```

### æ—¥å¿—èšåˆé…ç½®

#### ELK Stacké…ç½®
```yaml
# docker-compose.logging.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    ports:
      - "5044:5044"
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config:/usr/share/logstash/config
    depends_on:
      - elasticsearch

  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

#### Logstashé…ç½®
```ruby
# logstash/pipeline/football-prediction.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [fields][service] == "football-prediction" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    if [level] == "ERROR" {
      mutate {
        add_tag => [ "error" ]
      }
    }

    if [user_id] {
      mutate {
        add_field => { "user_identifier" => "%{user_id}" }
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "football-prediction-%{+YYYY.MM.dd}"
  }

  if "error" in [tags] {
    email {
      to => "admin@football-prediction.com"
      subject => "Error Alert: %{[fields][service]}"
      body => "Error occurred: %{message}"
    }
  }
}
```

---

## ğŸ”§ DevOpså·¥å…·é“¾

### Makefileé›†æˆ

```makefile
# Makefile (DevOpsç›¸å…³å‘½ä»¤)

.PHONY: docker-build docker-push deploy deploy-staging rollback

# Dockeræ„å»º
docker-build:
	docker build -t football-prediction:$(VERSION) .

# Dockeræ¨é€
docker-push:
	docker tag football-prediction:$(VERSION) $(DOCKER_REGISTRY)/football-prediction:$(VERSION)
	docker push $(DOCKER_REGISTRY)/football-prediction:$(VERSION)

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy-staging:
	@echo "ğŸš€ éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ..."
	ENVIRONMENT=staging VERSION=$(VERSION) scripts/deploy.sh

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy-production:
	@echo "ğŸš€ éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ..."
	ENVIRONMENT=production VERSION=$(VERSION) scripts/deploy.sh

# å›æ»š
rollback:
	@echo "ğŸ”„ å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬..."
	scripts/rollback.sh $(BACKUP_FILE)

# æ•°æ®åº“è¿ç§»
migrate:
	@echo "ğŸ“Š æ‰§è¡Œæ•°æ®åº“è¿ç§»..."
	alembic upgrade head

# å¤‡ä»½æ•°æ®åº“
backup:
	@echo "ğŸ’¾ å¤‡ä»½æ•°æ®åº“..."
	./scripts/backup_database.sh

# æ¢å¤æ•°æ®åº“
restore:
	@echo "ğŸ”„ æ¢å¤æ•°æ®åº“..."
	./scripts/restore_database.sh $(BACKUP_FILE)

# å¥åº·æ£€æŸ¥
health-check:
	@echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥..."
	curl -f $(API_URL)/health || exit 1

# æ€§èƒ½æµ‹è¯•
performance-test:
	@echo "âš¡ æ‰§è¡Œæ€§èƒ½æµ‹è¯•..."
	./scripts/performance_test.sh

# å®‰å…¨æ‰«æ
security-scan:
	@echo "ğŸ”’ æ‰§è¡Œå®‰å…¨æ‰«æ..."
	docker run --rm -v $(PWD):/app clair-scanner:latest

# ä¾èµ–æ£€æŸ¥
dependency-check:
	@echo "ğŸ“¦ æ£€æŸ¥ä¾èµ–å®‰å…¨..."
	pip-audit
	safety check

# ä»£ç è´¨é‡æ£€æŸ¥
quality-check:
	@echo "ğŸ” æ‰§è¡Œä»£ç è´¨é‡æ£€æŸ¥..."
	make lint
	make type-check
	make security-scan

# CIå®Œæ•´æµç¨‹
ci-full:
	@echo "ğŸ”„ æ‰§è¡Œå®Œæ•´CIæµç¨‹..."
	make quality-check
	make test.unit
	make test.integration
	make coverage
	make security-scan

# CDå®Œæ•´æµç¨‹
cd-full:
	@echo "ğŸš€ æ‰§è¡Œå®Œæ•´CDæµç¨‹..."
	make ci-full
	make docker-build
	make docker-push
	make deploy-staging
	make health-check
```

### è‡ªåŠ¨åŒ–è„šæœ¬

#### æ•°æ®åº“å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# scripts/backup_database.sh

set -e

BACKUP_DIR="/opt/backups"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/football_prediction_backup_$DATE.sql"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ‰§è¡Œå¤‡ä»½
echo "ğŸ“Š å¼€å§‹å¤‡ä»½æ•°æ®åº“..."
docker-compose -f docker-compose.prod.yml exec -T db pg_dump -U postgres -Fc football_prediction > $BACKUP_FILE

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip $BACKUP_FILE

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
find $BACKUP_DIR -name "*.gz" -mtime +30 -delete

echo "âœ… æ•°æ®åº“å¤‡ä»½å®Œæˆ: $BACKUP_FILE.gz"

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
if [ -n "$S3_BUCKET" ]; then
    aws s3 cp $BACKUP_FILE.gz s3://$S3_BUCKET/backups/
    echo "â˜ï¸ å¤‡ä»½å·²ä¸Šä¼ åˆ°S3"
fi
```

#### å¥åº·æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# scripts/health_check.sh

API_URL=${1:-http://localhost:8000}
MAX_RETRIES=30
RETRY_INTERVAL=5

echo "ğŸ” æ‰§è¡Œå¥åº·æ£€æŸ¥: $API_URL"

for i in $(seq 1 $MAX_RETRIES); do
    if curl -f -s "$API_URL/health" > /dev/null; then
        echo "âœ… å¥åº·æ£€æŸ¥é€šè¿‡"
        exit 0
    fi

    echo "â³ å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œé‡è¯•ä¸­... ($i/$MAX_RETRIES)"
    sleep $RETRY_INTERVAL
done

echo "âŒ å¥åº·æ£€æŸ¥å¤±è´¥"
exit 1
```

---

## ğŸ›¡ï¸ å®‰å…¨å’Œå¤‡ä»½

### å®‰å…¨é…ç½®

#### SSL/TLSé…ç½®
```nginx
# nginx/ssl.conf
server {
    listen 443 ssl http2;
    server_name api.football-prediction.com;

    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;

    # å®‰å…¨å¤´
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains" always;

    location / {
        proxy_pass http://app:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}
```

#### ç½‘ç»œå®‰å…¨
```yaml
# docker-compose.security.yml
version: '3.8'

services:
  app:
    networks:
      - app-network
      - db-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
    security_opt:
      - no-new-privileges:true
    user: "1000:1000"
    read_only: true
    tmpfs:
      - /tmp
      - /var/run

  db:
    networks:
      - db-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
    environment:
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    command: >
      postgres
      -c ssl=on
      -c ssl_cert_file=/var/lib/postgresql/server.crt
      -c ssl_key_file=/var/lib/postgresql/server.key

networks:
  app-network:
    driver: bridge
    internal: false
  db-network:
    driver: bridge
    internal: true
```

### å¤‡ä»½ç­–ç•¥

#### è‡ªåŠ¨åŒ–å¤‡ä»½
```bash
#!/bin/bash
# scripts/automated_backup.sh

set -e

BACKUP_TYPE=${1:-full}  # full, incremental, differential
RETENTION_DAYS=${2:-30}

echo "ğŸ“¦ å¼€å§‹ $BACKUP_TYPE å¤‡ä»½ï¼Œä¿ç•™ $RETENTION_DAYS å¤©"

case $BACKUP_TYPE in
    "full")
        echo "ğŸ“Š å®Œæ•´æ•°æ®åº“å¤‡ä»½..."
        docker-compose exec db pg_dump -U postgres -Fc football_prediction > /backups/full_$(date +%Y%m%d_%H%M%S).dump
        ;;
    "incremental")
        echo "ğŸ“Š å¢é‡å¤‡ä»½..."
        # åŸºäºWALæ—¥å¿—çš„å¢é‡å¤‡ä»½
        docker-compose exec db pg_basebackup -U postgres -D /backups/incremental_$(date +%Y%m%d_%H%M%S) -Ft -z -P
        ;;
    "differential")
        echo "ğŸ“Š å·®å¼‚å¤‡ä»½..."
        # ä»ä¸Šä¸€ä¸ªå®Œæ•´å¤‡ä»½çš„å·®å¼‚
        ;;
esac

# å¤‡ä»½åº”ç”¨æ•°æ®
echo "ğŸ“ å¤‡ä»½åº”ç”¨æ•°æ®..."
tar -czf /backups/app_data_$(date +%Y%m%d_%H%M%S).tar.gz /app/uploads

# æ¸…ç†è¿‡æœŸå¤‡ä»½
echo "ğŸ§¹ æ¸…ç†è¿‡æœŸå¤‡ä»½..."
find /backups -name "*$(date -d "$RETENTION_DAYS days ago" +%Y%m%d)*" -delete

# ä¸Šä¼ åˆ°äº‘å­˜å‚¨
echo "â˜ï¸ ä¸Šä¼ å¤‡ä»½åˆ°äº‘å­˜å‚¨..."
aws s3 sync /backups s3://$S3_BUCKET/backups/ --delete

echo "âœ… å¤‡ä»½å®Œæˆ"
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### åº”ç”¨æ€§èƒ½ä¼˜åŒ–

#### æ•°æ®åº“ä¼˜åŒ–
```sql
-- æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–
-- åˆ›å»ºç´¢å¼•
CREATE INDEX CONCURRENTLY idx_predictions_match_id ON predictions(match_id);
CREATE INDEX CONCURRENTLY idx_predictions_created_at ON predictions(created_at);
CREATE INDEX CONCURRENTLY idx_matches_date ON matches(match_date);

-- åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE predictions;
ANALYZE matches;
ANALYZE teams;

-- é…ç½®è¿æ¥æ± 
-- postgresql.conf
max_connections = 200
shared_buffers = 256MB
effective_cache_size = 1GB
work_mem = 4MB
maintenance_work_mem = 64MB
```

#### ç¼“å­˜ç­–ç•¥
```python
# src/cache/strategies.py
from functools import wraps
import redis
import json
import hashlib

class RedisCache:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.default_ttl = 3600  # 1å°æ—¶

    def cache_result(self, key_prefix, ttl=None):
        """ç¼“å­˜è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = self._generate_cache_key(key_prefix, args, kwargs)

                # å°è¯•ä»ç¼“å­˜è·å–
                cached_result = await self.redis.get(cache_key)
                if cached_result:
                    return json.loads(cached_result)

                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)

                # å­˜å…¥ç¼“å­˜
                await self.redis.setex(
                    cache_key,
                    ttl or self.default_ttl,
                    json.dumps(result, default=str)
                )

                return result
            return wrapper
        return decorator

    def _generate_cache_key(self, prefix, args, kwargs):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = f"{prefix}:{str(args)}:{str(sorted(kwargs.items()))}"
        return hashlib.md5(key_data.encode()).hexdigest()

# ä½¿ç”¨ç¤ºä¾‹
cache = RedisCache(redis_client)

@cache.cache_result("prediction", ttl=1800)  # 30åˆ†é’Ÿç¼“å­˜
async def get_prediction(match_id, strategy_type):
    # å¤æ‚çš„é¢„æµ‹é€»è¾‘
    pass
```

### è´Ÿè½½æµ‹è¯•

#### Locustæ€§èƒ½æµ‹è¯•
```python
# tests/performance/locustfile.py
from locust import HttpUser, task, between
import random

class FootballPredictionUser(HttpUser):
    wait_time = between(1, 3)

    def on_start(self):
        """ç”¨æˆ·å¼€å§‹æ—¶æ‰§è¡Œ"""
        # ç™»å½•è·å–token
        response = self.client.post("/api/auth/login", json={
            "username": "test_user",
            "password": "test_password"
        })
        if response.status_code == 200:
            self.token = response.json()["access_token"]
            self.headers = {"Authorization": f"Bearer {self.token}"}
        else:
            self.headers = {}

    @task(3)
    def get_matches(self):
        """è·å–æ¯”èµ›åˆ—è¡¨"""
        self.client.get("/api/matches", headers=self.headers)

    @task(2)
    def create_prediction(self):
        """åˆ›å»ºé¢„æµ‹"""
        match_ids = ["match_1", "match_2", "match_3"]
        strategies = ["ml_model", "historical", "statistical"]

        self.client.post("/api/predictions", json={
            "match_id": random.choice(match_ids),
            "strategy_type": random.choice(strategies)
        }, headers=self.headers)

    @task(1)
    def get_prediction(self):
        """è·å–é¢„æµ‹ç»“æœ"""
        prediction_id = f"pred_{random.randint(1, 1000)}"
        self.client.get(f"/api/predictions/{prediction_id}", headers=self.headers)

    @task(1)
    def health_check(self):
        """å¥åº·æ£€æŸ¥"""
        self.client.get("/health")

class AdminUser(FootballPredictionUser):
    wait_time = between(2, 5)

    @task(2)
    def get_system_stats(self):
        """è·å–ç³»ç»Ÿç»Ÿè®¡"""
        self.client.get("/api/admin/stats", headers=self.headers)

    @task(1)
    def get_users(self):
        """è·å–ç”¨æˆ·åˆ—è¡¨"""
        self.client.get("/api/admin/users", headers=self.headers)
```

---

## ğŸ” æ•…éšœæ’é™¤å’Œæ¢å¤

### å¸¸è§é—®é¢˜è¯Šæ–­

#### æœåŠ¡æ•…éšœæ’é™¤
```bash
#!/bin/bash
# scripts/diagnose.sh

echo "ğŸ” ç³»ç»Ÿè¯Šæ–­å¼€å§‹..."

# æ£€æŸ¥å®¹å™¨çŠ¶æ€
echo "ğŸ“¦ æ£€æŸ¥å®¹å™¨çŠ¶æ€..."
docker-compose -f docker-compose.prod.yml ps

# æ£€æŸ¥èµ„æºä½¿ç”¨
echo "ğŸ’¾ æ£€æŸ¥èµ„æºä½¿ç”¨..."
docker stats --no-stream

# æ£€æŸ¥æ—¥å¿—é”™è¯¯
echo "ğŸ“ æ£€æŸ¥æœ€è¿‘é”™è¯¯æ—¥å¿—..."
docker-compose -f docker-compose.prod.yml logs --tail=100 app | grep ERROR

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo "ğŸ—„ï¸ æ£€æŸ¥æ•°æ®åº“è¿æ¥..."
docker-compose -f docker-compose.prod.yml exec db pg_isready -U postgres

# æ£€æŸ¥Redisè¿æ¥
echo "ğŸ—„ï¸ æ£€æŸ¥Redisè¿æ¥..."
docker-compose -f docker-compose.prod.yml exec redis redis-cli ping

# æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "ğŸ’¾ æ£€æŸ¥ç£ç›˜ç©ºé—´..."
df -h

# æ£€æŸ¥ç½‘ç»œè¿æ¥
echo "ğŸŒ æ£€æŸ¥ç½‘ç»œè¿æ¥..."
curl -f http://localhost:8000/health || echo "âŒ åº”ç”¨å¥åº·æ£€æŸ¥å¤±è´¥"

echo "ğŸ” ç³»ç»Ÿè¯Šæ–­å®Œæˆ"
```

#### è‡ªåŠ¨æ•…éšœæ¢å¤
```python
# src/monitoring/auto_recovery.py
import asyncio
import logging
from typing import Dict, Any
import docker

class AutoRecovery:
    def __init__(self):
        self.docker_client = docker.from_env()
        self.logger = logging.getLogger(__name__)

    async def check_and_recover(self):
        """æ£€æŸ¥ç³»ç»ŸçŠ¶æ€å¹¶è‡ªåŠ¨æ¢å¤"""
        while True:
            try:
                await self._check_services()
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                self.logger.error(f"è‡ªåŠ¨æ¢å¤æ£€æŸ¥å¤±è´¥: {e}")

    async def _check_services(self):
        """æ£€æŸ¥å„é¡¹æœåŠ¡"""
        # æ£€æŸ¥åº”ç”¨æœåŠ¡
        if not await self._check_app_health():
            await self._recover_app()

        # æ£€æŸ¥æ•°æ®åº“
        if not await self._check_database():
            await self._recover_database()

        # æ£€æŸ¥Redis
        if not await self._check_redis():
            await self._recover_redis()

    async def _check_app_health(self) -> bool:
        """æ£€æŸ¥åº”ç”¨å¥åº·çŠ¶æ€"""
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                async with session.get('http://localhost:8000/health') as response:
                    return response.status == 200
        except:
            return False

    async def _recover_app(self):
        """æ¢å¤åº”ç”¨æœåŠ¡"""
        self.logger.warning("åº”ç”¨æœåŠ¡å¼‚å¸¸ï¼Œå¼€å§‹è‡ªåŠ¨æ¢å¤...")

        try:
            # é‡å¯åº”ç”¨å®¹å™¨
            container = self.docker_client.containers.get('football-prediction-app')
            container.restart()
            self.logger.info("åº”ç”¨å®¹å™¨é‡å¯æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"åº”ç”¨æ¢å¤å¤±è´¥: {e}")
            # å‘é€å‘Šè­¦
            await self._send_alert("åº”ç”¨æœåŠ¡è‡ªåŠ¨æ¢å¤å¤±è´¥", str(e))

    async def _check_database(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            import asyncpg
            conn = await asyncpg.connect("postgresql://postgres:password@localhost:5432/football_prediction")
            await conn.execute('SELECT 1')
            await conn.close()
            return True
        except:
            return False

    async def _recover_database(self):
        """æ¢å¤æ•°æ®åº“"""
        self.logger.warning("æ•°æ®åº“å¼‚å¸¸ï¼Œå¼€å§‹è‡ªåŠ¨æ¢å¤...")

        try:
            # é‡å¯æ•°æ®åº“å®¹å™¨
            container = self.docker_client.containers.get('football-prediction-db')
            container.restart()
            self.logger.info("æ•°æ®åº“å®¹å™¨é‡å¯æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æ¢å¤å¤±è´¥: {e}")
            await self._send_alert("æ•°æ®åº“è‡ªåŠ¨æ¢å¤å¤±è´¥", str(e))

    async def _check_redis(self) -> bool:
        """æ£€æŸ¥Redisè¿æ¥"""
        try:
            import aioredis
            redis = aioredis.from_url("redis://localhost:6379/0")
            await redis.ping()
            await redis.close()
            return True
        except:
            return False

    async def _recover_redis(self):
        """æ¢å¤Redis"""
        self.logger.warning("Rediså¼‚å¸¸ï¼Œå¼€å§‹è‡ªåŠ¨æ¢å¤...")

        try:
            # é‡å¯Rediså®¹å™¨
            container = self.docker_client.containers.get('football-prediction-redis')
            container.restart()
            self.logger.info("Rediså®¹å™¨é‡å¯æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"Redisæ¢å¤å¤±è´¥: {e}")
            await self._send_alert("Redisè‡ªåŠ¨æ¢å¤å¤±è´¥", str(e))

    async def _send_alert(self, title: str, message: str):
        """å‘é€å‘Šè­¦"""
        # å‘é€é‚®ä»¶ã€Slackæˆ–å…¶ä»–å‘Šè­¦æ–¹å¼
        self.logger.error(f"å‘Šè­¦: {title} - {message}")
        # è¿™é‡Œå¯ä»¥é›†æˆå…·ä½“çš„å‘Šè­¦ç³»ç»Ÿ
```

### ç¾éš¾æ¢å¤è®¡åˆ’

#### RTO/RPOç›®æ ‡
| æœåŠ¡ | RTO (æ¢å¤æ—¶é—´ç›®æ ‡) | RPO (æ¢å¤ç‚¹ç›®æ ‡) | å¤‡ä»½é¢‘ç‡ |
|------|-------------------|-------------------|----------|
| **åº”ç”¨æœåŠ¡** | 5åˆ†é’Ÿ | 1åˆ†é’Ÿ | å®æ—¶é•œåƒ |
| **æ•°æ®åº“** | 30åˆ†é’Ÿ | 15åˆ†é’Ÿ | æ¯15åˆ†é’Ÿå¢é‡ |
| **Redisç¼“å­˜** | 5åˆ†é’Ÿ | 0åˆ†é’Ÿ | æ— çŠ¶æ€ |
| **æ–‡ä»¶å­˜å‚¨** | 1å°æ—¶ | 1å°æ—¶ | æ¯æ—¥å…¨é‡ |

#### ç¾éš¾æ¢å¤æµç¨‹
```bash
#!/bin/bash
# scripts/disaster_recovery.sh

set -e

DISASTER_TYPE=$1
BACKUP_LOCATION=$2

echo "ğŸš¨ å¼€å§‹ç¾éš¾æ¢å¤æµç¨‹"
echo "ç¾éš¾ç±»å‹: $DISASTER_TYPE"
echo "å¤‡ä»½ä½ç½®: $BACKUP_LOCATION"

case $DISASTER_TYPE in
    "data_corruption")
        echo "ğŸ“Š æ•°æ®æŸåæ¢å¤..."
        ./scripts/restore_database.sh $BACKUP_LOCATION/latest_full_backup.dump
        ;;
    "server_failure")
        echo "ğŸ–¥ï¸ æœåŠ¡å™¨æ•…éšœæ¢å¤..."
        # 1. åœ¨æ–°æœåŠ¡å™¨ä¸Šéƒ¨ç½²ç¯å¢ƒ
        ./scripts/setup_new_server.sh
        # 2. æ¢å¤æ•°æ®
        ./scripts/restore_all_data.sh $BACKUP_LOCATION
        # 3. æ›´æ–°DNS
        ./scripts/update_dns.sh
        ;;
    "network_outage")
        echo "ğŸŒ ç½‘ç»œä¸­æ–­æ¢å¤..."
        # åˆ‡æ¢åˆ°å¤‡ç”¨ç½‘ç»œ
        ./scripts/failover_network.sh
        ;;
    "security_breach")
        echo "ğŸ”’ å®‰å…¨äº‹ä»¶æ¢å¤..."
        # 1. éš”ç¦»å—æ„ŸæŸ“ç³»ç»Ÿ
        ./scripts/isolate_compromised_systems.sh
        # 2. ä»å¹²å‡€å¤‡ä»½æ¢å¤
        ./scripts/restore_from_clean_backup.sh $BACKUP_LOCATION
        # 3. é‡ç½®æ‰€æœ‰å¯†ç å’Œå¯†é’¥
        ./_scripts/reset_credentials.sh
        ;;
esac

echo "âœ… ç¾éš¾æ¢å¤å®Œæˆ"
echo "ğŸ” æ‰§è¡Œæ¢å¤åéªŒè¯..."
./scripts/post_recovery_verification.sh

echo "ğŸ“¢ é€šçŸ¥ç›¸å…³äººå‘˜æ¢å¤å®Œæˆ"
./scripts/notify_recovery_completion.sh
```

---

## ğŸ¯ éƒ¨ç½²æœ€ä½³å®è·µ

### 1. éƒ¨ç½²æ¸…å•
- [ ] ç¯å¢ƒå˜é‡é…ç½®æ­£ç¡®
- [ ] æ•°æ®åº“è¿ç§»å®Œæˆ
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡
- [ ] æ€§èƒ½åŸºå‡†è¾¾æ ‡
- [ ] å®‰å…¨æ‰«æé€šè¿‡
- [ ] ç›‘æ§å‘Šè­¦é…ç½®
- [ ] æ—¥å¿—èšåˆæ­£å¸¸
- [ ] å¤‡ä»½ç­–ç•¥éªŒè¯

### 2. å‘å¸ƒç­–ç•¥
- **è“ç»¿éƒ¨ç½²**: é›¶åœæœºæ—¶é—´éƒ¨ç½²
- **é‡‘ä¸é›€å‘å¸ƒ**: é€æ­¥æµé‡åˆ‡æ¢
- **åŠŸèƒ½å¼€å…³**: åŠ¨æ€åŠŸèƒ½æ§åˆ¶
- **å›æ»šæœºåˆ¶**: å¿«é€Ÿæ•…éšœæ¢å¤

### 3. ç›‘æ§æŒ‡æ ‡
- **åº”ç”¨æŒ‡æ ‡**: å“åº”æ—¶é—´ã€é”™è¯¯ç‡ã€ååé‡
- **ç³»ç»ŸæŒ‡æ ‡**: CPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
- **ä¸šåŠ¡æŒ‡æ ‡**: é¢„æµ‹å‡†ç¡®ç‡ã€ç”¨æˆ·æ´»è·ƒåº¦
- **å®‰å…¨æŒ‡æ ‡**: ç™»å½•å¤±è´¥ã€å¼‚å¸¸è®¿é—®

### 4. è¿ç»´è‡ªåŠ¨åŒ–
- **è‡ªåŠ¨éƒ¨ç½²**: CI/CDæµæ°´çº¿
- **è‡ªåŠ¨æ‰©ç¼©å®¹**: åŸºäºè´Ÿè½½çš„å¼¹æ€§æ‰©å±•
- **è‡ªåŠ¨æ•…éšœæ¢å¤**: å¼‚å¸¸æ£€æµ‹å’Œè‡ªåŠ¨å¤„ç†
- **è‡ªåŠ¨å¤‡ä»½**: å®šæœŸæ•°æ®å¤‡ä»½å’ŒéªŒè¯

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0 | æ›´æ–°æ—¶é—´: 2025-11-16*