# ğŸ¤– æœºå™¨å­¦ä¹ æ¨¡å—æŒ‡å—

**ç‰ˆæœ¬**: v1.0.0
**æœ€åæ›´æ–°**: 2025-10-24
**ç»´æŠ¤è€…**: Claude AI Assistant

---

## ğŸ“š æ–‡æ¡£å¯¼èˆª

### ç›¸å…³æ–‡æ¡£
- **[ğŸ“Š æ•°æ®é‡‡é›†é…ç½®](../data/DATA_COLLECTION_SETUP.md)** - æ•°æ®æ”¶é›†å’Œé¢„å¤„ç†
- **[ğŸ—ï¸ ç³»ç»Ÿæ¶æ„](../architecture/ARCHITECTURE.md)** - ç³»ç»Ÿæ•´ä½“æ¶æ„
- **[ğŸ“‹ APIå‚è€ƒ](../reference/API_REFERENCE.md)** - APIæ¥å£æ–‡æ¡£
- **[ğŸ§ª æµ‹è¯•ç­–ç•¥](../testing/TEST_IMPROVEMENT_GUIDE.md)** - æµ‹è¯•æ–¹æ³•

### å›½é™…åŒ–è¯´æ˜
æœ¬æ–‡æ¡£æä¾›ä¸­æ–‡ç‰ˆæœ¬ï¼ˆæ¨èï¼‰ï¼Œè‹±æ–‡ç‰ˆæœ¬ä½œä¸ºå‚è€ƒï¼š
- ğŸ‡¨ğŸ‡³ **ä¸­æ–‡ç‰ˆæœ¬** (ä¸»è¦) - å½“å‰æ–‡æ¡£
- ğŸ‡ºğŸ‡¸ **English Version** (è®¡åˆ’ä¸­)

---

## ğŸ“‘ ç›®å½•

- [ğŸ“Š æ¨¡å—æ¦‚è¿°](#-æ¨¡å—æ¦‚è¿°)
- [ğŸ—ï¸ æ¶æ„è®¾è®¡](#ï¸-æ¶æ„è®¾è®¡)
- [ğŸ¤– æ¨¡å‹ç±»å‹](#-æ¨¡å‹ç±»å‹)
- [ğŸ“Š ç‰¹å¾å·¥ç¨‹](#-ç‰¹å¾å·¥ç¨‹)
- [ğŸš€ æ¨¡å‹è®­ç»ƒ](#-æ¨¡å‹è®­ç»ƒ)
- [ğŸ“ˆ æ¨¡å‹è¯„ä¼°](#-æ¨¡å‹è¯„ä¼°)
- [ğŸ’¾ æ¨¡å‹ç®¡ç†](#-æ¨¡å‹ç®¡ç†)
- [ğŸ”§ é…ç½®è¯´æ˜](#-é…ç½®è¯´æ˜)
- [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [ğŸ“š ä½¿ç”¨ç¤ºä¾‹](#-ä½¿ç”¨ç¤ºä¾‹)
- [ğŸ” ç›‘æ§å’Œæ—¥å¿—](#-ç›‘æ§å’Œæ—¥å¿—)
- [âš ï¸ æ•…éšœæ’é™¤](#ï¸-æ•…éšœæ’é™¤)
- [ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [ğŸ”— ç›¸å…³èµ„æº](#-ç›¸å…³èµ„æº)

---

## ğŸ“Š æ¨¡å—æ¦‚è¿°

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

æœºå™¨å­¦ä¹ æ¨¡å—æ˜¯è¶³çƒé¢„æµ‹ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œè´Ÿè´£ï¼š

- **æ¨¡å‹è®­ç»ƒ**: æ”¯æŒå¤šç§æœºå™¨å­¦ä¹ ç®—æ³•çš„è®­ç»ƒ
- **é¢„æµ‹æ¨ç†**: å®æ—¶é¢„æµ‹è¶³çƒæ¯”èµ›ç»“æœ
- **ç‰¹å¾å·¥ç¨‹**: æ•°æ®é¢„å¤„ç†å’Œç‰¹å¾ç”Ÿæˆ
- **æ¨¡å‹è¯„ä¼°**: å…¨é¢çš„æ¨¡å‹æ€§èƒ½è¯„ä¼°
- **æ¨¡å‹ç®¡ç†**: æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’Œéƒ¨ç½²

### ğŸ”§ æŠ€æœ¯æ ˆ

- **æ ¸å¿ƒæ¡†æ¶**: Python 3.11+, scikit-learn, pandas, numpy
- **æ·±åº¦å­¦ä¹ **: TensorFlow/PyTorch (è®¡åˆ’ä¸­)
- **æ¨¡å‹åºåˆ—åŒ–**: pickle, joblib
- **æ•°æ®å¤„ç†**: pandas, numpy
- **ç‰¹å¾å·¥ç¨‹**: scikit-learn preprocessing
- **æ¨¡å‹è¯„ä¼°**: scikit-learn metrics

### ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡

- **æ”¯æŒæ¨¡å‹ç±»å‹**: 8ç§ä¸»è¦ç®—æ³•
- **è®­ç»ƒæ•°æ®è§„æ¨¡**: æ”¯æŒç™¾ä¸‡çº§æ ·æœ¬
- **é¢„æµ‹å»¶è¿Ÿ**: < 10ms (å•æ¬¡é¢„æµ‹)
- **æ¨¡å‹å‡†ç¡®ç‡**: ç›®æ ‡ > 80%
- **å¹¶å‘æ”¯æŒ**: 1000+ QPS

## ğŸ—ï¸ MLæ¶æ„æ¦‚è§ˆ

### ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ML Pipeline                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®é¢„å¤„ç†      â”‚   æ¨¡å‹è®­ç»ƒ        â”‚        æ¨¡å‹éƒ¨ç½²               â”‚
â”‚  (Preprocessing) â”‚  (Training)     â”‚     (Deployment)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ ç‰¹å¾å·¥ç¨‹        â”‚ â€¢ æ¨¡å‹é€‰æ‹©        â”‚ â€¢ æ¨¡å‹æœåŠ¡åŒ–                 â”‚
â”‚ â€¢ æ•°æ®æ¸…æ´—        â”‚ â€¢ è¶…å‚æ•°è°ƒä¼˜      â”‚ â€¢ APIæ¥å£                   â”‚
â”‚ â€¢ ç‰¹å¾é€‰æ‹©        â”‚ â€¢ äº¤å‰éªŒè¯        â”‚ â€¢ æ‰¹é‡é¢„æµ‹                   â”‚
â”‚ â€¢ æ•°æ®åˆ†å‰²        â”‚ â€¢ æ¨¡å‹è¯„ä¼°        â”‚ â€¢ å®æ—¶é¢„æµ‹                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MLOpså±‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ¨¡å‹ç›‘æ§        â”‚   å®éªŒç®¡ç†        â”‚        æ¨¡å‹ç‰ˆæœ¬               â”‚
â”‚  (Monitoring)    â”‚ (Experiments)   â”‚    (Versioning)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æ€§èƒ½ç›‘æ§        â”‚ â€¢ MLflow         â”‚ â€¢ æ¨¡å‹æ³¨å†Œè¡¨                 â”‚
â”‚ â€¢ æ¼‚ç§»æ£€æµ‹        â”‚ â€¢ å®éªŒè·Ÿè¸ª        â”‚ â€¢ ç‰ˆæœ¬æ§åˆ¶                   â”‚
â”‚ â€¢ å‘Šè­¦é€šçŸ¥        â”‚ â€¢ å‚æ•°è®°å½•        â”‚ â€¢ å›æ»šæœºåˆ¶                   â”‚
â”‚ â€¢ è‡ªåŠ¨é‡è®­ç»ƒ      â”‚ â€¢ ç»“æœå¯¹æ¯”        â”‚ â€¢ A/Bæµ‹è¯•                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | æŠ€æœ¯æ ˆ | çŠ¶æ€ |
|------|------|--------|------|
| **PredictionModel** | é¢„æµ‹æ¨¡å‹åŸºç±» | Python, scikit-learn | âœ… è¿è¡Œä¸­ |
| **FootballPredictionModel** | è¶³çƒä¸“ç”¨é¢„æµ‹æ¨¡å‹ | éšæœºæ£®æ—, XGBoost | âœ… è¿è¡Œä¸­ |
| **BaselineModelTrainer** | åŸºå‡†æ¨¡å‹è®­ç»ƒå™¨ | MLflow, Optuna | ğŸš§ å¼€å‘ä¸­ |
| **FeatureProcessor** | ç‰¹å¾å¤„ç†å™¨ | pandas, numpy | âœ… è¿è¡Œä¸­ |
| **ModelMonitor** | æ¨¡å‹ç›‘æ§ | Prometheus, Grafana | âœ… è¿è¡Œä¸­ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.11+
- **æœºå™¨å­¦ä¹ åº“**: scikit-learn, XGBoost
- **å®éªŒè·Ÿè¸ª**: MLflow (å¯é€‰)
- **æ•°æ®å­˜å‚¨**: PostgreSQL, Redis

### 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

```bash
# 1. å®‰è£…MLä¾èµ–
pip install scikit-learn xgboost pandas numpy

# 2. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œé¢„æµ‹
python -c "
from src.models.prediction_model import FootballPredictionModel
model = FootballPredictionModel()
result = model.predict_match('Team A', 'Team B', {})
print(result)
"

# 3. è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹
python -c "
from src.models.prediction_model import FootballPredictionModel
import pandas as pd

# å‡†å¤‡è®­ç»ƒæ•°æ®
X = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})
y = pd.Series([0, 1, 0])

model = FootballPredictionModel('my_model')
metrics = model.train(X, y)
print(f'è®­ç»ƒå®Œæˆï¼Œå‡†ç¡®ç‡: {metrics[\"accuracy\"]:.3f}')
"

# 4. å¯åŠ¨MLå®éªŒè·Ÿè¸ª (å¯é€‰)
mlflow server --host 0.0.0.0 --port 5000
```

## ğŸ§  æ¨¡å‹è®¾è®¡

### 1. é¢„æµ‹æ¨¡å‹åŸºç±»

```python
# src/models/prediction_model.py

from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
import pandas as pd
import numpy as np
from datetime import datetime
import logging

class PredictionModel(ABC):
    """é¢„æµ‹æ¨¡å‹æŠ½è±¡åŸºç±»"""

    def __init__(
        self,
        model_name: str,
        model_type: str = "classification",
        **kwargs
    ):
        """
        åˆå§‹åŒ–é¢„æµ‹æ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°
            model_type: æ¨¡å‹ç±»å‹ (classification/regression)
            **kwargs: å…¶ä»–å‚æ•°
        """
        self.model_name = model_name
        self.model_type = model_type
        self.logger = logging.getLogger(f"ml.{self.__class__.__name__}")

        # æ¨¡å‹çŠ¶æ€
        self.is_trained = False
        self.model = None
        self.feature_columns = []
        self.target_column = "result"

        # å…ƒæ•°æ®
        self.metadata = {
            "created_at": datetime.now().isoformat(),
            "version": "1.0.0",
            "description": f"{model_name} prediction model",
            "hyperparameters": kwargs,
        }

    @abstractmethod
    def train(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> Dict[str, Any]:
        """
        è®­ç»ƒæ¨¡å‹

        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            **kwargs: è®­ç»ƒå‚æ•°

        Returns:
            è®­ç»ƒç»“æœæŒ‡æ ‡
        """
        pass

    @abstractmethod
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        è¿›è¡Œé¢„æµ‹

        Args:
            X: ç‰¹å¾æ•°æ®

        Returns:
            é¢„æµ‹ç»“æœ
        """
        pass

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹æ¦‚ç‡ (ä»…åˆ†ç±»æ¨¡å‹)

        Args:
            X: ç‰¹å¾æ•°æ®

        Returns:
            é¢„æµ‹æ¦‚ç‡
        """
        if self.model_type != "classification":
            raise ValueError("predict_proba only available for classification models")

        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")

        return self.model.predict_proba(X)

    def evaluate(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            X: ç‰¹å¾æ•°æ®
            y: çœŸå®æ ‡ç­¾

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

        if not self.is_trained:
            raise ValueError("Model must be trained before evaluation")

        # è·å–é¢„æµ‹ç»“æœ
        y_pred = self.predict(X)

        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = {}

        if self.model_type == "classification":
            metrics["accuracy"] = accuracy_score(y, y_pred)
            metrics["precision"] = precision_score(y, y_pred, average="weighted", zero_division=0)
            metrics["recall"] = recall_score(y, y_pred, average="weighted", zero_division=0)
            metrics["f1_score"] = f1_score(y, y_pred, average="weighted", zero_division=0)

            # å¦‚æœæ˜¯äºŒåˆ†ç±»æˆ–å¤šåˆ†ç±»ï¼Œè®¡ç®—AUC
            try:
                y_proba = self.predict_proba(X)
                if len(np.unique(y)) == 2:  # äºŒåˆ†ç±»
                    metrics["auc"] = roc_auc_score(y, y_proba[:, 1])
                else:  # å¤šåˆ†ç±»
                    metrics["auc"] = roc_auc_score(y, y_proba, multi_class="ovr", average="weighted")
            except Exception:
                self.logger.warning("Could not calculate AUC score")

        return metrics

    def get_feature_importance(self) -> Dict[str, float]:
        """
        è·å–ç‰¹å¾é‡è¦æ€§

        Returns:
            ç‰¹å¾é‡è¦æ€§å­—å…¸
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before getting feature importance")

        importance = {}

        if hasattr(self.model, "feature_importances_"):
            # æ ‘æ¨¡å‹ç­‰æ”¯æŒç‰¹å¾é‡è¦æ€§
            for feature, importance_score in zip(self.feature_columns, self.model.feature_importances_):
                importance[feature] = float(importance_score)
        elif hasattr(self.model, "coef_"):
            # çº¿æ€§æ¨¡å‹ç­‰
            coef = self.model.coef_
            if coef.ndim > 1:
                coef = np.abs(coef).mean(axis=0)

            for feature, coef_score in zip(self.feature_columns, coef):
                importance[feature] = float(coef_score)
        else:
            # ä½¿ç”¨SHAPå€¼æˆ–å…¶ä»–æ–¹æ³• (ç®€åŒ–å®ç°)
            for feature in self.feature_columns:
                importance[feature] = 1.0 / len(self.feature_columns)

        return importance

    def save_model(self, file_path: str) -> bool:
        """
        ä¿å­˜æ¨¡å‹åˆ°æ–‡ä»¶

        Args:
            file_path: ä¿å­˜è·¯å¾„

        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            import joblib

            model_data = {
                "model": self.model,
                "model_name": self.model_name,
                "model_type": self.model_type,
                "is_trained": self.is_trained,
                "feature_columns": self.feature_columns,
                "target_column": self.target_column,
                "metadata": self.metadata,
            }

            joblib.dump(model_data, file_path)
            self.logger.info(f"Model saved to: {file_path}")
            return True

        except Exception as e:
            self.logger.error(f"Failed to save model: {e}")
            return False

    def load_model(self, file_path: str) -> bool:
        """
        ä»æ–‡ä»¶åŠ è½½æ¨¡å‹

        Args:
            file_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            import joblib

            model_data = joblib.load(file_path)

            self.model = model_data["model"]
            self.model_name = model_data["model_name"]
            self.model_type = model_data["model_type"]
            self.is_trained = model_data["is_trained"]
            self.feature_columns = model_data["feature_columns"]
            self.target_column = model_data["target_column"]
            self.metadata = model_data["metadata"]

            self.logger.info(f"Model loaded from: {file_path}")
            return True

        except Exception as e:
            self.logger.error(f"Failed to load model: {e}")
            return False
```

### 2. è¶³çƒé¢„æµ‹æ¨¡å‹å®ç°

```python
# src/models/football_prediction_model.py

import pandas as pd
import numpy as np
from typing import Any, Dict, List, Optional
from datetime import datetime
from enum import Enum
import logging

from .prediction_model import PredictionModel

class PredictionType(Enum):
    """é¢„æµ‹ç±»å‹æšä¸¾"""
    MATCH_RESULT = "match_result"           # æ¯”èµ›ç»“æœ
    OVER_UNDER = "over_under"              # å¤§å°çƒ
    CORRECT_SCORE = "correct_score"        # æ­£ç¡®æ¯”åˆ†
    BOTH_TEAMS_SCORE = "both_teams_score"  # åŒæ–¹éƒ½è¿›çƒ

class FootballPredictionModel(PredictionModel):
    """è¶³çƒæ¯”èµ›é¢„æµ‹æ¨¡å‹"""

    def __init__(
        self,
        model_name: str = "football_predictor",
        prediction_type: PredictionType = PredictionType.MATCH_RESULT,
        **kwargs
    ):
        """
        åˆå§‹åŒ–è¶³çƒé¢„æµ‹æ¨¡å‹

        Args:
            model_name: æ¨¡å‹åç§°
            prediction_type: é¢„æµ‹ç±»å‹
            **kwargs: å…¶ä»–å‚æ•°
        """
        super().__init__(model_name, "classification", **kwargs)
        self.prediction_type = prediction_type

        # æ ¹æ®é¢„æµ‹ç±»å‹è®¾ç½®ç›®æ ‡ç±»åˆ«
        self._setup_target_classes()

        # æ¨¡å‹è¶…å‚æ•°
        self.hyperparameters = {
            "n_estimators": kwargs.get("n_estimators", 100),
            "max_depth": kwargs.get("max_depth", 10),
            "min_samples_split": kwargs.get("min_samples_split", 5),
            "min_samples_leaf": kwargs.get("min_samples_leaf", 2),
            "random_state": kwargs.get("random_state", 42),
        }

    def _setup_target_classes(self):
        """è®¾ç½®ç›®æ ‡ç±»åˆ«"""
        if self.prediction_type == PredictionType.MATCH_RESULT:
            self.target_classes = ["home_win", "draw", "away_win"]
        elif self.prediction_type == PredictionType.OVER_UNDER:
            self.target_classes = ["under", "over"]
        elif self.prediction_type == PredictionType.BOTH_TEAMS_SCORE:
            self.target_classes = ["no", "yes"]
        elif self.prediction_type == PredictionType.CORRECT_SCORE:
            # å¸¸è§æ¯”åˆ†
            self.target_classes = [
                "1-0", "2-0", "2-1", "1-1", "0-0",
                "0-1", "0-2", "1-2", "3-0", "0-3"
            ]
        else:
            raise ValueError(f"Unsupported prediction type: {self.prediction_type}")

    def train(self, X: pd.DataFrame, y: pd.Series, **kwargs) -> Dict[str, Any]:
        """
        è®­ç»ƒè¶³çƒé¢„æµ‹æ¨¡å‹

        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡å˜é‡
            **kwargs: è®­ç»ƒå‚æ•°

        Returns:
            è®­ç»ƒç»“æœæŒ‡æ ‡
        """
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.model_selection import cross_val_score
        from sklearn.preprocessing import LabelEncoder

        self.logger.info(f"Training {self.prediction_type.value} model with {len(X)} samples")

        # ç‰¹å¾å¤„ç†
        self.feature_columns = list(X.columns)

        # æ ‡ç­¾ç¼–ç 
        label_encoder = LabelEncoder()
        y_encoded = label_encoder.fit_transform(y)

        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
        self.label_encoder = label_encoder
        self.label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
        self.reverse_label_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))

        # åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
        self.model = RandomForestClassifier(**self.hyperparameters)

        # äº¤å‰éªŒè¯è¯„ä¼°
        cv_scores = cross_val_score(self.model, X, y_encoded, cv=5, scoring='accuracy')

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        self.model.fit(X, y_encoded)
        self.is_trained = True

        # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
        train_metrics = self.evaluate(X, y)

        # æ·»åŠ äº¤å‰éªŒè¯ç»“æœ
        train_metrics["cv_mean_accuracy"] = float(cv_scores.mean())
        train_metrics["cv_std_accuracy"] = float(cv_scores.std())

        # æ›´æ–°å…ƒæ•°æ®
        self.metadata.update({
            "last_trained": datetime.now().isoformat(),
            "training_samples": len(X),
            "feature_count": len(self.feature_columns),
            "prediction_type": self.prediction_type.value,
            "label_mapping": self.label_mapping,
            "metrics": train_metrics,
        })

        self.logger.info(
            f"Model trained successfully. "
            f"Accuracy: {train_metrics['accuracy']:.3f}, "
            f"CV Accuracy: {train_metrics['cv_mean_accuracy']:.3f} Â± {train_metrics['cv_std_accuracy']:.3f}"
        )

        return train_metrics

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹æ¯”èµ›ç»“æœ

        Args:
            X: ç‰¹å¾æ•°æ®

        Returns:
            é¢„æµ‹ç»“æœ
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")

        predictions_encoded = self.model.predict(X)

        # è½¬æ¢å›åŸå§‹æ ‡ç­¾
        predictions = [self.reverse_label_mapping[pred] for pred in predictions_encoded]

        return np.array(predictions)

    def predict_match(
        self,
        home_team: str,
        away_team: str,
        features: Dict[str, Any],
        return_probabilities: bool = True
    ) -> Dict[str, Any]:
        """
        é¢„æµ‹å•åœºæ¯”èµ›ç»“æœ

        Args:
            home_team: ä¸»é˜Ÿåç§°
            away_team: å®¢é˜Ÿåç§°
            features: ç‰¹å¾å­—å…¸
            return_probabilities: æ˜¯å¦è¿”å›æ¦‚ç‡

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        if not self.is_trained:
            raise ValueError("Model must be trained before prediction")

        # è½¬æ¢ç‰¹å¾ä¸ºDataFrame
        feature_df = pd.DataFrame([features])

        # ç¡®ä¿ç‰¹å¾åˆ—é¡ºåºæ­£ç¡®
        feature_df = feature_df[self.feature_columns]

        # é¢„æµ‹
        prediction_encoded = self.model.predict(feature_df)[0]
        prediction = self.reverse_label_mapping[prediction_encoded]

        result = {
            "home_team": home_team,
            "away_team": away_team,
            "prediction": prediction,
            "prediction_type": self.prediction_type.value,
            "features_used": list(features.keys()),
            "model_name": self.model_name,
            "prediction_time": datetime.now().isoformat(),
        }

        # æ·»åŠ æ¦‚ç‡ä¿¡æ¯
        if return_probabilities:
            probabilities = self.model.predict_proba(feature_df)[0]

            # æ„å»ºæ¦‚ç‡å­—å…¸
            prob_dict = {}
            for i, class_name in enumerate(self.label_encoder.classes_):
                prob_dict[class_name] = float(probabilities[i])

            result["probabilities"] = prob_dict
            result["confidence"] = float(np.max(probabilities))

        return result

    def batch_predict(self, matches: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡é¢„æµ‹æ¯”èµ›ç»“æœ

        Args:
            matches: æ¯”èµ›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« home_team, away_team, features

        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []

        for match in matches:
            try:
                result = self.predict_match(
                    home_team=match.get("home_team", ""),
                    away_team=match.get("away_team", ""),
                    features=match.get("features", {}),
                    return_probabilities=True
                )
                results.append(result)
            except Exception as e:
                self.logger.error(f"Failed to predict match: {e}")
                # æ·»åŠ é”™è¯¯ç»“æœ
                error_result = {
                    "home_team": match.get("home_team", ""),
                    "away_team": match.get("away_team", ""),
                    "error": str(e),
                    "prediction": None,
                }
                results.append(error_result)

        return results

    def explain_prediction(
        self,
        home_team: str,
        away_team: str,
        features: Dict[str, Any],
        top_features: int = 5
    ) -> Dict[str, Any]:
        """
        è§£é‡Šé¢„æµ‹ç»“æœ

        Args:
            home_team: ä¸»é˜Ÿåç§°
            away_team: å®¢é˜Ÿåç§°
            features: ç‰¹å¾å­—å…¸
            top_features: è¿”å›æœ€é‡è¦çš„ç‰¹å¾æ•°é‡

        Returns:
            è§£é‡Šç»“æœ
        """
        # è·å–é¢„æµ‹ç»“æœ
        prediction_result = self.predict_match(home_team, away_team, features)

        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = self.get_feature_importance()

        # è·å–å½“å‰ç‰¹å¾çš„é‡è¦æ€§
        current_features_importance = {}
        for feature, value in features.items():
            if feature in feature_importance:
                current_features_importance[feature] = {
                    "value": value,
                    "importance": feature_importance[feature],
                }

        # æ’åºå¹¶å–å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾
        sorted_features = sorted(
            current_features_importance.items(),
            key=lambda x: x[1]["importance"],
            reverse=True
        )[:top_features]

        explanation = {
            "match": f"{home_team} vs {away_team}",
            "prediction": prediction_result["prediction"],
            "confidence": prediction_result.get("confidence", 0),
            "top_features": [
                {
                    "name": feature,
                    "value": info["value"],
                    "importance": info["importance"],
                    "impact": self._calculate_feature_impact(feature, info["value"])
                }
                for feature, info in sorted_features
            ],
            "model_info": {
                "model_name": self.model_name,
                "prediction_type": self.prediction_type.value,
                "feature_count": len(self.feature_columns),
            }
        }

        return explanation

    def _calculate_feature_impact(self, feature: str, value: Any) -> str:
        """è®¡ç®—ç‰¹å¾å¯¹é¢„æµ‹çš„å½±å“"""
        if value > 0:
            return f"æ­£å‘å½±å“ (+{value:.2f})"
        elif value < 0:
            return f"è´Ÿå‘å½±å“ ({value:.2f})"
        else:
            return "ä¸­æ€§å½±å“ (0.00)"
```

## ğŸ”§ æ¨¡å‹è®­ç»ƒ

### 1. è®­ç»ƒæ•°æ®å‡†å¤‡

```python
# src/ml/data_preparation.py

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import logging

class FootballDataPreparer:
    """è¶³çƒæ•°æ®é¢„å¤„ç†å™¨"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.label_encoders = {}
        self.feature_columns = []
        self.logger = logging.getLogger(__name__)

    def prepare_match_data(
        self,
        matches_df: pd.DataFrame,
        target_column: str = "result",
        test_size: float = 0.2,
        random_state: int = 42
    ) -> Tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:
        """
        å‡†å¤‡æ¯”èµ›æ•°æ®ç”¨äºè®­ç»ƒ

        Args:
            matches_df: æ¯”èµ›æ•°æ®DataFrame
            target_column: ç›®æ ‡åˆ—å
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­

        Returns:
            (X_train, y_train, X_test, y_test)
        """
        self.logger.info(f"Preparing data with {len(matches_df)} matches")

        # æ•°æ®æ¸…æ´—
        cleaned_df = self._clean_data(matches_df)

        # ç‰¹å¾å·¥ç¨‹
        features_df = self._engineer_features(cleaned_df)

        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        X = features_df.drop(columns=[target_column])
        y = features_df[target_column]

        # ç¼–ç åˆ†ç±»å˜é‡
        X_encoded = self._encode_categorical_features(X)

        # ç‰¹å¾æ ‡å‡†åŒ–
        X_scaled = self._scale_features(X_encoded)

        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=test_size, random_state=random_state, stratify=y
        )

        self.feature_columns = list(X_train.columns)
        self.logger.info(f"Data prepared: {len(X_train)} training, {len(X_test)} testing samples")

        return X_train, y_train, X_test, y_test

    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        # ç§»é™¤ç¼ºå¤±å€¼è¿‡å¤šçš„è¡Œ
        df_clean = df.dropna(thresh=len(df.columns) * 0.7)

        # å¡«å……ç¼ºå¤±å€¼
        numeric_columns = df_clean.select_dtypes(include=[np.number]).columns
        df_clean[numeric_columns] = df_clean[numeric_columns].fillna(df_clean[numeric_columns].median())

        categorical_columns = df_clean.select_dtypes(include=['object']).columns
        for col in categorical_columns:
            df_clean[col] = df_clean[col].fillna('unknown')

        return df_clean

    def _engineer_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾å·¥ç¨‹"""
        df_features = df.copy()

        # ä¸»åœºä¼˜åŠ¿ç‰¹å¾
        if 'home_team' in df_features.columns and 'away_team' in df_features.columns:
            df_features['is_home_favorite'] = (
                df_features['home_team_rating'] > df_features['away_team_rating']
            ).astype(int)

        # æœ€è¿‘è¡¨ç°ç‰¹å¾
        if 'home_team_recent_form' in df_features.columns:
            df_features['home_team_form_trend'] = (
                df_features['home_team_recent_form'].rolling(window=3).mean()
            )

        # å†å²äº¤é”‹ç‰¹å¾
        if 'head_to_head_wins' in df_features.columns:
            df_features['head_to_head_win_rate'] = (
                df_features['head_to_head_wins'] / df_features['head_to_head_matches']
            )

        return df_features

    def _encode_categorical_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """ç¼–ç åˆ†ç±»ç‰¹å¾"""
        X_encoded = X.copy()

        categorical_columns = X_encoded.select_dtypes(include=['object']).columns

        for col in categorical_columns:
            if col not in self.label_encoders:
                self.label_encoders[col] = LabelEncoder()
                X_encoded[col] = self.label_encoders[col].fit_transform(X_encoded[col])
            else:
                # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
                known_classes = set(self.label_encoders[col].classes_)
                X_encoded[col] = X_encoded[col].apply(
                    lambda x: x if x in known_classes else 'unknown'
                )

                # å¦‚æœæœ‰æ–°ç±»åˆ«ï¼Œé‡æ–°æ‹Ÿåˆç¼–ç å™¨
                if not set(X_encoded[col]).issubset(known_classes):
                    self.label_encoders[col].classes_ = np.append(
                        self.label_encoders[col].classes_, 'unknown'
                    )

                X_encoded[col] = self.label_encoders[col].transform(X_encoded[col])

        return X_encoded

    def _scale_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """ç‰¹å¾æ ‡å‡†åŒ–"""
        X_scaled = X.copy()
        numeric_columns = X_scaled.select_dtypes(include=[np.number]).columns

        if len(self.feature_columns) == 0:  # é¦–æ¬¡æ‹Ÿåˆ
            X_scaled[numeric_columns] = self.scaler.fit_transform(X_scaled[numeric_columns])
        else:
            X_scaled[numeric_columns] = self.scaler.transform(X_scaled[numeric_columns])

        return X_scaled

    def prepare_prediction_features(self, match_data: Dict[str, Any]) -> pd.DataFrame:
        """
        ä¸ºå•åœºæ¯”èµ›é¢„æµ‹å‡†å¤‡ç‰¹å¾

        Args:
            match_data: æ¯”èµ›æ•°æ®å­—å…¸

        Returns:
            é¢„å¤„ç†åçš„ç‰¹å¾DataFrame
        """
        df = pd.DataFrame([match_data])
        df_features = self._engineer_features(df)
        df_encoded = self._encode_categorical_features(df_features)
        df_scaled = self._scale_features(df_encoded)

        # ç¡®ä¿ç‰¹å¾é¡ºåºä¸€è‡´
        if self.feature_columns:
            for col in self.feature_columns:
                if col not in df_scaled.columns:
                    df_scaled[col] = 0

            df_scaled = df_scaled[self.feature_columns]

        return df_scaled
```

### 2. æ¨¡å‹è®­ç»ƒå™¨

```python
# src/ml/model_trainer.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from datetime import datetime
import logging
import json
import os

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.metrics import classification_report, confusion_matrix
import mlflow
import mlflow.sklearn

from ..models.football_prediction_model import FootballPredictionModel, PredictionType
from .data_preparation import FootballDataPreparer

class FootballModelTrainer:
    """è¶³çƒæ¨¡å‹è®­ç»ƒå™¨"""

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®
        """
        self.config = config or {}
        self.logger = logging.getLogger(__name__)
        self.data_preparer = FootballDataPreparer()

        # MLflowé…ç½®
        self.mlflow_enabled = self.config.get("mlflow_enabled", False)
        if self.mlflow_enabled:
            mlflow.set_tracking_uri(self.config.get("mlflow_tracking_uri", "http://localhost:5000"))
            mlflow.set_experiment(self.config.get("mlflow_experiment", "football-prediction"))

    def train_match_result_model(
        self,
        matches_df: pd.DataFrame,
        target_column: str = "result",
        hyperparameter_tuning: bool = True
    ) -> FootballPredictionModel:
        """
        è®­ç»ƒæ¯”èµ›ç»“æœé¢„æµ‹æ¨¡å‹

        Args:
            matches_df: æ¯”èµ›æ•°æ®
            target_column: ç›®æ ‡åˆ—å
            hyperparameter_tuning: æ˜¯å¦è¿›è¡Œè¶…å‚æ•°è°ƒä¼˜

        Returns:
            è®­ç»ƒå¥½çš„æ¨¡å‹
        """
        self.logger.info("Starting match result model training")

        # å‡†å¤‡æ•°æ®
        X_train, y_train, X_test, y_test = self.data_preparer.prepare_match_data(
            matches_df, target_column
        )

        # åˆ›å»ºæ¨¡å‹
        model = FootballPredictionModel(
            model_name="match_result_predictor",
            prediction_type=PredictionType.MATCH_RESULT
        )

        if self.mlflow_enabled:
            with mlflow.start_run(run_name=f"match_result_{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
                return self._train_with_mlflow(model, X_train, y_train, X_test, y_test, hyperparameter_tuning)
        else:
            return self._train_model(model, X_train, y_train, X_test, y_test, hyperparameter_tuning)

    def _train_model(
        self,
        model: FootballPredictionModel,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        hyperparameter_tuning: bool
    ) -> FootballPredictionModel:
        """è®­ç»ƒæ¨¡å‹"""

        if hyperparameter_tuning:
            # è¶…å‚æ•°è°ƒä¼˜
            best_params = self._hyperparameter_tuning(X_train, y_train)
            model.hyperparameters.update(best_params)

        # è®­ç»ƒæ¨¡å‹
        train_metrics = model.train(X_train, y_train)

        # è¯„ä¼°æ¨¡å‹
        test_metrics = model.evaluate(X_test, y_test)

        # è®°å½•ç»“æœ
        self.logger.info(f"Training completed. Test accuracy: {test_metrics['accuracy']:.3f}")

        # ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
        self._generate_training_report(model, train_metrics, test_metrics, X_test, y_test)

        return model

    def _train_with_mlflow(
        self,
        model: FootballPredictionModel,
        X_train: pd.DataFrame,
        y_train: pd.Series,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        hyperparameter_tuning: bool
    ) -> FootballPredictionModel:
        """ä½¿ç”¨MLflowè®­ç»ƒæ¨¡å‹"""

        if hyperparameter_tuning:
            best_params = self._hyperparameter_tuning(X_train, y_train)
            model.hyperparameters.update(best_params)
            mlflow.log_params(best_params)

        # è®°å½•å‚æ•°
        mlflow.log_params({
            "model_type": model.model_type,
            "prediction_type": model.prediction_type.value,
            "training_samples": len(X_train),
            "test_samples": len(X_test),
            "feature_count": len(X_train.columns),
        })

        # è®­ç»ƒæ¨¡å‹
        train_metrics = model.train(X_train, y_train)

        # è¯„ä¼°æ¨¡å‹
        test_metrics = model.evaluate(X_test, y_test)

        # è®°å½•æŒ‡æ ‡
        for metric_name, metric_value in train_metrics.items():
            if isinstance(metric_value, (int, float)):
                mlflow.log_metric(f"train_{metric_name}", metric_value)

        for metric_name, metric_value in test_metrics.items():
            if isinstance(metric_value, (int, float)):
                mlflow.log_metric(f"test_{metric_name}", metric_value)

        # è®°å½•æ¨¡å‹
        mlflow.sklearn.log_model(model.model, "model")

        # è®°å½•ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_feature_importance()
        mlflow.log_dict(feature_importance, "feature_importance.json")

        self.logger.info(f"Training completed with MLflow. Test accuracy: {test_metrics['accuracy']:.3f}")

        return model

    def _hyperparameter_tuning(
        self,
        X: pd.DataFrame,
        y: pd.Series
    ) -> Dict[str, Any]:
        """è¶…å‚æ•°è°ƒä¼˜"""

        param_grid = {
            'n_estimators': [50, 100, 200],
            'max_depth': [5, 10, 15, None],
            'min_samples_split': [2, 5, 10],
            'min_samples_leaf': [1, 2, 4],
            'max_features': ['sqrt', 'log2', None]
        }

        rf = RandomForestClassifier(random_state=42)
        grid_search = GridSearchCV(
            rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1
        )

        grid_search.fit(X, y)

        self.logger.info(f"Best parameters: {grid_search.best_params_}")
        self.logger.info(f"Best CV score: {grid_search.best_score_:.3f}")

        return grid_search.best_params_

    def _generate_training_report(
        self,
        model: FootballPredictionModel,
        train_metrics: Dict[str, float],
        test_metrics: Dict[str, float],
        X_test: pd.DataFrame,
        y_test: pd.Series
    ) -> None:
        """ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š"""

        # é¢„æµ‹æµ‹è¯•é›†
        y_pred = model.predict(X_test)

        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(y_test, y_pred, output_dict=True)

        # ç”Ÿæˆæ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(y_test, y_pred)

        # è·å–ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_feature_importance()

        # åˆ›å»ºæŠ¥å‘Š
        report = {
            "model_info": {
                "model_name": model.model_name,
                "prediction_type": model.prediction_type.value,
                "training_time": model.metadata.get("last_trained"),
                "model_version": model.metadata.get("version"),
            },
            "data_info": {
                "training_samples": len(X_test) * 4 // 3,  # ä¼°ç®—è®­ç»ƒé›†å¤§å°
                "test_samples": len(X_test),
                "feature_count": len(model.feature_columns),
                "target_classes": list(model.label_mapping.keys()) if hasattr(model, 'label_mapping') else [],
            },
            "performance": {
                "train_metrics": train_metrics,
                "test_metrics": test_metrics,
            },
            "classification_report": class_report,
            "confusion_matrix": conf_matrix.tolist(),
            "feature_importance": dict(
                sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]
            ),
        }

        # ä¿å­˜æŠ¥å‘Š
        reports_dir = self.config.get("reports_dir", "ml_reports")
        os.makedirs(reports_dir, exist_ok=True)

        report_path = os.path.join(
            reports_dir,
            f"training_report_{model.model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )

        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        self.logger.info(f"Training report saved to: {report_path}")
```

## ğŸ“Š æ¨¡å‹è¯„ä¼°

### 1. è¯„ä¼°æŒ‡æ ‡

```python
# src/ml/model_evaluation.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report,
    precision_recall_curve, roc_curve
)
import logging

class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def comprehensive_evaluation(
        self,
        model,
        X_test: pd.DataFrame,
        y_test: pd.Series,
        target_classes: List[str] = None
    ) -> Dict[str, Any]:
        """
        ç»¼åˆæ¨¡å‹è¯„ä¼°

        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•æ ‡ç­¾
            target_classes: ç›®æ ‡ç±»åˆ«åˆ—è¡¨

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        self.logger.info(f"Evaluating model with {len(X_test)} test samples")

        # è·å–é¢„æµ‹ç»“æœ
        y_pred = model.predict(X_test)
        y_proba = None

        try:
            y_proba = model.predict_proba(X_test)
        except Exception:
            self.logger.warning("Model does not support probability prediction")

        # åŸºç¡€æŒ‡æ ‡
        basic_metrics = self._calculate_basic_metrics(y_test, y_pred, y_proba)

        # è¯¦ç»†åˆ†ç±»æŠ¥å‘Š
        class_report = classification_report(y_test, y_pred, target_names=target_classes, output_dict=True)

        # æ··æ·†çŸ©é˜µ
        conf_matrix = confusion_matrix(y_test, y_pred)

        # ROCæ›²çº¿æ•°æ® (å¦‚æœæ˜¯äºŒåˆ†ç±»)
        roc_data = None
        if y_proba is not None and len(np.unique(y_test)) == 2:
            roc_data = self._calculate_roc_curve(y_test, y_proba[:, 1])

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = model.get_feature_importance()

        # é¢„æµ‹åˆ†å¸ƒåˆ†æ
        prediction_distribution = self._analyze_prediction_distribution(y_test, y_pred)

        # é”™è¯¯åˆ†æ
        error_analysis = self._analyze_predictions(X_test, y_test, y_pred, y_proba)

        evaluation_results = {
            "basic_metrics": basic_metrics,
            "classification_report": class_report,
            "confusion_matrix": conf_matrix.tolist(),
            "feature_importance": feature_importance,
            "prediction_distribution": prediction_distribution,
            "error_analysis": error_analysis,
            "roc_curve": roc_data,
            "evaluation_time": pd.Timestamp.now().isoformat(),
        }

        return evaluation_results

    def _calculate_basic_metrics(
        self,
        y_true: pd.Series,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None
    ) -> Dict[str, float]:
        """è®¡ç®—åŸºç¡€è¯„ä¼°æŒ‡æ ‡"""

        metrics = {
            "accuracy": accuracy_score(y_true, y_pred),
            "precision": precision_score(y_true, y_pred, average="weighted", zero_division=0),
            "recall": recall_score(y_true, y_pred, average="weighted", zero_division=0),
            "f1_score": f1_score(y_true, y_pred, average="weighted", zero_division=0),
        }

        # AUC (å¦‚æœæ”¯æŒæ¦‚ç‡é¢„æµ‹)
        if y_proba is not None:
            try:
                if len(np.unique(y_true)) == 2:  # äºŒåˆ†ç±»
                    metrics["auc"] = roc_auc_score(y_true, y_proba[:, 1])
                else:  # å¤šåˆ†ç±»
                    metrics["auc"] = roc_auc_score(y_true, y_proba, multi_class="ovr", average="weighted")
            except Exception:
                self.logger.warning("Could not calculate AUC score")

        return metrics

    def _calculate_roc_curve(self, y_true: pd.Series, y_scores: np.ndarray) -> Dict[str, List]:
        """è®¡ç®—ROCæ›²çº¿æ•°æ®"""
        fpr, tpr, thresholds = roc_curve(y_true, y_scores)
        auc_score = roc_auc_score(y_true, y_scores)

        return {
            "false_positive_rate": fpr.tolist(),
            "true_positive_rate": tpr.tolist(),
            "thresholds": thresholds.tolist(),
            "auc": auc_score,
        }

    def _analyze_prediction_distribution(
        self,
        y_true: pd.Series,
        y_pred: np.ndarray
    ) -> Dict[str, Any]:
        """åˆ†æé¢„æµ‹åˆ†å¸ƒ"""

        # å®é™…åˆ†å¸ƒ
        actual_distribution = y_true.value_counts().to_dict()

        # é¢„æµ‹åˆ†å¸ƒ
        pred_series = pd.Series(y_pred)
        predicted_distribution = pred_series.value_counts().to_dict()

        # æŒ‰ç±»åˆ«çš„å‡†ç¡®æ€§
        class_accuracy = {}
        for class_name in np.unique(y_true):
            class_mask = (y_true == class_name)
            if class_mask.sum() > 0:
                class_accuracy[class_name] = accuracy_score(
                    y_true[class_mask], y_pred[class_mask]
                )

        return {
            "actual_distribution": actual_distribution,
            "predicted_distribution": predicted_distribution,
            "class_accuracy": class_accuracy,
        }

    def _analyze_predictions(
        self,
        X_test: pd.DataFrame,
        y_true: pd.Series,
        y_pred: np.ndarray,
        y_proba: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """åˆ†æé¢„æµ‹ç»“æœ"""

        # é”™è¯¯é¢„æµ‹çš„æ ·æœ¬
        incorrect_mask = (y_true != y_pred)
        incorrect_indices = np.where(incorrect_mask)[0]

        # ç½®ä¿¡åº¦åˆ†æ (å¦‚æœæœ‰æ¦‚ç‡)
        confidence_analysis = None
        if y_proba is not None:
            predicted_confidence = np.max(y_proba, axis=1)

            confidence_analysis = {
                "mean_confidence": float(np.mean(predicted_confidence)),
                "correct_predictions_mean_confidence": float(
                    np.mean(predicted_confidence[~incorrect_mask])
                ),
                "incorrect_predictions_mean_confidence": float(
                    np.mean(predicted_confidence[incorrect_mask])
                ),
            }

        # é”™è¯¯æ ·æœ¬åˆ†æ
        error_samples = []
        for idx in incorrect_indices[:10]:  # åªåˆ†æå‰10ä¸ªé”™è¯¯æ ·æœ¬
            error_info = {
                "index": int(idx),
                "true_label": str(y_true.iloc[idx]),
                "predicted_label": str(y_pred[idx]),
                "features": X_test.iloc[idx].to_dict(),
            }

            if y_proba is not None:
                error_info["predicted_probabilities"] = y_proba[idx].tolist()
                error_info["confidence"] = float(np.max(y_proba[idx]))

            error_samples.append(error_info)

        return {
            "total_errors": int(len(incorrect_indices)),
            "error_rate": float(len(incorrect_indices) / len(y_true)),
            "confidence_analysis": confidence_analysis,
            "error_samples": error_samples,
        }

    def generate_evaluation_report(
        self,
        evaluation_results: Dict[str, Any],
        model_name: str,
        save_path: Optional[str] = None
    ) -> str:
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""

        report = f"""
# æ¨¡å‹è¯„ä¼°æŠ¥å‘Š

## æ¨¡å‹ä¿¡æ¯
- **æ¨¡å‹åç§°**: {model_name}
- **è¯„ä¼°æ—¶é—´**: {evaluation_results['evaluation_time']}

## åŸºç¡€æ€§èƒ½æŒ‡æ ‡
"""

        # æ·»åŠ åŸºç¡€æŒ‡æ ‡
        metrics = evaluation_results['basic_metrics']
        for metric_name, metric_value in metrics.items():
            report += f"- **{metric_name.upper()}**: {metric_value:.4f}\n"

        # æ·»åŠ é¢„æµ‹åˆ†å¸ƒ
        pred_dist = evaluation_results['prediction_distribution']
        report += f"""
## é¢„æµ‹åˆ†å¸ƒåˆ†æ

### å®é™…åˆ†å¸ƒ
"""
        for class_name, count in pred_dist['actual_distribution'].items():
            report += f"- {class_name}: {count}\n"

        report += "\n### é¢„æµ‹åˆ†å¸ƒ\n"
        for class_name, count in pred_dist['predicted_distribution'].items():
            report += f"- {class_name}: {count}\n"

        report += "\n### å„ç±»åˆ«å‡†ç¡®ç‡\n"
        for class_name, accuracy in pred_dist['class_accuracy'].items():
            report += f"- {class_name}: {accuracy:.4f}\n"

        # æ·»åŠ é”™è¯¯åˆ†æ
        error_analysis = evaluation_results['error_analysis']
        report += f"""
## é”™è¯¯åˆ†æ
- **æ€»é”™è¯¯æ•°**: {error_analysis['total_errors']}
- **é”™è¯¯ç‡**: {error_analysis['error_rate']:.4f}
"""

        if error_analysis.get('confidence_analysis'):
            conf_analysis = error_analysis['confidence_analysis']
            report += f"""
### ç½®ä¿¡åº¦åˆ†æ
- **å¹³å‡ç½®ä¿¡åº¦**: {conf_analysis['mean_confidence']:.4f}
- **æ­£ç¡®é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦**: {conf_analysis['correct_predictions_mean_confidence']:.4f}
- **é”™è¯¯é¢„æµ‹å¹³å‡ç½®ä¿¡åº¦**: {conf_analysis['incorrect_predictions_mean_confidence']:.4f}
"""

        # æ·»åŠ ç‰¹å¾é‡è¦æ€§
        feature_importance = evaluation_results['feature_importance']
        report += f"""
## ç‰¹å¾é‡è¦æ€§ (Top 10)
"""
        for i, (feature, importance) in enumerate(
            sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]
        ):
            report += f"{i+1}. **{feature}**: {importance:.4f}\n"

        # ä¿å­˜æŠ¥å‘Š
        if save_path:
            with open(save_path, 'w', encoding='utf-8') as f:
                f.write(report)
            self.logger.info(f"Evaluation report saved to: {save_path}")

        return report
```

## ğŸš€ æ¨¡å‹éƒ¨ç½²

### 1. æ¨¡å‹æœåŠ¡åŒ–

```python
# src/ml/model_service.py

from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
import asyncio
import logging
from datetime import datetime
import uuid

from ..models.football_prediction_model import FootballPredictionModel

class PredictionRequest(BaseModel):
    """é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    home_team: str = Field(..., description="ä¸»é˜Ÿåç§°")
    away_team: str = Field(..., description="å®¢é˜Ÿåç§°")
    features: Dict[str, Any] = Field(..., description="ç‰¹å¾æ•°æ®")
    include_explanation: bool = Field(False, description="æ˜¯å¦åŒ…å«è§£é‡Š")

class BatchPredictionRequest(BaseModel):
    """æ‰¹é‡é¢„æµ‹è¯·æ±‚æ¨¡å‹"""
    matches: List[Dict[str, Any]] = Field(..., description="æ¯”èµ›åˆ—è¡¨")
    include_explanations: bool = Field(False, description="æ˜¯å¦åŒ…å«è§£é‡Š")

class PredictionResponse(BaseModel):
    """é¢„æµ‹å“åº”æ¨¡å‹"""
    request_id: str = Field(..., description="è¯·æ±‚ID")
    prediction: Dict[str, Any] = Field(..., description="é¢„æµ‹ç»“æœ")
    explanation: Optional[Dict[str, Any]] = Field(None, description="é¢„æµ‹è§£é‡Š")
    timestamp: str = Field(..., description="é¢„æµ‹æ—¶é—´")

class ModelService:
    """æ¨¡å‹æœåŠ¡"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.models: Dict[str, FootballPredictionModel] = {}
        self.prediction_cache = {}
        self.request_queue = asyncio.Queue()
        self.is_processing = False

    def register_model(self, model: FootballPredictionModel) -> None:
        """æ³¨å†Œæ¨¡å‹"""
        self.models[model.model_name] = model
        self.logger.info(f"Model registered: {model.model_name}")

    def get_model(self, model_name: str) -> Optional[FootballPredictionModel]:
        """è·å–æ¨¡å‹"""
        return self.models.get(model_name)

    async def predict_match(
        self,
        request: PredictionRequest,
        model_name: str = "football_predictor"
    ) -> PredictionResponse:
        """
        é¢„æµ‹å•åœºæ¯”èµ›

        Args:
            request: é¢„æµ‹è¯·æ±‚
            model_name: æ¨¡å‹åç§°

        Returns:
            é¢„æµ‹å“åº”
        """
        request_id = str(uuid.uuid4())

        try:
            model = self.get_model(model_name)
            if not model:
                raise HTTPException(status_code=404, detail=f"Model {model_name} not found")

            if not model.is_trained:
                raise HTTPException(status_code=400, detail="Model is not trained")

            # æ£€æŸ¥ç¼“å­˜
            cache_key = self._generate_cache_key(request, model_name)
            if cache_key in self.prediction_cache:
                cached_result = self.prediction_cache[cache_key]
                self.logger.debug(f"Cache hit for request: {request_id}")
                return PredictionResponse(
                    request_id=request_id,
                    prediction=cached_result["prediction"],
                    explanation=cached_result.get("explanation"),
                    timestamp=cached_result["timestamp"]
                )

            # æ‰§è¡Œé¢„æµ‹
            prediction = model.predict_match(
                home_team=request.home_team,
                away_team=request.away_team,
                features=request.features,
                return_probabilities=True
            )

            # ç”Ÿæˆè§£é‡Š (å¦‚æœéœ€è¦)
            explanation = None
            if request.include_explanation:
                explanation = model.explain_prediction(
                    home_team=request.home_team,
                    away_team=request.away_team,
                    features=request.features
                )

            # ç¼“å­˜ç»“æœ
            result = {
                "prediction": prediction,
                "explanation": explanation,
                "timestamp": datetime.now().isoformat()
            }
            self.prediction_cache[cache_key] = result

            self.logger.info(f"Prediction completed: {request_id}")

            return PredictionResponse(
                request_id=request_id,
                prediction=prediction,
                explanation=explanation,
                timestamp=result["timestamp"]
            )

        except Exception as e:
            self.logger.error(f"Prediction failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    async def batch_predict(
        self,
        request: BatchPredictionRequest,
        model_name: str = "football_predictor"
    ) -> List[PredictionResponse]:
        """
        æ‰¹é‡é¢„æµ‹

        Args:
            request: æ‰¹é‡é¢„æµ‹è¯·æ±‚
            model_name: æ¨¡å‹åç§°

        Returns:
            é¢„æµ‹å“åº”åˆ—è¡¨
        """
        model = self.get_model(model_name)
        if not model:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")

        if not model.is_trained:
            raise HTTPException(status_code=400, detail="Model is not trained")

        # å‡†å¤‡æ‰¹é‡é¢„æµ‹æ•°æ®
        matches = []
        for match in request.matches:
            matches.append({
                "home_team": match.get("home_team", ""),
                "away_team": match.get("away_team", ""),
                "features": match.get("features", {})
            })

        # æ‰§è¡Œæ‰¹é‡é¢„æµ‹
        batch_results = model.batch_predict(matches)

        # ç”Ÿæˆå“åº”
        responses = []
        for i, result in enumerate(batch_results):
            if "error" in result:
                # é”™è¯¯å“åº”
                responses.append(PredictionResponse(
                    request_id=f"batch_{i}_{str(uuid.uuid4())[:8]}",
                    prediction=result,
                    explanation=None,
                    timestamp=datetime.now().isoformat()
                ))
            else:
                # æˆåŠŸå“åº”
                explanation = None
                if request.include_explanations:
                    explanation = model.explain_prediction(
                        home_team=result["home_team"],
                        away_team=result["away_team"],
                        features=matches[i]["features"]
                    )

                responses.append(PredictionResponse(
                    request_id=f"batch_{i}_{str(uuid.uuid4())[:8]}",
                    prediction=result,
                    explanation=explanation,
                    timestamp=result["prediction_time"]
                ))

        return responses

    def _generate_cache_key(self, request: PredictionRequest, model_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib

        key_data = f"{model_name}:{request.home_team}:{request.away_team}:{sorted(request.features.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def clear_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self.prediction_cache.clear()
        self.logger.info("Prediction cache cleared")

    def get_model_info(self, model_name: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        model = self.get_model(model_name)
        if not model:
            raise HTTPException(status_code=404, detail=f"Model {model_name} not found")

        return {
            "model_name": model.model_name,
            "model_type": model.model_type,
            "prediction_type": model.prediction_type.value,
            "is_trained": model.is_trained,
            "feature_count": len(model.feature_columns),
            "target_classes": model.target_classes,
            "metadata": model.metadata,
        }

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(title="Football Prediction API", version="1.0.0")
model_service = ModelService()

# æ³¨å†Œé»˜è®¤æ¨¡å‹
from ..models.prediction_model import default_model
model_service.register_model(default_model)

# APIç«¯ç‚¹
@app.post("/predict", response_model=PredictionResponse)
async def predict_match(request: PredictionRequest):
    """é¢„æµ‹å•åœºæ¯”èµ›"""
    return await model_service.predict_match(request)

@app.post("/predict/batch", response_model=List[PredictionResponse])
async def batch_predict(request: BatchPredictionRequest):
    """æ‰¹é‡é¢„æµ‹æ¯”èµ›"""
    return await model_service.batch_predict(request)

@app.get("/models")
async def list_models():
    """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
    return {"models": list(model_service.models.keys())}

@app.get("/models/{model_name}")
async def get_model_info(model_name: str):
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return model_service.get_model_info(model_name)

@app.post("/models/{model_name}/register")
async def register_model(model_name: str, model_path: str):
    """æ³¨å†Œæ–°æ¨¡å‹"""
    model = FootballPredictionModel(model_name)
    if model.load_model(model_path):
        model_service.register_model(model)
        return {"message": f"Model {model_name} registered successfully"}
    else:
        raise HTTPException(status_code=400, detail="Failed to load model")

@app.delete("/cache")
async def clear_cache():
    """æ¸…ç©ºé¢„æµ‹ç¼“å­˜"""
    model_service.clear_cache()
    return {"message": "Cache cleared"}
```

## ğŸ“ˆ æ¨¡å‹ç›‘æ§

### 1. æ€§èƒ½ç›‘æ§

```python
# src/ml/model_monitor.py

import pandas as pd
import numpy as np
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import logging
from prometheus_client import Counter, Histogram, Gauge
import asyncio

class ModelMonitor:
    """æ¨¡å‹ç›‘æ§å™¨"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # PrometheusæŒ‡æ ‡
        self.prediction_requests_total = Counter(
            'model_prediction_requests_total',
            'Total number of prediction requests',
            ['model_name', 'status']
        )

        self.prediction_duration = Histogram(
            'model_prediction_duration_seconds',
            'Time spent making predictions',
            ['model_name'],
            buckets=[0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0]
        )

        self.prediction_accuracy = Gauge(
            'model_prediction_accuracy',
            'Current model accuracy',
            ['model_name']
        )

        self.data_drift_score = Gauge(
            'model_data_drift_score',
            'Data drift score',
            ['model_name', 'feature']
        )

        # ç›‘æ§æ•°æ®å­˜å‚¨
        self.monitoring_data = {}
        self.alert_thresholds = {
            "accuracy_drop": 0.1,      # å‡†ç¡®ç‡ä¸‹é™10%
            "prediction_latency": 2.0, # é¢„æµ‹å»¶è¿Ÿè¶…è¿‡2ç§’
            "error_rate": 0.05,        # é”™è¯¯ç‡è¶…è¿‡5%
            "data_drift": 0.2,         # æ•°æ®æ¼‚ç§»è¶…è¿‡20%
        }

    async def record_prediction(
        self,
        model_name: str,
        prediction_time: float,
        success: bool,
        prediction_result: Optional[Dict[str, Any]] = None
    ) -> None:
        """è®°å½•é¢„æµ‹è¯·æ±‚"""

        status = "success" if success else "error"
        self.prediction_requests_total.labels(
            model_name=model_name,
            status=status
        ).inc()

        self.prediction_duration.labels(
            model_name=model_name
        ).observe(prediction_time)

    async def update_model_accuracy(
        self,
        model_name: str,
        accuracy: float
    ) -> None:
        """æ›´æ–°æ¨¡å‹å‡†ç¡®ç‡"""

        self.prediction_accuracy.labels(
            model_name=model_name
        ).set(accuracy)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        await self._check_accuracy_drop(model_name, accuracy)

    async def detect_data_drift(
        self,
        model_name: str,
        current_features: pd.DataFrame,
        reference_features: pd.DataFrame
    ) -> Dict[str, float]:
        """æ£€æµ‹æ•°æ®æ¼‚ç§»"""

        drift_scores = {}

        for feature in current_features.columns:
            if feature in reference_features.columns:
                # è®¡ç®—åˆ†å¸ƒå·®å¼‚ (ç®€åŒ–å®ç°)
                current_mean = current_features[feature].mean()
                ref_mean = reference_features[feature].mean()
                current_std = current_features[feature].std()
                ref_std = reference_features[feature].std()

                # è®¡ç®—æ¼‚ç§»åˆ†æ•°
                mean_diff = abs(current_mean - ref_mean) / (ref_std + 1e-8)
                std_ratio = current_std / (ref_std + 1e-8)

                drift_score = max(mean_diff, abs(std_ratio - 1))
                drift_scores[feature] = drift_score

                # æ›´æ–°PrometheusæŒ‡æ ‡
                self.data_drift_score.labels(
                    model_name=model_name,
                    feature=feature
                ).set(drift_score)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        await self._check_data_drift_alert(model_name, drift_scores)

        return drift_scores

    async def _check_accuracy_drop(self, model_name: str, current_accuracy: float) -> None:
        """æ£€æŸ¥å‡†ç¡®ç‡ä¸‹é™å‘Šè­¦"""

        if model_name not in self.monitoring_data:
            self.monitoring_data[model_name] = {}

        last_accuracy = self.monitoring_data[model_name].get("last_accuracy")

        if last_accuracy is not None:
            accuracy_drop = last_accuracy - current_accuracy

            if accuracy_drop > self.alert_thresholds["accuracy_drop"]:
                await self._send_alert(
                    "accuracy_drop",
                    f"Model {model_name} accuracy dropped by {accuracy_drop:.3f}",
                    {
                        "model_name": model_name,
                        "last_accuracy": last_accuracy,
                        "current_accuracy": current_accuracy,
                        "drop": accuracy_drop,
                    }
                )

        self.monitoring_data[model_name]["last_accuracy"] = current_accuracy

    async def _check_data_drift_alert(
        self,
        model_name: str,
        drift_scores: Dict[str, float]
    ) -> None:
        """æ£€æŸ¥æ•°æ®æ¼‚ç§»å‘Šè­¦"""

        max_drift = max(drift_scores.values())

        if max_drift > self.alert_thresholds["data_drift"]:
            # æ‰¾åˆ°æ¼‚ç§»æœ€å¤§çš„ç‰¹å¾
            max_drift_feature = max(drift_scores.items(), key=lambda x: x[1])

            await self._send_alert(
                "data_drift",
                f"Significant data drift detected in model {model_name}",
                {
                    "model_name": model_name,
                    "max_drift": max_drift,
                    "max_drift_feature": max_drift_feature[0],
                    "drift_scores": drift_scores,
                }
            )

    async def _send_alert(
        self,
        alert_type: str,
        message: str,
        details: Dict[str, Any]
    ) -> None:
        """å‘é€å‘Šè­¦"""

        alert = {
            "alert_type": alert_type,
            "message": message,
            "details": details,
            "timestamp": datetime.now().isoformat(),
            "severity": self._get_alert_severity(alert_type),
        }

        self.logger.warning(f"ALERT: {message}")

        # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å‘Šè­¦ç³»ç»Ÿ (é‚®ä»¶ã€Slackç­‰)
        # await self.alert_sender.send_alert(alert)

    def _get_alert_severity(self, alert_type: str) -> str:
        """è·å–å‘Šè­¦ä¸¥é‡ç¨‹åº¦"""

        severity_map = {
            "accuracy_drop": "warning",
            "data_drift": "warning",
            "high_latency": "critical",
            "high_error_rate": "critical",
        }

        return severity_map.get(alert_type, "info")

    async def generate_monitoring_report(
        self,
        model_name: str,
        time_range: timedelta = timedelta(hours=24)
    ) -> Dict[str, Any]:
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""

        end_time = datetime.now()
        start_time = end_time - time_range

        report = {
            "model_name": model_name,
            "report_period": {
                "start": start_time.isoformat(),
                "end": end_time.isoformat(),
                "duration_hours": time_range.total_seconds() / 3600,
            },
            "performance_metrics": {
                "total_requests": 0,
                "success_rate": 0,
                "average_latency": 0,
                "current_accuracy": 0,
            },
            "data_drift": {},
            "alerts": [],
            "recommendations": [],
        }

        # æ”¶é›†ç›‘æ§æ•°æ® (ç®€åŒ–å®ç°)
        if model_name in self.monitoring_data:
            model_data = self.monitoring_data[model_name]

            report["performance_metrics"]["current_accuracy"] = model_data.get("last_accuracy", 0)

            # ç”Ÿæˆå»ºè®®
            if model_data.get("last_accuracy", 1.0) < 0.7:
                report["recommendations"].append("è€ƒè™‘é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥æé«˜å‡†ç¡®ç‡")

            if len(model_data.get("recent_drift_scores", {})) > 0:
                report["recommendations"].append("æ£€æµ‹åˆ°æ•°æ®æ¼‚ç§»ï¼Œå»ºè®®æ›´æ–°è®­ç»ƒæ•°æ®")

        return report
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. æ¨¡å‹å¼€å‘æœ€ä½³å®è·µ

1. **æ•°æ®è´¨é‡**
   - ç¡®ä¿è®­ç»ƒæ•°æ®çš„å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
   - è¿›è¡Œé€‚å½“çš„æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
   - å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼

2. **ç‰¹å¾å·¥ç¨‹**
   - é€‰æ‹©ä¸é¢„æµ‹ç›®æ ‡ç›¸å…³çš„ç‰¹å¾
   - é¿å…æ•°æ®æ³„éœ² (feature leakage)
   - è¿›è¡Œç‰¹å¾æ ‡å‡†åŒ–å’Œå½’ä¸€åŒ–

3. **æ¨¡å‹é€‰æ‹©**
   - ä»ç®€å•çš„åŸºçº¿æ¨¡å‹å¼€å§‹
   - æ ¹æ®é—®é¢˜ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ç®—æ³•
   - è€ƒè™‘æ¨¡å‹çš„è§£é‡Šæ€§éœ€æ±‚

4. **éªŒè¯ç­–ç•¥**
   - ä½¿ç”¨æ—¶é—´åºåˆ—åˆ†å‰²é¿å…æœªæ¥æ•°æ®æ³„éœ²
   - è¿›è¡Œäº¤å‰éªŒè¯ç¡®ä¿æ¨¡å‹ç¨³å®šæ€§
   - ä¿ç•™ç‹¬ç«‹çš„æµ‹è¯•é›†è¿›è¡Œæœ€ç»ˆè¯„ä¼°

### 2. éƒ¨ç½²æœ€ä½³å®è·µ

1. **æ¨¡å‹ç‰ˆæœ¬ç®¡ç†**
   - ä½¿ç”¨MLflowç­‰å·¥å…·è·Ÿè¸ªæ¨¡å‹ç‰ˆæœ¬
   - ä¿å­˜å®Œæ•´çš„æ¨¡å‹å…ƒæ•°æ®
   - å»ºç«‹æ¨¡å‹å›æ»šæœºåˆ¶

2. **æ€§èƒ½ç›‘æ§**
   - ç›‘æ§é¢„æµ‹å»¶è¿Ÿå’Œååé‡
   - è·Ÿè¸ªæ¨¡å‹æ€§èƒ½æŒ‡æ ‡å˜åŒ–
   - è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼

3. **å®‰å…¨è€ƒè™‘**
   - ä¿æŠ¤æ¨¡å‹æ–‡ä»¶å’Œè®­ç»ƒæ•°æ®
   - å®ç°APIè®¿é—®æ§åˆ¶
   - è®°å½•é¢„æµ‹è¯·æ±‚æ—¥å¿—

4. **å¯æ‰©å±•æ€§**
   - è®¾è®¡æ°´å¹³æ‰©å±•çš„æ¶æ„
   - å®ç°æ¨¡å‹è´Ÿè½½å‡è¡¡
   - ä¼˜åŒ–é¢„æµ‹æœåŠ¡æ€§èƒ½

## ğŸ“‹ ç›¸å…³æ–‡æ¡£

- [æ•°æ®åº“æ¶æ„æ–‡æ¡£](../reference/DATABASE_SCHEMA.md)
- [å¼€å‘æŒ‡å—](../reference/DEVELOPMENT_GUIDE.md)
- [æ•°æ®é‡‡é›†é…ç½®æŒ‡å—](../reference/DATA_COLLECTION_SETUP.md)
- [APIæ–‡æ¡£](../reference/API_REFERENCE.md)
- [æœ¯è¯­è¡¨](../reference/glossary.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-10-23
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
