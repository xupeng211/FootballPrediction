# ğŸš€ è¶³çƒé¢„æµ‹ç³»ç»Ÿè‡ªåŠ¨åŒ–æµ‹è¯•ä¸è¦†ç›–ç‡æå‡å®æ–½æ–¹æ¡ˆ

> æœ¬æ–‡æ¡£ç”± Claude Code è‡ªåŠ¨ç”Ÿæˆï¼Œä½œä¸ºæµ‹è¯•ä¸è¦†ç›–ç‡æå‡çš„æ‰§è¡Œè®¡åˆ’å’Œè´¨é‡ä¿éšœæ ‡å‡†ï¼Œéœ€éšç€é¡¹ç›®è¿­ä»£æŒç»­æ›´æ–°ã€‚

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

åŸºäºç°çŠ¶åˆ†ææŠ¥å‘Šï¼Œæœ¬æ–¹æ¡ˆé‡‡ç”¨5é˜¶æ®µæ¸è¿›å¼å®æ–½ç­–ç•¥ï¼Œæ—¨åœ¨å°†ç³»ç»Ÿæµ‹è¯•è¦†ç›–ç‡ä»å½“å‰çš„80%æå‡è‡³85%+ï¼Œå¹¶ä¿®å¤æ‰€æœ‰æŠ€æœ¯å€ºåŠ¡ã€‚æ–¹æ¡ˆé‡ç‚¹è§£å†³OpenLineageä¾èµ–é—®é¢˜ï¼Œè¡¥é½database/scheduler/dataæ¨¡å—è¦†ç›–ç¼ºå£ï¼Œå»ºç«‹å®Œæ•´çš„æµ‹è¯•è´¨é‡ä¿éšœä½“ç³»ã€‚

---

## ğŸ¯ æ€»ä½“ç›®æ ‡

### è¦†ç›–ç‡ç›®æ ‡
- **å½“å‰è¦†ç›–ç‡**: 80% (å­˜åœ¨5ä¸ªå¤±è´¥ç”¨ä¾‹)
- **Phase 1ç›®æ ‡**: ä¿®å¤æ‰€æœ‰å¤±è´¥ç”¨ä¾‹ï¼Œä¿æŒ80%è¦†ç›–ç‡
- **Phase 2ç›®æ ‡**: databaseæ¨¡å—è¦†ç›–ç‡æå‡è‡³â‰¥60%
- **Phase 3ç›®æ ‡**: scheduler/dataæ¨¡å—è¦†ç›–ç‡æå‡è‡³â‰¥70%
- **Phase 4ç›®æ ‡**: å…¨å±€è¦†ç›–ç‡æå‡è‡³â‰¥85%
- **Phase 5ç›®æ ‡**: æ¸…ç†æ‰€æœ‰skipç”¨ä¾‹ï¼Œè¦†ç›–ç‡ç¨³å®šâ‰¥85%

### è´¨é‡ç›®æ ‡
- **é›¶å¤±è´¥ç”¨ä¾‹**: æ‰€æœ‰æµ‹è¯•å¿…é¡»é€šè¿‡
- **é›¶æ— æ•ˆskip**: æ¸…ç†æˆ–ä¿®å¤æ‰€æœ‰è·³è¿‡çš„æµ‹è¯•
- **CIé—¨ç¦**: 85%è¦†ç›–ç‡ç¡¬æ€§è¦æ±‚
- **ä»£ç è´¨é‡**: æ‰€æœ‰æ–°å¢ä»£ç å¿…é¡»æœ‰å¯¹åº”æµ‹è¯•

---

## ğŸ“… åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### Phase 1: ä¿®å¤ç°æœ‰å¤±è´¥ç”¨ä¾‹ï¼ˆ1-2å‘¨ï¼‰

#### ğŸ¯ ç›®æ ‡
- ä¿®å¤OpenLineageä¾èµ–å¯¼è‡´çš„5ä¸ªå¤±è´¥æµ‹è¯•ç”¨ä¾‹
- ç¡®ä¿æ‰€æœ‰ç°æœ‰æµ‹è¯•ç¨³å®šé€šè¿‡
- å»ºç«‹åŸºç¡€æµ‹è¯•è´¨é‡åŸºçº¿

#### ğŸ“‹ ä»»åŠ¡æ¸…å•

**1.1 OpenLineageä¾èµ–é—®é¢˜ä¿®å¤**
```bash
# ä»»åŠ¡1: åˆ†æOpenLineageå¯¼å…¥é—®é¢˜
make analyze-lineage-issues

# ä»»åŠ¡2: ç»Ÿä¸€OpenLineageç‰ˆæœ¬
make fix-openlineage-deps

# ä»»åŠ¡3: ä¿®å¤SchemaDatasetFacetå¯¼å…¥é”™è¯¯
make fix-schema-facet-import

# ä»»åŠ¡4: éªŒè¯ä¿®å¤æ•ˆæœ
make test-lineage-module
```

**1.2 æ¸…ç†æ— æ•ˆæµ‹è¯•**
```bash
# ä»»åŠ¡5: åˆ†æskipç”¨ä¾‹
make analyze-skipped-tests

# ä»»åŠ¡6: ä¿®å¤æˆ–ç§»é™¤æ— æ•ˆskip
make cleanup-skipped-tests

# ä»»åŠ¡7: éªŒè¯æµ‹è¯•ç¨³å®šæ€§
make test-stability-check
```

**1.3 åŸºç¡€è®¾æ–½å®Œå–„**
```bash
# ä»»åŠ¡8: å®Œå–„conftest.py
make enhance-conftest

# ä»»åŠ¡9: ç»Ÿä¸€Mocké…ç½®
make standardize-mocks

# ä»»åŠ¡10: å»ºç«‹æµ‹è¯•æ•°æ®å·¥å‚
make setup-test-factories
```

#### ğŸ“Š éªŒæ”¶æ ‡å‡†
- [ ] æ‰€æœ‰lineageæ¨¡å—æµ‹è¯•é€šè¿‡
- [ ] æ— è·³è¿‡æµ‹è¯•ç”¨ä¾‹ï¼ˆé™¤æ¡ä»¶æ€§è·³è¿‡ï¼‰
- [ ] æµ‹è¯•è¿è¡Œç¨³å®šæ€§100%
- [ ] åŸºç¡€è¦†ç›–ç‡ä¿æŒ80%

---

### Phase 2: æå‡databaseæ¨¡å—è¦†ç›–ç‡ï¼ˆ2-3å‘¨ï¼‰

#### ğŸ¯ ç›®æ ‡
- databaseæ¨¡å—è¦†ç›–ç‡ä»23%æå‡è‡³â‰¥60%
- å»ºç«‹æ•°æ®åº“æµ‹è¯•åŸºç¡€è®¾æ–½
- å®Œå–„æ•°æ®æ¨¡å‹æµ‹è¯•è¦†ç›–

#### ğŸ“‹ ä»»åŠ¡æ¸…å•

**2.1 æ•°æ®åº“æµ‹è¯•åŸºç¡€è®¾æ–½**
```bash
# ä»»åŠ¡1: å®Œå–„pytest-postgresqlé…ç½®
make setup-db-testing

# ä»»åŠ¡2: å»ºç«‹æ•°æ®åº“æ¨¡å‹å·¥å‚
make create-db-factories

# ä»»åŠ¡3: ç»Ÿä¸€æ•°æ®åº“è¿æ¥æµ‹è¯•fixture
make standardize-db-fixtures

# ä»»åŠ¡4: å»ºç«‹æµ‹è¯•æ•°æ®æ¸…ç†æœºåˆ¶
make setup-test-cleanup
```

**2.2 æ•°æ®æ¨¡å‹æµ‹è¯•**
```bash
# ä»»åŠ¡5: è¡¥å……connection.pyæµ‹è¯•
make test-db-connection

# ä»»åŠ¡6: è¡¥å……modelsæµ‹è¯•ï¼ˆ11ä¸ªæ¨¡å‹ï¼‰
make test-db-models

# ä»»åŠ¡7: è¡¥å……migrationæµ‹è¯•
make test-db-migrations

# ä»»åŠ¡8: è¡¥å……configæµ‹è¯•
make test-db-config
```

**2.3 æ•°æ®åº“é›†æˆæµ‹è¯•**
```bash
# ä»»åŠ¡9: æ•°æ®åº“CRUDæ“ä½œæµ‹è¯•
make test-db-crud

# ä»»åŠ¡10: æ•°æ®åº“äº‹åŠ¡æµ‹è¯•
make test-db-transactions

# ä»»åŠ¡11: æ•°æ®åº“è¿æ¥æ± æµ‹è¯•
make test-db-connection-pool

# ä»»åŠ¡12: æ•°æ®åº“æ€§èƒ½æµ‹è¯•
make test-db-performance
```

#### ğŸ“Š éªŒæ”¶æ ‡å‡†
- [ ] databaseæ¨¡å—è¦†ç›–ç‡â‰¥60%
- [ ] æ‰€æœ‰æ•°æ®åº“æ¨¡å‹æœ‰å¯¹åº”æµ‹è¯•
- [ ] é›†æˆæµ‹è¯•è¦†ç›–ä¸»è¦æ•°æ®åº“æ“ä½œ
- [ ] æ•°æ®åº“æµ‹è¯•è¿è¡Œç¨³å®š

---

### Phase 3: æå‡schedulerå’Œdataæ¨¡å—è¦†ç›–ç‡ï¼ˆ2-3å‘¨ï¼‰

#### ğŸ¯ ç›®æ ‡
- scheduleræ¨¡å—è¦†ç›–ç‡ä»17%æå‡è‡³â‰¥70%
- dataæ¨¡å—è¦†ç›–ç‡ä»64%æå‡è‡³â‰¥70%
- å®Œå–„å¼‚æ­¥æµ‹è¯•æ¨¡å¼

#### ğŸ“‹ ä»»åŠ¡æ¸…å•

**3.1 Scheduleræ¨¡å—æµ‹è¯•**
```bash
# ä»»åŠ¡1: è¡¥å……celery_configæµ‹è¯•
make test-celery-config

# ä»»åŠ¡2: è¡¥å……task_scheduleræµ‹è¯•
make test-task-scheduler

# ä»»åŠ¡3: è¡¥å……job_manageræµ‹è¯•
make test-job-manager

# ä»»åŠ¡4: è¡¥å……recovery_handleræµ‹è¯•
make test-recovery-handler

# ä»»åŠ¡5: Celeryé›†æˆæµ‹è¯•
make test-celery-integration
```

**3.2 Dataæ¨¡å—æµ‹è¯•**
```bash
# ä»»åŠ¡6: è¡¥å……collectorsæµ‹è¯•ï¼ˆ4ä¸ªæ”¶é›†å™¨ï¼‰
make test-data-collectors

# ä»»åŠ¡7: è¡¥å……processingæµ‹è¯•ï¼ˆæ•°æ®æ¸…æ´—ï¼‰
make test-data-processing

# ä»»åŠ¡8: è¡¥å……qualityæµ‹è¯•ï¼ˆæ•°æ®è´¨é‡ï¼‰
make test-data-quality

# ä»»åŠ¡9: è¡¥å……storageæµ‹è¯•ï¼ˆæ•°æ®å­˜å‚¨ï¼‰
make test-data-storage

# ä»»åŠ¡10: è¡¥å……featuresæµ‹è¯•ï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰
make test-data-features
```

**3.3 å¼‚æ­¥æµ‹è¯•æ ‡å‡†åŒ–**
```bash
# ä»»åŠ¡11: ç»Ÿä¸€å¼‚æ­¥æµ‹è¯•æ¨¡å¼
make standardize-async-tests

# ä»»åŠ¡12: å®Œå–„AsyncMocké…ç½®
make enhance-async-mocks

# ä»»åŠ¡13: å¼‚æ­¥é›†æˆæµ‹è¯•
make test-async-integration
```

#### ğŸ“Š éªŒæ”¶æ ‡å‡†
- [ ] scheduleræ¨¡å—è¦†ç›–ç‡â‰¥70%
- [ ] dataæ¨¡å—è¦†ç›–ç‡â‰¥70%
- [ ] å¼‚æ­¥æµ‹è¯•æ¨¡å¼ç»Ÿä¸€
- [ ] æ•°æ®å¤„ç†ç®¡é“æµ‹è¯•å®Œæ•´

---

### Phase 4: å…¨å±€è¦†ç›–ç‡æå‡åˆ°â‰¥85%ï¼ˆ2-3å‘¨ï¼‰

#### ğŸ¯ ç›®æ ‡
- å…¨å±€è¦†ç›–ç‡ä»80%æå‡è‡³â‰¥85%
- é”å®šè¦†ç›–ç‡é˜ˆå€¼
- å»ºç«‹è¦†ç›–ç‡ç›‘æ§æœºåˆ¶

#### ğŸ“‹ ä»»åŠ¡æ¸…å•

**4.1 è¦†ç›–ç‡åˆ†æ**
```bash
# ä»»åŠ¡1: è¦†ç›–ç‡ç¼ºå£åˆ†æ
make analyze-coverage-gaps

# ä»»åŠ¡2: è¯†åˆ«ä½è¦†ç›–ç‡æ–‡ä»¶
make find-low-coverage-files

# ä»»åŠ¡3: åˆ¶å®šè¡¥å…¨ç­–ç•¥
make plan-coverage-improvement
```

**4.2 è¦†ç›–ç‡è¡¥å…¨**
```bash
# ä»»åŠ¡4: è¡¥å…¨utilsæ¨¡å—æµ‹è¯•
make test-utils-comprehensive

# ä»»åŠ¡5: è¡¥å…¨servicesæ¨¡å—æµ‹è¯•
make test-services-comprehensive

# ä»»åŠ¡6: è¡¥å…¨streamingæ¨¡å—æµ‹è¯•
make test-streaming-comprehensive

# ä»»åŠ¡7: è¡¥å…¨monitoringæ¨¡å—æµ‹è¯•
make test-monitoring-comprehensive
```

**4.3 è¦†ç›–ç‡é—¨ç¦å»ºç«‹**
```bash
# ä»»åŠ¡8: è®¾ç½®CIè¦†ç›–ç‡é—¨ç¦
make setup-coverage-gate

# ä»»åŠ¡9: å»ºç«‹è¦†ç›–ç‡æŠ¥å‘Š
make setup-coverage-reports

# ä»»åŠ¡10: è¦†ç›–ç‡è¶‹åŠ¿ç›‘æ§
make setup-coverage-trends
```

#### ğŸ“Š éªŒæ”¶æ ‡å‡†
- [ ] å…¨å±€è¦†ç›–ç‡â‰¥85%
- [ ] è¦†ç›–ç‡é—¨ç¦ç”Ÿæ•ˆ
- [ ] è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆ
- [ ] æ— æ˜¾è‘—è¦†ç›–ç‡ä¸‹é™

---

### Phase 5: æ¸…ç†/ä¿®å¤æ‰€æœ‰skipç”¨ä¾‹ï¼ˆ1-2å‘¨ï¼‰

#### ğŸ¯ ç›®æ ‡
- æ¸…ç†æ‰€æœ‰æ— æ•ˆskipç”¨ä¾‹
- ä¿®å¤å¿…è¦çš„è·³è¿‡æµ‹è¯•
- å»ºç«‹æµ‹è¯•è´¨é‡æ ‡å‡†

#### ğŸ“‹ ä»»åŠ¡æ¸…å•

**5.1 Skipç”¨ä¾‹åˆ†æ**
```bash
# ä»»åŠ¡1: åˆ†ææ‰€æœ‰skipç”¨ä¾‹
make analyze-all-skips

# ä»»åŠ¡2: åˆ†ç±»skipç”¨ä¾‹ï¼ˆæœ‰æ•ˆ/æ— æ•ˆï¼‰
make categorize-skips

# ä»»åŠ¡3: åˆ¶å®šä¿®å¤è®¡åˆ’
make plan-skip-fixes
```

**5.2 Skipç”¨ä¾‹ä¿®å¤**
```bash
# ä»»åŠ¡4: ä¿®å¤å¯ä¿®å¤çš„skipç”¨ä¾‹
make fix-repairable-skips

# ä»»åŠ¡5: ç§»é™¤æ— æ•ˆskipç”¨ä¾‹
make remove-invalid-skips

# ä»»åŠ¡6: ä¼˜åŒ–æ¡ä»¶æ€§skip
make optimize-conditional-skips
```

**5.3 è´¨é‡ä¿éšœ**
```bash
# ä»»åŠ¡7: å»ºç«‹æµ‹è¯•è´¨é‡æ£€æŸ¥
make setup-test-quality-check

# ä»»åŠ¡8: é˜²æ­¢æ–°å¢æ— æ•ˆskip
make prevent-bad-skips

# ä»»åŠ¡9: æœ€ç»ˆéªŒè¯
make final-validation
```

#### ğŸ“Š éªŒæ”¶æ ‡å‡†
- [ ] æ— æ— æ•ˆskipç”¨ä¾‹
- [ ] æ¡ä»¶æ€§skipåˆç†ä¸”å¿…è¦
- [ ] æµ‹è¯•è´¨é‡æ£€æŸ¥ç”Ÿæ•ˆ
- [ ] è¦†ç›–ç‡ç¨³å®šâ‰¥85%

---

## ğŸ—ï¸ æµ‹è¯•ç±»å‹è®¾è®¡

### å•å…ƒæµ‹è¯•è®¾è®¡

#### æ•°æ®åº“æ¨¡å‹æµ‹è¯•
```python
# tests/unit/database/test_models_comprehensive.py
import pytest
from factory.alchemy import SQLAlchemyModelFactory
from src.database.models.match import Match
from tests.factories.match_factory import MatchFactory

class TestMatchModel:
    """æµ‹è¯•Matchæ¨¡å‹çš„å„ç§æ“ä½œ"""

    def test_match_creation(self, db_session):
        """æµ‹è¯•æ¯”èµ›è®°å½•åˆ›å»º"""
        match = MatchFactory()
        db_session.add(match)
        db_session.commit()

        assert match.id is not None
        assert match.home_score >= 0
        assert match.away_score >= 0

    def test_match_relationships(self, db_session):
        """æµ‹è¯•æ¨¡å‹å…³ç³»"""
        match = MatchFactory(home_team__name="Team A", away_team__name="Team B")
        db_session.add(match)
        db_session.commit()

        assert match.home_team.name == "Team A"
        assert match.away_team.name == "Team B"
```

#### è°ƒåº¦å™¨é…ç½®æµ‹è¯•
```python
# tests/unit/scheduler/test_celery_config_comprehensive.py
import pytest
from unittest.mock import Mock, patch
from src.scheduler.celery_config import get_celery_config

class TestCeleryConfig:
    """æµ‹è¯•Celeryé…ç½®"""

    def test_celery_config_structure(self):
        """æµ‹è¯•é…ç½®ç»“æ„"""
        config = get_celery_config()

        assert 'broker_url' in config
        assert 'result_backend' in config
        assert 'task_serializer' in config

    @patch('src.scheduler.celery_config.Celery')
    def test_celery_app_creation(self, mock_celery):
        """æµ‹è¯•Celeryåº”ç”¨åˆ›å»º"""
        app = get_celery_app()

        mock_celery.assert_called_once()
        assert app is not None
```

#### æ•°æ®å¤„ç†å‡½æ•°æµ‹è¯•
```python
# tests/unit/data/processing/test_football_data_cleaner_comprehensive.py
import pytest
import pandas as pd
from src.data.processing.football_data_cleaner import FootballDataCleaner

class TestFootballDataCleaner:
    """æµ‹è¯•è¶³çƒæ•°æ®æ¸…æ´—"""

    def test_clean_match_data(self):
        """æµ‹è¯•æ¯”èµ›æ•°æ®æ¸…æ´—"""
        cleaner = FootballDataCleaner()
        dirty_data = pd.DataFrame({
            'home_team': ['Team A ', ' TEAM B', None],
            'away_team': ['Team B', 'Team A ', 'Invalid']
        })

        clean_data = cleaner.clean_match_data(dirty_data)

        assert clean_data['home_team'].iloc[0] == 'Team A'
        assert clean_data['home_team'].iloc[1] == 'TEAM B'
        assert pd.isna(clean_data['home_team'].iloc[2])
```

### é›†æˆæµ‹è¯•è®¾è®¡

#### æ•°æ®åº“è¿æ¥é›†æˆæµ‹è¯•
```python
# tests/integration/database/test_connection_integration.py
import pytest
from src.database.connection import DatabaseManager
from tests.fixtures.database import test_db_config

class TestDatabaseConnectionIntegration:
    """æµ‹è¯•æ•°æ®åº“è¿æ¥é›†æˆ"""

    @pytest.mark.asyncio
    async def test_database_connection_pool(self, test_db_config):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥æ± """
        db_manager = DatabaseManager(test_db_config)

        # æµ‹è¯•è¿æ¥æ± è·å–
        conn1 = await db_manager.get_connection()
        conn2 = await db_manager.get_connection()

        assert conn1 != conn2
        assert conn1.is_connected()
        assert conn2.is_connected()

        # æµ‹è¯•è¿æ¥é‡Šæ”¾
        await db_manager.release_connection(conn1)
        await db_manager.release_connection(conn2)
```

#### Celeryè°ƒåº¦é›†æˆæµ‹è¯•
```python
# tests/integration/scheduler/test_celery_integration.py
import pytest
from src.scheduler.celery_app import celery_app
from src.tasks.maintenance_tasks import cleanup_old_logs

class TestCeleryIntegration:
    """æµ‹è¯•Celeryé›†æˆ"""

    @pytest.mark.asyncio
    async def test_task_execution(self):
        """æµ‹è¯•ä»»åŠ¡æ‰§è¡Œ"""
        # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
        result = cleanup_old_logs.delay(days=30)

        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        task_result = await result.get(timeout=30)

        assert result.successful()
        assert task_result['cleaned_count'] >= 0
```

#### æ•°æ®é‡‡é›†æ¥å£é›†æˆæµ‹è¯•
```python
# tests/integration/data/collectors/test_api_integration.py
import pytest
import respx
from httpx import Response
from src.data.collectors.fixtures_collector import FixturesCollector

class TestAPIIntegration:
    """æµ‹è¯•APIé›†æˆ"""

    @pytest.mark.asyncio
    @respx.mock
    async def test_fixtures_collection(self):
        """æµ‹è¯•æ¯”èµ›æ•°æ®é‡‡é›†"""
        # Mock APIå“åº”
        mock_response = {
            "matches": [
                {"id": 1, "home_team": "Team A", "away_team": "Team B"}
            ]
        }
        respx.get("https://api.football-data.org/v4/matches").mock(
            Response(200, json=mock_response)
        )

        collector = FixturesCollector()
        matches = await collector.collect_fixtures()

        assert len(matches) == 1
        assert matches[0].home_team == "Team A"
```

### ç«¯åˆ°ç«¯æµ‹è¯•è®¾è®¡

#### å®Œæ•´æ•°æ®æµæµ‹è¯•
```python
# tests/e2e/test_complete_data_pipeline.py
import pytest
from src.data.collectors.fixtures_collector import FixturesCollector
from src.data.processing.football_data_cleaner import FootballDataCleaner
from src.features.feature_store import FeatureStore
from src.models.prediction_service import PredictionService

class TestCompleteDataPipeline:
    """æµ‹è¯•å®Œæ•´æ•°æ®æµ"""

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_end_to_end_prediction_pipeline(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯é¢„æµ‹æµç¨‹"""
        # 1. æ•°æ®é‡‡é›†
        collector = FixturesCollector()
        raw_matches = await collector.collect_fixtures()

        # 2. æ•°æ®å¤„ç†
        cleaner = FootballDataCleaner()
        clean_matches = cleaner.clean_match_data(raw_matches)

        # 3. ç‰¹å¾å·¥ç¨‹
        feature_store = FeatureStore()
        features = feature_store.extract_features(clean_matches)

        # 4. æ¨¡å‹é¢„æµ‹
        prediction_service = PredictionService()
        predictions = await prediction_service.predict(features)

        # éªŒè¯å®Œæ•´æµç¨‹
        assert len(predictions) > 0
        assert all('home_win_prob' in pred for pred in predictions)
```

### Mock/Fixtureè®¾è®¡

#### ç»Ÿä¸€å¤–éƒ¨ä¾èµ–Mock
```python
# tests/conftest.py
import pytest
from unittest.mock import Mock, AsyncMock
import redis.asyncio as redis

@pytest.fixture
def mock_redis():
    """ç»Ÿä¸€çš„Redis Mock"""
    mock_redis = AsyncMock()
    mock_redis.ping = AsyncMock(return_value=True)
    mock_redis.set = AsyncMock(return_value=True)
    mock_redis.get = AsyncMock(return_value=None)
    mock_redis.delete = AsyncMock(return_value=1)
    return mock_redis

@pytest.fixture
def mock_mlflow_client():
    """ç»Ÿä¸€çš„MLflow Mock"""
    mock_client = Mock()
    mock_client.get_latest_versions.return_value = [
        Mock(version="1", current_stage="Production")
    ]
    mock_client.transition_model_version_stage.return_value = None
    return mock_client

@pytest.fixture
def mock_kafka_producer():
    """ç»Ÿä¸€çš„Kafka Producer Mock"""
    mock_producer = AsyncMock()
    mock_producer.send = AsyncMock(return_value=None)
    mock_producer.flush = AsyncMock(return_value=None)
    return mock_producer
```

#### æ•°æ®åº“æµ‹è¯•Fixture
```python
# tests/fixtures/database.py
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from src.database.base import Base
from src.database.config import get_test_database_config

@pytest.fixture
def test_db_config():
    """æµ‹è¯•æ•°æ®åº“é…ç½®"""
    return get_test_database_config()

@pytest.fixture
def test_engine(test_db_config):
    """æµ‹è¯•æ•°æ®åº“å¼•æ“"""
    engine = create_engine(test_db_config['url'])
    Base.metadata.create_all(engine)
    yield engine
    Base.metadata.drop_all(engine)

@pytest.fixture
def test_session(test_engine):
    """æµ‹è¯•æ•°æ®åº“ä¼šè¯"""
    Session = sessionmaker(bind=test_engine)
    session = Session()
    yield session
    session.close()
```

---

## ğŸ”§ æŠ€æœ¯å®ç°ç»†èŠ‚

### æµ‹è¯•ç›®å½•ç»“æ„è§„èŒƒ

```
tests/
â”œâ”€â”€ unit/                     # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ api/                  # APIå±‚æµ‹è¯•
â”‚   â”œâ”€â”€ cache/                # ç¼“å­˜å±‚æµ‹è¯•
â”‚   â”œâ”€â”€ core/                 # æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•
â”‚   â”œâ”€â”€ data/                 # æ•°æ®å¤„ç†æµ‹è¯•
â”‚   â”œâ”€â”€ database/             # æ•°æ®åº“æµ‹è¯•
â”‚   â”œâ”€â”€ features/             # ç‰¹å¾å·¥ç¨‹æµ‹è¯•
â”‚   â”œâ”€â”€ models/               # æœºå™¨å­¦ä¹ æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ monitoring/           # ç›‘æ§æµ‹è¯•
â”‚   â”œâ”€â”€ scheduler/            # è°ƒåº¦å™¨æµ‹è¯•
â”‚   â”œâ”€â”€ services/             # ä¸šåŠ¡æœåŠ¡æµ‹è¯•
â”‚   â”œâ”€â”€ streaming/            # æµå¤„ç†æµ‹è¯•
â”‚   â”œâ”€â”€ tasks/                # ä»»åŠ¡æµ‹è¯•
â”‚   â””â”€â”€ utils/                # å·¥å…·å‡½æ•°æµ‹è¯•
â”œâ”€â”€ integration/              # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ api/                  # APIé›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ cache/                # ç¼“å­˜é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ database/             # æ•°æ®åº“é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ features/             # ç‰¹å¾å­˜å‚¨é›†æˆæµ‹è¯•
â”‚   â””â”€â”€ mlflow/               # MLflowé›†æˆæµ‹è¯•
â”œâ”€â”€ e2e/                      # ç«¯åˆ°ç«¯æµ‹è¯•
â”‚   â”œâ”€â”€ test_complete_workflow.py
â”‚   â””â”€â”€ test_prediction_flow.py
â”œâ”€â”€ fixtures/                 # æµ‹è¯•æ•°æ®
â”‚   â”œâ”€â”€ database.py
â”‚   â”œâ”€â”€ api_data.py
â”‚   â””â”€â”€ ml_models.py
â”œâ”€â”€ factories/                # å·¥å‚æ¨¡å¼
â”‚   â”œâ”€â”€ match_factory.py
â”‚   â”œâ”€â”€ team_factory.py
â”‚   â””â”€â”€ prediction_factory.py
â”œâ”€â”€ conftest.py               # pytesté…ç½®
â””â”€â”€ requirements.txt          # æµ‹è¯•ä¾èµ–
```

### Makefile å¢å¼ºç›®æ ‡

```makefile
# æµ‹è¯•ç›¸å…³ç›®æ ‡
test-unit: ## è¿è¡Œå•å…ƒæµ‹è¯•
	@$(ACTIVATE) && pytest tests/unit/ -v --cov=src --cov-report=term-missing --cov-fail-under=80

test-integration: ## è¿è¡Œé›†æˆæµ‹è¯•
	@$(ACTIVATE) && pytest tests/integration/ -v --cov=src --cov-report=term-missing --cov-fail-under=70

test-e2e: ## è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
	@$(ACTIVATE) && pytest tests/e2e/ -v --cov=src --cov-report=term-missing --cov-fail-under=60

test-all: ## è¿è¡Œæ‰€æœ‰æµ‹è¯•
	@$(ACTIVATE) && pytest tests/ -v --cov=src --cov-report=term-missing --cov-report=html --cov-fail-under=85

test-coverage: ## ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
	@$(ACTIVATE) && pytest tests/ --cov=src --cov-report=html --cov-report=term-missing --cov-fail-under=85
	@echo "Coverage report generated in htmlcov/"

test-stability: ## æµ‹è¯•ç¨³å®šæ€§æ£€æŸ¥
	@$(ACTIVATE) && pytest tests/ --maxfail=0 --reruns=3 --reruns-delay=1

coverage-gate: ## è¦†ç›–ç‡é—¨ç¦æ£€æŸ¥
	@$(ACTIVATE) && pytest tests/ --cov=src --cov-report=term-missing --cov-fail-under=85
	@echo "âœ… Coverage gate passed (â‰¥85%)"
```

### å¼‚æ­¥æµ‹è¯•æœ€ä½³å®è·µ

```python
# tests/unit/test_async_best_practices.py
import pytest
from unittest.mock import AsyncMock, patch

@pytest.mark.asyncio
class TestAsyncBestPractices:
    """å¼‚æ­¥æµ‹è¯•æœ€ä½³å®è·µç¤ºä¾‹"""

    async def test_async_function_with_real_async_mock(self):
        """ä½¿ç”¨çœŸå®AsyncMockæµ‹è¯•å¼‚æ­¥å‡½æ•°"""
        mock_service = AsyncMock()
        mock_service.process_data.return_value = {"result": "success"}

        result = await mock_service.process_data({"input": "data"})

        assert result["result"] == "success"
        mock_service.process_data.assert_called_once_with({"input": "data"})

    @patch('src.services.external_service.AsyncExternalService')
    async def test_async_function_with_patch(self, mock_service_class):
        """ä½¿ç”¨patchæµ‹è¯•å¼‚æ­¥å‡½æ•°"""
        mock_service = AsyncMock()
        mock_service_class.return_value = mock_service
        mock_service.fetch_data.return_value = {"data": "test"}

        from src.services.my_service import MyService
        service = MyService()
        result = await service.fetch_external_data()

        assert result == {"data": "test"}
        mock_service.fetch_data.assert_called_once()

    async def test_async_context_manager(self):
        """æµ‹è¯•å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        async with AsyncMock() as mock_context:
            mock_context.__aenter__.return_value = {"connection": "active"}
            mock_context.__aexit__.return_value = None

            async with mock_context as conn:
                assert conn["connection"] == "active"
```

### CI è¦†ç›–ç‡é—¨ç¦é…ç½®

```yaml
# .github/workflows/test.yml
name: Test and Coverage

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        make install

    - name: Run tests with coverage
      run: |
        make test-all

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

    - name: Coverage gate check
      run: |
        make coverage-gate

    - name: Generate coverage report
      run: |
        make test-coverage

    - name: Upload coverage reports
      uses: actions/upload-artifact@v3
      with:
        name: coverage-reports
        path: htmlcov/
```

---

## ğŸ“Š ç›‘æ§ä¸æŒç»­æ”¹è¿›

### è¦†ç›–ç‡è¶‹åŠ¿ç›‘æ§

```python
# scripts/coverage_monitor.py
import json
import os
from datetime import datetime
import matplotlib.pyplot as plt
import pandas as pd

class CoverageMonitor:
    """è¦†ç›–ç‡ç›‘æ§å·¥å…·"""

    def __init__(self, coverage_file="coverage.json"):
        self.coverage_file = coverage_file
        self.history_file = "coverage_history.json"

    def record_coverage(self, coverage_data):
        """è®°å½•è¦†ç›–ç‡æ•°æ®"""
        history = self.load_history()

        record = {
            "timestamp": datetime.now().isoformat(),
            "coverage": coverage_data,
            "git_sha": os.getenv("GITHUB_SHA", "local")
        }

        history.append(record)
        self.save_history(history)

        return record

    def generate_trend_chart(self):
        """ç”Ÿæˆè¦†ç›–ç‡è¶‹åŠ¿å›¾"""
        history = self.load_history()

        if not history:
            return

        df = pd.DataFrame(history)
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        plt.figure(figsize=(12, 6))
        plt.plot(df['timestamp'], df['coverage']['total'], marker='o')
        plt.axhline(y=85, color='r', linestyle='--', label='Target (85%)')
        plt.title('Test Coverage Trend')
        plt.xlabel('Date')
        plt.ylabel('Coverage %')
        plt.legend()
        plt.grid(True)

        plt.savefig('docs/coverage_trend.png')
        plt.close()

    def generate_report(self):
        """ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š"""
        history = self.load_history()

        if not history:
            return "No coverage data available"

        latest = history[-1]
        previous = history[-2] if len(history) > 1 else latest

        report = f"""
# Coverage Report

## Latest Coverage: {latest['coverage']['total']}%

### Trend: {'â†—ï¸' if latest['coverage']['total'] > previous['coverage']['total'] else 'â†˜ï¸' if latest['coverage']['total'] < previous['coverage']['total'] else 'â¡ï¸'} {latest['coverage']['total'] - previous['coverage']['total']:+.1f}%

### By Module:
"""

        for module, coverage in latest['coverage']['by_module'].items():
            report += f"- {module}: {coverage}%\n"

        return report
```

### è¦†ç›–ç‡è¿›åº¦æ–‡æ¡£

```markdown
<!-- docs/COVERAGE_PROGRESS.md -->
# Test Coverage Progress

## Current Status: 85.2% âœ…

### Coverage Trend
- **Start (Phase 0)**: 80.0% (with 5 failing tests)
- **Phase 1 Complete**: 80.0% (all tests passing)
- **Phase 2 Complete**: 82.5% (database module improved)
- **Phase 3 Complete**: 84.1% (scheduler/data modules improved)
- **Phase 4 Complete**: 85.2% (global target achieved)
- **Phase 5 Complete**: 85.2% (all skip cases cleaned)

### Module Coverage
| Module | Coverage | Status |
|--------|----------|---------|
| api | 95% | âœ… Excellent |
| core | 92% | âœ… Excellent |
| data | 88% | âœ… Good |
| database | 75% | âœ… Good |
| features | 90% | âœ… Excellent |
| lineage | 85% | âœ… Good |
| models | 91% | âœ… Excellent |
| monitoring | 89% | âœ… Good |
| scheduler | 78% | âœ… Good |
| services | 93% | âœ… Excellent |
| streaming | 87% | âœ… Good |
| tasks | 86% | âœ… Good |
| utils | 94% | âœ… Excellent |

### Next Steps
- [ ] Maintain 85%+ coverage
- [ ] Focus on integration test quality
- [ ] Improve test performance
```

### Pre-commit Hooké…ç½®

```yaml
# .pre-commit-config.yaml
repos:
  - repo: local
    hooks:
      - id: pytest-check
        name: pytest-check
        entry: make test-unit
        language: system
        pass_filenames: false
        always_run: true

      - id: coverage-check
        name: coverage-check
        entry: make coverage-gate
        language: system
        pass_filenames: false
        always_run: true

      - id: lint-check
        name: lint-check
        entry: make lint
        language: system
        pass_filenames: false
        always_run: true
```

---

## ğŸ“ˆ æ‰§è¡Œæ—¶é—´çº¿ä¸é‡Œç¨‹ç¢‘

### æ€»ä½“æ—¶é—´çº¿ï¼š10-13å‘¨

#### Phase 1 (1-2å‘¨)
- **Week 1**: OpenLineageé—®é¢˜ä¿®å¤
- **Week 2**: åŸºç¡€è®¾æ–½å®Œå–„å’Œæµ‹è¯•æ¸…ç†
- **é‡Œç¨‹ç¢‘**: æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œæ— å¤±è´¥ç”¨ä¾‹

#### Phase 2 (2-3å‘¨)
- **Week 3-4**: æ•°æ®åº“æµ‹è¯•åŸºç¡€è®¾æ–½
- **Week 5**: æ•°æ®æ¨¡å‹å’Œé›†æˆæµ‹è¯•
- **é‡Œç¨‹ç¢‘**: databaseæ¨¡å—è¦†ç›–ç‡â‰¥60%

#### Phase 3 (2-3å‘¨)
- **Week 6-7**: scheduleræ¨¡å—æµ‹è¯•
- **Week 8**: dataæ¨¡å—æµ‹è¯•
- **é‡Œç¨‹ç¢‘**: scheduler/dataæ¨¡å—è¦†ç›–ç‡â‰¥70%

#### Phase 4 (2-3å‘¨)
- **Week 9-10**: è¦†ç›–ç‡è¡¥å…¨
- **Week 11**: è¦†ç›–ç‡é—¨ç¦å»ºç«‹
- **é‡Œç¨‹ç¢‘**: å…¨å±€è¦†ç›–ç‡â‰¥85%

#### Phase 5 (1-2å‘¨)
- **Week 12**: Skipç”¨ä¾‹æ¸…ç†
- **Week 13**: æœ€ç»ˆéªŒè¯
- **é‡Œç¨‹ç¢‘**: ç³»ç»Ÿæµ‹è¯•è´¨é‡è¾¾æ ‡

---

## ğŸ¯ æˆåŠŸæ ‡å‡†ä¸éªŒè¯

### æŠ€æœ¯æŒ‡æ ‡
- [ ] è¦†ç›–ç‡â‰¥85%
- [ ] é›¶å¤±è´¥æµ‹è¯•ç”¨ä¾‹
- [ ] é›¶æ— æ•ˆskipç”¨ä¾‹
- [ ] æ‰€æœ‰å¤–éƒ¨æœåŠ¡æœ‰Mockè¦†ç›–
- [ ] CIè¦†ç›–ç‡é—¨ç¦ç”Ÿæ•ˆ

### è´¨é‡æŒ‡æ ‡
- [ ] æµ‹è¯•è¿è¡Œæ—¶é—´â‰¤5åˆ†é’Ÿ
- [ ] æµ‹è¯•ç¨³å®šæ€§100%
- [ ] ä»£ç è´¨é‡æ£€æŸ¥100%é€šè¿‡
- [ ] æ–‡æ¡£å®Œæ•´æ€§100%

### ä¸šåŠ¡æŒ‡æ ‡
- [ ] å¼€å‘æ•ˆç‡æå‡â‰¥20%
- [ ] Bugä¿®å¤æ—¶é—´å‡å°‘â‰¥30%
- [ ] ä»£ç å®¡æŸ¥æ•ˆç‡æå‡â‰¥25%
- [ ] å›¢é˜Ÿæµ‹è¯•èƒ½åŠ›æå‡

---

## ğŸ“‹ æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡5é˜¶æ®µæ¸è¿›å¼å®æ–½ï¼Œå°†ç³»ç»Ÿæµ‹è¯•è´¨é‡ä»å½“å‰çš„80%è¦†ç›–ç‡ã€å­˜åœ¨æŠ€æœ¯å€ºåŠ¡çš„çŠ¶æ€ï¼Œæå‡è‡³85%+è¦†ç›–ç‡ã€é›¶æŠ€æœ¯å€ºåŠ¡çš„é«˜è´¨é‡çŠ¶æ€ã€‚æ–¹æ¡ˆé‡ç‚¹è§£å†³äº†OpenLineageä¾èµ–é—®é¢˜ï¼Œè¡¥é½äº†å…³é”®æ¨¡å—è¦†ç›–ç¼ºå£ï¼Œå»ºç«‹äº†å®Œæ•´çš„æµ‹è¯•è´¨é‡ä¿éšœä½“ç³»ã€‚

**é¢„æœŸæ”¶ç›Š**:
- ä»£ç è´¨é‡æ˜¾è‘—æå‡
- æŠ€æœ¯å€ºåŠ¡å½»åº•æ¸…ç†
- å¼€å‘æ•ˆç‡å¤§å¹…æé«˜
- ç³»ç»Ÿç¨³å®šæ€§å¢å¼º
- å›¢é˜Ÿæµ‹è¯•èƒ½åŠ›æå‡

æ–¹æ¡ˆå®æ–½åï¼Œç³»ç»Ÿå°†å…·å¤‡ä¼ä¸šçº§çš„æµ‹è¯•è´¨é‡æ ‡å‡†ï¼Œä¸ºåç»­åŠŸèƒ½è¿­ä»£å’Œç³»ç»Ÿæ¼”è¿›æä¾›åšå®ä¿éšœã€‚