# æ—¥å¿—é‡‡é›†ä¸ç®¡ç†ç­–ç•¥

## æ¦‚è¿°

FootballPrediction ç³»ç»Ÿçš„æ—¥å¿—ç®¡ç†ç­–ç•¥ï¼Œæ¶µç›–æ—¥å¿—åˆ†çº§ã€é‡‡é›†ã€å­˜å‚¨ã€åˆ†æå’Œç›‘æ§ï¼Œç¡®ä¿ç³»ç»Ÿå¯è§‚æµ‹æ€§å’Œé—®é¢˜è¿½è¸ªèƒ½åŠ›ã€‚

## ğŸ“Š æ—¥å¿—åˆ†çº§ç­–ç•¥

### æ—¥å¿—çº§åˆ«å®šä¹‰

#### DEBUG (è°ƒè¯•çº§åˆ«)
- **ç”¨é€”**: å¼€å‘å’Œè°ƒè¯•é˜¶æ®µçš„è¯¦ç»†ä¿¡æ¯
- **å†…å®¹**: å‡½æ•°è°ƒç”¨ã€å˜é‡å€¼ã€æ‰§è¡Œè·¯å¾„
- **ç¯å¢ƒ**: ä»…åœ¨å¼€å‘ç¯å¢ƒå¯ç”¨
- **ç¤ºä¾‹**:
```python
logger.debug("Processing match data: %s", match_data)
logger.debug("Feature calculation result: %s", features)
```

#### INFO (ä¿¡æ¯çº§åˆ«)
- **ç”¨é€”**: é‡è¦ä¸šåŠ¡äº‹ä»¶å’Œæ“ä½œè®°å½•
- **å†…å®¹**: ä¸šåŠ¡æµç¨‹å…³é”®èŠ‚ç‚¹ã€çŠ¶æ€å˜æ›´
- **ç¯å¢ƒ**: æ‰€æœ‰ç¯å¢ƒ
- **ç¤ºä¾‹**:
```python
logger.info("Data collection started for date: %s", date)
logger.info("Model prediction completed, confidence: %.2f", confidence)
logger.info("User %s requested predictions for %d matches", user_id, count)
```

#### WARNING (è­¦å‘Šçº§åˆ«)
- **ç”¨é€”**: æ½œåœ¨é—®é¢˜æˆ–å¼‚å¸¸æƒ…å†µ
- **å†…å®¹**: é‡è¯•æ“ä½œã€æ•°æ®å¼‚å¸¸ã€æ€§èƒ½é—®é¢˜
- **ç¯å¢ƒ**: æ‰€æœ‰ç¯å¢ƒ
- **ç¤ºä¾‹**:
```python
logger.warning("API rate limit reached, retrying in %d seconds", delay)
logger.warning("Data quality check failed for %s, using fallback", source)
logger.warning("Response time exceeded threshold: %.2fs", duration)
```

#### ERROR (é”™è¯¯çº§åˆ«)
- **ç”¨é€”**: ä¸¥é‡é”™è¯¯å’Œå¼‚å¸¸
- **å†…å®¹**: æ“ä½œå¤±è´¥ã€å¼‚å¸¸å †æ ˆã€é”™è¯¯æ¢å¤
- **ç¯å¢ƒ**: æ‰€æœ‰ç¯å¢ƒ
- **ç¤ºä¾‹**:
```python
logger.error("Database connection failed: %s", str(e))
logger.error("Model prediction failed for match %d: %s", match_id, str(e))
logger.error("Failed to process message: %s", message, exc_info=True)
```

#### CRITICAL (è‡´å‘½çº§åˆ«)
- **ç”¨é€”**: ç³»ç»Ÿçº§åˆ«çš„ä¸¥é‡é—®é¢˜
- **å†…å®¹**: æœåŠ¡ä¸å¯ç”¨ã€æ•°æ®ä¸¢å¤±ã€å®‰å…¨äº‹ä»¶
- **ç¯å¢ƒ**: æ‰€æœ‰ç¯å¢ƒ
- **ç¤ºä¾‹**:
```python
logger.critical("Application startup failed: %s", str(e))
logger.critical("Data corruption detected in table: %s", table_name)
logger.critical("Security breach attempt from IP: %s", ip_address)
```

## ğŸ—ï¸ æ—¥å¿—é‡‡é›†æ¶æ„

### æ¨èæ–¹æ¡ˆï¼šELK Stack

#### ç»„ä»¶æ¶æ„
```
Application Logs â†’ Filebeat â†’ Logstash â†’ Elasticsearch â†’ Kibana
                       â†“
               Structured JSON Logs
```

#### å¤‡é€‰æ–¹æ¡ˆï¼šLoki Stack
```
Application Logs â†’ Promtail â†’ Loki â†’ Grafana
                       â†“
              Label-based Indexing
```

### æ—¥å¿—æ ¼å¼æ ‡å‡†

#### ç»“æ„åŒ–æ—¥å¿—æ ¼å¼ (æ¨è)
```json
{
  "timestamp": "2023-01-01T10:00:00.000Z",
  "level": "INFO",
  "logger": "src.api.predictions",
  "message": "Prediction request processed successfully",
  "request_id": "req_123456",
  "user_id": "user_789",
  "match_id": 12345,
  "prediction_confidence": 0.85,
  "processing_time_ms": 250,
  "metadata": {
    "endpoint": "/api/v1/predictions",
    "method": "POST",
    "status_code": 200
  }
}
```

#### åº”ç”¨é…ç½®ç¤ºä¾‹
```python
# src/core/logging.py
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
        }

        # æ·»åŠ è¯·æ±‚ä¸Šä¸‹æ–‡
        if hasattr(record, 'request_id'):
            log_entry["request_id"] = record.request_id
        if hasattr(record, 'user_id'):
            log_entry["user_id"] = record.user_id

        # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
        if record.exc_info:
            log_entry["exception"] = self.formatException(record.exc_info)

        return json.dumps(log_entry)
```

## ğŸ“ è¯·æ±‚è¿½è¸ªä¸å…³è”

### Request ID ç”Ÿæˆ
```python
# middleware/request_tracking.py
import uuid
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware

class RequestTrackingMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        request_id = str(uuid.uuid4())
        request.state.request_id = request_id

        # æ·»åŠ åˆ°å“åº”å¤´
        response = await call_next(request)
        response.headers["X-Request-ID"] = request_id
        return response
```

### Trace ID é›†æˆ
```python
# src/core/tracing.py
import logging
from contextvars import ContextVar

# ä¸Šä¸‹æ–‡å˜é‡
trace_id_var: ContextVar[str] = ContextVar('trace_id', default='')
span_id_var: ContextVar[str] = ContextVar('span_id', default='')

class TraceLoggerAdapter(logging.LoggerAdapter):
    def process(self, msg, kwargs):
        trace_id = trace_id_var.get()
        span_id = span_id_var.get()

        if trace_id:
            self.extra['trace_id'] = trace_id
        if span_id:
            self.extra['span_id'] = span_id

        return msg, kwargs
```

## ğŸ”§ ELK Stack é›†æˆé…ç½®

### 1. Filebeat é…ç½®
```yaml
# filebeat.yml
filebeat.inputs:
- type: log
  enabled: true
  paths:
    - /app/logs/*.log
  fields:
    service: football-prediction
    environment: production
  fields_under_root: true
  multiline.pattern: '^{'
  multiline.negate: true
  multiline.match: after

output.logstash:
  hosts: ["logstash:5044"]

processors:
- add_host_metadata:
    when.not.contains.tags: forwarded
- add_docker_metadata: ~
```

### 2. Logstash é…ç½®
```ruby
# logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [service] == "football-prediction" {
    json {
      source => "message"
    }

    date {
      match => [ "timestamp", "ISO8601" ]
    }

    mutate {
      add_field => { "[@metadata][index]" => "football-logs-%{+YYYY.MM.dd}" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "%{[@metadata][index]}"
  }
}
```

### 3. Elasticsearch ç´¢å¼•æ¨¡æ¿
```json
{
  "template": "football-logs-*",
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index.mapping.total_fields.limit": 2000
  },
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date"
      },
      "level": {
        "type": "keyword"
      },
      "logger": {
        "type": "keyword"
      },
      "message": {
        "type": "text",
        "analyzer": "standard"
      },
      "request_id": {
        "type": "keyword"
      },
      "user_id": {
        "type": "keyword"
      },
      "processing_time_ms": {
        "type": "integer"
      }
    }
  }
}
```

## ğŸ¯ Loki Stack é›†æˆé…ç½® (å¤‡é€‰)

### 1. Promtail é…ç½®
```yaml
# promtail.yml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
- job_name: football-app
  static_configs:
  - targets:
      - localhost
    labels:
      job: football-app
      service: football-prediction
      __path__: /app/logs/*.log

  pipeline_stages:
  - json:
      expressions:
        level: level
        timestamp: timestamp
        logger: logger
        request_id: request_id

  - labels:
      level:
      logger:

  - timestamp:
      source: timestamp
      format: RFC3339
```

### 2. Loki é…ç½®
```yaml
# loki.yml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/boltdb-shipper-active
    cache_location: /loki/boltdb-shipper-cache
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
```

### 3. Grafana æ—¥å¿—æŸ¥è¯¢ç¤ºä¾‹
```logql
# æŸ¥è¯¢é”™è¯¯æ—¥å¿—
{service="football-prediction"} |= "ERROR"

# æŸ¥è¯¢ç‰¹å®šè¯·æ±‚çš„æ‰€æœ‰æ—¥å¿—
{service="football-prediction"} | json | request_id="req_123456"

# ç»Ÿè®¡é”™è¯¯ç‡
rate({service="football-prediction"} |= "ERROR" [5m])

# æŸ¥è¯¢æ…¢è¯·æ±‚
{service="football-prediction"} | json | processing_time_ms > 1000
```

## ğŸ“ æ—¥å¿—å­˜å‚¨å’Œè½®è½¬

### æœ¬åœ°æ—¥å¿—é…ç½®
```python
# src/core/logging.py
import logging.config

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'json': {
            '()': 'src.core.logging.JSONFormatter',
        },
        'standard': {
            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        },
    },
    'handlers': {
        'file': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/app/logs/app.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'json',
        },
        'error_file': {
            'level': 'ERROR',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': '/app/logs/error.log',
            'maxBytes': 10485760,  # 10MB
            'backupCount': 5,
            'formatter': 'json',
        },
        'console': {
            'level': 'INFO',
            'class': 'logging.StreamHandler',
            'formatter': 'standard',
        },
    },
    'loggers': {
        'src': {
            'handlers': ['file', 'error_file', 'console'],
            'level': 'INFO',
            'propagate': False,
        },
        'uvicorn': {
            'handlers': ['file', 'console'],
            'level': 'INFO',
        },
    },
    'root': {
        'level': 'INFO',
        'handlers': ['console'],
    }
}
```

### Docker æ—¥å¿—é…ç½®
```yaml
# docker-compose.yml
services:
  app:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,environment"
```

## ğŸ” æ—¥å¿—åˆ†æå’Œç›‘æ§

### å¸¸ç”¨æŸ¥è¯¢æ¨¡å¼

#### 1. é”™è¯¯åˆ†æ
```bash
# Elasticsearch
GET football-logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        {"term": {"level": "ERROR"}},
        {"range": {"timestamp": {"gte": "now-1h"}}}
      ]
    }
  },
  "aggs": {
    "error_by_logger": {
      "terms": {"field": "logger"}
    }
  }
}

# Loki
{service="football-prediction"} |= "ERROR" | json | __error__=""
```

#### 2. æ€§èƒ½åˆ†æ
```bash
# æ…¢è¯·æ±‚åˆ†æ
{service="football-prediction"}
| json
| processing_time_ms > 1000
| line_format "{{.timestamp}} - {{.message}} ({{.processing_time_ms}}ms)"
```

#### 3. ç”¨æˆ·è¡Œä¸ºåˆ†æ
```bash
# ç”¨æˆ·è¯·æ±‚æ¨¡å¼
{service="football-prediction"}
| json
| user_id != ""
| stats count() by user_id
```

### å‘Šè­¦è§„åˆ™é…ç½®

#### Prometheus å‘Šè­¦è§„åˆ™
```yaml
# log_alerts.yml
groups:
  - name: log_alerts
    rules:
    - alert: HighErrorRate
      expr: rate(log_entries_total{level="ERROR"}[5m]) > 0.1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "High error rate detected"

    - alert: NoLogsReceived
      expr: absent(rate(log_entries_total[5m]))
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "No logs received in the last 5 minutes"
```

#### æ—¥å¿—ç›‘æ§æŒ‡æ ‡
```python
# src/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# æ—¥å¿—è®¡æ•°å™¨
log_entries_total = Counter(
    'log_entries_total',
    'Total number of log entries',
    ['level', 'logger']
)

# å“åº”æ—¶é—´ç›´æ–¹å›¾
request_duration_seconds = Histogram(
    'request_duration_seconds',
    'Request duration in seconds',
    ['endpoint', 'method', 'status']
)

# é”™è¯¯ç‡è®¡é‡å™¨
error_rate = Gauge(
    'error_rate',
    'Current error rate',
    ['service']
)
```

## ğŸ›¡ï¸ æ—¥å¿—å®‰å…¨å’Œåˆè§„

### æ•æ„Ÿä¿¡æ¯è„±æ•
```python
# src/core/sanitizer.py
import re
from typing import Any, Dict

class LogSanitizer:
    SENSITIVE_PATTERNS = {
        'password': re.compile(r'"password":\s*"[^"]*"'),
        'token': re.compile(r'"token":\s*"[^"]*"'),
        'api_key': re.compile(r'"api_key":\s*"[^"]*"'),
        'credit_card': re.compile(r'\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b'),
    }

    @classmethod
    def sanitize_log_data(cls, data: Dict[str, Any]) -> Dict[str, Any]:
        """è„±æ•æ—¥å¿—æ•°æ®ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        sanitized = data.copy()

        for field in ['password', 'token', 'api_key']:
            if field in sanitized:
                sanitized[field] = '***MASKED***'

        return sanitized
```

### è®¿é—®æ§åˆ¶
```yaml
# elasticsearch.yml
xpack.security.enabled: true
xpack.security.authc:
  realms:
    native:
      native1:
        order: 0

# kibana.yml
elasticsearch.username: "kibana_user"
elasticsearch.password: "${KIBANA_PASSWORD}"
```

## ğŸ“Š æ—¥å¿—ä»ªè¡¨ç›˜æ¨¡æ¿

### Kibana ä»ªè¡¨ç›˜ç»„ä»¶

#### 1. ç³»ç»Ÿæ¦‚è§ˆ
- æ—¥å¿—æ€»é‡è¶‹åŠ¿
- é”™è¯¯ç‡å˜åŒ–
- å“åº”æ—¶é—´åˆ†å¸ƒ
- æ´»è·ƒç”¨æˆ·æ•°

#### 2. é”™è¯¯åˆ†æ
- é”™è¯¯ç±»å‹åˆ†å¸ƒ
- é”™è¯¯å‘ç”Ÿé¢‘ç‡
- å¼‚å¸¸å †æ ˆåˆ†æ
- å—å½±å“ç”¨æˆ·ç»Ÿè®¡

#### 3. æ€§èƒ½ç›‘æ§
- æ¥å£å“åº”æ—¶é—´
- æ•°æ®åº“æŸ¥è¯¢è€—æ—¶
- ç¼“å­˜å‘½ä¸­ç‡
- èµ„æºä½¿ç”¨æƒ…å†µ

#### 4. ä¸šåŠ¡ç›‘æ§
- é¢„æµ‹è¯·æ±‚é‡
- æ•°æ®é‡‡é›†çŠ¶æ€
- ç”¨æˆ·æ´»è·ƒåº¦
- åŠŸèƒ½ä½¿ç”¨ç»Ÿè®¡

### Grafana æ—¥å¿—é¢æ¿
```json
{
  "dashboard": {
    "title": "Football Prediction Logs",
    "panels": [
      {
        "title": "Log Volume",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(loki_log_entries_total[5m]))",
            "legendFormat": "Logs/sec"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate({service=\"football-prediction\"} |= \"ERROR\" [5m])",
            "legendFormat": "Errors/sec"
          }
        ]
      }
    ]
  }
}
```

## ğŸš€ éƒ¨ç½²å’Œç»´æŠ¤

### å®¹å™¨åŒ–éƒ¨ç½²
```dockerfile
# Dockerfile
FROM python:3.11-slim

# åˆ›å»ºæ—¥å¿—ç›®å½•
RUN mkdir -p /app/logs
VOLUME ["/app/logs"]

# æ—¥å¿—è½®è½¬é…ç½®
COPY logrotate.conf /etc/logrotate.d/app
```

### æ—¥å¿—è½®è½¬é…ç½®
```bash
# logrotate.conf
/app/logs/*.log {
    daily
    missingok
    rotate 7
    compress
    delaycompress
    notifempty
    copytruncate
    postrotate
        /bin/kill -USR1 `cat /var/run/app.pid 2> /dev/null` 2> /dev/null || true
    endscript
}
```

### ç›‘æ§è„šæœ¬
```bash
#!/bin/bash
# scripts/log_health_check.sh

LOG_DIR="/app/logs"
MAX_LOG_AGE=300  # 5åˆ†é’Ÿ

# æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨å’Œæ–°é²œåº¦
if [ ! -f "$LOG_DIR/app.log" ]; then
    echo "ERROR: app.log not found"
    exit 1
fi

LAST_MODIFIED=$(stat -f "%m" "$LOG_DIR/app.log" 2>/dev/null || stat -c "%Y" "$LOG_DIR/app.log")
CURRENT_TIME=$(date +%s)
AGE=$((CURRENT_TIME - LAST_MODIFIED))

if [ $AGE -gt $MAX_LOG_AGE ]; then
    echo "WARNING: app.log is too old (${AGE}s)"
    exit 1
fi

echo "OK: Logs are healthy"
```

## ğŸ“ æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§é—®é¢˜

#### 1. æ—¥å¿—ä¸¢å¤±
```bash
# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h /app/logs

# æ£€æŸ¥æ–‡ä»¶æƒé™
ls -la /app/logs/

# æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿è¡Œ
ps aux | grep filebeat
```

#### 2. æ—¥å¿—æ ¼å¼é”™è¯¯
```bash
# éªŒè¯JSONæ ¼å¼
tail -n 10 /app/logs/app.log | jq .

# æ£€æŸ¥Logstashå¤„ç†
curl -X GET "elasticsearch:9200/_cat/indices/football-logs-*"
```

#### 3. æœç´¢æ€§èƒ½é—®é¢˜
```bash
# æ£€æŸ¥Elasticsearché›†ç¾¤çŠ¶æ€
curl -X GET "elasticsearch:9200/_cluster/health"

# ä¼˜åŒ–æŸ¥è¯¢
curl -X POST "elasticsearch:9200/football-logs-*/_forcemerge"
```

---

**æ³¨æ„**: æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´é…ç½®å‚æ•°ï¼Œå®šæœŸå®¡æŸ¥æ—¥å¿—ç­–ç•¥ä»¥é€‚åº”ä¸šåŠ¡å˜åŒ–ã€‚