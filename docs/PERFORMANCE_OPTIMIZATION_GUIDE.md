# æ€§èƒ½ä¼˜åŒ–ä½¿ç”¨æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»è¶³çƒé¢„æµ‹ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–åŠŸèƒ½çš„ä½¿ç”¨æ–¹æ³•ï¼ŒåŒ…æ‹¬ç¼“å­˜ç®¡ç†ã€æ€§èƒ½ç›‘æ§å’Œç³»ç»Ÿä¼˜åŒ–ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æ€§èƒ½ä¼˜åŒ–æœåŠ¡

```bash
# ç¡®ä¿RedisæœåŠ¡è¿è¡Œ (å¯é€‰)
redis-server

# å¯åŠ¨åº”ç”¨ (æ€§èƒ½ä¼˜åŒ–è‡ªåŠ¨å¯ç”¨)
python src/main.py
```

### 2. éªŒè¯æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½

```bash
# æ£€æŸ¥å¥åº·çŠ¶æ€
curl http://localhost:8000/api/predictions/v2/health

# è·å–ä¼˜åŒ–é¢„æµ‹ (å¸¦ç¼“å­˜)
curl http://localhost:8000/api/predictions/v2/matches/123/prediction
```

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶

### 1. ç»Ÿä¸€ç¼“å­˜ç®¡ç†å™¨

#### åŸºæœ¬ä½¿ç”¨
```python
from src.cache.unified_cache import get_cache_manager

# è·å–ç¼“å­˜ç®¡ç†å™¨
cache_mgr = get_cache_manager()

# ç¼“å­˜æ•°æ®
await cache_mgr.set(
    key="prediction_123",
    value={"outcome": "home_win", "confidence": 0.75},
    cache_type="prediction_result",
    ttl=1800  # 30åˆ†é’Ÿ
)

# è·å–ç¼“å­˜æ•°æ®
cached_data = await cache_mgr.get("prediction_123", "prediction_result")
if cached_data:
    print("ç¼“å­˜å‘½ä¸­:", cached_data)
else:
    print("ç¼“å­˜æœªå‘½ä¸­ï¼Œéœ€è¦é‡æ–°è®¡ç®—")

# åˆ é™¤ç¼“å­˜
await cache_mgr.delete("prediction_123", "prediction_result")

# æ‰¹é‡æ¸…é™¤åŒ¹é…æ¨¡å¼çš„ç¼“å­˜
await cache_mgr.invalidate_pattern("prediction_", "prediction_result")
```

#### è£…é¥°å™¨ä½¿ç”¨
```python
from src.cache.unified_cache import cached, performance_monitor

# è‡ªåŠ¨ç¼“å­˜å‡½æ•°ç»“æœ
@cached(cache_type="prediction_result", ttl=1800)
async def calculate_prediction(match_id: int):
    # å¤æ‚çš„é¢„æµ‹è®¡ç®—
    result = await complex_prediction_algorithm(match_id)
    return result

# æ€§èƒ½ç›‘æ§è£…é¥°å™¨
@performance_monitor(threshold=1.0)  # 1ç§’é˜ˆå€¼
async def slow_operation():
    # è€—æ—¶æ“ä½œ
    await asyncio.sleep(0.5)
    return "result"

# ç»„åˆä½¿ç”¨
@cached(cache_type="team_stats", ttl=900)
@performance_monitor(threshold=0.5)
async def get_team_statistics(team_id: int):
    # è·å–å›¢é˜Ÿç»Ÿè®¡
    return await fetch_team_stats(team_id)
```

### 2. ç³»ç»Ÿæ€§èƒ½ç›‘æ§

#### ç›‘æ§ç³»ç»Ÿèµ„æº
```python
from src.performance.monitoring import get_system_monitor

# è·å–ç³»ç»Ÿç›‘æ§å™¨
monitor = get_system_monitor()

# è·å–å½“å‰ç³»ç»ŸæŒ‡æ ‡
current_metrics = monitor.get_current_metrics()
print(f"CPUä½¿ç”¨ç‡: {current_metrics.cpu_percent}%")
print(f"å†…å­˜ä½¿ç”¨ç‡: {current_metrics.memory_percent}%")
print(f"å¹³å‡å“åº”æ—¶é—´: {current_metrics.response_time_avg}ms")

# è·å–å†å²æŒ‡æ ‡æ‘˜è¦
summary = monitor.get_metrics_summary(minutes=5)
print(f"5åˆ†é’Ÿå†…å¹³å‡CPU: {summary['cpu']['avg']}%")

# æ£€æŸ¥æ€§èƒ½è­¦æŠ¥
alerts = monitor.check_performance_alerts()
if alerts:
    for alert in alerts:
        print(f"è­¦æŠ¥: {alert['message']} (ä¸¥é‡ç¨‹åº¦: {alert['severity']})")
```

#### æ€§èƒ½åˆ†æ
```python
from src.performance.monitoring import get_performance_analyzer

# è·å–æ€§èƒ½åˆ†æå™¨
analyzer = get_performance_analyzer()

# åˆ†ææ€§èƒ½è¶‹åŠ¿
trends = analyzer.analyze_trends(hours=1)
print(f"CPUè¶‹åŠ¿: {trends['trends']['cpu']['trend']}")

# è·å–æ€§èƒ½å»ºè®®
metrics_summary = monitor.get_metrics_summary(minutes=10)
recommendations = analyzer.get_performance_recommendations(metrics_summary)
for rec in recommendations:
    print(f"å»ºè®®: {rec}")
```

### 3. æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–

#### ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–å™¨
```python
from src.database.base import get_db
from src.performance.optimizer import get_performance_optimizer

# è·å–æ•°æ®åº“ä¼šè¯
async with get_db() as db_session:
    # è·å–æ€§èƒ½ä¼˜åŒ–å™¨
    optimizer = get_performance_optimizer(db_session)

    # ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
    index_results = await optimizer.optimize_database_indexes()
    print(f"åˆ›å»ºç´¢å¼•: {len(index_results['created'])}")
    print(f"å·²å­˜åœ¨ç´¢å¼•: {len(index_results['existing'])}")

    # åˆ†ææ…¢æŸ¥è¯¢
    query_analysis = await optimizer.optimize_slow_queries()
    for query_name, analysis in query_analysis.items():
        if 'recommendations' in analysis:
            print(f"æŸ¥è¯¢ {query_name} å»ºè®®:")
            for rec in analysis['recommendations']:
                print(f"  - {rec}")

    # å®ç°æŸ¥è¯¢ç¼“å­˜
    cache_results = await optimizer.implement_query_cache()
    print(f"ç¼“å­˜æŸ¥è¯¢æ•°é‡: {len(cache_results['cached'])}")

    # ä¼˜åŒ–ç¼“å­˜é…ç½®
    cache_optimization = await optimizer.optimize_cache_configuration()
    print("ç¼“å­˜ä¼˜åŒ–å»ºè®®:", cache_optimization['recommendations'])
```

## ğŸ”§ é…ç½®ç®¡ç†

### 1. ç¼“å­˜é…ç½®

```python
from src.performance.config import get_performance_config

# è·å–é…ç½®ç®¡ç†å™¨
config = get_performance_config()

# æŸ¥çœ‹å½“å‰ç¼“å­˜é…ç½®
cache_config = config.get_cache_config()
print("é¢„æµ‹ç»“æœTTL:", cache_config['prediction']['ttl'])  # 1800ç§’
print("æœ¬åœ°ç¼“å­˜å¤§å°:", cache_config['local']['size'])    # 1000æ¡ç›®

# æ›´æ–°ç¼“å­˜é…ç½®
config.update_cache_config(
    prediction_ttl=3600,      # é¢„æµ‹ç»“æœç¼“å­˜1å°æ—¶
    local_cache_size=2000     # æœ¬åœ°ç¼“å­˜2000æ¡ç›®
)
```

### 2. ç›‘æ§é…ç½®

```python
# æŸ¥çœ‹ç›‘æ§é…ç½®
monitoring_config = config.get_monitoring_config()
thresholds = monitoring_config['thresholds']

print("CPUè­¦å‘Šé˜ˆå€¼:", thresholds['cpu']['warning'])     # 70%
print("CPUä¸¥é‡é˜ˆå€¼:", thresholds['cpu']['critical'])    # 90%
print("å“åº”æ—¶é—´è­¦å‘Š:", thresholds['response_time']['warning'])  # 1.0ç§’

# æ›´æ–°ç›‘æ§é…ç½®
config.update_monitoring_config(
    cpu_warning_threshold=80.0,    # CPUè­¦å‘Šé˜ˆå€¼è°ƒè‡³80%
    response_time_warning=1.5      # å“åº”æ—¶é—´è­¦å‘Šè°ƒè‡³1.5ç§’
)
```

### 3. ä¼˜åŒ–é…ç½®

```python
# æŸ¥çœ‹ä¼˜åŒ–é…ç½®
optimization_config = config.get_optimization_config()
print("å¯ç”¨æŸ¥è¯¢ä¼˜åŒ–:", optimization_config['database']['enable_query_optimization'])
print("æœ€å¤§è¿æ¥æ•°:", optimization_config['connection_pool']['max_connections'])

# æ›´æ–°ä¼˜åŒ–é…ç½®
config.update_optimization_config(
    max_connections=30,           # æœ€å¤§è¿æ¥æ•°å¢è‡³30
    slow_query_threshold=0.5      # æ…¢æŸ¥è¯¢é˜ˆå€¼è°ƒè‡³0.5ç§’
)
```

## ğŸ“Š APIç«¯ç‚¹ä½¿ç”¨

### 1. å¥åº·æ£€æŸ¥

```bash
# åŸºæœ¬å¥åº·æ£€æŸ¥
curl http://localhost:8000/api/predictions/v2/health

# å“åº”ç¤ºä¾‹
{
  "status": "healthy",
  "timestamp": "2025-11-06T20:00:00Z",
  "cache_stats": {
    "hit_rate": 0.75,
    "local_cache_size": 850
  },
  "system_metrics": {
    "cpu_percent": 45.2,
    "memory_percent": 62.8,
    "response_time_avg": 0.35
  }
}
```

### 2. ä¼˜åŒ–é¢„æµ‹ç«¯ç‚¹

```bash
# è·å–é¢„æµ‹ç»“æœ (è‡ªåŠ¨ç¼“å­˜)
curl "http://localhost:8000/api/predictions/v2/matches/123/prediction?include_details=true"

# å“åº”ç¤ºä¾‹
{
  "status": "success",
  "data": {
    "match_id": 123,
    "predicted_outcome": "home_win",
    "confidence_score": 0.78,
    "probabilities": {
      "home_win": 0.65,
      "draw": 0.25,
      "away_win": 0.10
    }
  },
  "cached": false,
  "execution_time_ms": 245.5
}
```

### 3. çƒ­é—¨é¢„æµ‹

```bash
# è·å–çƒ­é—¨é¢„æµ‹
curl "http://localhost:8000/api/predictions/v2/popular?limit=20&time_range=24h"

# è·å–ç”¨æˆ·é¢„æµ‹å†å²
curl "http://localhost:8000/api/predictions/v2/user/456/history?page=1&size=10"
```

### 4. ç»Ÿè®¡ä¿¡æ¯

```bash
# è·å–é¢„æµ‹ç»Ÿè®¡
curl "http://localhost:8000/api/predictions/v2/statistics?time_range=7d"

# å“åº”ç¤ºä¾‹
{
  "status": "success",
  "data": {
    "total_predictions": 15420,
    "accuracy_rate": 0.73,
    "average_confidence": 0.76,
    "performance_metrics": {
      "avg_response_time_ms": 280.5,
      "cache_hit_rate": 0.78,
      "daily_predictions": 185
    }
  }
}
```

### 5. ç¼“å­˜ç®¡ç†

```bash
# ç¼“å­˜é¢„çƒ­ (éœ€è¦ç®¡ç†å‘˜æƒé™)
curl -X POST http://localhost:8000/api/predictions/v2/cache/warmup \
  -H "Authorization: Bearer <admin_token>"

# æ¸…é™¤ç¼“å­˜ (éœ€è¦ç®¡ç†å‘˜æƒé™)
curl -X DELETE "http://localhost:8000/api/predictions/v2/cache/clear?pattern=prediction" \
  -H "Authorization: Bearer <admin_token>"
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. ç¼“å­˜ç­–ç•¥

```python
# âœ… æ¨è: ä¸ºä¸åŒç±»å‹çš„æ•°æ®è®¾ç½®åˆé€‚çš„TTL
@cached(cache_type="prediction_result", ttl=1800)  # é¢„æµ‹ç»“æœ30åˆ†é’Ÿ
async def get_prediction(match_id):
    pass

@cached(cache_type="team_stats", ttl=3600)        # å›¢é˜Ÿç»Ÿè®¡1å°æ—¶
async def get_team_stats(team_id):
    pass

@cached(cache_type="user_preferences", ttl=7200)  # ç”¨æˆ·åå¥½2å°æ—¶
async def get_user_preferences(user_id):
    pass
```

### 2. æ€§èƒ½ç›‘æ§

```python
# âœ… æ¨è: ä¸ºå…³é”®æ“ä½œæ·»åŠ æ€§èƒ½ç›‘æ§
@performance_monitor(threshold=1.0)  # 1ç§’é˜ˆå€¼
async def critical_business_operation():
    # å…³é”®ä¸šåŠ¡é€»è¾‘
    pass

# âœ… æ¨è: å®šæœŸæ£€æŸ¥æ€§èƒ½è­¦æŠ¥
monitor = get_system_monitor()
alerts = monitor.check_performance_alerts()
if alerts:
    # å‘é€å‘Šè­¦é€šçŸ¥
    await send_alert_notification(alerts)
```

### 3. æ•°æ®åº“ä¼˜åŒ–

```python
# âœ… æ¨è: å®šæœŸè¿è¡Œæ•°æ®åº“ä¼˜åŒ–
async def scheduled_optimization():
    async with get_db() as db_session:
        optimizer = get_performance_optimizer(db_session)

        # æ¯å‘¨ä¼˜åŒ–ç´¢å¼•
        await optimizer.optimize_database_indexes()

        # åˆ†ææ…¢æŸ¥è¯¢
        await optimizer.optimize_slow_queries()
```

### 4. é”™è¯¯å¤„ç†

```python
# âœ… æ¨è: å®Œå–„çš„é”™è¯¯å¤„ç†
try:
    cached_data = await cache_mgr.get(key, cache_type)
    if cached_data is None:
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œé‡æ–°è®¡ç®—
        data = await expensive_computation()
        await cache_mgr.set(key, data, cache_type, ttl)
    else:
        data = cached_data
except Exception as e:
    logger.error(f"ç¼“å­˜æ“ä½œå¤±è´¥: {e}")
    # é™çº§åˆ°ç›´æ¥è®¡ç®—
    data = await expensive_computation()
```

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. ç¼“å­˜ä¼˜åŒ–

- **çƒ­ç‚¹æ•°æ®**: å¯¹è®¿é—®é¢‘ç¹çš„æ•°æ®ä½¿ç”¨è¾ƒçŸ­çš„TTL (5-15åˆ†é’Ÿ)
- **ç¨³å®šæ•°æ®**: å¯¹å˜åŒ–è¾ƒå°‘çš„æ•°æ®ä½¿ç”¨è¾ƒé•¿çš„TTL (1-24å°æ—¶)
- **æœ¬åœ°ç¼“å­˜**: åˆç†è®¾ç½®æœ¬åœ°ç¼“å­˜å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
- **ç¼“å­˜é¢„çƒ­**: ç³»ç»Ÿå¯åŠ¨æ—¶é¢„çƒ­å…³é”®æ•°æ®

### 2. ç›‘æ§è°ƒä¼˜

- **é˜ˆå€¼è®¾ç½®**: æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´æ€§èƒ½é˜ˆå€¼
- **ç›‘æ§é¢‘ç‡**: å¹³è¡¡ç›‘æ§ç²¾åº¦å’Œç³»ç»Ÿå¼€é”€
- **å‘Šè­¦ç­–ç•¥**: è®¾ç½®åˆç†çš„å‘Šè­¦çº§åˆ«å’Œé€šçŸ¥æ–¹å¼
- **è¶‹åŠ¿åˆ†æ**: å®šæœŸåˆ†ææ€§èƒ½è¶‹åŠ¿ï¼ŒåŠæ—¶å‘ç°é—®é¢˜

### 3. æ•°æ®åº“ä¼˜åŒ–

- **ç´¢å¼•ç­–ç•¥**: ä¸ºå¸¸ç”¨æŸ¥è¯¢åˆ›å»ºåˆé€‚çš„ç´¢å¼•
- **æŸ¥è¯¢ä¼˜åŒ–**: é¿å…N+1æŸ¥è¯¢ï¼Œä½¿ç”¨JOINä¼˜åŒ–
- **è¿æ¥æ± **: åˆç†é…ç½®æ•°æ®åº“è¿æ¥æ± å¤§å°
- **æ…¢æŸ¥è¯¢**: å®šæœŸåˆ†æå’Œä¼˜åŒ–æ…¢æŸ¥è¯¢

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **ç¼“å­˜å‘½ä¸­ç‡ä½**
   ```python
   # æ£€æŸ¥ç¼“å­˜ç»Ÿè®¡
   stats = await cache_mgr.get_cache_stats()
   print(f"å‘½ä¸­ç‡: {stats['hit_rate']}")

   # ä¼˜åŒ–ç¼“å­˜é”®è®¾è®¡
   # ç¡®ä¿ç¼“å­˜é”®çš„ä¸€è‡´æ€§
   ```

2. **å†…å­˜ä½¿ç”¨è¿‡é«˜**
   ```python
   # è°ƒæ•´æœ¬åœ°ç¼“å­˜å¤§å°
   config.update_cache_config(local_cache_size=500)

   # æ¸…ç†è¿‡æœŸç¼“å­˜
   await cache_mgr.cleanup_expired_entries()
   ```

3. **å“åº”æ—¶é—´æ…¢**
   ```python
   # æ£€æŸ¥æ€§èƒ½è­¦æŠ¥
   alerts = monitor.check_performance_alerts()

   # ä¼˜åŒ–æ…¢æŸ¥è¯¢
   async with get_db() as db_session:
       optimizer = get_performance_optimizer(db_session)
       await optimizer.optimize_slow_queries()
   ```

### è°ƒè¯•å·¥å…·

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger('src.cache').setLevel(logging.DEBUG)
logging.getLogger('src.performance').setLevel(logging.DEBUG)

# æ€§èƒ½åˆ†æ
@performance_monitor(threshold=0.1)  # è®¾ç½®è¾ƒä½é˜ˆå€¼ç”¨äºè°ƒè¯•
async def debug_function():
    pass
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ€§èƒ½ä¼˜åŒ–å®ŒæˆæŠ¥å‘Š](./ISSUE337_PERFORMANCE_OPTIMIZATION_COMPLETION_REPORT.md)
- [APIå‚è€ƒæ–‡æ¡£](./API_REFERENCE.md)
- [ç³»ç»Ÿæ¶æ„æ–‡æ¡£](./ARCHITECTURE.md)
- [æµ‹è¯•æ”¹è¿›æŒ‡å—](./TEST_IMPROVEMENT_GUIDE.md)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0 | **æ›´æ–°æ—¶é—´**: 2025-11-06 | **ç»´æŠ¤è€…**: Claude Code