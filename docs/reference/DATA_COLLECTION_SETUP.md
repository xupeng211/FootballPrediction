# æ•°æ®é‡‡é›†é…ç½®æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†ä»‹ç»è¶³çƒé¢„æµ‹ç³»ç»Ÿçš„æ•°æ®é‡‡é›†æ¶æ„ã€é…ç½®æ–¹æ³•å’Œæœ€ä½³å®è·µã€‚ç³»ç»Ÿé‡‡ç”¨å¤šå±‚æ¶æ„è®¾è®¡ï¼Œæ”¯æŒå¤šç§æ•°æ®æºçš„é«˜æ•ˆé‡‡é›†ï¼Œç¡®ä¿æ•°æ®è´¨é‡å’Œå®æ—¶æ€§ã€‚

## ğŸ—ï¸ æ•°æ®é‡‡é›†æ¶æ„

### æ•´ä½“æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®é‡‡é›†å±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   å¤–éƒ¨æ•°æ®æº     â”‚   é‡‡é›†å¼•æ“       â”‚        æ•°æ®è´¨é‡               â”‚
â”‚  (External)     â”‚  (Collectors)   â”‚     (Quality)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ è¶³çƒAPI       â”‚ â€¢ BaseCollector  â”‚ â€¢ æ•°æ®éªŒè¯                   â”‚
â”‚ â€¢ åšå½©API       â”‚ â€¢ OddsCollector  â”‚ â€¢ å¼‚å¸¸æ£€æµ‹                   â”‚
â”‚ â€¢ å®æ—¶æ•°æ®æº     â”‚ â€¢ FixturesCollectorâ”‚ â€¢ æ•°æ®æ¸…æ´—                   â”‚
â”‚ â€¢ å†å²æ•°æ®åº“     â”‚ â€¢ ScoresCollectorâ”‚ â€¢ æ ¼å¼æ ‡å‡†åŒ–                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ•°æ®å­˜å‚¨å±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Bronzeå±‚       â”‚   Silverå±‚       â”‚        Goldå±‚                â”‚
â”‚   (åŸå§‹æ•°æ®)      â”‚   (æ¸…æ´—æ•°æ®)      â”‚      (ä¸šåŠ¡æ•°æ®)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ raw_match_data â”‚ â€¢ matches       â”‚ â€¢ predictions               â”‚
â”‚ â€¢ raw_odds_data  â”‚ â€¢ odds          â”‚ â€¢ features                  â”‚
â”‚ â€¢ raw_scores_dataâ”‚ â€¢ teams         â”‚ â€¢ analytics                 â”‚
â”‚ â€¢ é‡‡é›†æ—¥å¿—       â”‚ â€¢ leagues       â”‚ â€¢ reports                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | æŠ€æœ¯æ ˆ | çŠ¶æ€ |
|------|------|--------|------|
| **BaseCollector** | é‡‡é›†å™¨åŸºç±»ï¼Œæä¾›é€šç”¨åŠŸèƒ½ | Python asyncio | âœ… è¿è¡Œä¸­ |
| **OddsCollector** | èµ”ç‡æ•°æ®é‡‡é›†å™¨ | aiohttp, é‡è¯•æœºåˆ¶ | âœ… è¿è¡Œä¸­ |
| **FixturesCollector** | èµ›ç¨‹æ•°æ®é‡‡é›†å™¨ | REST APIè°ƒç”¨ | âœ… è¿è¡Œä¸­ |
| **ScoresCollector** | æ¯”åˆ†æ•°æ®é‡‡é›†å™¨ | WebSocket/è½®è¯¢ | âœ… è¿è¡Œä¸­ |
| **DataQualityMonitor** | æ•°æ®è´¨é‡ç›‘æ§ | è‡ªå®šä¹‰éªŒè¯è§„åˆ™ | âœ… è¿è¡Œä¸­ |

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- **Python**: 3.11+
- **æ•°æ®åº“**: PostgreSQL 14+
- **ç¼“å­˜**: Redis 6+
- **ç½‘ç»œ**: ç¨³å®šçš„äº’è”ç½‘è¿æ¥

### 5åˆ†é’Ÿå¿«é€Ÿé…ç½®

```bash
# 1. é…ç½®æ•°æ®æºç¯å¢ƒå˜é‡
cp .env.example .env
nano .env

# 2. è®¾ç½®å¿…è¦çš„APIå¯†é’¥
export FOOTBALL_API_KEY="your-football-api-key"
export ODDS_API_KEY="your-odds-api-key"

# 3. å¯åŠ¨æ•°æ®é‡‡é›†æœåŠ¡
make data-collect-start

# 4. éªŒè¯é‡‡é›†çŠ¶æ€
make data-collect-status

# 5. æŸ¥çœ‹é‡‡é›†æ—¥å¿—
make data-collect-logs
```

## âš™ï¸ æ•°æ®æºé…ç½®

### 1. è¶³çƒæ•°æ®APIé…ç½®

#### ç¯å¢ƒå˜é‡è®¾ç½®

```bash
# .env
# è¶³çƒæ•°æ®APIé…ç½®
FOOTBALL_API_BASE_URL="https://api.football-data.org/v4"
FOOTBALL_API_KEY="your-football-api-key-here"
FOOTBALL_API_RATE_LIMIT="10"  # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
FOOTBALL_API_TIMEOUT="30"     # è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

# æ•°æ®é‡‡é›†é¢‘ç‡é…ç½®
FIXTURES_COLLECTION_INTERVAL="3600"  # èµ›ç¨‹é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
LIVE_SCORES_COLLECTION_INTERVAL="60" # å®æ—¶æ¯”åˆ†é‡‡é›†é—´éš”ï¼ˆç§’ï¼‰
```

#### APIå¯†é’¥è·å–

1. **football-data.org**
   - è®¿é—® [https://www.football-data.org/login](https://www.football-data.org/login)
   - æ³¨å†Œè´¦æˆ·å¹¶è·å–å…è´¹APIå¯†é’¥
   - å…è´¹ç‰ˆæœ¬ï¼šæ¯åˆ†é’Ÿ10æ¬¡è¯·æ±‚

2. **TheOdds API** (å¯é€‰)
   - è®¿é—® [https://the-odds-api.com/](https://the-odds-api.com/)
   - æ³¨å†Œè·å–APIå¯†é’¥
   - æ”¯æŒèµ”ç‡æ•°æ®é‡‡é›†

### 2. æ•°æ®åº“é…ç½®

#### Bronzeå±‚è¡¨ç»“æ„

```sql
-- åŸå§‹æ¯”èµ›æ•°æ®è¡¨
CREATE TABLE raw_match_data (
    id SERIAL PRIMARY KEY,
    data_source VARCHAR(100) NOT NULL,
    external_match_id VARCHAR(100) NOT NULL,
    external_league_id VARCHAR(100),
    raw_data JSONB NOT NULL,
    match_time TIMESTAMP,
    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed BOOLEAN DEFAULT FALSE,
    UNIQUE(data_source, external_match_id)
);

-- åŸå§‹èµ”ç‡æ•°æ®è¡¨
CREATE TABLE raw_odds_data (
    id SERIAL PRIMARY KEY,
    data_source VARCHAR(100) NOT NULL,
    external_match_id VARCHAR(100) NOT NULL,
    bookmaker VARCHAR(50),
    market_type VARCHAR(50),
    raw_data JSONB NOT NULL,
    odds_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed BOOLEAN DEFAULT FALSE,
    UNIQUE(data_source, external_match_id, bookmaker, market_type)
);

-- åŸå§‹æ¯”åˆ†æ•°æ®è¡¨
CREATE TABLE raw_scores_data (
    id SERIAL PRIMARY KEY,
    data_source VARCHAR(100) NOT NULL,
    external_match_id VARCHAR(100) NOT NULL,
    match_status VARCHAR(50),
    home_score INTEGER,
    away_score INTEGER,
    match_minute INTEGER,
    raw_data JSONB NOT NULL,
    collected_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed BOOLEAN DEFAULT FALSE,
    UNIQUE(data_source, external_match_id)
);

-- æ•°æ®é‡‡é›†æ—¥å¿—è¡¨
CREATE TABLE data_collection_logs (
    id SERIAL PRIMARY KEY,
    data_source VARCHAR(100) NOT NULL,
    collection_type VARCHAR(50) NOT NULL,
    status VARCHAR(20) DEFAULT 'pending',
    start_time TIMESTAMP,
    end_time TIMESTAMP,
    records_processed INTEGER DEFAULT 0,
    records_failed INTEGER DEFAULT 0,
    error_message TEXT,
    metadata_json JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### 3. Redisç¼“å­˜é…ç½®

```bash
# .env
# Redisé…ç½®
REDIS_URL="redis://localhost:6379/0"
REDIS_PASSWORD="your-redis-password"
REDIS_DB="0"

# ç¼“å­˜ç­–ç•¥é…ç½®
CACHE_TTL_FIXTURES="3600"     # èµ›ç¨‹ç¼“å­˜1å°æ—¶
CACHE_TTL_ODDS="300"          # èµ”ç‡ç¼“å­˜5åˆ†é’Ÿ
CACHE_TTL_SCORES="60"         # æ¯”åˆ†ç¼“å­˜1åˆ†é’Ÿ
```

## ğŸ”§ é‡‡é›†å™¨é…ç½®

### 1. BaseCollector åŸºç¡€é…ç½®

```python
# src/data/collectors/base_collector.py é…ç½®ç¤ºä¾‹

from src.data.collectors.base_collector import DataCollector

class CustomDataCollector(DataCollector):
    def __init__(
        self,
        data_source: str = "custom_api",
        max_retries: int = 3,
        retry_delay: int = 5,
        timeout: int = 30,
    ):
        super().__init__(
            data_source=data_source,
            max_retries=max_retries,
            retry_delay=retry_delay,
            timeout=timeout,
        )

        # è‡ªå®šä¹‰é…ç½®
        self.rate_limit = 10  # æ¯åˆ†é’Ÿè¯·æ±‚é™åˆ¶
        self.batch_size = 100  # æ‰¹å¤„ç†å¤§å°

    async def collect_fixtures(self, **kwargs) -> CollectionResult:
        """å®ç°èµ›ç¨‹æ•°æ®é‡‡é›†é€»è¾‘"""
        pass

    async def collect_odds(self, **kwargs) -> CollectionResult:
        """å®ç°èµ”ç‡æ•°æ®é‡‡é›†é€»è¾‘"""
        pass

    async def collect_live_scores(self, **kwargs) -> CollectionResult:
        """å®ç°å®æ—¶æ¯”åˆ†é‡‡é›†é€»è¾‘"""
        pass
```

### 2. OddsCollector èµ”ç‡é‡‡é›†å™¨é…ç½®

```python
# src/data/collectors/odds_collector.py ä½¿ç”¨ç¤ºä¾‹

from src.data.collectors.odds_collector import OddsCollector

# åˆå§‹åŒ–èµ”ç‡é‡‡é›†å™¨
odds_collector = OddsCollector(
    data_source="the_odds_api",
    api_key="your-odds-api-key",
    base_url="https://api.the-odds-api.com/v4",
    time_window_minutes=5,  # å»é‡æ—¶é—´çª—å£
    max_retries=3,
    retry_delay=5,
    timeout=30
)

# é‡‡é›†ç‰¹å®šæ¯”èµ›çš„èµ”ç‡
result = await odds_collector.collect_odds(
    match_ids=["match_123", "match_456"],
    bookmakers=["bet365", "pinnacle", "williamhill"],
    markets=["h2h", "spreads", "totals"]
)

print(f"é‡‡é›†ç»“æœ: {result.status}")
print(f"æˆåŠŸè®°å½•: {result.success_count}")
print(f"å¤±è´¥è®°å½•: {result.error_count}")
```

### 3. æ•°æ®é‡‡é›†è°ƒåº¦å™¨é…ç½®

```python
# src/scheduler/data_collection_scheduler.py

import asyncio
from datetime import datetime
from src.data.collectors.odds_collector import OddsCollector
from src.data.collectors.fixtures_collector import FixturesCollector
from src.data.collectors.scores_collector import ScoresCollector

class DataCollectionScheduler:
    """æ•°æ®é‡‡é›†è°ƒåº¦å™¨"""

    def __init__(self):
        self.collectors = {
            'odds': OddsCollector(),
            'fixtures': FixturesCollector(),
            'scores': ScoresCollector(),
        }
        self.running = False

    async def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨"""
        self.running = True
        tasks = [
            asyncio.create_task(self._schedule_odds_collection()),
            asyncio.create_task(self._schedule_fixtures_collection()),
            asyncio.create_task(self._schedule_scores_collection()),
        ]

        try:
            await asyncio.gather(*tasks)
        except Exception as e:
            self.logger.error(f"è°ƒåº¦å™¨è¿è¡Œå‡ºé”™: {e}")
        finally:
            self.running = False

    async def _schedule_odds_collection(self):
        """å®šæ—¶é‡‡é›†èµ”ç‡æ•°æ®"""
        while self.running:
            try:
                result = await self.collectors['odds'].collect_odds()
                self.logger.info(f"èµ”ç‡é‡‡é›†å®Œæˆ: {result.status}")

                # ç­‰å¾…5åˆ†é’Ÿåå†æ¬¡é‡‡é›†
                await asyncio.sleep(300)

            except Exception as e:
                self.logger.error(f"èµ”ç‡é‡‡é›†å¤±è´¥: {e}")
                await asyncio.sleep(60)  # å‡ºé”™æ—¶ç­‰å¾…1åˆ†é’Ÿé‡è¯•

    async def _schedule_fixtures_collection(self):
        """å®šæ—¶é‡‡é›†èµ›ç¨‹æ•°æ®"""
        while self.running:
            try:
                result = await self.collectors['fixtures'].collect_fixtures()
                self.logger.info(f"èµ›ç¨‹é‡‡é›†å®Œæˆ: {result.status}")

                # ç­‰å¾…1å°æ—¶åå†æ¬¡é‡‡é›†
                await asyncio.sleep(3600)

            except Exception as e:
                self.logger.error(f"èµ›ç¨‹é‡‡é›†å¤±è´¥: {e}")
                await asyncio.sleep(300)  # å‡ºé”™æ—¶ç­‰å¾…5åˆ†é’Ÿé‡è¯•

    async def _schedule_scores_collection(self):
        """å®šæ—¶é‡‡é›†æ¯”åˆ†æ•°æ®"""
        while self.running:
            try:
                result = await self.collectors['scores'].collect_live_scores()
                self.logger.info(f"æ¯”åˆ†é‡‡é›†å®Œæˆ: {result.status}")

                # ç­‰å¾…1åˆ†é’Ÿåå†æ¬¡é‡‡é›†
                await asyncio.sleep(60)

            except Exception as e:
                self.logger.error(f"æ¯”åˆ†é‡‡é›†å¤±è´¥: {e}")
                await asyncio.sleep(30)  # å‡ºé”™æ—¶ç­‰å¾…30ç§’é‡è¯•

# å¯åŠ¨è°ƒåº¦å™¨
scheduler = DataCollectionScheduler()
await scheduler.start()
```

## ğŸ“Š æ•°æ®è´¨é‡ç®¡ç†

### 1. æ•°æ®éªŒè¯è§„åˆ™

```python
# src/data/quality/data_validator.py

from typing import Dict, Any, List, Optional
from datetime import datetime
import re

class DataValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""

    def __init__(self):
        self.validation_rules = {
            'match_data': self._validate_match_data,
            'odds_data': self._validate_odds_data,
            'scores_data': self._validate_scores_data,
        }

    async def validate_data(
        self,
        data_type: str,
        data: Dict[str, Any]
    ) -> tuple[bool, List[str]]:
        """
        éªŒè¯æ•°æ®è´¨é‡

        Returns:
            tuple[bool, List[str]]: (æ˜¯å¦æœ‰æ•ˆ, é”™è¯¯ä¿¡æ¯åˆ—è¡¨)
        """
        if data_type not in self.validation_rules:
            return False, [f"æœªçŸ¥çš„æ•°æ®ç±»å‹: {data_type}"]

        return await self.validation_rules[data_type](data)

    async def _validate_match_data(self, data: Dict[str, Any]) -> tuple[bool, List[str]]:
        """éªŒè¯æ¯”èµ›æ•°æ®"""
        errors = []

        # å¿…å¡«å­—æ®µæ£€æŸ¥
        required_fields = ['match_id', 'home_team', 'away_team', 'match_time']
        for field in required_fields:
            if field not in data or not data[field]:
                errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

        # æ—¶é—´æ ¼å¼éªŒè¯
        if 'match_time' in data:
            try:
                if isinstance(data['match_time'], str):
                    datetime.fromisoformat(data['match_time'].replace('Z', '+00:00'))
            except ValueError:
                errors.append("æ¯”èµ›æ—¶é—´æ ¼å¼æ— æ•ˆ")

        # çƒé˜Ÿåç§°æ ¼å¼éªŒè¯
        for team_field in ['home_team', 'away_team']:
            if team_field in data and data[team_field]:
                team_name = data[team_field]
                if not isinstance(team_name, str) or len(team_name) < 2:
                    errors.append(f"çƒé˜Ÿåç§°æ ¼å¼æ— æ•ˆ: {team_field}")

        return len(errors) == 0, errors

    async def _validate_odds_data(self, data: Dict[str, Any]) -> tuple[bool, List[str]]:
        """éªŒè¯èµ”ç‡æ•°æ®"""
        errors = []

        # å¿…å¡«å­—æ®µæ£€æŸ¥
        required_fields = ['match_id', 'bookmaker', 'market_type', 'outcomes']
        for field in required_fields:
            if field not in data or not data[field]:
                errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

        # èµ”ç‡å€¼éªŒè¯
        if 'outcomes' in data and isinstance(data['outcomes'], list):
            for outcome in data['outcomes']:
                if 'price' in outcome:
                    try:
                        price = float(outcome['price'])
                        if price <= 1.0:
                            errors.append(f"èµ”ç‡å€¼å¿…é¡»å¤§äº1: {price}")
                    except (ValueError, TypeError):
                        errors.append(f"èµ”ç‡å€¼æ ¼å¼æ— æ•ˆ: {outcome.get('price')}")

        return len(errors) == 0, errors

    async def _validate_scores_data(self, data: Dict[str, Any]) -> tuple[bool, List[str]]:
        """éªŒè¯æ¯”åˆ†æ•°æ®"""
        errors = []

        # å¿…å¡«å­—æ®µæ£€æŸ¥
        required_fields = ['match_id', 'match_status']
        for field in required_fields:
            if field not in data or not data[field]:
                errors.append(f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

        # æ¯”åˆ†éªŒè¯
        for score_field in ['home_score', 'away_score']:
            if score_field in data and data[score_field] is not None:
                try:
                    score = int(data[score_field])
                    if score < 0:
                        errors.append(f"æ¯”åˆ†ä¸èƒ½ä¸ºè´Ÿæ•°: {score_field}")
                except (ValueError, TypeError):
                    errors.append(f"æ¯”åˆ†æ ¼å¼æ— æ•ˆ: {score_field}")

        # æ¯”èµ›åˆ†é’ŸéªŒè¯
        if 'match_minute' in data and data['match_minute'] is not None:
            try:
                minute = int(data['match_minute'])
                if minute < 0 or minute > 120:
                    errors.append(f"æ¯”èµ›åˆ†é’Ÿæ•°æ— æ•ˆ: {minute}")
            except (ValueError, TypeError):
                errors.append(f"æ¯”èµ›åˆ†é’Ÿæ•°æ ¼å¼æ— æ•ˆ: {data['match_minute']}")

        return len(errors) == 0, errors
```

### 2. å¼‚å¸¸æ£€æµ‹

```python
# src/data/quality/anomaly_detector.py

import numpy as np
from typing import Dict, Any, List
from datetime import datetime, timedelta

class AnomalyDetector:
    """æ•°æ®å¼‚å¸¸æ£€æµ‹å™¨"""

    def __init__(self):
        self.odds_history = {}  # èµ”ç‡å†å²æ•°æ®
        self.score_history = {}  # æ¯”åˆ†å†å²æ•°æ®

    async def detect_odds_anomaly(self, odds_data: Dict[str, Any]) -> List[str]:
        """æ£€æµ‹èµ”ç‡æ•°æ®å¼‚å¸¸"""
        anomalies = []

        match_id = odds_data.get('match_id')
        bookmaker = odds_data.get('bookmaker')
        market_type = odds_data.get('market_type')

        # æ„å»ºå†å²é”®
        history_key = f"{match_id}:{bookmaker}:{market_type}"

        # æ£€æŸ¥èµ”ç‡å€¼å¼‚å¸¸
        if 'outcomes' in odds_data:
            for outcome in odds_data['outcomes']:
                price = outcome.get('price')
                if price:
                    # æ£€æŸ¥æç«¯èµ”ç‡
                    if price > 1000:
                        anomalies.append(f"æç«¯èµ”ç‡å€¼: {price}")

                    # æ£€æŸ¥å†å²å¯¹æ¯”å¼‚å¸¸
                    if history_key in self.odds_history:
                        last_price = self.odds_history[history_key].get(outcome.get('name'))
                        if last_price:
                            # æ£€æŸ¥èµ”ç‡å˜åŒ–å¹…åº¦
                            change_ratio = abs(price - last_price) / last_price
                            if change_ratio > 0.5:  # å˜åŒ–è¶…è¿‡50%
                                anomalies.append(f"èµ”ç‡å˜åŒ–å¼‚å¸¸: {last_price} -> {price}")

        # æ›´æ–°å†å²è®°å½•
        if history_key not in self.odds_history:
            self.odds_history[history_key] = {}

        for outcome in odds_data.get('outcomes', []):
            self.odds_history[history_key][outcome.get('name')] = outcome.get('price')

        return anomalies

    async def detect_score_anomaly(self, score_data: Dict[str, Any]) -> List[str]:
        """æ£€æµ‹æ¯”åˆ†æ•°æ®å¼‚å¸¸"""
        anomalies = []

        match_id = score_data.get('match_id')
        home_score = score_data.get('home_score', 0)
        away_score = score_data.get('away_score', 0)
        match_minute = score_data.get('match_minute', 0)

        # æ£€æŸ¥æ¯”åˆ†é€»è¾‘å¼‚å¸¸
        if match_minute > 90 and (home_score == 0 and away_score == 0):
            anomalies.append("90åˆ†é’Ÿåä»ä¸º0-0ï¼Œå¯èƒ½æ˜¯æ•°æ®é”™è¯¯")

        # æ£€æŸ¥æ¯”åˆ†è·³è·ƒå¼‚å¸¸
        if match_id in self.score_history:
            last_score = self.score_history[match_id]
            last_home = last_score.get('home_score', 0)
            last_away = last_score.get('away_score', 0)
            last_minute = last_score.get('match_minute', 0)

            # æ£€æŸ¥æ—¶é—´å€’é€€
            if match_minute < last_minute:
                anomalies.append(f"æ¯”èµ›æ—¶é—´å€’é€€: {last_minute} -> {match_minute}")

            # æ£€æŸ¥æ¯”åˆ†è·³è·ƒ
            goal_diff = (home_score + away_score) - (last_home + last_away)
            if goal_diff > 3:  # å•æ¬¡æ›´æ–°è¿›çƒè¶…è¿‡3ä¸ª
                anomalies.append(f"æ¯”åˆ†è·³è·ƒå¼‚å¸¸: {last_home}-{last_away} -> {home_score}-{away_score}")

        # æ›´æ–°å†å²è®°å½•
        self.score_history[match_id] = {
            'home_score': home_score,
            'away_score': away_score,
            'match_minute': match_minute,
            'updated_at': datetime.now()
        }

        return anomalies
```

## ğŸ”„ æ•°æ®å¤„ç†æµç¨‹

### 1. æ•°æ®é‡‡é›†æµç¨‹

```python
# src/data/pipeline/collection_pipeline.py

import asyncio
from typing import Dict, Any, List
from src.data.collectors.base_collector import CollectionResult
from src.data.quality.data_validator import DataValidator
from src.data.quality.anomaly_detector import AnomalyDetector

class DataCollectionPipeline:
    """æ•°æ®é‡‡é›†ç®¡é“"""

    def __init__(self):
        self.validator = DataValidator()
        self.anomaly_detector = AnomalyDetector()

    async def process_collection_result(
        self,
        data_type: str,
        result: CollectionResult
    ) -> Dict[str, Any]:
        """å¤„ç†é‡‡é›†ç»“æœ"""

        processed_data = []
        validation_errors = []
        anomaly_alerts = []

        if result.collected_data:
            for data_item in result.collected_data:
                try:
                    # 1. æ•°æ®éªŒè¯
                    is_valid, errors = await self.validator.validate_data(data_type, data_item)
                    if not is_valid:
                        validation_errors.extend(errors)
                        continue

                    # 2. å¼‚å¸¸æ£€æµ‹
                    anomalies = await self._detect_anomalies(data_type, data_item)
                    if anomalies:
                        anomaly_alerts.extend(anomalies)

                    # 3. æ•°æ®æ¸…æ´—å’Œæ ‡å‡†åŒ–
                    cleaned_data = await self._clean_data(data_type, data_item)
                    if cleaned_data:
                        processed_data.append(cleaned_data)

                except Exception as e:
                    self.logger.error(f"å¤„ç†æ•°æ®é¡¹å¤±è´¥: {e}")
                    continue

        # 4. ä¿å­˜å¤„ç†åçš„æ•°æ®
        if processed_data:
            await self._save_processed_data(data_type, processed_data)

        # 5. ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        report = {
            'data_type': data_type,
            'original_count': len(result.collected_data) if result.collected_data else 0,
            'processed_count': len(processed_data),
            'validation_errors': validation_errors,
            'anomaly_alerts': anomaly_alerts,
            'processing_time': datetime.now().isoformat(),
        }

        return report

    async def _detect_anomalies(self, data_type: str, data: Dict[str, Any]) -> List[str]:
        """æ£€æµ‹æ•°æ®å¼‚å¸¸"""
        if data_type == 'odds':
            return await self.anomaly_detector.detect_odds_anomaly(data)
        elif data_type == 'scores':
            return await self.anomaly_detector.detect_score_anomaly(data)
        return []

    async def _clean_data(self, data_type: str, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æ¸…æ´—å’Œæ ‡å‡†åŒ–æ•°æ®"""
        # å®ç°æ•°æ®æ¸…æ´—é€»è¾‘
        cleaned = data.copy()

        # æ ‡å‡†åŒ–å­—æ®µå
        field_mapping = {
            'matchId': 'match_id',
            'homeTeam': 'home_team',
            'awayTeam': 'away_team',
            'matchTime': 'match_time',
        }

        for old_field, new_field in field_mapping.items():
            if old_field in cleaned:
                cleaned[new_field] = cleaned.pop(old_field)

        # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
        cleaned['processed_at'] = datetime.now().isoformat()

        return cleaned

    async def _save_processed_data(self, data_type: str, data: List[Dict[str, Any]]) -> None:
        """ä¿å­˜å¤„ç†åçš„æ•°æ®åˆ°Silverå±‚"""
        # å®ç°æ•°æ®ä¿å­˜é€»è¾‘
        pass
```

### 2. æ•°æ®è½¬æ¢è§„åˆ™

```python
# src/data/processing/transformation_rules.py

from typing import Dict, Any, List
from datetime import datetime
import re

class DataTransformer:
    """æ•°æ®è½¬æ¢å™¨"""

    def __init__(self):
        self.transformation_rules = {
            'match_data': self._transform_match_data,
            'odds_data': self._transform_odds_data,
            'scores_data': self._transform_scores_data,
        }

    async def transform_data(self, data_type: str, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢åŸå§‹æ•°æ®ä¸ºæ ‡å‡†åŒ–æ ¼å¼"""
        if data_type not in self.transformation_rules:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {data_type}")

        return await self.transformation_rules[data_type](raw_data)

    async def _transform_match_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢æ¯”èµ›æ•°æ®"""
        transformed = {
            'external_match_id': raw_data.get('id'),
            'external_league_id': raw_data.get('competition', {}).get('id'),
            'home_team_name': raw_data.get('homeTeam', {}).get('name'),
            'away_team_name': raw_data.get('awayTeam', {}).get('name'),
            'home_team_id': raw_data.get('homeTeam', {}).get('id'),
            'away_team_id': raw_data.get('awayTeam', {}).get('id'),
            'match_date': self._parse_datetime(raw_data.get('utcDate')),
            'match_status': raw_data.get('status'),
            'venue': raw_data.get('venue', {}).get('name'),
            'referee': raw_data.get('referees', [{}])[0].get('name') if raw_data.get('referees') else None,
            'competition_name': raw_data.get('competition', {}).get('name'),
            'season': raw_data.get('season', {}).get('startDate', '')[:4] if raw_data.get('season') else None,
            'raw_data': raw_data,
            'transformed_at': datetime.now().isoformat(),
        }

        return transformed

    async def _transform_odds_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢èµ”ç‡æ•°æ®"""
        outcomes = raw_data.get('outcomes', [])

        # è§£æä¸åŒç±»å‹çš„èµ”ç‡
        home_win_odds = None
        draw_odds = None
        away_win_odds = None
        over_odds = None
        under_odds = None

        for outcome in outcomes:
            name = outcome.get('name', '').lower()
            price = outcome.get('price')

            if 'home' in name or '1' in name:
                home_win_odds = price
            elif 'away' in name or '2' in name:
                away_win_odds = price
            elif 'draw' in name or 'x' in name:
                draw_odds = price
            elif 'over' in name:
                over_odds = price
            elif 'under' in name:
                under_odds = price

        transformed = {
            'external_match_id': raw_data.get('match_id'),
            'bookmaker': raw_data.get('bookmaker'),
            'market_type': raw_data.get('market_type'),
            'home_win_odds': home_win_odds,
            'draw_odds': draw_odds,
            'away_win_odds': away_win_odds,
            'over_odds': over_odds,
            'under_odds': under_odds,
            'odds_time': self._parse_datetime(raw_data.get('last_update')),
            'raw_outcomes': outcomes,
            'raw_data': raw_data,
            'transformed_at': datetime.now().isoformat(),
        }

        return transformed

    async def _transform_scores_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """è½¬æ¢æ¯”åˆ†æ•°æ®"""
        transformed = {
            'external_match_id': raw_data.get('match_id'),
            'match_status': raw_data.get('match_status'),
            'home_score': raw_data.get('home_score'),
            'away_score': raw_data.get('away_score'),
            'match_minute': raw_data.get('match_minute'),
            'score_time': self._parse_datetime(raw_data.get('score_time')),
            'extra_time_home': raw_data.get('extra_time_home'),
            'extra_time_away': raw_data.get('extra_time_away'),
            'penalty_shootout_home': raw_data.get('penalty_shootout_home'),
            'penalty_shootout_away': raw_data.get('penalty_shootout_away'),
            'raw_data': raw_data,
            'transformed_at': datetime.now().isoformat(),
        }

        return transformed

    def _parse_datetime(self, datetime_str: str) -> Optional[datetime]:
        """è§£ææ—¥æœŸæ—¶é—´å­—ç¬¦ä¸²"""
        if not datetime_str:
            return None

        try:
            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
            formats = [
                '%Y-%m-%dT%H:%M:%S',
                '%Y-%m-%dT%H:%M:%SZ',
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%dT%H:%M:%S.%f',
                '%Y-%m-%dT%H:%M:%S.%fZ',
            ]

            for fmt in formats:
                try:
                    return datetime.strptime(datetime_str, fmt)
                except ValueError:
                    continue

        except Exception:
            pass

        return None
```

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### 1. é‡‡é›†ç›‘æ§æŒ‡æ ‡

```python
# src/monitoring/collection_metrics.py

from prometheus_client import Counter, Histogram, Gauge
import time

class CollectionMetrics:
    """æ•°æ®é‡‡é›†ç›‘æ§æŒ‡æ ‡"""

    def __init__(self):
        # è®¡æ•°å™¨
        self.collection_requests_total = Counter(
            'data_collection_requests_total',
            'Total number of data collection requests',
            ['data_source', 'data_type', 'status']
        )

        self.records_collected_total = Counter(
            'data_records_collected_total',
            'Total number of records collected',
            ['data_source', 'data_type']
        )

        self.validation_errors_total = Counter(
            'data_validation_errors_total',
            'Total number of validation errors',
            ['data_type', 'error_type']
        )

        # ç›´æ–¹å›¾
        self.collection_duration = Histogram(
            'data_collection_duration_seconds',
            'Time spent collecting data',
            ['data_source', 'data_type'],
            buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
        )

        # ä»ªè¡¨
        self.active_collectors = Gauge(
            'active_data_collectors',
            'Number of active data collectors'
        )

        self.collection_queue_size = Gauge(
            'data_collection_queue_size',
            'Size of data collection queue'
        )

        self.last_collection_success_time = Gauge(
            'last_collection_success_time',
            'Unix timestamp of last successful collection',
            ['data_source', 'data_type']
        )

    def record_collection_start(self, data_source: str, data_type: str):
        """è®°å½•é‡‡é›†å¼€å§‹"""
        self.active_collectors.inc()

    def record_collection_complete(
        self,
        data_source: str,
        data_type: str,
        status: str,
        records_count: int,
        duration: float
    ):
        """è®°å½•é‡‡é›†å®Œæˆ"""
        self.active_collectors.dec()
        self.collection_requests_total.labels(
            data_source=data_source,
            data_type=data_type,
            status=status
        ).inc()

        if records_count > 0:
            self.records_collected_total.labels(
                data_source=data_source,
                data_type=data_type
            ).inc(records_count)

        self.collection_duration.labels(
            data_source=data_source,
            data_type=data_type
        ).observe(duration)

        if status == 'success':
            self.last_collection_success_time.labels(
                data_source=data_source,
                data_type=data_type
            ).set(time.time())

    def record_validation_error(self, data_type: str, error_type: str):
        """è®°å½•éªŒè¯é”™è¯¯"""
        self.validation_errors_total.labels(
            data_type=data_type,
            error_type=error_type
        ).inc()
```

### 2. å‘Šè­¦è§„åˆ™é…ç½®

```yaml
# config/monitoring/data_collection_alerts.yml

groups:
  - name: data_collection_alerts
    rules:
      - alert: DataCollectionFailure
        expr: increase(data_collection_requests_total{status="failed"}[5m]) > 0
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®é‡‡é›†å¤±è´¥"
          description: "æ•°æ®æº {{ $labels.data_source }} çš„ {{ $labels.data_type }} æ•°æ®é‡‡é›†åœ¨æœ€è¿‘5åˆ†é’Ÿå†…å¤±è´¥"

      - alert: HighCollectionLatency
        expr: histogram_quantile(0.95, rate(data_collection_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®é‡‡é›†å»¶è¿Ÿè¿‡é«˜"
          description: "95%çš„æ•°æ®é‡‡é›†è¯·æ±‚è€—æ—¶è¶…è¿‡30ç§’"

      - alert: NoRecentCollection
        expr: time() - last_collection_success_time > 3600
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "æ•°æ®é‡‡é›†ä¸­æ–­"
          description: "æ•°æ®æº {{ $labels.data_source }} çš„ {{ $labels.data_type }} æ•°æ®é‡‡é›†å·²è¶…è¿‡1å°æ—¶æœªæˆåŠŸ"

      - alert: HighValidationErrorRate
        expr: rate(data_validation_errors_total[5m]) / rate(data_records_collected_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®éªŒè¯é”™è¯¯ç‡è¿‡é«˜"
          description: "æ•°æ®ç±»å‹ {{ $labels.data_type }} çš„éªŒè¯é”™è¯¯ç‡è¶…è¿‡10%"

      - name: data_quality_alerts
    rules:
      - alert: DataAnomalyDetected
        expr: increase(data_anomaly_alerts_total[5m]) > 0
        for: 1m
        labels:
          severity: info
        annotations:
          summary: "æ£€æµ‹åˆ°æ•°æ®å¼‚å¸¸"
          description: "åœ¨ {{ $labels.data_type }} æ•°æ®ä¸­æ£€æµ‹åˆ°å¼‚å¸¸: {{ $labels.anomaly_type }}"
```

## ğŸ› ï¸ è¿ç»´æ“ä½œ

### 1. æ—¥å¸¸è¿ç»´å‘½ä»¤

```bash
# æ•°æ®é‡‡é›†æœåŠ¡ç®¡ç†
make data-collect-start      # å¯åŠ¨æ•°æ®é‡‡é›†æœåŠ¡
make data-collect-stop       # åœæ­¢æ•°æ®é‡‡é›†æœåŠ¡
make data-collect-restart    # é‡å¯æ•°æ®é‡‡é›†æœåŠ¡
make data-collect-status     # æŸ¥çœ‹é‡‡é›†æœåŠ¡çŠ¶æ€
make data-collect-logs       # æŸ¥çœ‹é‡‡é›†æ—¥å¿—

# æ•°æ®è´¨é‡ç®¡ç†
make data-quality-check      # è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥
make data-validation-report  # ç”Ÿæˆæ•°æ®éªŒè¯æŠ¥å‘Š
make data-anomaly-scan       # æ‰«ææ•°æ®å¼‚å¸¸
make data-cleanup           # æ¸…ç†è¿‡æœŸæ•°æ®

# ç›‘æ§å’Œå‘Šè­¦
make data-metrics           # æŸ¥çœ‹é‡‡é›†æŒ‡æ ‡
make data-alerts           # æŸ¥çœ‹å‘Šè­¦çŠ¶æ€
make data-dashboard        # æ‰“å¼€ç›‘æ§é¢æ¿
```

### 2. æ•…éšœæ’æŸ¥æŒ‡å—

#### é‡‡é›†å¤±è´¥æ’æŸ¥

```bash
# 1. æ£€æŸ¥ç½‘ç»œè¿æ¥
ping api.football-data.org
curl -I https://api.football-data.org/v4/

# 2. éªŒè¯APIå¯†é’¥
curl -H "X-Auth-Token: YOUR_API_KEY" \
     https://api.football-data.org/v4/competitions

# 3. æ£€æŸ¥æ•°æ®åº“è¿æ¥
docker exec -it football-prediction_db_1 psql -U football_user -d football_prediction_dev
SELECT COUNT(*) FROM data_collection_logs WHERE created_at > NOW() - INTERVAL '1 hour';

# 4. æŸ¥çœ‹è¯¦ç»†é”™è¯¯æ—¥å¿—
docker logs football-prediction_app_1 | grep "ERROR" | tail -20
```

#### æ•°æ®è´¨é‡é—®é¢˜æ’æŸ¥

```sql
-- æŸ¥çœ‹åŸå§‹æ•°æ®ä¸­çš„é—®é¢˜è®°å½•
SELECT
    data_source,
    collection_type,
    COUNT(*) as error_count,
    error_message
FROM data_collection_logs
WHERE status = 'failed'
AND created_at > NOW() - INTERVAL '24 hours'
GROUP BY data_source, collection_type, error_message
ORDER BY error_count DESC;

-- æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
SELECT
    'raw_match_data' as table_name,
    COUNT(*) as total_records,
    COUNT(CASE WHEN processed THEN 1 END) as processed_records,
    COUNT(CASE WHEN NOT processed THEN 1 END) as unprocessed_records
FROM raw_match_data
UNION ALL
SELECT
    'raw_odds_data' as table_name,
    COUNT(*) as total_records,
    COUNT(CASE WHEN processed THEN 1 END) as processed_records,
    COUNT(CASE WHEN NOT processed THEN 1 END) as unprocessed_records
FROM raw_odds_data;
```

### 3. æ€§èƒ½ä¼˜åŒ–

#### æ•°æ®åº“ä¼˜åŒ–

```sql
-- åˆ›å»ºç´¢å¼•ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½
CREATE INDEX IF NOT EXISTS idx_raw_match_data_source_time
ON raw_match_data(data_source, collected_at);

CREATE INDEX IF NOT EXISTS idx_raw_odds_match_bookmaker
ON raw_odds_data(external_match_id, bookmaker);

CREATE INDEX IF NOT EXISTS idx_collection_logs_type_status
ON data_collection_logs(collection_type, status);

-- åˆ†åŒºè¡¨ä¼˜åŒ–ï¼ˆå¤§æ•°æ®é‡æ—¶ï¼‰
CREATE TABLE raw_match_data_partitioned (
    LIKE raw_match_data INCLUDING ALL
) PARTITION BY RANGE (collected_at);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE raw_match_data_2024_01
PARTITION OF raw_match_data_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');
```

#### é‡‡é›†å™¨ä¼˜åŒ–

```python
# å¹¶å‘é‡‡é›†ä¼˜åŒ–
import asyncio
from asyncio import Semaphore

class OptimizedDataCollector(DataCollector):
    def __init__(self, max_concurrent_requests: int = 10, **kwargs):
        super().__init__(**kwargs)
        self.semaphore = Semaphore(max_concurrent_requests)

    async def collect_odds_batch(self, match_ids: List[str]) -> CollectionResult:
        """æ‰¹é‡å¹¶å‘é‡‡é›†èµ”ç‡æ•°æ®"""
        tasks = []

        for match_id in match_ids:
            task = self._collect_single_match_odds(match_id)
            tasks.append(task)

        # é™åˆ¶å¹¶å‘æ•°
        results = await asyncio.gather(
            *[self.semaphore.acquire() and task for task in tasks],
            return_exceptions=True
        )

        # å¤„ç†ç»“æœ
        successful_results = []
        error_count = 0

        for result in results:
            if isinstance(result, Exception):
                error_count += 1
                self.logger.error(f"é‡‡é›†ä»»åŠ¡å¤±è´¥: {result}")
            else:
                successful_results.extend(result)

        return CollectionResult(
            data_source=self.data_source,
            collection_type="odds",
            records_collected=len(successful_results),
            success_count=len(successful_results),
            error_count=error_count,
            status="success" if error_count == 0 else "partial"
        )
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. æ•°æ®é‡‡é›†æœ€ä½³å®è·µ

1. **é”™è¯¯å¤„ç†å’Œé‡è¯•**
   - å®ç°æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶
   - è®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
   - è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—

2. **æ•°æ®å»é‡**
   - ä½¿ç”¨æ—¶é—´çª—å£å»é‡é¿å…é‡å¤æ•°æ®
   - åŸºäºä¸šåŠ¡å…³é”®å­—æ®µç”Ÿæˆå”¯ä¸€é”®
   - å®šæœŸæ¸…ç†è¿‡æœŸçš„å»é‡ç¼“å­˜

3. **æ€§èƒ½ä¼˜åŒ–**
   - ä½¿ç”¨å¼‚æ­¥IOæé«˜å¹¶å‘æ€§èƒ½
   - å®ç°æ‰¹é‡æ•°æ®å¤„ç†
   - åˆç†è®¾ç½®å¹¶å‘è¯·æ±‚æ•°é™åˆ¶

4. **ç›‘æ§å’Œå‘Šè­¦**
   - ç›‘æ§é‡‡é›†æˆåŠŸç‡ã€å»¶è¿Ÿã€é”™è¯¯ç‡
   - è®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼
   - å»ºç«‹å€¼ç­å“åº”æœºåˆ¶

### 2. æ•°æ®è´¨é‡ç®¡ç†

1. **æ•°æ®éªŒè¯**
   - å®ç°å¤šå±‚æ¬¡æ•°æ®éªŒè¯è§„åˆ™
   - éªŒè¯æ•°æ®æ ¼å¼ã€å®Œæ•´æ€§ã€ä¸€è‡´æ€§
   - è®°å½•éªŒè¯ç»“æœå’Œé”™è¯¯ä¿¡æ¯

2. **å¼‚å¸¸æ£€æµ‹**
   - åŸºäºå†å²æ•°æ®æ£€æµ‹å¼‚å¸¸å€¼
   - è®¾ç½®åˆç†çš„å¼‚å¸¸é˜ˆå€¼
   - åŠæ—¶å‘Šè­¦é€šçŸ¥ç›¸å…³äººå‘˜

3. **æ•°æ®æ¸…æ´—**
   - æ ‡å‡†åŒ–æ•°æ®æ ¼å¼å’Œå­—æ®µå
   - å¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
   - ä¿æŒæ•°æ®çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§

### 3. ç³»ç»Ÿå¯é æ€§

1. **å®¹é”™è®¾è®¡**
   - å®ç°ä¼˜é›…é™çº§æœºåˆ¶
   - è®¾ç½®ç†”æ–­å™¨é˜²æ­¢é›ªå´©
   - å»ºç«‹å¤‡ç”¨æ•°æ®æº

2. **å¤‡ä»½å’Œæ¢å¤**
   - å®šæœŸå¤‡ä»½åŸå§‹æ•°æ®
   - å»ºç«‹æ•°æ®æ¢å¤æµç¨‹
   - æµ‹è¯•å¤‡ä»½çš„æœ‰æ•ˆæ€§

3. **æ–‡æ¡£å’Œè¿ç»´**
   - ç»´æŠ¤è¯¦ç»†çš„é…ç½®æ–‡æ¡£
   - å»ºç«‹æ•…éšœå¤„ç†æ‰‹å†Œ
   - å®šæœŸè¿›è¡Œç³»ç»Ÿæ¼”ç»ƒ

## ğŸ“š ç›¸å…³æ–‡æ¡£

### ğŸ”— æ ¸å¿ƒæ–‡æ¡£
- **æ•°æ®åº“æ¶æ„**: [DATABASE_SCHEMA.md](DATABASE_SCHEMA.md) - æ•°æ®åº“è¡¨ç»“æ„å’Œè®¾è®¡
- **å¼€å‘æŒ‡å—**: [DEVELOPMENT_GUIDE.md](DEVELOPMENT_GUIDE.md) - å¼€å‘ç¯å¢ƒæ­å»ºå’Œè§„èŒƒ
- **æœºå™¨å­¦ä¹ æ¨¡å‹**: [../ml/ML_MODEL_GUIDE.md](../ml/ML_MODEL_GUIDE.md) - MLæ¨¡å‹å¼€å‘å’Œè®­ç»ƒ
- **ç›‘æ§ç³»ç»Ÿ**: [MONITORING_GUIDE.md](MONITORING_GUIDE.md) - ç³»ç»Ÿç›‘æ§å’Œå‘Šè­¦
- **APIæ–‡æ¡£**: [API_REFERENCE.md](API_REFERENCE.md) - REST APIæ¥å£è¯´æ˜
- **æœ¯è¯­è¡¨**: [glossary.md](glossary.md) - é¡¹ç›®æœ¯è¯­å’Œæ¦‚å¿µå®šä¹‰

### ğŸ—ï¸ æ¶æ„ç›¸å…³
- [ç³»ç»Ÿæ¶æ„æ–‡æ¡£](../architecture/SYSTEM_ARCHITECTURE.md) - æ•´ä½“ç³»ç»Ÿæ¶æ„
- [æ•°æ®æ¶æ„è®¾è®¡](../architecture/DATA_DESIGN.md) - æ•°æ®å±‚æ¶æ„è¯¦æƒ…
- [é‡è¯•æœºåˆ¶è®¾è®¡](../architecture/RETRY_MECHANISM_DESIGN.md) - å®¹é”™å’Œé‡è¯•æœºåˆ¶

### ğŸ› ï¸ è¿ç»´å’Œéƒ¨ç½²
- [ç”Ÿäº§éƒ¨ç½²æŒ‡å—](../ops/PRODUCTION_DEPLOYMENT_GUIDE.md) - ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- [è¿ç»´æ‰‹å†Œ](../ops/runbooks/README.md) - è¿ç»´æ“ä½œæŒ‡å—
- [Stagingç¯å¢ƒé…ç½®](../how-to/STAGING_ENVIRONMENT.md) - æµ‹è¯•ç¯å¢ƒé…ç½®

### ğŸ§ª æµ‹è¯•ç›¸å…³
- [æµ‹è¯•ç­–ç•¥æ–‡æ¡£](../testing/TESTING_STRATEGY.md) - æµ‹è¯•ç­–ç•¥å’Œæ–¹æ³•
- [CI Guardianç³»ç»Ÿ](../testing/CI_GUARDIAN_GUIDE.md) - CI/CDè´¨é‡é—¨ç¦
- [æ€§èƒ½æµ‹è¯•æ–¹æ¡ˆ](../testing/performance_tests.md) - æ€§èƒ½æµ‹è¯•æŒ‡å—

### ğŸ“– å¿«é€Ÿå¼€å§‹
- [å®Œæ•´ç³»ç»Ÿæ¼”ç¤º](../how-to/COMPLETE_DEMO.md) - ç³»ç»ŸåŠŸèƒ½æ¼”ç¤º
- [å¿«é€Ÿå¼€å§‹å·¥å…·æŒ‡å—](../how-to/QUICKSTART_TOOLS.md) - å¼€å‘å·¥å…·ä½¿ç”¨
- [Makefileä½¿ç”¨æŒ‡å—](../how-to/MAKEFILE_GUIDE.md) - é¡¹ç›®å·¥å…·å‘½ä»¤

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-10-23
**ç»´æŠ¤è€…**: å¼€å‘å›¢é˜Ÿ
