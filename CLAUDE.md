# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

**ÈáçË¶ÅÊèêÈÜí**: ËØ∑ÂßãÁªà‰ΩøÁî®ÁÆÄ‰Ωì‰∏≠ÊñáÂõûÂ§çÁî®Êà∑ÈóÆÈ¢òÂíå‰∫§ÊµÅ„ÄÇ

**È°πÁõÆÁ±ªÂûã**: ‰ºÅ‰∏öÁ∫ßË∂≥ÁêÉÈ¢ÑÊµãÁ≥ªÁªü (Enterprise Football Prediction System)
**Êû∂ÊûÑÊ®°Âºè**: DDD + CQRS + Event-Driven + Async-First
**ÊäÄÊúØÊ†à**: FastAPI 0.104+ + SQLAlchemy 2.0+ + Redis 5.0+ + PostgreSQL 15 + React 19.2.0 + TypeScript 4.9.5 + XGBoost 2.0+

## üõ†Ô∏è Core Development Commands

### Environment Setup
```bash
# DockerÂºÄÂèëÁéØÂ¢É (‰∏ªË¶ÅÂºÄÂèëÊñπÂºè)
make dev                # ÂêØÂä®ÂÆåÊï¥ÂºÄÂèëÁéØÂ¢É (app + db + redis + frontend + nginx + worker + beat)
make dev-rebuild        # ÈáçÊñ∞ÊûÑÂª∫ÈïúÂÉèÂπ∂ÂêØÂä®ÂºÄÂèëÁéØÂ¢É
make dev-logs           # Êü•ÁúãÂºÄÂèëÁéØÂ¢ÉÊó•Âøó
make dev-stop           # ÂÅúÊ≠¢ÂºÄÂèëÁéØÂ¢É
make down               # ÂÅúÊ≠¢ÊâÄÊúâÊúçÂä°

# Áîü‰∫ßÁéØÂ¢É
make prod               # ÂêØÂä®Áîü‰∫ßÁéØÂ¢É (‰ΩøÁî® docker-compose.prod.yml)
make prod-rebuild       # ÈáçÊñ∞ÊûÑÂª∫Áîü‰∫ßÁéØÂ¢É

# ÁéØÂ¢ÉÁÆ°ÁêÜ
make clean              # Ê∏ÖÁêÜDockerËµÑÊ∫êÂíåÁºìÂ≠ò
make clean-all          # ÂΩªÂ∫ïÊ∏ÖÁêÜÊâÄÊúâÁõ∏ÂÖ≥ËµÑÊ∫ê
make status             # Êü•ÁúãÊâÄÊúâÊúçÂä°Áä∂ÊÄÅ

# Âø´Êç∑ÂëΩ‰ª§
make quick-start        # Âø´ÈÄüÂêØÂä®ÂºÄÂèëÁéØÂ¢É (Âà´Âêç: dev)
make quick-stop         # Âø´ÈÄüÂÅúÊ≠¢ÂºÄÂèëÁéØÂ¢É (Âà´Âêç: dev-stop)
```

### Code Quality & Testing
```bash
# ‰ª£Á†ÅË¥®ÈáèÊ£ÄÊü•Âíå‰øÆÂ§ç (Âú®DockerÂÆπÂô®‰∏≠ËøêË°å)
make test               # Âú®ÂÆπÂô®‰∏≠ËøêË°åÊâÄÊúâÊµãËØï
make lint               # Âú®ÂÆπÂô®‰∏≠ËøêË°å‰ª£Á†ÅÊ£ÄÊü• (Ruff + MyPy)
make format             # Âú®ÂÆπÂô®‰∏≠ËøêË°å‰ª£Á†ÅÊ†ºÂºèÂåñ (ruff format)
make fix-code           # Âú®ÂÆπÂô®‰∏≠ËøêË°å‰ª£Á†ÅËá™Âä®‰øÆÂ§ç
make type-check         # Âú®ÂÆπÂô®‰∏≠ËøêË°åÁ±ªÂûãÊ£ÄÊü•
make security-check     # Âú®ÂÆπÂô®‰∏≠ËøêË°åÂÆâÂÖ®Êâ´Êèè
make coverage           # Âú®ÂÆπÂô®‰∏≠ÁîüÊàêË¶ÜÁõñÁéáÊä•Âëä

# ÊµãËØïÂàÜÁ±ªÊâßË°å
make test.unit          # Âú®ÂÆπÂô®‰∏≠ËøêË°åÂçïÂÖÉÊµãËØï
make test.integration   # Âú®ÂÆπÂô®‰∏≠ËøêË°åÈõÜÊàêÊµãËØï
make test.all           # Âú®ÂÆπÂô®‰∏≠ËøêË°åÊâÄÊúâÊµãËØï

# Âø´ÈÄü‰ºòÂåñËÑöÊú¨
./quick_optimize.sh     # Âø´ÈÄü‰ºòÂåñ‰ª£Á†ÅË¥®Èáè (‰∏ÄÈîÆ‰øÆÂ§çÊâÄÊúâÈóÆÈ¢ò)

# Êú¨Âú∞ÊµãËØïÊâßË°å (ÂèØÈÄâÔºåÈúÄË¶ÅÊú¨Âú∞ÁéØÂ¢É)
pytest tests/unit/test_specific.py::test_function -v        # ËøêË°åÂçï‰∏™ÊµãËØï
pytest tests/unit/test_module.py -k "test_keyword" -v        # ÂÖ≥ÈîÆËØçËøáÊª§ÊµãËØï
pytest tests/unit/ -m "unit and not slow" -v                # Ê†áËÆ∞ÁªÑÂêàÊµãËØï
pytest tests/unit/ --maxfail=3 -x                           # Â§±Ë¥•Êó∂Âø´ÈÄüÂÅúÊ≠¢

# Ë¶ÜÁõñÁéáÂàÜÊûê
pytest --cov=src --cov-report=html --cov-report=term-missing

# CI Ë¶ÜÁõñÁéáÈó®ÊßõÈ™åËØÅ (Ë¶ÅÊ±Ç31%)
pytest --cov=src --cov-fail-under=31 --cov-report=term-missing
```

### Container Management & Access
```bash
# ÂÆπÂô®ÁÆ°ÁêÜÂíåËÆøÈóÆ
make shell              # ËøõÂÖ•ÂêéÁ´ØÂÆπÂô®ÁªàÁ´Ø
make shell-db           # ËøõÂÖ•Êï∞ÊçÆÂ∫ìÂÆπÂô®
make db-shell           # ËøûÊé•PostgreSQLÊï∞ÊçÆÂ∫ì
make redis-shell        # ËøûÊé•Redis
make logs               # Êü•ÁúãÂ∫îÁî®Êó•Âøó
make logs-db            # Êü•ÁúãÊï∞ÊçÆÂ∫ìÊó•Âøó
make logs-redis         # Êü•ÁúãRedisÊó•Âøó
make status             # Êü•ÁúãÊâÄÊúâÊúçÂä°Áä∂ÊÄÅ

# Docker ÂÆπÂô®ÁÆ°ÁêÜ (ÂéüÁîüÂëΩ‰ª§)
docker-compose up -d            # ÂêØÂä®ÂÆåÊï¥ÂºÄÂèëÁéØÂ¢É
docker-compose down             # ÂÅúÊ≠¢ÂºÄÂèëÁéØÂ¢É
docker-compose logs -f app      # Êü•ÁúãÂ∫îÁî®Êó•Âøó
docker-compose exec app bash     # ËøõÂÖ•Â∫îÁî®ÂÆπÂô®
docker-compose exec db psql      # ËøûÊé•PostgreSQLÊï∞ÊçÆÂ∫ì

# ÁõëÊéßÂíåË∞ÉËØï
make monitor            # ÂÆûÊó∂ÁõëÊéßÂ∫îÁî®ËµÑÊ∫ê‰ΩøÁî®
make monitor-all        # ÁõëÊéßÊâÄÊúâÂÆπÂô®ËµÑÊ∫ê‰ΩøÁî®

# Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ
make db-reset           # ÈáçÁΩÆÊï∞ÊçÆÂ∫ì
make db-migrate         # ËøêË°åÊï∞ÊçÆÂ∫ìËøÅÁßª
```

### Security & Validation
```bash
make security-check     # BanditÂÆâÂÖ®Êâ´Êèè
make audit              # ‰æùËµñÂÆâÂÖ®ÂÆ°ËÆ°
make ci-check          # ÂÆåÊï¥CIÈ™åËØÅ
```

## üèóÔ∏è Architecture Overview

### Technology Stack
- **Backend**: FastAPI 0.104+, SQLAlchemy 2.0+, Pydantic v2+, Redis 5.0+, PostgreSQL 15
- **Frontend**: React 19.2.0, TypeScript 4.9.5, Ant Design 5.27.6, Redux Toolkit 2.9.2
- **Machine Learning**: XGBoost 2.0+, scikit-learn 1.3+, pandas 2.1+, MLflow 2.22.2+
- **Testing**: pytest 8.4+ with asyncio support (385 test cases)
- **Code Quality**: Ruff 0.14+, MyPy 1.18+, Bandit 1.8.6+

### Clean Architecture Pattern
```
src/
‚îú‚îÄ‚îÄ api/           # FastAPI routers, HTTP concerns only
‚îú‚îÄ‚îÄ domain/        # Business logic, entities (pure Python)
‚îú‚îÄ‚îÄ services/      # Application services, orchestration
‚îú‚îÄ‚îÄ database/      # SQLAlchemy models, repositories
‚îú‚îÄ‚îÄ adapters/      # External API integrations
‚îú‚îÄ‚îÄ ml/           # Machine learning models and pipelines
‚îú‚îÄ‚îÄ cache/        # Redis caching layer
‚îî‚îÄ‚îÄ utils/        # Shared utilities
```

### Key Integration Points
- **Main Application**: `src/main.py` with 40+ API endpoints
- **ML Prediction Engine**: XGBoost models for match predictions
- **Real-time Features**: WebSocket support at `/api/v1/realtime/ws`
- **Caching**: Redis-based performance optimization
- **Database**: Async PostgreSQL with SQLAlchemy 2.0

### API Endpoints Structure
- **Health Checks**: `/health`, `/health/system`, `/health/database`
- **Predictions**: `/api/v1/predictions/`, `/api/v2/predictions/`
- **Data Management**: `/api/v1/data_management/`
- **System**: `/api/v1/system/`
- **Adapters**: `/api/v1/adapters/`
- **Monitoring**: `/metrics`

## üß™ Testing Architecture

### Test Structure (385 test cases)
- **Unit Tests** (85%): Fast, isolated component testing
- **Integration Tests** (12%): Real dependency testing
- **E2E Tests** (2%): Complete user workflow testing
- **Performance Tests** (1%): Load and stress testing

### Standardized Test Markers
```python
# Core test types
@pytest.mark.unit           # Unit tests (85% of tests)
@pytest.mark.integration    # Integration tests (12% of tests)
@pytest.mark.e2e           # End-to-end tests (2% of tests)
@pytest.mark.performance   # Performance tests (1% of tests)

# Execution characteristics
@pytest.mark.critical       # Must-pass core functionality
@pytest.mark.smoke         # Basic functionality validation
@pytest.mark.slow          # Long-running tests (>30s)
@pytest.mark.regression    # Regression testing

# Functional domain
@pytest.mark.api           # HTTP endpoint testing
@pytest.mark.database      # Database connection tests
@pytest.mark.ml            # Machine learning tests
@pytest.mark.cache         # Redis and caching logic
@pytest.mark.auth          # Authentication and authorization
@pytest.mark.monitoring    # Metrics and health checks

# Development dependencies
@pytest.mark.external_api  # Requires external API calls
@pytest.mark.docker        # Requires Docker container environment
@pytest.mark.network       # Requires network connection
```

### Recommended Testing Workflow
```bash
# Êó•Â∏∏ÂºÄÂèë - Âø´ÈÄüÈ™åËØÅ
make test.smart         # Quick validation (<2 minutes, excludes slow tests)

# ÂäüËÉΩÂºÄÂèëÂÆåÊàê - ÂÆåÊï¥ÊµãËØï
make test.unit          # Full unit test suite with coverage
make coverage           # Generate HTML coverage report

# ÂèëÂ∏ÉÂâçÈ™åËØÅ - ÂÖ®Èù¢Ê£ÄÊü•
make test.all           # Complete test suite (unit + integration + e2e)
make lint && make test  # Code quality + full testing

# CI/CD pipeline validation
make ci-check          # Full pipeline validation (quality + security + tests)
```

## üîß Development Standards

### Code Style Requirements
- **Type Hints**: All functions must have complete type annotations
- **Async/Await**: All I/O operations must be async (database, external APIs)
- **Logging**: Use structured logging with `logger` (never use `print()`)
- **Error Handling**: Comprehensive exception handling with proper logging

### Function Template
```python
from typing import Optional, Dict, Any
import logging

logger = logging.getLogger(__name__)

async def process_data(
    input_data: Dict[str, Any],
    *,
    timeout: Optional[int] = None,
    retry_count: int = 3
) -> ResultModel:
    """Process input data with async operations.

    Args:
        input_data: Dictionary containing input parameters
        timeout: Optional timeout in seconds
        retry_count: Number of retry attempts

    Returns:
        ResultModel: Processed result

    Raises:
        ValueError: When input data is invalid
        TimeoutError: When operation exceeds timeout
    """
    logger.info(f"Processing data: {len(input_data)} items")

    try:
        result = await database_service.fetch_data(input_data, timeout)
        logger.debug(f"Successfully processed {len(result)} items")
        return result
    except Exception as e:
        logger.error(f"Data processing failed: {e}")
        raise
```

## üê≥ Docker Development

### Multi-Environment Support
- **Development**: `docker-compose.yml` (ÂÆåÊï¥ÂºÄÂèëÊ†à - app + db + redis + frontend + nginx + worker + beat)
- **Production**: `docker-compose.prod.yml` (Áîü‰∫ß‰ºòÂåñÊ†à - app + db + redis + nginx + monitoring + logging)

### Service Stack
#### Development Environment
- **app**: FastAPI application (port: 8000)
- **frontend**: React application (ports: 3000, 3001)
- **db**: PostgreSQL 15 (port: 5432)
- **redis**: Redis 7.0 (port: 6379)
- **nginx**: Reverse proxy (port: 80)
- **worker**: Celery worker (ÂºÇÊ≠•‰ªªÂä°Â§ÑÁêÜ)
- **beat**: Celery beat (ÂÆöÊó∂‰ªªÂä°Ë∞ÉÂ∫¶)

#### Production Environment (Additional Services)
- **prometheus**: ÊåáÊ†áÊî∂ÈõÜÂíåÂ≠òÂÇ® (port: 9090)
- **grafana**: ÂèØËßÜÂåñ‰ª™Ë°®Êùø (port: 3000)
- **loki**: Êó•ÂøóËÅöÂêà (port: 3100)

### Container Development Features
- Hot reload with volume mounting
- Health checks for all services
- Environment-specific configurations
- Multi-stage builds for optimized images
- Development vs production targets

## ü§ñ Machine Learning Pipeline

### ML Architecture
- **Prediction Engine**: XGBoost 2.0+ gradient boosting models with hyperparameter optimization
- **Feature Engineering**: pandas 2.1+ and numpy 1.25+ data preprocessing with automated pipelines
- **Model Training**: scikit-learn 1.3+ training pipelines with cross-validation
- **Model Management**: MLflow 2.22.2+ version control with security patches and experiment tracking
- **Model Ensemble**: Multiple prediction strategies (LSTM, Poisson, Ensemble) with performance comparison

### ML Service Integration
```python
# Inference service usage
from src.services.inference_service import inference_service

prediction_result = await inference_service.predict_match(match_id)
# Returns: PredictionResult with confidence scores and probabilities

# Batch prediction
batch_results = await inference_service.batch_predict_match(match_ids)
```

### Model Lifecycle
- **Training**: `src/ml/train_model.py` - Automated model training with hyperparameter tuning
- **Validation**: `src/ml/model_validation.py` - Cross-validation and performance evaluation
- **Deployment**: `src/ml/model_deployment.py` - Model packaging and API deployment
- **Monitoring**: `src/ml/model_monitoring.py` - Real-time performance tracking and drift detection
- **Feature Store**: `src/ml/feature_store.py` - Centralized feature management
- **Experiments**: `src/ml/experiments/` - A/B testing and model comparison

### ML Model Management
```bash
# ML model operations
python -m src.ml.train_model --config configs/ml/xgboost_v2.yaml
python -m src.ml.model_validation --model-version latest
python -m src.ml.batch_predict --date-range 2024-01-01:2024-01-31

# MLflow UI (when running)
mlflow ui --port 5000  # Access experiment tracking
```

## üö® Crisis Management Tools

### Test Crisis Recovery
```bash
# When tests are failing (>30%)
make test.smart          # Run quick tests to identify issues
make fix-code           # Auto-fix code quality issues
make lint              # Run full code quality check
make coverage          # Check coverage impact

# Manual test recovery
pytest tests/unit/ --maxfail=5 -x  # Run tests with fast failure
pytest tests/unit/ -k "not slow"   # Exclude slow tests
pytest --cov=src --cov-report=term-missing  # Identify uncovered code
```

### Environment Recovery
```bash
# Environment issues
make clean && make install    # Fresh environment setup
make doctor                   # Run health check
make status                   # Check project status

# Docker environment recovery
docker-compose down -v        # Stop and remove volumes
docker-compose up -d          # Fresh start
docker-compose logs -f app    # Check startup issues
```

### Code Quality Recovery
```bash
# Quality issues
make fix-code               # Auto-fix most issues
make format                 # Format all code
make lint                  # Identify remaining issues

# Import and dependency issues
pip install --upgrade pip   # Update pip
pip install -e ".[dev]"     # Reinstall development dependencies
```

## ü§ñ Ëá™Âä®ÂåñËÑöÊú¨Â∑•ÂÖ∑

### Âø´ÈÄü‰ºòÂåñËÑöÊú¨
```bash
./quick_optimize.sh     # Âø´ÈÄü‰ºòÂåñ‰ª£Á†ÅË¥®Èáè (‰∏ÄÈîÆ‰øÆÂ§çÊâÄÊúâÈóÆÈ¢ò)
```

### Áª¥Êä§ËÑöÊú¨ (scripts/)
```bash
# Êï∞ÊçÆÂ∫ìÁÆ°ÁêÜ
scripts/backup_db.sh     # Â§á‰ªΩÊï∞ÊçÆÂ∫ì
scripts/restore_db.sh    # ÊÅ¢Â§çÊï∞ÊçÆÂ∫ì
scripts/db-migrate.sh    # ËøêË°åÊï∞ÊçÆÂ∫ìËøÅÁßª

# Ë¥®Èáè‰øùËØÅ
scripts/quality/intelligent_quality_analyzer.py        # Êô∫ËÉΩË¥®ÈáèÂàÜÊûê
scripts/quality/continuous_improvement_engine.py      # ÊåÅÁª≠ÊîπËøõÂºïÊìé
scripts/maintenance/scheduled_maintenance.py         # ÂÆöÊúüÁª¥Êä§
scripts/maintenance/coverage_trend_analyzer.py        # Ë¶ÜÁõñÁéáË∂ãÂäøÂàÜÊûê

# ÈÉ®ÁΩ≤ÂíåÈ™åËØÅ
scripts/deploy.sh         # ÈÉ®ÁΩ≤ËÑöÊú¨
scripts/deploy_verify.sh  # ÈÉ®ÁΩ≤È™åËØÅ
scripts/security_check.sh # ÂÆâÂÖ®Ê£ÄÊü•

# ÁõëÊéßÂíåÊó•Âøó
scripts/start_monitoring.sh # ÂêØÂä®ÁõëÊéßÁ≥ªÁªü
```

### GitHub Actions CI/CD Â∑•‰ΩúÊµÅ
```yaml
# .github/workflows/
‚îú‚îÄ‚îÄ ci_pipeline_v2.yml         # CI/CDÊµÅÊ∞¥Á∫ø
‚îú‚îÄ‚îÄ production-deploy.yml      # Áîü‰∫ßÈÉ®ÁΩ≤
‚îú‚îÄ‚îÄ smart-fixer-ci.yml          # Êô∫ËÉΩ‰øÆÂ§çCI
‚îú‚îÄ‚îÄ docs.yml                    # ÊñáÊ°£ÊûÑÂª∫
‚îú‚îÄ‚îÄ branch-protection.yml       # ÂàÜÊîØ‰øùÊä§
‚îú‚îÄ‚îÄ issues-cleanup.yml         # IssueÊ∏ÖÁêÜ
‚îú‚îÄ‚îÄ ai-feedback.yml            # AIÂèçÈ¶à
‚îî‚îÄ‚îÄ deploy.yml                  # ÈÉ®ÁΩ≤ÊµÅÁ®ã
```

### CI/CD ÊµÅÊ∞¥Á∫øÁâπÊÄß
- **Ëá™Âä®ÂåñÊµãËØï**: 385‰∏™ÊµãËØïÁî®‰æãËá™Âä®ÊâßË°å
- **‰ª£Á†ÅË¥®ÈáèÊ£ÄÊü•**: Ruff + MyPy + Bandit ‰∏âÈáçÊ£ÄÊü•
- **ÂÆâÂÖ®Êâ´Êèè**: ‰æùËµñÊºèÊ¥ûÊ£ÄÊµãÂíå‰ª£Á†ÅÂÆâÂÖ®ÂÆ°ËÆ°
- **Ë¶ÜÁõñÁéáÈó®Êßõ**: 31%Ë¶ÜÁõñÁéáÂàöÊÄßË¶ÅÊ±Ç
- **Êô∫ËÉΩ‰øÆÂ§ç**: Ëá™Âä®‰øÆÂ§çÂ∏∏ËßÅ‰ª£Á†ÅÈóÆÈ¢ò
- **ÈÉ®ÁΩ≤È™åËØÅ**: Áîü‰∫ßÈÉ®ÁΩ≤ÂâçÂÆåÊï¥ÊÄßÊ£ÄÊü•
- **ÁéØÂ¢ÉÊ∏ÖÁêÜ**: ÂÆöÊúüÊ∏ÖÁêÜ‰∏¥Êó∂ËµÑÊ∫êÂíåËøáÊúüÊï∞ÊçÆ

## üîÑ Git Workflow

### Commit Message Format
```bash
# Features
feat(api): add user authentication endpoint
feat(ml): implement XGBoost prediction model

# Fixes
fix(database): resolve async connection timeout issue
fix(tests): restore 100+ core test functionality

# Quality
refactor(api): extract validation logic to service layer
style(core): apply ruff formatting to all files

# Maintenance
chore(deps): update FastAPI to 0.121.2
chore(security): upgrade MLflow to 2.22.2 for security patches
```

### Pre-Commit Checklist
- [ ] Tests pass: `make test.smart`
- [ ] Code quality: `make fix-code`
- [ ] Security check: `make security-check`
- [ ] Coverage maintained: `make coverage`
- [ ] Full validation: `make ci-check`

## üìä Current Project Status

### Quality Metrics
- **Test Coverage**: Use `make test.unit --cov=src --cov-report=term-missing`
- **Test Cases**: 385 active test cases across unit/integration/e2e
- **Code Quality**: Ruff + Black + Bandit validation passing
- **Security**: No critical vulnerabilities, ECDSA vulnerability patched
- **CI/CD**: Green pipeline with automated recovery

### Health Score
```
Overall Health: 85/100 ‚úÖ
‚îú‚îÄ‚îÄ Code Quality: 90/100 ‚úÖ
‚îú‚îÄ‚îÄ Testing: 70/100 ‚ö†Ô∏è
‚îú‚îÄ‚îÄ Documentation: 95/100 ‚úÖ
‚îú‚îÄ‚îÄ Security: 95/100 ‚úÖ
‚îú‚îÄ‚îÄ CI/CD: 90/100 ‚úÖ
‚îî‚îÄ‚îÄ Dependencies: 90/100 ‚úÖ
```

## üé® Frontend Architecture

### React + TypeScript Stack
- **React 19.2.0**: Modern React with concurrent features
- **TypeScript 4.9.5**: Full type safety with strict mode
- **Ant Design 5.27.6**: Enterprise-grade component library
- **Redux Toolkit 2.9.2**: State management with RTK Query
- **React Query**: Server state management and caching
- **Vite**: Fast build tool with HMR

### Frontend Development Commands
```bash
# Frontend development (Âú® frontend/ ÁõÆÂΩï‰∏≠)
cd frontend
npm install              # ÂÆâË£Ö‰æùËµñ
npm run dev             # ÂêØÂä®ÂºÄÂèëÊúçÂä°Âô® (port 3000)
npm run build           # Áîü‰∫ßÊûÑÂª∫
npm run preview         # È¢ÑËßàÁîü‰∫ßÊûÑÂª∫
npm run test            # ËøêË°åÂâçÁ´ØÊµãËØï
npm run lint            # ESLint + TypeScript Ê£ÄÊü•
```

### Frontend Project Structure
```
frontend/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/     # ÂèØÈáçÁî®UIÁªÑ‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ pages/         # È°µÈù¢ÁªÑ‰ª∂
‚îÇ   ‚îú‚îÄ‚îÄ hooks/         # Ëá™ÂÆö‰πâReact hooks
‚îÇ   ‚îú‚îÄ‚îÄ store/         # Redux storeÈÖçÁΩÆ
‚îÇ   ‚îú‚îÄ‚îÄ services/      # APIÊúçÂä°ÂáΩÊï∞
‚îÇ   ‚îú‚îÄ‚îÄ types/         # TypeScriptÁ±ªÂûãÂÆö‰πâ
‚îÇ   ‚îú‚îÄ‚îÄ utils/         # ÂâçÁ´ØÂ∑•ÂÖ∑ÂáΩÊï∞
‚îÇ   ‚îî‚îÄ‚îÄ styles/        # CSSÂíåÊ†∑Âºè
‚îú‚îÄ‚îÄ public/            # ÈùôÊÄÅËµÑÊ∫ê
‚îî‚îÄ‚îÄ tests/             # ÂâçÁ´ØÊµãËØïÊñá‰ª∂
```

### Frontend State Management
```typescript
// Redux Toolkit store configuration
import { configureStore } from '@reduxjs/toolkit'
import { predictionApi } from './services/predictionApi'

export const store = configureStore({
  reducer: {
    [predictionApi.reducerPath]: predictionApi.reducer,
  },
  middleware: (getDefaultMiddleware) =>
    getDefaultMiddleware().concat(predictionApi.middleware),
})
```

### ÂâçÁ´ØÂºÄÂèëÁéØÂ¢ÉËÆøÈóÆ
- **Frontend Dev Server**: http://localhost:3000 (ViteÂºÄÂèëÊúçÂä°Âô®)
- **Backend API**: http://localhost:8000 (FastAPIÂ∫îÁî®)
- **API Documentation**: http://localhost:8000/docs (‰∫§‰∫íÂºèOpenAPI)
- **Frontend in Production**: http://localhost (ÈÄöËøánginxÂèçÂêë‰ª£ÁêÜ)

## ü§ñ AI Assistant Configuration

### Claude Code Integration
This project is AI-first maintained with these configurations:

#### Development Workflow
1. **Environment Check**: `make env-check` - Verify development environment
2. **Context Loading**: `make context` - Load project architecture into AI memory
3. **Development**: Write code following DDD + CQRS patterns
4. **Quality Validation**: `make ci-check` - Full pipeline validation
5. **Pre-commit**: `make prepush` - Complete validation before push

#### AI Tool Guidelines
- **Architecture Priority**: Maintain DDD layer separation and CQRS patterns
- **Testing First**: Write tests before implementation (TDD approach)
- **Async Everywhere**: All I/O operations must be async (database, APIs)
- **Type Safety**: Complete type annotations required for all functions
- **Error Handling**: Comprehensive exception handling with proper logging
- **Security First**: Never expose secrets, validate all inputs

#### Preferred Patterns
```python
# ‚úÖ Preferred: Service layer with dependency injection
async def get_prediction_use_case(
    match_id: int,
    prediction_service: PredictionService,
    prediction_repo: PredictionRepository
) -> Dict[str, Any]:
    prediction = await prediction_service.generate_prediction(match_id)
    await prediction_repo.save_prediction(prediction)
    return prediction

# ‚ùå Avoid: Direct database access in API layer
@app.get("/predictions/{match_id}")
async def get_prediction(match_id: int, db: AsyncSession):
    # Business logic should be in service layer
    result = await db.execute(select(Prediction).where(Prediction.match_id == match_id))
    return result.scalar_one_or_none()
```

## üìä ÁõëÊéßÂíåËøêÁª¥

### Áîü‰∫ßÁéØÂ¢ÉÁõëÊéßÊ†à
- **Prometheus**: ÊåáÊ†áÊî∂ÈõÜÂíåÂ≠òÂÇ® (port: 9090)
- **Grafana**: ÂèØËßÜÂåñ‰ª™Ë°®Êùø (port: 3000)
- **Loki**: Êó•ÂøóËÅöÂêà (port: 3100)
- **ÂÅ•Â∫∑Ê£ÄÊü•**: Â§öÂ±ÇÊ¨°ÂÅ•Â∫∑ÁõëÊµã

### ÁõëÊéßÂëΩ‰ª§
```bash
# ÂÆπÂô®ÁõëÊéß
make monitor              # ÂÆûÊó∂ÁõëÊéßÂ∫îÁî®ËµÑÊ∫ê‰ΩøÁî®
make monitor-all          # ÁõëÊéßÊâÄÊúâÂÆπÂô®ËµÑÊ∫ê‰ΩøÁî®

# Êó•ÂøóÊü•Áúã
make logs                 # Êü•ÁúãÂ∫îÁî®Êó•Âøó
make logs-db             # Êü•ÁúãÊï∞ÊçÆÂ∫ìÊó•Âøó
make logs-redis          # Êü•ÁúãRedisÊó•Âøó

# ÂÅ•Â∫∑Ê£ÄÊü•
curl http://localhost:8000/health               # Â∫îÁî®ÂÅ•Â∫∑Ê£ÄÊü•
curl http://localhost:8000/health/system          # Á≥ªÁªüÂÅ•Â∫∑Ê£ÄÊü•
curl http://localhost:8000/health/database       # Êï∞ÊçÆÂ∫ìÂÅ•Â∫∑Ê£ÄÊü•
curl http://localhost:8000/api/v1/health/inference # Êé®ÁêÜÊúçÂä°ÂÅ•Â∫∑Ê£ÄÊü•
```

### ÊÄßËÉΩÁõëÊéß
```bash
# Prometheus ÊåáÊ†áËÆøÈóÆ
curl http://localhost:9090/metrics              # PrometheusÊåáÊ†á
curl http://localhost:8000/metrics               # Â∫îÁî®ÊåáÊ†á

# Á≥ªÁªüËµÑÊ∫êÁõëÊéß
docker stats                                     # Êü•ÁúãÂÆπÂô®ËµÑÊ∫ê‰ΩøÁî®
docker-compose ps                               # Ê£ÄÊü•ÊúçÂä°Áä∂ÊÄÅ
```

### Ë¥®Èáè‰øùËØÅ
- **ÊµãËØïË¶ÜÁõñÁéá**: 31% CIÈó®ÊßõË¶ÅÊ±Ç
- **‰ª£Á†ÅË¥®Èáè**: Ruff + MyPy + Bandit ‰∏âÈáçÊ£ÄÊü•
- **ÂÆâÂÖ®Êâ´Êèè**: ÂÆöÊúüÂÆâÂÖ®ÂÆ°ËÆ°ÂíåÊºèÊ¥û‰øÆÂ§ç
- **ÊÄßËÉΩÁõëÊéß**: ÂÆûÊó∂ÊÄßËÉΩÊåáÊ†áÂíåË∂ãÂäøÂàÜÊûê

## üåê Application URLs

### Development Access
- **Frontend**: http://localhost:3000 (React development server)
- **Backend API**: http://localhost:8000 (FastAPI application)
- **API Documentation**: http://localhost:8000/docs (Interactive OpenAPI)
- **Health Check**: http://localhost:8000/health
- **System Status**: http://localhost:8000/system/status
- **Inference Service**: http://localhost:8000/api/v1/health/inference
- **WebSocket**: ws://localhost:8000/api/v1/realtime/ws
- **MLflow UI**: http://localhost:5000 (ML experiment tracking)

### Production Monitoring Access
- **Grafana Dashboard**: http://localhost:3000 (ÁõëÊéß‰ª™Ë°®Êùø)
- **Prometheus**: http://localhost:9090 (ÊåáÊ†áÂ≠òÂÇ®)
- **Loki**: http://localhost:3100 (Êó•ÂøóËÅöÂêà)
- **Frontend Production**: http://localhost (nginxÂèçÂêë‰ª£ÁêÜ)

### Quality Dashboard
- **Quality Monitor**: http://localhost:3001 (Independent quality monitoring)

## üöÄ Quick Start for New Development

### Environment Prerequisites
```bash
# Á°Æ‰øù‰ª•‰∏ãÂ∑•ÂÖ∑Â∑≤ÂÆâË£Ö
docker --version
docker-compose --version
python3 --version  # Python 3.10+
make --version
```

### 5-Minute Setup
```bash
# 1. Environment initialization
make dev                    # ÂêØÂä®ÂÆåÊï¥ÂºÄÂèëÁéØÂ¢É

# 2. Validate setup
make test && make lint      # È™åËØÅÁéØÂ¢ÉÂíå‰ª£Á†ÅË¥®Èáè

# 3. Access services
# Frontend: http://localhost:3000
# Backend API: http://localhost:8000/docs
# Database: make db-shell
```

### Quick Development Commands
```bash
# Âø´ÈÄüÂêØÂä®ÂÆåÊï¥ÂºÄÂèëÁéØÂ¢É (Êé®Ëçê)
make dev

# ÂêØÂä®Áîü‰∫ßÁéØÂ¢É (Â∏¶ÁõëÊéß)
make prod

# ‰ª£Á†ÅË¥®ÈáèÂø´ÈÄü‰øÆÂ§ç
./quick_optimize.sh

# Êü•ÁúãÊâÄÊúâÊúçÂä°Áä∂ÊÄÅ
make status

# ÂÅúÊ≠¢ÊâÄÊúâÊúçÂä°
make down
```

### First Development Tasks
1. Read `src/main.py` to understand the application structure
2. Review `pyproject.toml` for dependencies and tool configurations
3. Check `Makefile` for available development commands
4. Run `make test.smart` to verify the testing environment
5. Access API documentation at http://localhost:8000/docs

## üîç Common Development Patterns

### Database Operations (Async Only)
```python
# ‚úÖ Correct: Async database operations
from sqlalchemy.ext.asyncio import AsyncSession

async def get_user(db: AsyncSession, user_id: int) -> Optional[User]:
    stmt = select(User).where(User.id == user_id)
    result = await db.execute(stmt)
    return result.scalar_one_or_none()

# ‚ùå Wrong: Sync database operations
user = db.query(User).filter(User.id == user_id).first()
```

### API Response Patterns
```python
from src.api.schemas import PredictionResponse

@app.get("/api/v1/predictions/{match_id}")
async def get_prediction(match_id: int) -> PredictionResponse:
    try:
        prediction = await inference_service.predict_match(match_id)
        return PredictionResponse(success=True, data=prediction)
    except Exception as e:
        logger.error(f"Prediction failed: {e}")
        raise HTTPException(status_code=500, detail="Prediction service unavailable")
```

### Service Layer Integration
```python
# Service injection pattern
from src.services.prediction import PredictionService
from src.database.repositories import PredictionRepository

async def get_prediction_use_case(
    match_id: int,
    prediction_service: PredictionService,
    prediction_repo: PredictionRepository
) -> Dict[str, Any]:
    # Use case orchestration
    prediction = await prediction_service.generate_prediction(match_id)
    await prediction_repo.save_prediction(prediction)
    return prediction
```

## üîó API Usage Patterns

### Standard API Response Format
```python
from src.api.schemas import PredictionResponse

# Standard success response
{
    "success": True,
    "data": {...},
    "message": "Operation completed successfully",
    "timestamp": "2025-01-01T00:00:00Z"
}

# Error response format
{
    "success": False,
    "error": {
        "code": "VALIDATION_ERROR",
        "message": "Invalid input parameters",
        "details": {...}
    },
    "timestamp": "2025-01-01T00:00:00Z"
}
```

### Common API Endpoints Usage
```python
# Health checks
GET /health                    # Basic health check
GET /health/system             # System health (CPU, memory)
GET /health/database           # Database connectivity

# Predictions API (v1 and v2)
GET /api/v1/predictions/{match_id}              # Single match prediction
POST /api/v1/predictions/batch                  # Batch predictions
GET /api/v2/predictions/{match_id}/analysis      # Enhanced prediction with analysis
GET /predictions/{match_id}                     # Direct prediction endpoint

# Data management
GET /api/v1/data_management/teams               # Get all teams
POST /api/v1/data_management/matches/sync       # Sync match data
GET /api/v1/data_management/leagues/{id}/table  # League table

# System management
GET /api/v1/system/status                       # System status
POST /api/v1/system/cache/clear                 # Clear cache
GET /api/v1/system/metrics                      # Performance metrics

# Adapters and external integrations
GET /api/v1/adapters/{service}/status           # External service status
POST /api/v1/adapters/{service}/sync            # Sync external data

# Monitoring and metrics
GET /metrics                                    # Prometheus metrics
GET /api/v1/monitoring/health                   # Application health monitoring
```

### WebSocket Real-time Communication
```python
# WebSocket connection for real-time updates
import asyncio
import json
import websockets

async def handle_realtime_updates():
    uri = "ws://localhost:8000/api/v1/realtime/ws"
    async with websockets.connect(uri) as websocket:
        async for message in websocket:
            data = json.loads(message)

            if data['type'] == 'prediction_update':
                # Handle real-time prediction updates
                await handle_prediction_update(data['payload'])
            elif data['type'] == 'match_status':
                # Handle match status changes
                await handle_match_status_change(data['payload'])
            elif data['type'] == 'system_metrics':
                # Handle system monitoring data
                await handle_system_metrics(data['payload'])

# Client usage example
import websocket

def on_message(ws, message):
    data = json.loads(message)
    if data['type'] == 'prediction_update':
        handle_prediction_update(data['payload'])

ws = websocket.WebSocketApp(
    "ws://localhost:8000/api/v1/realtime/ws",
    on_message=on_message
)
ws.run_forever()
```

## üìû Troubleshooting Guide

### Common Issues
1. **Test Failures**: Run `make solve-test-crisis` immediately
2. **Type Errors**: Check imports and add missing type hints
3. **Database Issues**: Verify connection string and PostgreSQL status
4. **Redis Issues**: Check Redis service status and connection
5. **Import Errors**: Run `make fix-imports` to resolve import problems
6. **Port Conflicts**: Check if ports 8000, 3000, 5432, 6379 are available

### Environment Debugging
```bash
# Check service status
docker-compose ps                    # All containers status
docker-compose logs app              # Application logs
docker-compose logs db               # Database logs
docker-compose logs redis            # Redis logs

# Database debugging
make db-shell                        # Connect to PostgreSQL
\dt                                  # List tables
SELECT COUNT(*) FROM matches;        # Verify data

# Redis debugging
make redis-shell                     # Connect to Redis
KEYS *                               # List all keys
INFO memory                          # Memory usage
```

### Performance Issues
```bash
# Performance monitoring
curl http://localhost:8000/metrics   # Prometheus metrics
curl http://localhost:8000/system/status  # System status

# Load testing (if installed)
ab -n 100 -c 10 http://localhost:8000/health  # Apache bench
```

### Emergency Commands
```bash
# Complete environment reset
make clean && make install && make test.smart

# Health check all services
make up && make logs && make status

# Code quality emergency
make fix-code && make format && make lint

# Database recovery
docker-compose down -v && docker-compose up -d

# Dependency issues
pip install --upgrade pip && make install

# Test failures - quick diagnosis
pytest tests/unit/ --tb=short --maxfail=3
pytest tests/unit/ -k "critical or smoke"

# Performance issues
docker stats                    # Check container resource usage
curl http://localhost:8000/metrics  # Check application metrics
```

## üìö Additional Resources

### Documentation
- **API Documentation**: http://localhost:8000/docs (Interactive OpenAPI)
- **ReDoc**: http://localhost:8000/redoc (Alternative API docs)
- **MLflow Tracking**: http://localhost:5000 (ML experiments)
- **Coverage Report**: `htmlcov/index.html` (Test coverage visualization)

### Development Tools
- **Adminer**: http://localhost:8080 (Database admin, if enabled)
- **Redis Commander**: http://localhost:8081 (Redis admin, if enabled)
- **Prometheus**: http://localhost:9090 (Metrics, if configured)

### Key Configuration Files
- `pyproject.toml` - Python dependencies and tool configuration
- `docker-compose.*.yml` - Container orchestration
- `Makefile` - Development workflow commands
- `.env.*` - Environment configurations

---

**Remember**: This is an AI-first maintained project. Prioritize architectural integrity, code quality, and comprehensive testing. When in doubt, choose the conservative approach that preserves existing patterns and maintains the green CI pipeline.

## üìã Development Environment Reference

### Minimum Requirements
- **Python**: 3.10+ (recommended: 3.11)
- **Docker**: 20.0+ with docker-compose
- **Memory**: 4GB+ RAM for development
- **Storage**: 10GB+ free disk space

### Service Ports
- **Backend API**: http://localhost:8000
- **Frontend**: http://localhost:3000
- **PostgreSQL**: localhost:5432
- **Redis**: localhost:6379
- **API Docs**: http://localhost:8000/docs

### Key File Locations
- **Main App**: `src/main.py` - FastAPI application entry point
- **Config**: `pyproject.toml` - Dependencies and tool configuration
- **Docker**: `docker-compose.yml` - Development environment
- **Makefile**: Development commands and workflows
- **Tests**: `tests/` - 385 test cases across unit/integration/e2e

*Last Updated: 2025-11-24 | AI Maintainer: Claude Code | Version: 1.3*