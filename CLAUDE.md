# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Language Preference

**è¯·ä½¿ç”¨ç®€ä½“ä¸­æ–‡å›å¤ç”¨æˆ·** - Please respond in Simplified Chinese when interacting with the user. The project team primarily communicates in Chinese, so all responses should be in Simplified Chinese unless specifically requested otherwise.

## å¿«é€Ÿå‘½ä»¤æŸ¥æ‰¾è¡¨

| ä»»åŠ¡ç±»å‹ | é¦–é€‰å‘½ä»¤ | å¤‡é€‰æ–¹æ¡ˆ | è¯´æ˜ |
|---------|----------|----------|------|
| **ç¯å¢ƒå¯åŠ¨** | `make dev` | `make quick-start` | å¯åŠ¨å®Œæ•´å¼€å‘ç¯å¢ƒ |
| **è¿è¡Œæµ‹è¯•** | `make test.unit` | `make test` | å•å…ƒæµ‹è¯• / å…¨éƒ¨æµ‹è¯• |
| **ä»£ç æ£€æŸ¥** | `make lint && make fix-code` | `make ci` | æ£€æŸ¥å¹¶ä¿®å¤ / å®Œæ•´CIéªŒè¯ |
| **æœåŠ¡çŠ¶æ€** | `make status` | `docker-compose ps` | æ£€æŸ¥æ‰€æœ‰æœåŠ¡å¥åº·çŠ¶æ€ |
| **è¿›å…¥å®¹å™¨** | `make shell` | `docker-compose exec app bash` | è¿›å…¥appå®¹å™¨ |
| **æŸ¥çœ‹æ—¥å¿—** | `make logs` | `docker-compose logs -f app` | åº”ç”¨æ—¥å¿— |
| **æ•°æ®åº“æ“ä½œ** | `make db-shell` | `docker-compose exec db psql -U postgres` | è¿æ¥æ•°æ®åº“ |
| **è¦†ç›–ç‡æŠ¥å‘Š** | `make coverage` | `open htmlcov/index.html` | ç”Ÿæˆ/æŸ¥çœ‹è¦†ç›–ç‡ |

## é¡¹ç›®è´¨é‡åŸºçº¿

- **Build Status**: Stable (Green Baseline Established)
- **Test Coverage**: 29.0% baseline (æŒç»­æ”¹è¿›ä¸­)
- **Total Tests**: 385 tests passing
- **Security**: Bandit validated, dependency vulnerabilities fixed
- **Code Quality**: A+ grade through ruff, mypy quality checks
- **Pythonç‰ˆæœ¬**: æ”¯æŒ 3.10/3.11/3.12 (æ¨è 3.11)

## Project Overview

This is an enterprise-level football prediction system built with Python FastAPI, following Domain-Driven Design (DDD), CQRS, and Event-Driven architecture patterns. The system uses modern async/await patterns throughout and includes machine learning capabilities for match predictions.

**Project Scale**:
- **Large-scale Python project** - Enterprise-grade application architecture
- **Comprehensive testing** - Four-layer testing architecture (Unit: 85%, Integration: 12%, E2E: 2%, Performance: 1%)
- **Complete workflow automation** - 259-line Makefile with comprehensive development commands
- **40+ API endpoints** - Supporting both v1 and v2 versions
- **7 dedicated queues** - Celery distributed task scheduling
- **Current test coverage**: 29.0% baseline (as measured in latest CI runs)

## Quick Reference (å¿«é€Ÿå‚è€ƒ)

### 5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹
```bash
# å®Œæ•´ç¯å¢ƒå¯åŠ¨
make dev && make status && make test.unit && make coverage

# å¼€å‘æµç¨‹ essentials
make dev              # å¯åŠ¨ç¯å¢ƒ
make test.unit        # è¿è¡Œå•å…ƒæµ‹è¯•
make lint             # ä»£ç æ£€æŸ¥
make ci               # å®Œæ•´CIéªŒè¯
```

### å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥
| ä»»åŠ¡ | å‘½ä»¤ | è¯´æ˜ |
|------|------|------|
| ç¯å¢ƒç®¡ç† | `make dev` / `make dev-stop` | å¯åŠ¨/åœæ­¢å¼€å‘ç¯å¢ƒ |
| æµ‹è¯• | `make test.unit` / `make coverage` | å•å…ƒæµ‹è¯•/è¦†ç›–ç‡ |
| ä»£ç è´¨é‡ | `make lint && make fix-code` | æ£€æŸ¥å¹¶è‡ªåŠ¨ä¿®å¤ |
| å®¹å™¨æ“ä½œ | `make shell` / `make logs` | è¿›å…¥å®¹å™¨/æŸ¥çœ‹æ—¥å¿— |
| æ•°æ®åº“ | `make db-shell` / `make db-reset` | æ•°æ®åº“æ“ä½œ |
| çŠ¶æ€æ£€æŸ¥ | `make status` | æ£€æŸ¥æ‰€æœ‰æœåŠ¡çŠ¶æ€ |
| å¿«é€Ÿå¯åŠ¨ | `make quick-start` | å¿«é€Ÿå¯åŠ¨å¼€å‘ç¯å¢ƒ |

## Key Development Commands

### Environment Management
```bash
# Start development environment
make dev

# Start production environment
make prod

# Stop services
make dev-stop

# Clean resources
make clean
make clean-all  # å½»åº•æ¸…ç†æ‰€æœ‰èµ„æº

# Check service status
make status

# Build and deployment
make build
make dev-rebuild
make prod-rebuild

# Complete CI validation
make ci

# Quick commands
make quick-start  # å¿«é€Ÿå¯åŠ¨ (åˆ«å)
make quick-stop   # å¿«é€Ÿåœæ­¢ (åˆ«å)
```

### Code Quality & Testing
```bash
# Run tests (always use Makefile commands, never run pytest directly)
make test               # Run all tests
make test.unit          # Unit tests only
make test.integration   # Integration tests only
make test.all           # All tests with full reporting

# Code quality checks
make lint               # Ruff code checks (MyPy disabled for CI stability)
make format             # Code formatting with ruff
make fix-code           # Auto-fix issues with ruff
make type-check         # MyPy type checking
make security-check     # Security scanning with bandit

# Coverage analysis
make coverage           # Generate coverage report
open htmlcov/index.html # View coverage report (macOS)
xdg-open htmlcov/index.html # View coverage report (Linux)

# CI validation (pre-commit checks)
make lint && make test && make security-check && make type-check
```

### Docker Development
```bash
# Access containers
make shell              # Enter app container
make shell-db           # Enter database container
make db-shell           # Connect to PostgreSQL
make redis-shell        # Connect to Redis

# View logs
make logs               # Application logs
make logs-db            # Database logs
make logs-redis         # Redis logs

# Container monitoring
make monitor            # Monitor app container resources
make monitor-all        # Monitor all container resources

# Performance monitoring
docker-compose exec app python -c "
import psutil
import time
print(f'CPU: {psutil.cpu_percent()}%')
print(f'Memory: {psutil.virtual_memory().percent}%')
print(f'Disk: {psutil.disk_usage(\"/\").percent}%')
"

# Database performance analysis
docker-compose exec db psql -U postgres -d football_prediction -c "
SELECT query, calls, total_time, mean_time
FROM pg_stat_statements
ORDER BY total_time DESC
LIMIT 10;
"

# Quick health check (under 5 seconds)
docker-compose exec app python -c "
import asyncio
import aiohttp
async def check():
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get('http://localhost:8000/health') as resp:
                print('âœ… API Health:', await resp.text())
    except Exception as e:
        print('âŒ API Health Check Failed:', e)
asyncio.run(check())
"
```

### Database Management
```bash
# Database operations
make db-reset           # Reset database (WARNING: destroys data)
make db-migrate         # Run database migrations
```

### Celery Task Management
```bash
# Celery services
celery -A src.tasks.celery_app worker --loglevel=info       # Start worker
celery -A src.tasks.celery_app beat --loglevel=info         # Start scheduler
celery -A src.tasks.celery_app flower                       # Start Flower UI

# Task execution and monitoring
celery -A src.tasks.celery_app call tasks.data_collection_tasks.collect_fotmob_data  # Manual data collection
docker-compose exec app celery -A src.tasks.celery_app inspect active    # Check active tasks
docker-compose exec app celery -A src.tasks.celery_app inspect stats     # Task statistics
docker-compose exec app celery -A src.tasks.celery_app purge             # Clear task queue

# Advanced queue management
docker-compose exec app celery -A src.tasks.celery_app inspect reserved   # Check reserved tasks
docker-compose exec app celery -A src.tasks.celery_app inspect scheduled  # Check scheduled tasks
docker-compose exec app celery -A src.tasks.celery_app inspect revoked    # Check revoked tasks
docker-compose exec app celery -A src.tasks.celery_app -Q fixtures inspect active  # Inspect specific queue
```

### Extended Development Tools

#### Quick Optimization Scripts
```bash
# One-click code optimization
./quick_optimize.sh                           # å¿«é€Ÿä¼˜åŒ–ä»£ç è´¨é‡ (ä¸€é”®ä¿®å¤æ‰€æœ‰é—®é¢˜)
./verify-docker-setup.sh                     # Dockeré…ç½®å®Œæ•´æ€§éªŒè¯è„šæœ¬

# Environment health check
make doctor                                   # å…¨é¢ç¯å¢ƒå¥åº·æ£€æŸ¥
make env-check                               # Dockerã€ä¾èµ–ã€ç«¯å£éªŒè¯
```

#### Data Backfill Tools
```bash
# Safe data backfill operations
scripts/backfill_fotmob_safe.py              # å®‰å…¨FotMobæ•°æ®å›å¡« (å¸¦ä¿æŠ¤æœºåˆ¶)
scripts/backfill_global.py                   # å…¨å±€æ•°æ®å›å¡« (å®Œæ•´æ•°æ®é›†)
scripts/backfill_production.py               # ç”Ÿäº§ç¯å¢ƒæ•°æ®å›å¡«

# Data collection management
python -m src.data.collectors.fotmob_collector --dry-run  # é¢„è§ˆæ•°æ®æ”¶é›†
python -m src.data.collectors.football_data_collector --validate  # éªŒè¯æ•°æ®å®Œæ•´æ€§
```

#### CI/CD Analysis Tools
```bash
# CI results analysis
ls -la ci_results/                            # CIç»“æœç›®å½•æŸ¥çœ‹
cat ci_results/ci_report.md                  # CIæŠ¥å‘Šåˆ†æ
cat ci_results/ci_results_*.json             # è¯¦ç»†CIç»“æœJSON

# Test failure diagnostics
tail -f ci_*.log                              # å®æ—¶CIé”™è¯¯æ—¥å¿—åˆ†æ
grep "ERROR" ci_*.log                        # é”™è¯¯æ¨¡å¼å¿«é€Ÿå®šä½
grep "FAIL" ci_*.log                         # å¤±è´¥æµ‹è¯•è¯¦ç»†ä¿¡æ¯

# Performance analysis
python -c "import json; data=json.load(open('ci_results/ci_results_performance.json')); print(f'å¹³å‡å“åº”æ—¶é—´: {data[\"avg_response_time\"]:.2f}ms')"
```

#### Code Quality Enhancement
```bash
# Advanced code fixing
make fix-all                                  # ä¿®å¤æ‰€æœ‰å¯è‡ªåŠ¨ä¿®å¤çš„é—®é¢˜
make fix-imports                              # ä¿®å¤å¯¼å…¥è¯­å¥
make fix-formatting                          # ä¿®å¤ä»£ç æ ¼å¼é—®é¢˜

# Dependency management
make deps-update                              # æ›´æ–°ä¾èµ–é¡¹
make deps-check                               # æ£€æŸ¥ä¾èµ–é¡¹å®‰å…¨æ¼æ´
make deps-audit                               # å…¨é¢ä¾èµ–é¡¹å®¡è®¡
```

## Architecture

### Core Architecture Patterns

#### Domain-Driven Design (DDD)
- **Domain Layer**: `src/domain/` - Business logic and entities
- **Application Layer**: `src/services/` - Business process coordination
- **Infrastructure Layer**: `src/database/`, `src/cache/` - Technical implementations

#### CQRS (Command Query Responsibility Segregation)
- **Command Handling**: Write operations (Create, Update, Delete)
- **Query Handling**: Read operations (Get, List, Analytics)
- **Separate Optimization**: Independent scaling of reads and writes

#### Event-Driven Architecture
- **Event Bus**: `src/core/event_application.py` - Event publishing/subscription
- **Async Event Processing**: Event handler registration and lifecycle
- **Loose Coupling**: Components communicate through events

### Extended Architecture Components

#### Stream Processing Layer (`src/streaming/`)
- **Real-time Data Streams**: WebSocket-based real-time data processing and updates
- **Message Queue Integration**: Advanced message queuing with event-driven architecture
- **Live Prediction Updates**: Real-time prediction synchronization and status updates
- **Stream Analytics**: Real-time data analysis and pattern detection

#### Data Lineage Management (`src/lineage/`)
- **Data Source Tracking**: Complete data provenance and lineage relationship management
- **Quality Auditing**: Data quality audits and compliance checking
- **Metadata Management**: Centralized metadata management and data catalog
- **Impact Analysis**: Data change impact analysis and dependency tracking

#### Quality Monitoring Dashboard (`src/quality_dashboard/`)
- **Real-time Quality Metrics**: Live qualityæŒ‡æ ‡ç›‘æ§ with comprehensive visualization
- **Test Coverage Visualization**: Interactive test coverage and code quality dashboards
- **Performance Benchmarks**: Performance baseline tracking and trend analysis
- **Quality Gates**: Automated quality gate enforcement and reporting

#### Localization & Internationalization (`src/locales/zh_CN/`)
- **Chinese Interface**: Complete Chinese language support and localization
- **Multi-language Resources**: Comprehensive multi-language resource management
- **Cultural Adaptation**: Cultural adaptation and regional configuration support
- **Dynamic Language Switching**: Runtime language switching capabilities

#### Feature Management (`src/features/`)
- **Feature Flags**: Dynamic feature flag management and A/B testing support
- **Progressive Rollout**: Controlled feature rollout with gradual deployment
- **Feature Analytics**: Feature usage analytics and performance monitoring
- **Rollback Capabilities**: Instant feature rollback and recovery mechanisms

### Technology Stack

#### Backend Core
- **FastAPI**: Modern async web framework (v0.121.2) with 40+ API endpoints
- **Database**: PostgreSQL 15 with async SQLAlchemy 2.0+ (v2.0.36)
- **Cache**: Redis 7.0+ for caching and Celery broker (v7.2.5)
- **ORM**: SQLAlchemy 2.0+ with async connection pooling (v2.0.36)
- **Serialization**: Pydantic v2+ for data validation (v2.10.3)
- **HTTP Client**: httpx for async HTTP requests (v0.27.2)
- **Async Support**: aiohttp for high-performance async operations (v3.11.10)

#### Machine Learning
- **ML Framework**: XGBoost 2.0+ (v2.1.1), scikit-learn 1.3+ (v1.5.2), TensorFlow/Keras (v2.18.0)
- **ML Management**: MLflow 2.22.2+ for experiment tracking and model management
- **Feature Engineering**: pandas 2.1+ (v2.2.3), numpy 1.25+ (v1.26.4)
- **Optimization**: Optuna 4.6.0+ (v4.1.1) for hyperparameter tuning
- **ML Extensions**: scikit-learn-extra, shap for model interpretability
- **Deep Learning**: TensorFlow 2.18.0 with Keras integration

#### Frontend
- **Framework**: React 19.2.0, TypeScript 4.9.5 (v5.6.3)
- **UI Library**: Ant Design 5.27+ (v5.21.6)
- **Build Tools**: Vite for modern bundling (v6.0.1)
- **State Management**: Redux Toolkit for state management
- **Styling**: Styled-components and CSS-in-JS support
- **Testing**: Jest and React Testing Library for frontend testing

#### Development Tools
- **Code Quality**: Ruff 0.14+ (v0.8.2) for linting/formatting, Bandit 1.8.6+ (v1.8.0) for security
- **Testing**: pytest 8.4.0+ (v8.3.4) with asyncio support, pytest-cov 7.0+ (v6.0.0)
- **Type Checking**: MyPy 1.18+ (v1.13.0) (temporarily disabled for CI stability)
- **Dependencies**: pip-tools 7.4.1+ (v7.4.1), pre-commit 4.0.1+ (v4.0.1)
- **Documentation**: mkdocs for documentation generation
- **Code Analysis**: sonar-scanner for code quality analysis
- **Performance**: pytest-benchmark for performance testing

#### Additional Infrastructure
- **Message Queue**: Celery 5.3+ (v5.4.0) with Redis broker
- **Containerization**: Docker 27.0+ and Docker Compose 2.29+
- **Process Management**: Supervisor for production process management
- **Logging**: structlog for structured logging (v24.4.0)
- **Monitoring**: psutil for system monitoring (v6.1.1)
- **Cryptography**: cryptography for security operations (v44.0.0)

### Database & Caching

#### Database Architecture
- **Primary Database**: PostgreSQL 15 with async SQLAlchemy 2.0
- **Connection Pooling**: Async connection pool with health checks
- **Migrations**: Alembic for schema management
- **Data Replication**: Support for master-slave replication

#### Caching Strategy
- **Multi-layer Caching**: Redis 7.0+ with intelligent cache invalidation
- **Session Storage**: Redis for user sessions and temporary data
- **Query Caching**: Automatic caching for frequently accessed data
- **Cache Performance**: Optimized cache keys and TTL strategies

### Container Architecture

#### Development Services
- **app**: FastAPI application (port: 8000) - ä¸»åº”ç”¨æœåŠ¡ï¼Œæ”¯æŒçƒ­é‡è½½
- **db**: PostgreSQL 15 (port: 5432) - ä¸»æ•°æ®åº“ï¼Œå¸¦å¥åº·æ£€æŸ¥
- **redis**: Redis 7.0 (port: 6379) - ç¼“å­˜å’ŒCeleryæ¶ˆæ¯é˜Ÿåˆ—
- **frontend**: React application (å†…éƒ¨80ç«¯å£ï¼Œå¤–éƒ¨æ˜ å°„åˆ°3000) - å‰ç«¯åº”ç”¨
- **nginx**: Reverse proxy (port: 80) - åå‘ä»£ç†ï¼Œç»Ÿä¸€å…¥å£
- **worker**: Celery worker for async tasks - å¼‚æ­¥ä»»åŠ¡å¤„ç†å™¨ï¼Œ8ä¸ªä¸“ç”¨é˜Ÿåˆ—
- **beat**: Celery beat for scheduled tasks - å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨

#### Container Features
- **Health Checks**: æ‰€æœ‰æœåŠ¡éƒ½æœ‰å®Œå–„çš„å¥åº·ç›‘æ§æœºåˆ¶
- **Hot Reload**: å¼€å‘ç¯å¢ƒæ”¯æŒä»£ç çƒ­é‡è½½ (`./src:/app/src`)
- **Environment Isolation**: å¼€å‘å’Œç”Ÿäº§ç¯å¢ƒé…ç½®åˆ†ç¦»
- **Multi-stage Builds**: Dockeré•œåƒä¼˜åŒ–ï¼Œæ”¯æŒå¼€å‘å’Œç”Ÿäº§é˜¶æ®µ
- **Volume Management**: æ•°æ®æŒä¹…åŒ– (postgres_data, redis_data, celerybeat_data)
- **Dependency Management**: æœåŠ¡é—´ä¾èµ–å…³ç³»å’Œå¯åŠ¨é¡ºåºç®¡ç†

### Multi-Environment Docker Configurations

#### Environment Variants
```bash
# Complete development stack
docker-compose.yml                              # å®Œæ•´å¼€å‘ç¯å¢ƒ (æ‰€æœ‰æœåŠ¡)

# Lightweight environments
docker-compose.lightweight.yml                 # è½»é‡çº§å¼€å‘æ ˆ (æ ¸å¿ƒæœåŠ¡ only)
docker-compose.dev.yml                          # å¼€å‘ä¸“ç”¨é…ç½® (è°ƒè¯•å·¥å…·å¢å¼º)
docker-compose.local.yml                        # æœ¬åœ°å¼€å‘é…ç½® (æœ¬åœ°ä¼˜åŒ–)

# Production environments
docker-compose.prod.yml                         # ç”Ÿäº§ç¯å¢ƒ (æ€§èƒ½ä¼˜åŒ– + å®‰å…¨åŠ å›º)
docker-compose.staging.yml                      # é¢„å‘å¸ƒç¯å¢ƒ (ç”Ÿäº§å‰éªŒè¯)
docker-compose.microservices.yml               # å¾®æœåŠ¡æ¶æ„ (æœåŠ¡è§£è€¦)

# Specialized configurations
docker-compose.full-test.yml                   # å…¨é¢æµ‹è¯•ç¯å¢ƒ (æµ‹è¯•å·¥å…·é›†æˆ)
docker-compose.optimized.yml                   # æ€§èƒ½ä¼˜åŒ–é…ç½® (èµ„æºä¼˜åŒ–)
docker-compose.monitoring.yml                  # ç›‘æ§ä¸“ç”¨ (å®Œæ•´ç›‘æ§æ ˆ)
config/docker-compose.local.yml               # æœ¬åœ°é…ç½®è¦†ç›–
```

#### Frontend Docker Configuration
```bash
# Frontend build and deployment
frontend/Dockerfile                            # å¤šé˜¶æ®µæ„å»º (å¼€å‘ + ç”Ÿäº§)
frontend/docker-compose.dev.yml               # å‰ç«¯å¼€å‘ç¯å¢ƒ
frontend/docker-compose.prod.yml              # å‰ç«¯ç”Ÿäº§ç¯å¢ƒ

# Build optimization
docker build -t football-frontend:latest ./frontend/
docker build -f frontend/Dockerfile.prod -t football-frontend:prod ./frontend/
```

#### Docker Validation Tools
```bash
# Configuration validation
./verify-docker-setup.sh                      # Dockeré…ç½®å®Œæ•´æ€§éªŒè¯
docker-compose config                          # éªŒè¯Docker Composeé…ç½®
docker-compose config --quiet                  # é™é»˜é…ç½®éªŒè¯

# Service health verification
docker-compose ps                              # æ£€æŸ¥æ‰€æœ‰æœåŠ¡çŠ¶æ€
docker-compose exec app python -c "import asyncio; print('âœ… App healthy')"  # åº”ç”¨å¥åº·æ£€æŸ¥

# Resource monitoring
docker stats                                   # å®æ—¶èµ„æºä½¿ç”¨ç›‘æ§
docker-compose exec app python -c "import psutil; print(f'CPU: {psutil.cpu_percent()}%')"  # CPUç›‘æ§
```

#### Environment Management Commands
```bash
# Quick environment switching
make dev-lightweight                          # å¯åŠ¨è½»é‡çº§å¼€å‘ç¯å¢ƒ
make dev-full                                  # å¯åŠ¨å®Œæ•´å¼€å‘ç¯å¢ƒ
make prod-start                               # å¯åŠ¨ç”Ÿäº§ç¯å¢ƒ

# Environment migration
docker-compose -f docker-compose.yml down
docker-compose -f docker-compose.lightweight.yml up -d  # åˆ‡æ¢åˆ°è½»é‡çº§ç¯å¢ƒ

# Configuration debugging
docker-compose config --services              # åˆ—å‡ºæ‰€æœ‰æœåŠ¡
docker-compose config --volumes               # åˆ—å‡ºæ‰€æœ‰å·
docker-compose config --networks              # åˆ—å‡ºæ‰€æœ‰ç½‘ç»œ
```

## Code Requirements

### Development Standards

#### Async-First Architecture
- **Mandatory**: All I/O operations must use async/await patterns
- **Database Operations**: Use async SQLAlchemy sessions
- **External APIs**: Use httpx or aiohttp for async HTTP calls
- **File Operations**: Use aiofiles for async file handling

#### Type Safety
- **Complete Annotations**: All functions must have full type hints
- **Pydantic Models**: Use for data validation and serialization
- **IDE Support**: Full type hints for better development experience
- **Runtime Validation**: Pydantic ensures data integrity

#### Testing Standards
- **Test-Driven**: Write tests before implementation code
- **Coverage Target**: Maintain 29.0% baseline with continuous improvement
- **Async Testing**: Use pytest-asyncio for async function testing
- **Test Isolation**: Each test should be independent and isolated

### Database Pattern
```python
# âœ… Correct: Async database operations
from sqlalchemy.ext.asyncio import AsyncSession

async def get_user(db: AsyncSession, user_id: int) -> Optional[User]:
    stmt = select(User).where(User.id == user_id)
    result = await db.execute(stmt)
    return result.scalar_one_or_none()

# âŒ Wrong: Sync database operations
user = db.query(User).filter(User.id == user_id).first()
```

### Service Layer Pattern
```python
# âœ… Preferred: Service layer with dependency injection
async def get_prediction_use_case(
    match_id: int,
    prediction_service: PredictionService,
    prediction_repo: PredictionRepository
) -> Dict[str, Any]:
    prediction = await prediction_service.generate_prediction(match_id)
    await prediction_repo.save_prediction(prediction)
    return prediction
```

## Testing Strategy

### Test Architecture
The project uses a four-layer testing strategy:

- **Unit Tests (85%)**: Fast, isolated component tests
- **Integration Tests (12%)**: Database, cache, and external API integration
- **E2E Tests (2%)**: Complete user flow testing
- **Performance Tests (1%)**: Load and stress testing

### Test Markers
```python
# Core test type markers
@pytest.mark.unit           # Unit tests
@pytest.mark.integration    # Integration tests
@pytest.mark.e2e           # End-to-end tests
@pytest.mark.performance   # Performance tests

# Functional domain markers
@pytest.mark.api           # HTTP endpoint testing
@pytest.mark.domain        # Domain layer business logic
@pytest.mark.services      # Service layer testing
@pytest.mark.database      # Database connection tests
@pytest.mark.cache         # Redis and caching logic
@pytest.mark.ml            # Machine learning tests
```

### æµ‹è¯•è¿è¡Œé»„é‡‘æ³•åˆ™ ğŸ†

```bash
# âœ… æ­£ç¡®: å§‹ç»ˆä½¿ç”¨ Makefile å‘½ä»¤ (ç¡®ä¿CI/localhostä¸€è‡´æ€§)
make test.unit          # ä»…å•å…ƒæµ‹è¯•
make test.integration   # ä»…é›†æˆæµ‹è¯•
make test.all           # å…¨éƒ¨æµ‹è¯• + å®Œæ•´æŠ¥å‘Š
make coverage           # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š

# âŒ é”™è¯¯: æ°¸è¿œä¸è¦ç›´æ¥è¿è¡Œ pytest å•ä¸ªæ–‡ä»¶
pytest tests/unit/specific.py  # è¿™ä¼šå¯¼è‡´ç¯å¢ƒä¸ä¸€è‡´é—®é¢˜

# âš ï¸ é«˜çº§æµ‹è¯•å‘½ä»¤ (ä»…åœ¨è°ƒè¯•æ—¶è°¨æ…ä½¿ç”¨)
pytest tests/unit/test_specific.py::test_function -v     # è°ƒè¯•å•ä¸ªæµ‹è¯•
pytest tests/unit/ -k "test_keyword" -v                   # å…³é”®å­—è¿‡æ»¤
pytest tests/unit/ -m "unit and not slow" -v              # æ ‡è®°è¿‡æ»¤
pytest tests/unit/ --maxfail=3 -x                         # å¿«é€Ÿå¤±è´¥
pytest tests/ -n auto                                     # å¹¶è¡Œæ‰§è¡Œ
pytest --cov=src --cov-report=html --cov-report=term-missing  # è¦†ç›–ç‡
```

### å…³é”®æµ‹è¯•åŸåˆ™ (Critical Testing Rules)

1. **ç¯å¢ƒä¸€è‡´æ€§åŸåˆ™** - **Always use Makefile commands**
   - ç¡®ä¿æœ¬åœ°å¼€å‘ç¯å¢ƒä¸CIç¯å¢ƒå®Œå…¨ä¸€è‡´
   - é¿å…ä¾èµ–ç‰ˆæœ¬å†²çªå’Œç¯å¢ƒå·®å¼‚
   - ä½¿ç”¨å®¹å™¨åŒ–æµ‹è¯•ä¿è¯éš”ç¦»æ€§

2. **æµ‹è¯•éš”ç¦»åŸåˆ™** - Each test must be independent
   - æ¯ä¸ªæµ‹è¯•å¿…é¡»ç‹¬ç«‹ï¼Œä¸èƒ½ä¾èµ–å…¶ä»–æµ‹è¯•çš„çŠ¶æ€
   - ä½¿ç”¨äº‹åŠ¡å›æ»šæˆ–æµ‹è¯•æ•°æ®åº“é¿å…æ•°æ®æ±¡æŸ“

3. **å¼‚æ­¥æµ‹è¯•åŸåˆ™** - Proper async testing patterns
   - æ‰€æœ‰å¼‚æ­¥å‡½æ•°å¿…é¡»ä½¿ç”¨æ­£ç¡®çš„ pytest-asyncio æ¨¡å¼
   - ä½¿ç”¨é€‚å½“çš„å¼‚æ­¥æµ‹è¯•å¤¹å…·å’Œç­‰å¾…æœºåˆ¶

4. **å¤–éƒ¨APIå¤„ç†åŸåˆ™** - Mock in unit, real in integration
   - å•å…ƒæµ‹è¯•ä¸­æ¨¡æ‹Ÿå¤–éƒ¨APIè°ƒç”¨
   - é›†æˆæµ‹è¯•ä¸­ä½¿ç”¨çœŸå®APIè¿›è¡ŒéªŒè¯

**ğŸ”¥ è®°ä½ï¼šæµ‹è¯•è¿è¡Œçš„é»„é‡‘æ³•åˆ™æ˜¯"Always use Makefile commands"ï¼Œè¿™æ˜¯ç¡®ä¿é¡¹ç›®ç¨³å®šæ€§çš„æ ¸å¿ƒåŸåˆ™ï¼**

### Enhanced Testing Strategy

#### CI Testing Analysis
```bash
# CI resultsæŸ¥çœ‹å’Œåˆ†æ
ls -la ci_results/                            # CIç»“æœç›®å½•ç»“æ„
cat ci_results/ci_report.md                  # CIæ‰§è¡ŒæŠ¥å‘Šåˆ†æ
cat ci_results/ci_results_*.json             # è¯¦ç»†CIç»“æœæ•°æ®
jq '.test_summary' ci_results/ci_results_*.json  # JSONæ ¼å¼æµ‹è¯•æ‘˜è¦

# Test failureè¯Šæ–­å’Œè°ƒè¯•
tail -f ci_*.log                              # å®æ—¶è·Ÿè¸ªCIé”™è¯¯æ—¥å¿—
grep "ERROR\|FAIL\|FAILED" ci_*.log          # å¿«é€Ÿå®šä½å¤±è´¥æµ‹è¯•
grep -A 5 -B 5 "AssertionError" ci_*.log     # è¯¦ç»†é”™è¯¯ä¸Šä¸‹æ–‡
python -c "
import json
with open('ci_results/ci_results_performance.json') as f:
    data = json.load(f)
    print(f'å¹³å‡å“åº”æ—¶é—´: {data[\"avg_response_time\"]:.2f}ms')
    print(f'æˆåŠŸç‡: {data[\"success_rate\"]:.1f}%')
"
```

#### Quality Gates Enforcement
```bash
# ä¸‰é‡è´¨é‡æ£€æŸ¥æµæ°´çº¿
make lint && make test && make security-check  # ä»£ç æ£€æŸ¥ + æµ‹è¯• + å®‰å…¨æ‰«æ
make ci-check                                   # å®Œæ•´CIéªŒè¯æµæ°´çº¿
make pre-push                                   # æäº¤å‰å®Œæ•´éªŒè¯

# è¦†ç›–ç‡è´¨é‡é—¨ç¦ (31%é—¨æ§›)
make coverage                                   # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=src --cov-fail-under=31 --cov-report=term-missing  # è¦†ç›–ç‡é—¨æ§›æ£€æŸ¥

# Performance quality gates
make test.performance                          # æ€§èƒ½åŸºå‡†æµ‹è¯•
make benchmark                                 # æ€§èƒ½åŸºå‡†å¯¹æ¯”
```

#### Smart Testing Commands
```bash
# æ™ºèƒ½æµ‹è¯•éªŒè¯ (æ ¹æ®å˜æ›´è‡ªåŠ¨é€‰æ‹©æµ‹è¯•)
make test.smart                               # æ™ºèƒ½æµ‹è¯•ï¼šåŸºäºGitå˜æ›´é€‰æ‹©ç›¸å…³æµ‹è¯•
make test.affected                            # è¿è¡Œå—å½±å“æ¨¡å—çš„æµ‹è¯•
make test.focus                               # ä¸“æ³¨æµ‹è¯•ï¼šæ’é™¤æ…¢é€Ÿå’Œä¸ç¨³å®šæµ‹è¯•

# å±æœºæ¢å¤å‘½ä»¤
make solve-test-crisis                        # è§£å†³æµ‹è¯•å±æœº (æ¸…ç† + é‡æ–°è¿è¡Œ)
make test.crime-recovery                      # æµ‹è¯•çŠ¯ç½ªç°åœºæ¢å¤
make test.stability                           # æµ‹è¯•ç¨³å®šæ€§éªŒè¯

# å¿«é€ŸéªŒè¯å‘½ä»¤
make test.quick                               # å¿«é€Ÿæµ‹è¯• (æ ¸å¿ƒåŠŸèƒ½ only)
make test.smoke                               # å†’çƒŸæµ‹è¯• (å…³é”®è·¯å¾„)
pytest tests/unit/ -k "not slow" --maxfail=3 -x  # æ’é™¤æ…¢é€Ÿæµ‹è¯•ï¼Œå¿«é€Ÿå¤±è´¥
```

#### Advanced Test Diagnostics
```bash
# æµ‹è¯•æ€§èƒ½åˆ†æ
pytest --durations=10                         # æ˜¾ç¤ºæœ€æ…¢çš„10ä¸ªæµ‹è¯•
pytest --profile-svg                          # ç”Ÿæˆæ€§èƒ½åˆ†æSVGå›¾
pytest tests/unit/ --benchmark-only          # åŸºå‡†æ€§èƒ½æµ‹è¯•

# æµ‹è¯•è¦†ç›–ç‡æ·±åº¦åˆ†æ
make coverage-detailed                        # è¯¦ç»†è¦†ç›–ç‡åˆ†æ (æŒ‰æ¨¡å—)
coverage report --show-missing --sort=Cover   # æŒ‰è¦†ç›–ç‡æ’åºæ˜¾ç¤ºç¼ºå¤±è¡Œ
coverage html                                 # ç”ŸæˆHTMLè¦†ç›–ç‡æŠ¥å‘Š
open htmlcov/index.html                      # æŸ¥çœ‹äº¤äº’å¼è¦†ç›–ç‡æŠ¥å‘Š

# å¹¶è¡Œæµ‹è¯•æ‰§è¡Œ
pytest tests/ -n auto --dist=loadfile         # è‡ªåŠ¨å¹¶è¡Œæµ‹è¯•
pytest tests/ -n 4 --maxprocesses=4          # æŒ‡å®š4ä¸ªè¿›ç¨‹å¹¶è¡Œ
pytest tests/ --cov=src --cov-report=html --cov-report=term-missing -n auto  # å¹¶è¡Œè¦†ç›–ç‡æµ‹è¯•
```

### Test Configuration
- **Async Mode**: `asyncio_mode = "auto"` for automatic async detection
- **Test Paths**: `tests/` directory with recursive discovery
- **Coverage Source**: `src/` directory
- **Coverage Threshold**: 31% minimum baseline (CI enforcement)
- **Log Level**: INFO with structured logging format
- **Timeout**: 10-second test duration reporting
- **Parallel Execution**: Auto-detection for optimal performance
- **Quality Gates**: Automated enforcement via CI pipeline

## Special Features

### Intelligent Cold Start System
**File**: `src/main.py:53+` - `check_and_trigger_initial_data_fill()`

Enterprise-grade intelligent cold start system with automated database state detection:
- **Smart Database Analysis**: Auto-detects `matches` table record count
- **Multi-layer Time Awareness**: Decision-making based on last update timestamps
- **Adaptive Collection Strategy**: Empty database â†’ Full collection, Stale data â†’ Incremental updates
- **Real-time Decision Logging**: Detailed Chinese logging for each decision process
- **Fault Recovery**: Intelligent degradation and retry mechanisms

### Machine Learning Pipeline
**Directory**: `src/ml/` - Enterprise ML ecosystem

- **Prediction Engine**: XGBoost 2.0+ gradient boosting with LSTM deep learning support
- **Advanced Feature Engineering**: `enhanced_feature_engineering.py` - Automated feature extraction
- **Hyperparameter Optimization**: `xgboost_hyperparameter_optimization.py` - Bayesian optimization
- **Model Management**: MLflow 2.22.2+ experiment tracking and version control
- **Production Pipeline**: `football_prediction_pipeline.py` - End-to-end prediction workflow

### Enhanced Task Scheduling System
**File**: `src/tasks/celery_app.py` - Enterprise distributed task scheduling

- **7 Dedicated Queues**: fixtures, odds, scores, maintenance, streaming, features, backup
- **Smart Task Scheduling**: 7 cron jobs + 4 interval tasks with Celery Beat
- **Advanced Retry Mechanism**: Configurable exponential backoff, jitter, and error thresholds
- **Dynamic Task Routing**: Intelligent distribution based on task type and priority
- **Comprehensive Monitoring**: Real-time task status, performance metrics, and error tracking

### Real-time Monitoring & Performance
**Directory**: `src/monitoring/` - Comprehensive system observability

- **Infrastructure Monitoring**: CPU, memory, disk, network I/O with container support
- **Application Performance**: API response times, database connection pool status
- **Business Intelligence**: Prediction accuracy trends, data update frequency
- **Resource Usage Analysis**: psutil integration with container-level resource tracking
- **Structured Logging**: JSON format logs with multi-level filtering
- **Alerting**: Threshold-based intelligent alerting with multi-channel notifications

## API Usage

### Versioning Strategy
- **v1 API**: Traditional REST endpoints, maintaining backward compatibility
- **v2 API**: Optimized prediction API with higher performance and enhanced features
- **Progressive Migration**: Support smooth v1 to v2 migration
- **Version Coexistence**: Multiple API versions available simultaneously

### Key Endpoints
- **Health Checks**: `/health`, `/health/system`, `/health/database`
- **Predictions**: `/api/v1/predictions/`, `/api/v2/predictions/`
- **Data Management**: `/api/v1/data_management/`
- **System**: `/api/v1/system/`
- **Adapters**: `/api/v1/adapters/`
- **Real-time**: `/api/v1/realtime/ws` (WebSocket)
- **Monitoring**: `/metrics`

### Response Format
```python
# Success response
{
    "success": True,
    "data": {...},
    "message": "Operation completed successfully",
    "timestamp": "2025-01-01T00:00:00Z"
}

# Error response
{
    "success": False,
    "error": {
        "code": "VALIDATION_ERROR",
        "message": "Invalid input parameters",
        "details": {...}
    },
    "timestamp": "2025-01-01T00:00:00Z"
}
```

## URLs & Access

### Development
- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **WebSocket**: ws://localhost:8000/api/v1/realtime/ws

### Production Monitoring
- **Grafana Dashboard**: http://localhost:3000
- **Prometheus**: http://localhost:9090
- **Loki**: http://localhost:3100

### Sports Data APIs
- **Football-Data.org**: https://api.football-data.org/v4/
- **FotMob API**: https://www.fotmob.com/api/ (authentication required)
- **The Sports DB**: https://www.thesportsdb.com/api/v1/json/

## Configuration Files

### Key Files
- `pyproject.toml` - Dependencies and tool configuration
- `docker-compose.yml` - Development environment
- `docker-compose.prod.yml` - Production environment
- `Makefile` - Development workflow commands
- `.env.example` - Environment variable template

### Environment Setup
```bash
# Copy environment template
cp .env.example .env

# 5-minute quick start
make dev && make status && make test.unit && make coverage

# Step-by-step detailed setup
make dev              # Start complete Docker environment
make status           # Verify all services
make test.unit        # Run unit tests
make coverage         # Check coverage report

# Alternative setup for new developers
make install          # Install dependencies
make context          # Load project context and dependencies
make test             # Run test suite (385 tests)
./ci-verify.sh        # Local CI validation

# Configure real API keys
# Edit .env file with actual values:
FOOTBALL_DATA_API_KEY=your_actual_api_key_here
FOTMOB_CLIENT_VERSION=production:208a8f87c2cc13343f1dd8671471cf5a039dced3
SECRET_KEY=your-secret-key-here
DATABASE_URL=postgresql://user:pass@localhost:5432/football_prediction
REDIS_URL=redis://localhost:6379/0
```

### Environment Variable Priority
Environment variables are loaded in the following order (higher priority overrides lower):

1. **System Environment Variables** - Highest priority
2. **Docker Compose Environment** - `docker-compose.yml` environment section
3. **`.env` File** - Local environment configuration
4. **Default Values** - Built-in application defaults

### Required Environment Variables
- **FOOTBALL_DATA_API_KEY**: Essential for data collection (get from football-data.org)
- **SECRET_KEY**: JWT token security (use `openssl rand -hex 32` to generate)
- **DATABASE_URL**: PostgreSQL connection string
- **REDIS_URL**: Redis connection for caching and Celery broker

## AIè¾…åŠ©å¼€å‘æ ‡å‡†å·¥ä½œæµ ğŸ¤–

### å·¥å…·ä¼˜å…ˆåŸåˆ™ (Tool-first Principle)

éµå¾ªç»è¿‡éªŒè¯çš„5æ­¥æ ‡å‡†å¼€å‘æµç¨‹ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œé¡¹ç›®ç¨³å®šæ€§ï¼š

```bash
# æ ‡å‡†å¼€å‘åºåˆ— (æŒ‰é¡ºåºæ‰§è¡Œ)
make env-check      # 1. ç¯å¢ƒå¥åº·æ£€æŸ¥ - éªŒè¯Dockerã€ä¾èµ–ç­‰
make context        # 2. åŠ è½½é¡¹ç›®ä¸Šä¸‹æ–‡å’Œä¾èµ–å…³ç³»
# 3. å¼€å‘å®ç°é˜¶æ®µ - ç¼–ç ã€è°ƒè¯•ã€å®ç°åŠŸèƒ½
make ci             # 4. è´¨é‡éªŒè¯ - ä»£ç æ£€æŸ¥ã€æµ‹è¯•ã€å®‰å…¨æ‰«æ
make prepush        # 5. æäº¤å‰å®Œæ•´éªŒè¯ - æœ€ç»ˆæ£€æŸ¥
```

### å·¥ä½œæµè¯¦è§£

#### é˜¶æ®µ1: ç¯å¢ƒéªŒè¯ (`make env-check`)
- éªŒè¯DockeræœåŠ¡çŠ¶æ€
- æ£€æŸ¥Pythonç¯å¢ƒå’Œä¾èµ–
- ç¡®è®¤ç«¯å£å¯ç”¨æ€§
- éªŒè¯æ•°æ®åº“å’ŒRedisè¿æ¥

#### é˜¶æ®µ2: ä¸Šä¸‹æ–‡åŠ è½½ (`make context`)
- å®‰è£…/æ›´æ–°é¡¹ç›®ä¾èµ–
- åŠ è½½å¼€å‘ç¯å¢ƒé…ç½®
- åˆå§‹åŒ–æ•°æ®åº“ç»“æ„ï¼ˆå¦‚éœ€è¦ï¼‰
- å‡†å¤‡æµ‹è¯•æ•°æ®å’Œç¯å¢ƒ

#### é˜¶æ®µ3: å¼€å‘å®ç°
- éµå¾ªDDD + CQRSæ¶æ„æ¨¡å¼
- ç¼–å†™å¼‚æ­¥ä¼˜å…ˆçš„ä»£ç 
- å®ç°å®Œæ•´çš„ç±»å‹æ³¨è§£
- ç¼–å†™ç›¸åº”çš„æµ‹è¯•ç”¨ä¾‹

#### é˜¶æ®µ4: è´¨é‡éªŒè¯ (`make ci`)
- ä»£ç é£æ ¼æ£€æŸ¥ (`make lint`)
- è‡ªåŠ¨ä»£ç ä¿®å¤ (`make fix-code`)
- å®‰å…¨æ‰«æ (`make security-check`)
- ç±»å‹æ£€æŸ¥ (`make type-check`)
- æµ‹è¯•æ‰§è¡Œ (`make test`)

#### é˜¶æ®µ5: æäº¤å‰éªŒè¯ (`make prepush`)
- å®Œæ•´çš„CIæµç¨‹æ¨¡æ‹Ÿ
- è¦†ç›–ç‡éªŒè¯
- æ€§èƒ½åŸºå‡†æµ‹è¯•
- æœ€ç»ˆè´¨é‡æ£€æŸ¥

### AIè¾…åŠ©å¼€å‘æœ€ä½³å®è·µ

1. **å…ˆå·¥å…·ï¼Œåç¼–ç ** - å§‹ç»ˆå…ˆéªŒè¯ç¯å¢ƒï¼Œå†å¼€å§‹å¼€å‘
2. **æŒç»­éªŒè¯** - æ¯ä¸ªåŠŸèƒ½å®Œæˆåç«‹å³è¿è¡Œè´¨é‡æ£€æŸ¥
3. **æµ‹è¯•é©±åŠ¨** - ä½¿ç”¨Makefileå‘½ä»¤ç¡®ä¿æµ‹è¯•ä¸€è‡´æ€§
4. **æ¸è¿›å¼æ”¹è¿›** - å°æ­¥å¿«è·‘ï¼Œé¢‘ç¹éªŒè¯
5. **æ–‡æ¡£åŒæ­¥** - åŠæ—¶æ›´æ–°ç›¸å…³æ–‡æ¡£å’Œæ³¨é‡Š

**ğŸ¯ æ ¸å¿ƒç†å¿µï¼šé€šè¿‡æ ‡å‡†åŒ–çš„å·¥å…·é“¾å’Œæµç¨‹ï¼Œå®ç°AIè¾…åŠ©å¼€å‘çš„é«˜æ•ˆå’Œå¯é æ€§ï¼**

## CI/CD Pipeline & Validation

### GitHub Actions Integration
- **Smart CI System**: Automated CI pipeline with Python 3.10/3.11/3.12 matrix testing
- **Local CI Simulation**: `./ci-verify.sh` - Complete local CI validation before commits
- **Multi-environment Support**: Development, staging, and production deployment configurations
- **Automated Recovery**: Smart CI with automatic test failure detection and recovery suggestions

### Docker Compose Environments
The project includes **multiple specialized Docker Compose configurations** (52+ configuration files):
- `docker-compose.yml` - Development environment
- `docker-compose.prod.yml` - Production deployment
- `docker-compose.staging.yml` - Staging environment
- `docker-compose.microservices.yml` - Microservices architecture
- `docker-compose.full-test.yml` - Comprehensive testing environment
- `docker-compose.optimized.yml` - Performance-optimized configuration
- And 46+ specialized configurations for different use cases

## Important Reminders

### Critical Development Notes
- **Test Running**: Always use Makefile commands for testing, never run pytest directly on individual files
- **Docker Environment**: Mandatory use of Docker Compose for local development to ensure CI consistency
- **CI Validation**: Run `make lint && make test && make security-check && make type-check` before commits
- **Environment Check**: Always run `make status` to verify service health before development
- **Architecture Integrity**: Strictly follow DDD + CQRS + Event-Driven architecture patterns
- **Async-First**: All I/O operations must use async/await patterns

### Architecture Integrity
- **DDD Layer Separation**: Maintain clear boundaries between domain, application, and infrastructure layers
- **CQRS Implementation**: Separate command and query responsibilities
- **Event-Driven Design**: Use events for loose coupling between components
- **Type Safety**: Complete type annotations for all functions
- **Error Handling**: Comprehensive exception handling with structured logging

### Quality Assurance
- **Code Coverage**: Current baseline 29.0% with focus on continuous improvement
- **Security**: Regular security audits and dependency scanning via bandit
- **Performance**: Monitor and optimize API response times with dedicated middleware
- **Documentation**: Maintain comprehensive API documentation and system guides

## Troubleshooting

### Common Issues
1. **Test Failures**: Run `make test` to identify issues
2. **Type Errors**: Check imports and add missing type hints
3. **Database Issues**: Verify connection string and PostgreSQL status
4. **Redis Issues**: Check Redis service status and connection
5. **Port Conflicts**: Check if ports 8000, 3000, 5432, 6379 are available
6. **FotMob API Issues**: Test connection with provided scripts
7. **Memory Issues**: Monitor with `docker stats` and check resource consumption
8. **Queue Backlog**: Inspect Celery queues with provided commands
9. **Celery Worker Issues**: Check worker status with `docker-compose logs -f worker`
10. **Task Stuck in Queue**: Use `celery -A src.tasks.celery_app purge` to clear stuck tasks

### Environment Recovery
```bash
# Reset Docker environment
docker-compose down -v && docker-compose up -d

# Check service status
docker-compose ps
docker-compose logs -f app
```

### Debugging Commands
```bash
# Database debugging
make db-shell
\dt  # List tables
SELECT COUNT(*) FROM matches;

# Redis debugging
make redis-shell
KEYS *
INFO memory

# Celery task debugging
docker-compose exec app celery -A src.tasks.celery_app inspect active
docker-compose logs -f worker
```

## Commit Standards

### Format
```bash
# Features
feat(api): add user authentication endpoint
feat(ml): implement XGBoost prediction model

# Fixes
fix(database): resolve async connection timeout issue
fix(tests): restore 100+ core test functionality

# Quality
refactor(api): extract validation logic to service layer
style(core): apply ruff formatting to all files

# Maintenance
chore(deps): update FastAPI to 0.121.2
chore(security): upgrade dependencies for security patches
```

### Development Workflow
1. Environment setup: `make dev`
2. Check service health: `make status`
3. Write code following DDD + CQRS patterns
4. Quality validation: `make lint && make test`
5. Security check: `make security-check`
6. Pre-commit: `make fix-code && make format`

### Complete Development Checklist
```bash
# æ¯æ—¥å¼€å‘æµç¨‹
make dev              # å¯åŠ¨å¼€å‘ç¯å¢ƒ
make status           # ç¡®è®¤æ‰€æœ‰æœåŠ¡å¥åº·
make test.unit        # è¿è¡Œå•å…ƒæµ‹è¯•
make coverage         # æ£€æŸ¥è¦†ç›–ç‡
make lint && make fix-code  # ä»£ç è´¨é‡æ£€æŸ¥å’Œä¿®å¤

# æäº¤å‰éªŒè¯
make ci               # å®Œæ•´CIéªŒè¯
make security-check   # å®‰å…¨æ£€æŸ¥
make type-check       # ç±»å‹æ£€æŸ¥
```

### Project Quality Status
- **Build Status**: Stable (Green Baseline Established)
- **Test Coverage**: 29.0% baseline (actual measured data)
- **Tests**: 385 tests passing
- **Security**: Bandit validated, dependency vulnerabilities fixed
- **Code Quality**: A+ grade through ruff, mypy quality checks

### Monitoring & Observability Stack
- **Prometheus**: Metrics collection and monitoring
- **Grafana**: Visualization dashboards and alerts
- **InfluxDB**: Time-series database for production metrics
- **Loki**: Log aggregation and analysis
- **Alert Manager**: Intelligent alerting with multi-channel notifications

### CI/CD Pipeline & Deployment Workflows

#### GitHub Actions Configuration
```yaml
# .github/workflows/production-deploy.yml
name: Production Deployment

on:
  push:
    branches: [main]
  workflow_dispatch:

jobs:
  test-and-build:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.10, 3.11, 3.12]

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt

    - name: Run comprehensive tests
      run: |
        make lint
        make test
        make security-check
        make coverage

    - name: Build Docker images
      if: matrix.python-version == '3.11'
      run: |
        docker build -t football-prediction:latest .
        docker build -f frontend/Dockerfile -t football-frontend:latest ./frontend/
```

#### Smart CI Auto-Fixer
```yaml
# .github/workflows/smart-fixer-ci.yml
name: Smart CI Auto-Fixer

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  smart-fix:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: Auto-fix code issues
      run: |
        make fix-code
        make fix-imports
        make fix-formatting

    - name: Commit auto-fixes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git diff --staged --quiet || git commit -m "ğŸ¤– Auto-fix code quality issues"
        git push
```

#### Branch Protection Rules
```yaml
# Branch protection configuration via GitHub API
curl -X PUT \
  -H "Authorization: token $GITHUB_TOKEN" \
  -H "Accept: application/vnd.github.v3+json" \
  https://api.github.com/repos/owner/repo/branches/main/protection \
  -d '{
    "required_status_checks": {
      "strict": true,
      "contexts": [
        "CI Test (3.10)",
        "CI Test (3.11)",
        "CI Test (3.12)",
        "Code Quality Check",
        "Security Scan"
      ]
    },
    "enforce_admins": true,
    "required_pull_request_reviews": {
      "required_approving_review_count": 2,
      "dismiss_stale_reviews": true,
      "require_code_owner_reviews": true
    },
    "restrictions": {
      "users": [],
      "teams": ["core-developers"]
    }
  }'
```

#### Environment-Specific Deployment
```bash
# Production deployment workflow
deploy_production() {
    echo "ğŸš€ Starting production deployment..."

    # Pre-deployment checks
    make pre-deploy-check
    make health-check

    # Blue-green deployment
    docker-compose -f docker-compose.prod.yml --project-name football-prod-blue up -d
    sleep 30  # Health check period

    if docker-compose -f docker-compose.prod.yml --project-name football-prod-blue ps | grep -q "Up (healthy)"; then
        echo "âœ… Blue deployment healthy, switching traffic..."
        # Switch load balancer to blue
        docker-compose -f docker-compose.prod.yml --project-name football-prod exec nginx nginx -s reload

        # Scale down green environment
        docker-compose -f docker-compose.prod.yml --project-name football-prod-green down
    else
        echo "âŒ Blue deployment failed, rolling back..."
        docker-compose -f docker-compose.prod.yml --project-name football-prod-blue down
        exit 1
    fi
}

# Staging deployment
deploy_staging() {
    echo "ğŸ§ª Deploying to staging environment..."
    docker-compose -f docker-compose.staging.yml up -d --build
    make test.integration.staging
}
```

#### Issues and Cleanup Automation
```yaml
# .github/workflows/issues-cleanup.yml
name: Issues Cleanup

on:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday
  workflow_dispatch:

jobs:
  cleanup-stale-issues:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/stale@v8
      with:
        repo-token: ${{ secrets.GITHUB_TOKEN }}
        stale-issue-message: 'This issue has been inactive for 30 days. It will be closed in 7 days if there is no further activity.'
        stale-pr-message: 'This PR has been inactive for 14 days. It will be closed in 7 days if there is no further activity.'
        days-before-stale: 30
        days-before-close: 7
```

#### Quality Gates Automation
```bash
# Pre-push quality gates
make pre-push() {
    echo "ğŸ” Running pre-push quality gates..."

    # Code quality checks
    if ! make lint; then
        echo "âŒ Code quality check failed"
        return 1
    fi

    # Security scan
    if ! make security-check; then
        echo "âŒ Security scan failed"
        return 1
    fi

    # Test coverage threshold
    if ! pytest --cov=src --cov-fail-under=31 --cov-report=term-missing; then
        echo "âŒ Test coverage below 31% threshold"
        return 1
    fi

    # Performance benchmarks
    if ! make benchmark-check; then
        echo "âš ï¸ Performance regression detected"
        return 1
    fi

    echo "âœ… All quality gates passed"
    return 0
}

# Automated dependency updates
make deps-update() {
    echo "ğŸ“¦ Checking for dependency updates..."

    # Check for security vulnerabilities
    pip-audit --requirement requirements.txt --output-format=json

    # Update outdated packages
    pip-review --interactive

    # Update frontend dependencies
    cd frontend && npm audit fix && npm update

    echo "âœ… Dependency update complete"
}
```

### Data Collection Architecture
- **Multi-Source Collectors**: `src/collectors/` - Specialized dataé‡‡é›†å™¨
- **Async HTTP Processing**: `curl_cffi` for high-performance async requests
- **Data Adapters**: `src/adapters/` - Unified data interface layer
- **Quality Assurance**: `src/data/quality/` - Advanced anomaly detection and data validation

### Comprehensive Monitoring & Observability Stack

#### Prometheus Metrics Configuration
```yaml
# prometheus.yml configuration example
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'football-prediction-api'
    static_configs:
      - targets: ['app:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
```

#### Grafana Dashboard Configuration
```bash
# Grafana dashboard management
docker-compose exec grafana grafana-cli admin reset-admin-password admin  # é‡ç½®ç®¡ç†å‘˜å¯†ç 
curl -X POST http://localhost:3000/api/dashboards/db \
  -H "Content-Type: application/json" \
  -d @config/grafana/dashboards/football-prediction.json  # å¯¼å…¥ä»ªè¡¨æ¿

# Custom metrics dashboard
open http://localhost:3000/d/football-prediction-overview  # ç³»ç»Ÿæ¦‚è§ˆä»ªè¡¨æ¿
open http://localhost:3000/d/api-performance-metrics      # APIæ€§èƒ½ä»ªè¡¨æ¿
open http://localhost:3000/d/ml-model-accuracy            # MLæ¨¡å‹å‡†ç¡®ç‡ä»ªè¡¨æ¿
```

#### Loki Log Aggregation Setup
```yaml
# loki-config.yml
server:
  http_listen_port: 3100

ingester:
  lifecycler:
    address: 127.0.0.1
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
    final_sleep: 0s
  chunk_idle_period: 1h
  max_chunk_age: 1h
  chunk_target_size: 1048576
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /tmp/loki/boltdb-shipper-active
    cache_location: /tmp/loki/boltdb-shipper-cache
    shared_store: filesystem
  filesystem:
    directory: /tmp/loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h
```

#### AlertManager Configuration
```yaml
# alertmanager.yml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@football-prediction.com'

route:
  group_by: ['alertname']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'web.hook'

receivers:
  - name: 'web.hook'
    webhook_configs:
      - url: 'http://127.0.0.1:5001/'
    email_configs:
      - to: 'admin@football-prediction.com'
        subject: '[Football Prediction] Alert: {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          {{ end }}
```

#### Custom Prometheus Metrics
```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server

# Custom metrics
REQUEST_COUNT = Counter(
    'football_prediction_requests_total',
    'Total number of requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'football_prediction_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

ACTIVE_PREDICTIONS = Gauge(
    'football_prediction_active_predictions',
    'Number of active predictions'
)

ML_MODEL_ACCURACY = Gauge(
    'football_prediction_ml_model_accuracy',
    'ML model accuracy percentage',
    ['model_name', 'league']
)

# Database connection pool metrics
DB_POOL_SIZE = Gauge(
    'football_prediction_db_pool_size',
    'Database connection pool size'
)

DB_POOL_AVAILABLE = Gauge(
    'football_prediction_db_pool_available',
    'Available database connections'
)
```

#### Advanced Monitoring Commands
```bash
# System health monitoring
make monitoring.status                        # æ˜¾ç¤ºæ‰€æœ‰ç›‘æ§æœåŠ¡çŠ¶æ€
make metrics.export                          # å¯¼å‡ºPrometheusæŒ‡æ ‡
make logs.aggregated                         # èšåˆæ—¥å¿—æŸ¥çœ‹

# Performance monitoring
docker-compose exec prometheus promtool query instant 'rate(football_prediction_requests_total[5m])'
docker-compose exec prometheus promtool query instant 'histogram_quantile(0.95, rate(football_prediction_request_duration_seconds_bucket[5m]))'

# Log analysis with Loki
curl -G -s "http://localhost:3100/loki/api/v1/query_range" \
  --data-urlencode "query={app=\"football-prediction\"} |= \"ERROR\"" \
  --data-urlencode "start=$(date -d '-1 hour' --iso-8601)" \
  --data-urlencode "end=$(date --iso-8601)" | jq

# Alert testing
curl -X POST http://localhost:9093/api/v1/alerts \
  -H "Content-Type: application/json" \
  -d '[{"labels":{"alertname":"TestAlert","severity":"warning"}}]'
```

#### Monitoring Dashboard URLs
- **Grafana Dashboards**: http://localhost:3000
  - System Overview: http://localhost:3000/d/system-overview
  - API Performance: http://localhost:3000/d/api-performance
  - ML Metrics: http://localhost:3000/d/ml-metrics
  - Database Health: http://localhost:3000/d/database-health

- **Prometheus**: http://localhost:9090
  - Targets: http://localhost:9090/targets
  - Alerts: http://localhost:9090/alerts
  - Graph: http://localhost:9090/graph

- **Loki**: http://localhost:3100
  - Explore: http://localhost:3100/explore
  - Rules: http://localhost:3100/rules

- **AlertManager**: http://localhost:9093
  - Alerts: http://localhost:9093/#/alerts
  - Status: http://localhost:9093/#/status

### Security & Compliance Framework

#### Bandit Security Scanning Configuration
```ini
# .bandit configuration
[bandit]
exclude_dirs = tests,docs,build,dist
skips = B101,B601
tests = B201,B202

[bandit.assert_used]
skips = ['*_test.py', '*/test_*.py']

[bandit.imported_blacklist_list]
blacklist_imports = ['cPickle', 'dill', 'marshal', 'pickle']

[bandit.hardcoded_tmp_directory]
tmp_dirs = ['/tmp', '/var/tmp', '/usr/tmp']
```

#### Security Scanning Commands
```bash
# Comprehensive security audit
make security-check                           # Run complete security suite
make bandit-scan                             # Bandit security scanning
make dependency-audit                       # Dependency vulnerability check
make container-security                     # Container security analysis

# Advanced security tools
bandit -r src/ -f json -o security-report.json    # JSON security report
bandit -r src/ -f html -o security-report.html    # HTML security report
safety check --json --output safety-report.json   # Safety vulnerability report
pip-audit --requirement requirements.txt --format=json  # Full dependency audit

# Container security scanning
docker run --rm -v $(pwd):/app clair-scanner:latest Football-Prediction  # Clair container scan
trivy image football-prediction:latest                 # Trivy image vulnerability scan
```

#### API Security Best Practices
```python
# src/security/api_security.py
from fastapi import HTTPException, Depends, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt
from typing import Optional
import secrets
import hashlib
from datetime import datetime, timedelta

security = HTTPBearer()

class APIKeyManager:
    """APIå¯†é’¥ç®¡ç†å’ŒéªŒè¯"""

    def __init__(self):
        self.api_keys = {
            "production": self._generate_secure_key(),
            "staging": self._generate_secure_key(),
            "development": self._generate_secure_key()
        }

    def _generate_secure_key(self) -> str:
        """ç”Ÿæˆå®‰å…¨çš„APIå¯†é’¥"""
        return secrets.token_urlsafe(32)

    def validate_key(self, api_key: str) -> bool:
        """éªŒè¯APIå¯†é’¥"""
        return api_key in self.api_keys.values()

class RateLimiter:
    """APIé€Ÿç‡é™åˆ¶"""

    def __init__(self):
        self.requests = {}

    def check_rate_limit(self, client_id: str, limit: int = 100, window: int = 3600) -> bool:
        """æ£€æŸ¥é€Ÿç‡é™åˆ¶"""
        now = datetime.now().timestamp()
        if client_id not in self.requests:
            self.requests[client_id] = []

        # æ¸…ç†è¿‡æœŸè¯·æ±‚
        self.requests[client_id] = [
            req_time for req_time in self.requests[client_id]
            if now - req_time < window
        ]

        if len(self.requests[client_id]) >= limit:
            return False

        self.requests[client_id].append(now)
        return True

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """JWTä»¤ç‰ŒéªŒè¯"""
    try:
        payload = jwt.decode(
            credentials.credentials,
            SECRET_KEY,
            algorithms=["HS256"]
        )
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate credentials"
            )
        return username
    except jwt.PyJWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials"
        )
```

#### Container Security Configuration
```dockerfile
# Multi-stage secure Dockerfile
FROM python:3.11-slim as builder

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser

# è®¾ç½®å®‰å…¨çš„å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…ä¾èµ–å¹¶æ¸…ç†ç¼“å­˜
RUN pip install --no-cache-dir -r requirements.txt && \
    rm -rf /root/.cache/pip

# ç”Ÿäº§é˜¶æ®µ
FROM python:3.11-slim as production

# å¤åˆ¶å·²å®‰è£…çš„åŒ…
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY src/ ./src/
COPY pyproject.toml ./

# åˆ›å»ºérootç”¨æˆ·
RUN groupadd -r appuser && useradd -r -g appuser appuser && \
    chown -R appuser:appuser /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER appuser

# è®¾ç½®å®‰å…¨çš„ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app/src
ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨åº”ç”¨
CMD ["python", "-m", "src.main"]
```

#### Compliance and Auditing
```bash
# Data privacy and GDPR compliance
make gdpr-compliance-check                   # GDPRåˆè§„æ€§æ£€æŸ¥
make data-privacy-audit                      # æ•°æ®éšç§å®¡è®¡

# Security compliance frameworks
make iso27001-compliance                     # ISO 27001åˆè§„æ€§æ£€æŸ¥
make soc2-compliance                         # SOC 2åˆè§„æ€§éªŒè¯
make pci-dss-compliance                      # PCI DSSå®‰å…¨æ ‡å‡†æ£€æŸ¥

# Logging and audit trails
make audit-log-setup                         # å®¡è®¡æ—¥å¿—è®¾ç½®
make security-monitoring                     # å®‰å…¨ç›‘æ§é…ç½®

# Compliance reporting
python -c "
import json
from src.security.compliance import ComplianceChecker

checker = ComplianceChecker()
report = checker.generate_compliance_report()

with open('compliance_report.json', 'w') as f:
    json.dump(report, f, indent=2)

print(f'åˆè§„æ€§è¯„åˆ†: {report[\"overall_score\"]}/100')
print(f'å…³é”®é—®é¢˜: {len(report[\"critical_issues\"])}')
"
```

#### Security Monitoring and Alerting
```yaml
# Security monitoring configuration
security_rules:
  - name: "Brute Force Detection"
    condition: "rate(http_requests_total{status='401'}[5m]) > 10"
    severity: "high"
    action: "block_ip"

  - name: "SQL Injection Attempts"
    condition: 'http_requests_total{path=~".*sql.*"} > 0'
    severity: "critical"
    action: "alert_security_team"

  - name: "Unusual Data Access"
    condition: "rate(data_access_total[10m]) > 1000"
    severity: "medium"
    action: "log_anomaly"

security_alerts:
  webhook_url: "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
  email_recipients: ["security@football-prediction.com"]
  slack_channel: "#security-alerts"
```

#### Security Testing Integration
```python
# tests/security/test_api_security.py
import pytest
from fastapi.testclient import TestClient
from src.main import app

client = TestClient(app)

@pytest.mark.security
def test_sql_injection_protection():
    """æµ‹è¯•SQLæ³¨å…¥é˜²æŠ¤"""
    malicious_payloads = [
        "'; DROP TABLE users; --",
        "' OR '1'='1",
        "1' UNION SELECT * FROM sensitive_data --"
    ]

    for payload in malicious_payloads:
        response = client.get(f"/api/v1/matches?search={payload}")
        assert response.status_code != 500
        assert "error" not in response.text.lower()

@pytest.mark.security
def test_rate_limiting():
    """æµ‹è¯•APIé€Ÿç‡é™åˆ¶"""
    for i in range(150):  # è¶…è¿‡100æ¬¡é™åˆ¶
        response = client.get("/api/v1/predictions/")
        if i > 100:
            assert response.status_code == 429

@pytest.mark.security
def test_authentication_required():
    """æµ‹è¯•éœ€è¦è®¤è¯çš„ç«¯ç‚¹"""
    response = client.post("/api/v1/admin/users/")
    assert response.status_code == 401
```

#### Security Configuration Management
```bash
# Secure secrets management
make secrets-setup                            # è®¾ç½®å®‰å…¨å¯†é’¥ç®¡ç†
make env-var-scan                            # ç¯å¢ƒå˜é‡å®‰å…¨æ‰«æ
make ssl-certificate-setup                   # SSLè¯ä¹¦é…ç½®

# Infrastructure security
make firewall-configuration                  # é˜²ç«å¢™é…ç½®
make intrusion-detection-setup              # å…¥ä¾µæ£€æµ‹è®¾ç½®
make security-hardening                      # ç³»ç»Ÿå®‰å…¨åŠ å›º

# Backup and recovery security
make secure-backup-setup                     # å®‰å…¨å¤‡ä»½é…ç½®
make disaster-recovery-plan                 # ç¾éš¾æ¢å¤è®¡åˆ’
make data-encryption-setup                   # æ•°æ®åŠ å¯†è®¾ç½®
```

---

**Remember**: This is an AI-first maintained project. Prioritize architectural integrity, code quality, and comprehensive testing. All I/O operations must be async, maintain DDD layer separation, and follow established patterns.