# CLAUDE_DETAILED.md

**è¯¦ç»†å‚è€ƒæ–‡æ¡£** - åŒ…å«å®Œæ•´çš„ä»£ç ç¤ºä¾‹ã€é…ç½®å‚æ•°å’Œæ•…éšœæ’é™¤æŒ‡å—

> ğŸ“– **å¿«é€Ÿå¯¼èˆª**: å¦‚éœ€æ ¸å¿ƒæ¦‚å¿µå’Œå¿«é€Ÿå…¥é—¨ï¼Œè¯·æŸ¥çœ‹ [CLAUDE.md](./CLAUDE.md)

---

## ğŸ“‘ ç›®å½•

- [ğŸ”§ ä»£ç ç¤ºä¾‹](#-ä»£ç ç¤ºä¾‹)
- [âš™ï¸ é…ç½®æ–‡ä»¶è¯¦è§£](#ï¸-é…ç½®æ–‡ä»¶è¯¦è§£)
- [ğŸš€ æ€§èƒ½ä¼˜åŒ–](#-æ€§èƒ½ä¼˜åŒ–)
- [ğŸ”§ è´¨é‡ä¿®å¤](#-è´¨é‡ä¿®å¤)
- [ğŸš¨ æ•…éšœæ’é™¤](#-æ•…éšœæ’é™¤)
- [ğŸ” é«˜çº§æµ‹è¯•ç­–ç•¥](#-é«˜çº§æµ‹è¯•ç­–ç•¥)
- [ğŸ³ Dockerå’Œéƒ¨ç½²](#-dockerå’Œéƒ¨ç½²)
- [ğŸ“Š ç›‘æ§å’ŒæŒ‡æ ‡](#-ç›‘æ§å’ŒæŒ‡æ ‡)

---

## ğŸ”§ ä»£ç ç¤ºä¾‹

### ç­–ç•¥å·¥å‚æ¨¡å¼

**ä½ç½®**: `src/domain/strategies/factory.py:35`

```python
from src.domain.strategies.factory import PredictionStrategyFactory
from src.domain.strategies.base import StrategyType
from src.domain.services.prediction_service import PredictionService

async def main():
    # åˆ›å»ºç­–ç•¥å·¥å‚ï¼ˆæ”¯æŒé…ç½®æ–‡ä»¶ï¼‰
    factory = PredictionStrategyFactory(config_path="config/strategies.yaml")

    # åŠ¨æ€é€‰æ‹©ç­–ç•¥ç±»å‹
    strategies = [
        ("ml_model", "enhanced_ml_model"),
        ("statistical", "poisson_distribution"),
        ("historical", "head_to_head"),
        ("ensemble", "weighted_voting")
    ]

    for strategy_type, strategy_name in strategies:
        strategy = await factory.create_strategy(strategy_type, strategy_name)
        service = PredictionService(strategy)

        # æ‰§è¡Œé¢„æµ‹
        prediction_data = {
            "match_id": 123,
            "home_team": "Team A",
            "away_team": "Team B",
            "league": "Premier League",
            "season": "2024-25"
        }
        prediction = await service.create_prediction(prediction_data)
        print(f"Strategy {strategy_name}: {prediction.confidence:.2f}")
```

### ä¾èµ–æ³¨å…¥å®¹å™¨

**ä½ç½®**: `src/core/di.py:23`

```python
from src.core.di import DIContainer, ServiceCollection, ServiceLifetime
from src.database.manager import DatabaseManager
from src.database.unit_of_work import UnitOfWork
from src.domain.services.prediction_service import PredictionService
from src.cache.redis_client import RedisClient

# é…ç½®æœåŠ¡å®¹å™¨ï¼ˆä¸‰ç§ç”Ÿå‘½å‘¨æœŸï¼‰
container = ServiceCollection()

# å•ä¾‹æ¨¡å¼ - å…¨å±€å”¯ä¸€å®ä¾‹
container.add_singleton(DatabaseManager)
container.add_singleton(RedisClient)

# ä½œç”¨åŸŸæ¨¡å¼ - æ¯ä¸ªè¯·æ±‚ä½œç”¨åŸŸå†…å”¯ä¸€
container.add_scoped(UnitOfWork)

# ç¬æ—¶æ¨¡å¼ - æ¯æ¬¡è¯·æ±‚åˆ›å»ºæ–°å®ä¾‹
container.add_transient(PredictionService)

# æ„å»ºå®¹å™¨å¹¶è§£ææœåŠ¡
di_container = container.build_container()

# åœ¨åº”ç”¨å¯åŠ¨æ—¶è§£ææœåŠ¡
async def initialize_app():
    db_manager = di_container.resolve(DatabaseManager)
    await db_manager.initialize()

    redis_client = di_container.resolve(RedisClient)
    await redis_client.connect()

# åœ¨APIç«¯ç‚¹ä¸­ä½¿ç”¨
async def create_prediction(request: PredictionRequest):
    # æ¯æ¬¡è¯·æ±‚éƒ½ä¼šåˆ›å»ºæ–°çš„UnitOfWorkå’ŒPredictionService
    unit_of_work = di_container.resolve(UnitOfWork)
    prediction_service = di_container.resolve(PredictionService)

    async with unit_of_work:
        return await prediction_service.create_prediction(request)
```

### CQRSæ¨¡å¼

**ä½ç½®**: `src/cqrs/bus.py:17`

```python
from src.cqrs.bus import CommandBus, QueryBus
from src.cqrs.commands import CreatePredictionCommand, UpdatePredictionCommand
from src.cqrs.queries import GetPredictionQuery, ListPredictionsQuery
from src.cqrs.handlers import (
    CreatePredictionHandler,
    UpdatePredictionHandler,
    GetPredictionHandler,
    ListPredictionsHandler
)

# åˆå§‹åŒ–æ€»çº¿å¹¶æ³¨å†Œå¤„ç†å™¨
command_bus = CommandBus()
query_bus = QueryBus()

# æ³¨å†Œå‘½ä»¤å¤„ç†å™¨ï¼ˆå†™æ“ä½œï¼‰
command_bus.register_handler(CreatePredictionCommand, CreatePredictionHandler())
command_bus.register_handler(UpdatePredictionCommand, UpdatePredictionHandler())

# æ³¨å†ŒæŸ¥è¯¢å¤„ç†å™¨ï¼ˆè¯»æ“ä½œï¼‰
query_bus.register_handler(GetPredictionQuery, GetPredictionHandler())
query_bus.register_handler(ListPredictionsQuery, ListPredictionsHandler())

# ä½¿ç”¨ç¤ºä¾‹ï¼šåˆ›å»ºé¢„æµ‹ï¼ˆå‘½ä»¤ï¼‰
async def create_new_prediction(match_data: dict):
    create_cmd = CreatePredictionCommand(
        match_id=match_data["match_id"],
        home_team=match_data["home_team"],
        away_team=match_data["away_team"],
        predicted_home_score=match_data["predicted_home_score"],
        predicted_away_score=match_data["predicted_away_score"],
        confidence=match_data["confidence"],
        strategy_used=match_data["strategy"]
    )

    # å‘½ä»¤æ‰§è¡Œï¼Œè¿”å›é¢„æµ‹ID
    result = await command_bus.dispatch(create_cmd)
    return result.prediction_id

# ä½¿ç”¨ç¤ºä¾‹ï¼šæŸ¥è¯¢é¢„æµ‹ï¼ˆæŸ¥è¯¢ï¼‰
async def get_prediction_details(prediction_id: int):
    query = GetPredictionQuery(prediction_id=prediction_id)
    prediction = await query_bus.dispatch(query)
    return prediction

# æ‰¹é‡æŸ¥è¯¢ç¤ºä¾‹
async def get_user_predictions(user_id: int, limit: int = 10):
    query = ListPredictionsQuery(user_id=user_id, limit=limit)
    predictions = await query_bus.dispatch(query)
    return predictions

# ä¸­é—´ä»¶æ”¯æŒï¼ˆæ—¥å¿—ã€ç¼“å­˜ã€éªŒè¯ï¼‰
command_bus.register_middleware(LoggingMiddleware())
command_bus.register_middleware(ValidationMiddleware())
query_bus.register_middleware(CachingMiddleware(ttl=300))
```

---

## âš™ï¸ é…ç½®æ–‡ä»¶è¯¦è§£

### pyproject.toml

```toml
[build-system]
requires = ["setuptools>=45", "wheel", "setuptools_scm[toml]>=6.2"]
build-backend = "setuptools.build_meta"

[project]
name = "football-prediction"
version = "1.0.0"
description = "ä¼ä¸šçº§è¶³çƒé¢„æµ‹ç³»ç»Ÿ"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Football Prediction Team", email = "team@footballprediction.com"}
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
]
requires-python = ">=3.11"
dependencies = [
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "sqlalchemy>=2.0.0",
    "alembic>=1.12.0",
    "pydantic>=2.5.0",
    "redis>=5.0.0",
    "psycopg2-binary>=2.9.0",
    "python-multipart>=0.0.6",
    "python-jose[cryptography]>=3.3.0",
    "passlib[bcrypt]>=1.7.4",
    "python-dotenv>=1.0.0",
    "httpx>=0.25.0",
    "pandas>=2.1.0",
    "numpy>=1.25.0",
    "scikit-learn>=1.3.0",
]

[project.optional-dependencies]
dev = [
    # æµ‹è¯•å·¥å…·
    "pytest>=8.4.2",
    "pytest-asyncio>=1.2.0",
    "pytest-cov>=7.0.0",
    "pytest-mock>=3.14.0",
    "pytest-xdist>=3.6.1",
    "factory-boy>=3.3.1",

    # ä»£ç è´¨é‡å’Œæ ¼å¼åŒ–
    "ruff>=0.14.3",
    "mypy>=1.18.2",
    "bandit>=1.8.6",

    # å¼€å‘å·¥å…·
    "pre-commit>=4.0.1",
    "pip-audit>=2.6.0",
    "pip-tools>=7.4.1",
    "ipython>=8.31.0",

    # æ–‡æ¡£å·¥å…·
    "mkdocs>=1.6.1",
    "mkdocs-material>=9.5.49",
]

[tool.setuptools.packages.find]
where = ["src"]

[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "W", "I", "N", "UP", "B", "A", "C4", "T20", "D"]
ignore = ["E501", "B008"]

[tool.ruff.lint.pydocstyle]
# ä½¿ç”¨ Google é£æ ¼çš„ docstrings
convention = "google"
exclude = [
    ".bzr", ".direnv", ".eggs", ".git", ".hg", ".mypy_cache",
    ".nox", ".pants.d", ".ruff_cache", ".svn", ".tox", ".venv",
    "__pypackages__", "_build", "buck-out", "dist", "node_modules", "venv",
]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
strict_optional = true
warn_no_return = true
warn_redundant_casts = true
warn_unused_ignores = true
check_untyped_defs = true
exclude = [
    "tests/", "scripts/", "docs/", "htmlcov/",
]

[[tool.mypy.overrides]]
module = [
    "pandas.*", "numpy.*", "sklearn.*",
    "redis.*", "psycopg2.*"
]
ignore_missing_imports = true

[tool.bandit]
exclude_dirs = ["tests", "build", "dist"]
skips = ["B101", "B601"]  # è·³è¿‡assert_testå’Œshell_injectionæ£€æŸ¥

[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --strict-markers --strict-config"
testpaths = ["tests"]
python_files = ["test_*.py", "*_test.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
markers = [
    "unit: Unit tests",
    "integration: Integration tests",
    "e2e: End-to-end tests",
    "slow: Slow running tests",
    "api: API related tests",
    "database: Database related tests",
    "auth: Authentication tests",
    "critical: Critical functionality tests",
    "smoke: Basic functionality verification",
    "regression: Regression tests",
    "performance: Performance tests",
    "metrics: Metrics and measurement tests",
    "edge_cases: Edge cases and boundary conditions",
    "asyncio: Async function tests",
    "external_api: Tests requiring external API calls",
    "docker: Tests requiring Docker environment",
    "network: Tests requiring network connection",
    "domain: Domain layer tests",
    "business: Business logic tests",
    "services: Service layer tests",
    "cache: Cache related tests",
    "monitoring: Monitoring related tests",
    "streaming: Streaming tests",
    "collectors: Data collector tests",
    "middleware: Middleware tests",
    "utils: Utility function tests",
    "core: Core module tests",
    "decorators: Decorator tests",
    "health: Health check tests",
    "validation: Validation tests",
    "ml: Machine learning tests",
    "issue94: Issue #94 API module fixes",
]

[tool.coverage.run]
source = ["src"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
    "*/venv/*",
    "*/.venv/*",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "raise AssertionError",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
]
```

### pytest.ini

```ini
[pytest]
# Smart Testsä¼˜åŒ–é…ç½®
addopts = --cov=src --cov-fail-under=40 --cov-report=term-missing

# æµ‹è¯•è·¯å¾„é…ç½®
testpaths = tests

# Pythonæ–‡ä»¶åŒ¹é…è§„åˆ™
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# ä¸¥æ ¼æ ‡è®°å’Œé…ç½®æ¨¡å¼
strict-markers = true
strict-config = true

# 40ä¸ªæ ‡å‡†åŒ–æµ‹è¯•æ ‡è®°å®šä¹‰
markers =
    # æ ¸å¿ƒç±»å‹æ ‡è®°ï¼ˆ4ä¸ªï¼‰
    unit: Unit tests (85% of tests)
    integration: Integration tests (12% of tests)
    e2e: End-to-end tests (2% of tests)
    performance: Performance tests (1% of tests)

    # åŠŸèƒ½åŸŸæ ‡è®°ï¼ˆ18ä¸ªï¼‰
    api: API tests - HTTP endpoints and interfaces
    domain: Domain layer tests - Business logic and algorithms
    business: Business rules tests - Business logic and rule engine
    services: Service layer tests - Business services and data processing
    database: Database tests - Requires database connection
    cache: Cache related tests - Redis and caching logic
    auth: Authentication related tests - JWT and permission validation
    monitoring: Monitoring related tests - Metrics and health checks
    streaming: Streaming tests - Kafka and real-time data
    collectors: Collector tests - Data collection and scraping modules
    middleware: Middleware tests - Request processing and pipeline components
    utils: Utility class tests - Common utilities and helper functions
    core: Core module tests - Configuration, dependency injection, infrastructure
    decorators: Decorator tests - Various decorator functions and performance tests
    health: Health check related tests
    validation: Validation and confirmation tests
    ml: Machine learning tests - ML model training, prediction and evaluation tests

    # æ‰§è¡Œç‰¹å¾æ ‡è®°ï¼ˆ9ä¸ªï¼‰
    slow: Slow tests - Running time >30s
    smoke: Smoke tests - Basic functionality verification
    critical: Critical tests - Must-pass core functionality tests
    regression: Regression tests - Verify fixed issues don't reoccur
    metrics: Metrics and measurement tests - Performance metrics and progress verification
    edge_cases: Edge cases tests - Extreme values and exception handling
    asyncio: Async tests - Test async functions and coroutines

    # ç¯å¢ƒä¾èµ–æ ‡è®°ï¼ˆ3ä¸ªï¼‰
    external_api: Requires external API calls
    docker: Requires Docker container environment
    network: Requires network connection

    # é—®é¢˜ç‰¹å®šæ ‡è®°ï¼ˆ1ä¸ªï¼‰
    issue94: Issue #94 API module systematic fixes

# Smart Testså…·ä½“é…ç½®
[tool:pytest_smart_tests]
# æ ¸å¿ƒç¨³å®šæµ‹è¯•æ¨¡å—ï¼ˆæ‰§è¡Œæ—¶é—´<2åˆ†é’Ÿï¼‰
testpaths_smarts = tests/unit/utils tests/unit/cache tests/unit/core

# æ’é™¤çš„é—®é¢˜æµ‹è¯•æ–‡ä»¶
ignore_files_smarts =
    tests/unit/services/test_prediction_service.py
    tests/unit/core/test_di.py
    tests/unit/core/test_path_manager_enhanced.py

# æ€§èƒ½ä¼˜åŒ–é…ç½®
addopts_smarts = -v --tb=short --maxfail=20 -m "not slow"
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### æ•°æ®åº“ä¼˜åŒ–

**ä½ç½®**: `src/core/config/database.py`

```python
from sqlalchemy import create_async_engine
from sqlalchemy.pool import QueuePool
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine

# é«˜æ€§èƒ½æ•°æ®åº“è¿æ¥é…ç½®
DATABASE_CONFIG = {
    "pool_size": 20,           # è¿æ¥æ± å¤§å°
    "max_overflow": 30,        # æœ€å¤§æº¢å‡ºè¿æ¥æ•°
    "pool_timeout": 30,        # è·å–è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    "pool_recycle": 3600,      # è¿æ¥å›æ”¶æ—¶é—´ï¼ˆç§’ï¼‰
    "pool_pre_ping": True,     # è¿æ¥å‰pingæ£€æŸ¥
    "echo": False,             # ç”Ÿäº§ç¯å¢ƒå…³é—­SQLæ—¥å¿—
}

# åˆ›å»ºé«˜æ€§èƒ½å¼•æ“
engine = create_async_engine(
    SQLALCHEMY_DATABASE_URL,
    poolclass=QueuePool,
    **DATABASE_CONFIG
)

# æ‰¹é‡æ“ä½œä¼˜åŒ–
async def bulk_create_predictions(predictions: List[dict]):
    """æ‰¹é‡åˆ›å»ºé¢„æµ‹ï¼Œæå‡æ’å…¥æ€§èƒ½"""
    async with AsyncSession(engine) as session:
        try:
            # ä½¿ç”¨æ‰¹é‡æ’å…¥è€Œéå•æ¡æ’å…¥
            await session.execute(
                insert(Prediction).values(predictions)
            )
            await session.commit()
        except Exception as e:
            await session.rollback()
            raise e

# æŸ¥è¯¢ä¼˜åŒ–
async def get_predictions_with_pagination(
    page: int = 1,
    size: int = 20,
    filters: dict = None
):
    """åˆ†é¡µæŸ¥è¯¢ä¼˜åŒ–"""
    query = select(Prediction)

    # åº”ç”¨è¿‡æ»¤å™¨
    if filters:
        if filters.get("team"):
            query = query.where(
                or_(
                    Prediction.home_team == filters["team"],
                    Prediction.away_team == filters["team"]
                )
            )
        if filters.get("date_from"):
            query = query.where(
                Prediction.match_date >= filters["date_from"]
            )

    # åˆ†é¡µ
    offset = (page - 1) * size
    query = query.offset(offset).limit(size)

    result = await session.execute(query)
    return result.scalars().all()
```

### Redisç¼“å­˜ä¼˜åŒ–

**ä½ç½®**: `src/cache/redis_client.py`

```python
import redis.asyncio as redis
from redis.asyncio import ConnectionPool
from typing import Optional, Any
import json
import pickle

# é«˜æ€§èƒ½Redisè¿æ¥æ± é…ç½®
REDIS_CONFIG = {
    "host": "localhost",
    "port": 6379,
    "max_connections": 50,     # æœ€å¤§è¿æ¥æ•°
    "retry_on_timeout": True,  # è¶…æ—¶é‡è¯•
    "socket_timeout": 5,       # Socketè¶…æ—¶
    "socket_connect_timeout": 5,
    "health_check_interval": 30,  # å¥åº·æ£€æŸ¥é—´éš”
    "decode_responses": False,  # ä¿æŒbytesä»¥ä¾¿pickle
}

# åˆ›å»ºè¿æ¥æ± 
pool = ConnectionPool(**REDIS_CONFIG)
redis_client = redis.Redis(connection_pool=pool)

class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.default_ttl = 3600  # é»˜è®¤1å°æ—¶

    async def get_prediction(self, prediction_id: int) -> Optional[dict]:
        """è·å–é¢„æµ‹ç¼“å­˜"""
        key = f"prediction:{prediction_id}"
        data = await self.redis.get(key)
        return pickle.loads(data) if data else None

    async def set_prediction(
        self,
        prediction_id: int,
        prediction_data: dict,
        ttl: int = None
    ):
        """è®¾ç½®é¢„æµ‹ç¼“å­˜"""
        key = f"prediction:{prediction_id}"
        ttl = ttl or self.default_ttl
        await self.redis.setex(
            key,
            ttl,
            pickle.dumps(prediction_data)
        )

    async def invalidate_user_cache(self, user_id: int):
        """å¤±æ•ˆç”¨æˆ·ç›¸å…³ç¼“å­˜"""
        pattern = f"user:{user_id}:*"
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)

    async def cache_team_stats(
        self,
        team_id: int,
        stats_data: dict,
        ttl: int = 7200  # 2å°æ—¶
    ):
        """ç¼“å­˜çƒé˜Ÿç»Ÿè®¡æ•°æ®"""
        key = f"team_stats:{team_id}"
        await self.redis.setex(
            key,
            ttl,
            json.dumps(stats_data)
        )

    async def get_team_stats(self, team_id: int) -> Optional[dict]:
        """è·å–çƒé˜Ÿç»Ÿè®¡ç¼“å­˜"""
        key = f"team_stats:{team_id}"
        data = await self.redis.get(key)
        return json.loads(data) if data else None

# ç¼“å­˜è£…é¥°å™¨ä¼˜åŒ–
def smart_cache(key_template: str, ttl: int = 3600):
    """æ™ºèƒ½ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = key_template.format(*args, **kwargs)

            # å°è¯•ä»ç¼“å­˜è·å–
            cached = await redis_client.get(cache_key)
            if cached:
                return pickle.loads(cached)

            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = await func(*args, **kwargs)
            await redis_client.setex(
                cache_key,
                ttl,
                pickle.dumps(result)
            )
            return result
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@smart_cache("prediction_stats:{team_id}:{season}", ttl=7200)
async def get_team_prediction_stats(team_id: int, season: str):
    """è·å–çƒé˜Ÿé¢„æµ‹ç»Ÿè®¡ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    # å¤æ‚çš„è®¡ç®—é€»è¾‘
    pass
```

### APIæ€§èƒ½ä¼˜åŒ–

**ä½ç½®**: `src/api/endpoints/predictions.py`

```python
from fastapi import FastAPI, BackgroundTasks, HTTPException, Depends
from fastapi.concurrency import run_in_threadpool
import asyncio
from typing import List

# å¼‚æ­¥æ‰¹å¤„ç†ç«¯ç‚¹
@app.post("/predictions/batch")
async def create_batch_predictions(
    predictions: List[PredictionCreate],
    background_tasks: BackgroundTasks,
    prediction_service: PredictionService = Depends(get_prediction_service)
):
    """
    å¼‚æ­¥æ‰¹é‡å¤„ç†é¢„æµ‹ï¼Œé¿å…é˜»å¡

    - æ‰¹é‡å¤§å°ï¼š50
    - åå°å¤„ç†ï¼šé¿å…è¶…æ—¶
    - è¿›åº¦è·Ÿè¸ªï¼šè¿”å›ä»»åŠ¡ID
    """
    total_predictions = len(predictions)
    batch_size = 50

    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())

    # åˆ†æ‰¹å¤„ç†
    for i in range(0, total_predictions, batch_size):
        batch = predictions[i:i + batch_size]
        background_tasks.add_task(
            process_prediction_batch,
            task_id,
            i // batch_size + 1,
            batch,
            prediction_service
        )

    return {
        "message": "Batch processing started",
        "task_id": task_id,
        "total_predictions": total_predictions,
        "estimated_batches": (total_predictions + batch_size - 1) // batch_size
    }

async def process_prediction_batch(
    task_id: str,
    batch_number: int,
    predictions: List[PredictionCreate],
    prediction_service: PredictionService
):
    """åå°æ‰¹é‡å¤„ç†ä»»åŠ¡"""
    try:
        results = []
        for pred_data in predictions:
            try:
                prediction = await prediction_service.create_prediction(pred_data)
                results.append({
                    "status": "success",
                    "prediction_id": prediction.id,
                    "match_id": pred_data.match_id
                })
            except Exception as e:
                results.append({
                    "status": "error",
                    "match_id": pred_data.match_id,
                    "error": str(e)
                })

        # å­˜å‚¨æ‰¹æ¬¡ç»“æœ
        await store_batch_results(task_id, batch_number, results)

    except Exception as e:
        logger.error(f"Batch {batch_number} failed: {e}")
        await store_batch_error(task_id, batch_number, str(e))

# å¹¶å‘ä¼˜åŒ–çš„æŸ¥è¯¢ç«¯ç‚¹
@app.get("/predictions/search")
async def search_predictions(
    q: str = Query(..., description="æœç´¢å…³é”®è¯"),
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
    prediction_service: PredictionService = Depends(get_prediction_service)
):
    """
    å¹¶å‘ä¼˜åŒ–çš„æœç´¢ç«¯ç‚¹

    - å¹¶å‘æŸ¥è¯¢ï¼šåŒæ—¶æœç´¢å¤šä¸ªå­—æ®µ
    - ç»“æœåˆå¹¶ï¼šå»é‡å’Œæ’åº
    - åˆ†é¡µæ”¯æŒï¼šoffset/limit
    """
    # å¹¶å‘æœç´¢å¤šä¸ªå­—æ®µ
    search_tasks = [
        prediction_service.search_by_team(q, limit),
        prediction_service.search_by_league(q, limit),
        prediction_service.search_by_date(q, limit)
    ]

    # ç­‰å¾…æ‰€æœ‰æœç´¢å®Œæˆ
    search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

    # åˆå¹¶å’Œå»é‡ç»“æœ
    all_predictions = []
    prediction_ids = set()

    for results in search_results:
        if not isinstance(results, Exception):
            for pred in results:
                if pred.id not in prediction_ids:
                    prediction_ids.add(pred.id)
                    all_predictions.append(pred)

    # åº”ç”¨åˆ†é¡µ
    total = len(all_predictions)
    paginated_results = all_predictions[offset:offset + limit]

    return {
        "predictions": paginated_results,
        "total": total,
        "offset": offset,
        "limit": limit
    }

# å“åº”å‹ç¼©ä¸­é—´ä»¶
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)

# ç¼“å­˜å¤´ä¼˜åŒ–
@app.get("/predictions/{prediction_id}")
async def get_prediction(
    prediction_id: int,
    prediction_service: PredictionService = Depends(get_prediction_service)
):
    prediction = await prediction_service.get_prediction(prediction_id)
    if not prediction:
        raise HTTPException(status_code=404, detail="Prediction not found")

    # è®¾ç½®ç¼“å­˜å¤´
    response = JSONResponse(content=prediction.dict())
    response.headers["Cache-Control"] = "public, max-age=300"  # 5åˆ†é’Ÿç¼“å­˜
    response.headers["ETag"] = f'"{prediction.updated_at.isoformat()}"'

    return response
```

---

## ğŸ”§ è´¨é‡ä¿®å¤

### è‡ªåŠ¨åŒ–è´¨é‡ä¿®å¤

```bash
#!/bin/bash
# scripts/quality_fix.sh

echo "ğŸ”§ å¼€å§‹è‡ªåŠ¨åŒ–è´¨é‡ä¿®å¤..."

# 1. Ruffè‡ªåŠ¨ä¿®å¤
echo "ğŸ“ è¿è¡ŒRuffä»£ç æ£€æŸ¥å’Œä¿®å¤..."
ruff check src/ tests/ --fix --unsafe-fixes
echo "âœ… Ruffä¿®å¤å®Œæˆ"

# 2. Ruffæ ¼å¼åŒ–
echo "ğŸ¨ è¿è¡ŒRuffæ ¼å¼åŒ–..."
ruff format src/ tests/
echo "âœ… æ ¼å¼åŒ–å®Œæˆ"

# 3. Importæ’åºä¼˜åŒ–
echo "ğŸ“¦ ä¼˜åŒ–importè¯­å¥..."
ruff check src/ tests/ --select=I --fix
echo "âœ… Importä¼˜åŒ–å®Œæˆ"

# 4. ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥
echo "ğŸ—‘ï¸ ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥..."
ruff check src/ --select=F401 --fix
echo "âœ… æœªä½¿ç”¨å¯¼å…¥æ¸…ç†å®Œæˆ"

# 5. ç±»å‹æ£€æŸ¥ä¿®å¤
echo "ğŸ” è¿è¡ŒMyPyç±»å‹æ£€æŸ¥..."
mypy src/ --ignore-missing-imports --show-error-codes --no-error-summary
echo "âœ… ç±»å‹æ£€æŸ¥å®Œæˆ"

# 6. å®‰å…¨æ£€æŸ¥
echo "ğŸ›¡ï¸ è¿è¡ŒBanditå®‰å…¨æ‰«æ..."
bandit -r src/ -f json -o bandit-report.json || echo "å®‰å…¨æ‰«æå®Œæˆï¼Œæœ‰è­¦å‘Š"
echo "âœ… å®‰å…¨æ£€æŸ¥å®Œæˆ"

echo "ğŸ‰ è‡ªåŠ¨åŒ–è´¨é‡ä¿®å¤å®Œæˆï¼"
echo "ğŸ“Š ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶ï¼š"
echo "   - bandit-report.json (å®‰å…¨æ‰«ææŠ¥å‘Š)"
```

### Pythonè´¨é‡ä¿®å¤è„šæœ¬

```python
#!/usr/bin/env python3
# scripts/advanced_quality_fixer.py

import subprocess
import sys
import os
from pathlib import Path

class AdvancedQualityFixer:
    """é«˜çº§è´¨é‡ä¿®å¤å·¥å…·"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.src_dir = project_root / "src"
        self.tests_dir = project_root / "tests"

    def fix_ruff_issues(self, unsafe_fixes: bool = False):
        """ä½¿ç”¨Ruffä¿®å¤ä»£ç è´¨é‡é—®é¢˜"""
        print("ğŸ”§ ä½¿ç”¨Ruffä¿®å¤ä»£ç é—®é¢˜...")

        cmd = ["ruff", "check", "src/", "tests/", "--fix"]
        if unsafe_fixes:
            cmd.append("--unsafe-fixes")

        result = subprocess.run(cmd, cwd=self.project_root)
        if result.returncode == 0:
            print("âœ… Ruffä¿®å¤æˆåŠŸ")
        else:
            print("âš ï¸ Ruffä¿®å¤è¿‡ç¨‹ä¸­æœ‰è­¦å‘Š")

    def format_code(self):
        """ä½¿ç”¨Ruffæ ¼å¼åŒ–ä»£ç """
        print("ğŸ¨ ä½¿ç”¨Ruffæ ¼å¼åŒ–ä»£ç ...")

        result = subprocess.run(
            ["ruff", "format", "src/", "tests/"],
            cwd=self.project_root
        )
        if result.returncode == 0:
            print("âœ… ä»£ç æ ¼å¼åŒ–æˆåŠŸ")
        else:
            print("âŒ ä»£ç æ ¼å¼åŒ–å¤±è´¥")

    def fix_imports(self):
        """ä¿®å¤å¯¼å…¥è¯­å¥é—®é¢˜"""
        print("ğŸ“¦ ä¿®å¤å¯¼å…¥è¯­å¥...")

        # æ’åºå¯¼å…¥
        subprocess.run(
            ["ruff", "check", "src/", "tests/", "--select=I", "--fix"],
            cwd=self.project_root
        )

        # ç§»é™¤æœªä½¿ç”¨çš„å¯¼å…¥
        subprocess.run(
            ["ruff", "check", "src/", "--select=F401", "--fix"],
            cwd=self.project_root
        )

        print("âœ… å¯¼å…¥è¯­å¥ä¿®å¤å®Œæˆ")

    def fix_docstrings(self):
        """ä¿®å¤æ–‡æ¡£å­—ç¬¦ä¸²é—®é¢˜"""
        print("ğŸ“ ä¿®å¤æ–‡æ¡£å­—ç¬¦ä¸²...")

        # æŸ¥æ‰¾éœ€è¦ä¿®å¤docstringçš„æ–‡ä»¶
        python_files = list(self.src_dir.rglob("*.py"))

        for file_path in python_files:
            self._fix_file_docstrings(file_path)

        print("âœ… æ–‡æ¡£å­—ç¬¦ä¸²ä¿®å¤å®Œæˆ")

    def _fix_file_docstrings(self, file_path: Path):
        """ä¿®å¤å•ä¸ªæ–‡ä»¶çš„docstring"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # ç®€å•çš„docstringä¿®å¤é€»è¾‘
            # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•æ›´å¤æ‚çš„ä¿®å¤è§„åˆ™
            lines = content.split('\n')
            fixed_lines = []

            for i, line in enumerate(lines):
                # ä¿®å¤å¸¸è§çš„docstringæ ¼å¼é—®é¢˜
                if '"""' in line and not line.strip().endswith('"""'):
                    if i + 1 < len(lines) and lines[i + 1].strip():
                        # å¦‚æœä¸‹ä¸€è¡Œæœ‰å†…å®¹ï¼Œå¯èƒ½éœ€è¦æ·»åŠ æ¢è¡Œ
                        fixed_lines.append(line)
                        fixed_lines.append("")
                    else:
                        fixed_lines.append(line)
                else:
                    fixed_lines.append(line)

            # å†™å›æ–‡ä»¶
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(fixed_lines))

        except Exception as e:
            print(f"âš ï¸ ä¿®å¤æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    def run_type_checks(self):
        """è¿è¡Œç±»å‹æ£€æŸ¥"""
        print("ğŸ” è¿è¡ŒMyPyç±»å‹æ£€æŸ¥...")

        result = subprocess.run(
            [
                "mypy", "src/",
                "--ignore-missing-imports",
                "--show-error-codes",
                "--no-error-summary"
            ],
            cwd=self.project_root
        )

        if result.returncode == 0:
            print("âœ… ç±»å‹æ£€æŸ¥é€šè¿‡")
        else:
            print("âš ï¸ ç±»å‹æ£€æŸ¥å‘ç°é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°è¾“å‡º")

    def run_security_check(self):
        """è¿è¡Œå®‰å…¨æ£€æŸ¥"""
        print("ğŸ›¡ï¸ è¿è¡ŒBanditå®‰å…¨æ£€æŸ¥...")

        result = subprocess.run(
            ["bandit", "-r", "src/", "-f", "json", "-o", "bandit-report.json"],
            cwd=self.project_root
        )

        print("âœ… å®‰å…¨æ£€æŸ¥å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜åˆ° bandit-report.json")

    def fix_syntax_errors(self):
        """ä¿®å¤è¯­æ³•é”™è¯¯"""
        print("ğŸ› æ£€æŸ¥å¹¶ä¿®å¤è¯­æ³•é”™è¯¯...")

        # ä½¿ç”¨Pythonç¼–è¯‘æ£€æŸ¥è¯­æ³•é”™è¯¯
        python_files = list(self.src_dir.rglob("*.py"))
        syntax_errors = []

        for file_path in python_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                compile(content, str(file_path), 'exec')
            except SyntaxError as e:
                syntax_errors.append((file_path, e))
                print(f"âŒ è¯­æ³•é”™è¯¯åœ¨ {file_path}: {e}")

        if not syntax_errors:
            print("âœ… æœªå‘ç°è¯­æ³•é”™è¯¯")
        else:
            print(f"âš ï¸ å‘ç° {len(syntax_errors)} ä¸ªè¯­æ³•é”™è¯¯ï¼Œéœ€è¦æ‰‹åŠ¨ä¿®å¤")

    def run_complete_fix(self):
        """è¿è¡Œå®Œæ•´çš„è´¨é‡ä¿®å¤æµç¨‹"""
        print("ğŸš€ å¼€å§‹å®Œæ•´è´¨é‡ä¿®å¤æµç¨‹...")

        try:
            self.fix_syntax_errors()
            self.fix_ruff_issues(unsafe_fixes=True)
            self.format_code()
            self.fix_imports()
            self.fix_docstrings()
            self.run_type_checks()
            self.run_security_check()

            print("ğŸ‰ å®Œæ•´è´¨é‡ä¿®å¤æµç¨‹å®Œæˆï¼")

        except Exception as e:
            print(f"âŒ è´¨é‡ä¿®å¤è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            sys.exit(1)

if __name__ == "__main__":
    project_root = Path(__file__).parent.parent
    fixer = AdvancedQualityFixer(project_root)
    fixer.run_complete_fix()
```

### Makefileé›†æˆ

```makefile
# é«˜çº§è´¨é‡ä¿®å¤å‘½ä»¤
quality-fix-advanced:
	@echo "ğŸ”§ è¿è¡Œé«˜çº§è´¨é‡ä¿®å¤..."
	@$(ACTIVATE) && python scripts/advanced_quality_fixer.py

# æ™ºèƒ½ä»£ç ä¿®å¤
smart-fix:
	@echo "ğŸ¤– è¿è¡Œæ™ºèƒ½ä»£ç ä¿®å¤..."
	@$(ACTIVATE) && \
		echo "ğŸ“ Ruffä¿®å¤..." && \
		ruff check src/ tests/ --fix --unsafe-fixes && \
		echo "ğŸ¨ æ ¼å¼åŒ–..." && \
		ruff format src/ tests/ && \
		echo "ğŸ“¦ Importä¼˜åŒ–..." && \
		ruff check src/ tests/ --select=I,F401 --fix && \
		echo "âœ… æ™ºèƒ½ä¿®å¤å®Œæˆ"

# ç±»å‹å®‰å…¨ä¿®å¤
type-fix:
	@echo "ğŸ” è¿è¡Œç±»å‹å®‰å…¨ä¿®å¤..."
	@$(ACTIVATE) && \
		mypy src/ --ignore-missing-imports --show-error-codes || true && \
		echo "âœ… ç±»å‹æ£€æŸ¥å®Œæˆ"

# å®‰å…¨æ¼æ´ä¿®å¤
security-fix:
	@echo "ğŸ›¡ï¸ è¿è¡Œå®‰å…¨æ¼æ´ä¿®å¤..."
	@$(ACTIVATE) && \
		bandit -r src/ -f json -o security-report.json || true && \
		echo "âœ… å®‰å…¨æ£€æŸ¥å®Œæˆï¼ŒæŠ¥å‘Š: security-report.json"
```

---

## ğŸš¨ æ•…éšœæ’é™¤

### ä¾èµ–å†²çªè§£å†³

```bash
#!/bin/bash
# scripts/fix_dependencies.sh

echo "ğŸ”§ å¼€å§‹ä¿®å¤ä¾èµ–å†²çª..."

# 1. æ¸…ç†è™šæ‹Ÿç¯å¢ƒ
echo "ğŸ—‘ï¸ æ¸…ç†è™šæ‹Ÿç¯å¢ƒ..."
rm -rf .venv
python3 -m venv .venv
source .venv/bin/activate

# 2. å‡çº§pip
echo "â¬†ï¸ å‡çº§pip..."
pip install --upgrade pip

# 3. å®‰è£…pip-tools
echo "ğŸ“¦ å®‰è£…pip-tools..."
pip install pip-tools

# 4. é‡æ–°ç¼–è¯‘ä¾èµ–
echo "ğŸ”„ é‡æ–°ç¼–è¯‘ä¾èµ–..."
pip-compile pyproject.toml --extra=dev --output-file requirements-dev.txt

# 5. åŒæ­¥å®‰è£…
echo "ğŸ“¥ åŒæ­¥å®‰è£…ä¾èµ–..."
pip-sync requirements-dev.txt

# 6. éªŒè¯æ ¸å¿ƒä¾èµ–
echo "âœ… éªŒè¯æ ¸å¿ƒä¾èµ–..."
python -c "
import sys
modules = ['fastapi', 'sqlalchemy', 'redis', 'pytest', 'ruff']
failed = []
for module in modules:
    try:
        __import__(module)
        print(f'âœ… {module}')
    except ImportError as e:
        failed.append((module, e))
        print(f'âŒ {module}: {e}')

if failed:
    print(f'\\nâŒ ä¾èµ–éªŒè¯å¤±è´¥: {len(failed)} ä¸ªæ¨¡å—')
    sys.exit(1)
else:
    print('\\nâœ… æ‰€æœ‰æ ¸å¿ƒä¾èµ–éªŒè¯é€šè¿‡')
"

echo "ğŸ‰ ä¾èµ–ä¿®å¤å®Œæˆï¼"
```

### æµ‹è¯•å¤±è´¥æ’æŸ¥

```bash
#!/bin/bash
# scripts/diagnose_tests.sh

echo "ğŸ” å¼€å§‹æµ‹è¯•è¯Šæ–­..."

# 1. æ£€æŸ¥æµ‹è¯•æ”¶é›†
echo "ğŸ“‹ æ£€æŸ¥æµ‹è¯•æ”¶é›†..."
pytest --collect-only -q 2>&1 | head -20

# 2. æ£€æŸ¥è¯­æ³•é”™è¯¯
echo "ğŸ› æ£€æŸ¥è¯­æ³•é”™è¯¯..."
python -c "
import subprocess
import sys

result = subprocess.run(['pytest', '--collect-only'],
                       capture_output=True, text=True)

if result.returncode != 0:
    print('âŒ å‘ç°æµ‹è¯•æ”¶é›†é”™è¯¯:')
    print(result.stderr)

    # å°è¯•å®šä½å…·ä½“é”™è¯¯æ–‡ä»¶
    lines = result.stderr.split('\n')
    for line in lines:
        if 'error' in line.lower() or 'failed' in line.lower():
            print(f'  ğŸ“ {line}')
else:
    print('âœ… æµ‹è¯•æ”¶é›†æ­£å¸¸')
"

# 3. æ£€æŸ¥å¯¼å…¥é—®é¢˜
echo "ğŸ“¦ æ£€æŸ¥å…³é”®æ¨¡å—å¯¼å…¥..."
python -c "
import sys
import traceback

critical_modules = [
    'src.core.di',
    'src.domain.services.prediction_service',
    'src.database.connection',
    'src.cache.redis_client'
]

failed_imports = []
for module in critical_modules:
    try:
        __import__(module)
        print(f'âœ… {module}')
    except Exception as e:
        failed_imports.append((module, e))
        print(f'âŒ {module}: {e}')

if failed_imports:
    print(f'\\nâŒ å¯¼å…¥å¤±è´¥çš„æ¨¡å—:')
    for module, error in failed_imports:
        print(f'  ğŸ“ {module}: {error}')
        print(f'     {traceback.format_exc()}')
"

# 4. è¿è¡Œæœ€å°æµ‹è¯•é›†
echo "ğŸ§ª è¿è¡Œæœ€å°æµ‹è¯•é›†..."
pytest tests/unit/utils/ -v --tb=short --maxfail=3

# 5. æ£€æŸ¥ç¯å¢ƒé—®é¢˜
echo "ğŸŒ æ£€æŸ¥ç¯å¢ƒçŠ¶æ€..."
python -c "
import os
import sys

print(f'Pythonç‰ˆæœ¬: {sys.version}')
print(f'å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}')
print(f'PYTHONPATH: {os.environ.get(\"PYTHONPATH\", \"æœªè®¾ç½®\")}')

# æ£€æŸ¥å…³é”®ç¯å¢ƒå˜é‡
env_vars = ['DATABASE_URL', 'REDIS_URL', 'ENVIRONMENT']
for var in env_vars:
    value = os.environ.get(var)
    if value:
        print(f'âœ… {var}: å·²è®¾ç½®')
    else:
        print(f'âš ï¸ {var}: æœªè®¾ç½®')
"

echo "ğŸ” æµ‹è¯•è¯Šæ–­å®Œæˆï¼"
```

### Dockerç¯å¢ƒé—®é¢˜

```bash
#!/bin/bash
# scripts/fix_docker_environment.sh

echo "ğŸ³ å¼€å§‹ä¿®å¤Dockerç¯å¢ƒ..."

# 1. å®Œå…¨é‡ç½®Dockerç¯å¢ƒ
echo "ğŸ—‘ï¸ æ¸…ç†Dockerç¯å¢ƒ..."
docker-compose down -v --remove-orphans
docker system prune -f
docker volume prune -f

# 2. æ¸…ç†æ‚¬ç©ºé•œåƒ
echo "ğŸ§¹ æ¸…ç†æ‚¬ç©ºé•œåƒ..."
docker image prune -f

# 3. é‡æ–°æ„å»ºå¯åŠ¨
echo "ğŸ”„ é‡æ–°æ„å»ºå¯åŠ¨..."
docker-compose up --build -d

# 4. ç­‰å¾…æœåŠ¡å¯åŠ¨
echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

# 5. æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
echo "ğŸ¥ æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€..."
docker-compose ps

# 6. æ£€æŸ¥æœåŠ¡æ—¥å¿—
echo "ğŸ“‹ æ£€æŸ¥æœåŠ¡æ—¥å¿—..."
for service in app db redis; do
    echo "\\n=== $service æœåŠ¡æ—¥å¿— ==="
    docker-compose logs --tail=20 $service
done

# 7. éªŒè¯æœåŠ¡è¿æ¥
echo "ğŸ”— éªŒè¯æœåŠ¡è¿æ¥..."
docker-compose exec -T app python -c "
import asyncio
import sys

async def check_services():
    try:
        # æ£€æŸ¥æ•°æ®åº“è¿æ¥
        from src.database.connection import DatabaseManager
        db_manager = DatabaseManager()
        await db_manager.check_connection()
        print('âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸')

        # æ£€æŸ¥Redisè¿æ¥
        from src.cache.redis_client import RedisClient
        redis_client = RedisClient()
        await redis_client.ping()
        print('âœ… Redisè¿æ¥æ­£å¸¸')

        return True
    except Exception as e:
        print(f'âŒ æœåŠ¡è¿æ¥å¤±è´¥: {e}')
        return False

result = asyncio.run(check_services())
sys.exit(0 if result else 1)
"

if [ $? -eq 0 ]; then
    echo "âœ… Dockerç¯å¢ƒä¿®å¤å®Œæˆï¼"
else
    echo "âŒ Dockerç¯å¢ƒä»æœ‰é—®é¢˜ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°æ—¥å¿—"
fi
```

### å†…å­˜å’Œæ€§èƒ½ç›‘æ§

```python
#!/usr/bin/env python3
# scripts/performance_monitor.py

import psutil
import time
import os
import sys
import subprocess
import tracemalloc
from memory_profiler import profile
from pathlib import Path

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å·¥å…·"""

    def __init__(self):
        self.start_time = time.time()
        self.process = psutil.Process()

    def monitor_memory_usage(self):
        """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        # å†…å­˜ä¿¡æ¯
        memory_info = self.process.memory_info()
        memory_percent = self.process.memory_percent()

        print(f"ğŸ§  å†…å­˜ä½¿ç”¨:")
        print(f"   RSS: {memory_info.rss / 1024 / 1024:.2f} MB")
        print(f"   VMS: {memory_info.vms / 1024 / 1024:.2f} MB")
        print(f"   å æ¯”: {memory_percent:.2f}%")

        # ç³»ç»Ÿå†…å­˜
        system_memory = psutil.virtual_memory()
        print(f"ğŸ–¥ï¸ ç³»ç»Ÿå†…å­˜:")
        print(f"   æ€»è®¡: {system_memory.total / 1024 / 1024 / 1024:.2f} GB")
        print(f"   å¯ç”¨: {system_memory.available / 1024 / 1024 / 1024:.2f} GB")
        print(f"   ä½¿ç”¨ç‡: {system_memory.percent:.2f}%")

    def monitor_cpu_usage(self):
        """ç›‘æ§CPUä½¿ç”¨æƒ…å†µ"""
        cpu_percent = self.process.cpu_percent(interval=1)
        cpu_count = psutil.cpu_count()

        print(f"âš¡ CPUä½¿ç”¨:")
        print(f"   è¿›ç¨‹CPU: {cpu_percent:.2f}%")
        print(f"   CPUæ ¸å¿ƒæ•°: {cpu_count}")
        print(f"   ç³»ç»Ÿè´Ÿè½½: {os.getloadavg()[0]:.2f}")

    def monitor_disk_io(self):
        """ç›‘æ§ç£ç›˜IO"""
        disk_io = self.process.io_counters()
        disk_usage = psutil.disk_usage('/')

        print(f"ğŸ’¾ ç£ç›˜IO:")
        print(f"   è¯»å–æ¬¡æ•°: {disk_io.read_count}")
        print(f"   å†™å…¥æ¬¡æ•°: {disk_io.write_count}")
        print(f"   è¯»å–å­—èŠ‚: {disk_io.read_bytes / 1024 / 1024:.2f} MB")
        print(f"   å†™å…¥å­—èŠ‚: {disk_io.write_bytes / 1024 / 1024:.2f} MB")

        print(f"ğŸ’¿ ç£ç›˜ä½¿ç”¨:")
        print(f"   æ€»è®¡: {disk_usage.total / 1024 / 1024 / 1024:.2f} GB")
        print(f"   å¯ç”¨: {disk_usage.free / 1024 / 1024 / 1024:.2f} GB")
        print(f"   ä½¿ç”¨ç‡: {(1 - disk_usage.free / disk_usage.total) * 100:.2f}%")

    def profile_memory_usage(self, duration: int = 60):
        """å†…å­˜ä½¿ç”¨åˆ†æ"""
        print(f"ğŸ” å¼€å§‹å†…å­˜ä½¿ç”¨åˆ†æ ({duration}ç§’)...")

        # å¯åŠ¨å†…å­˜è·Ÿè¸ª
        tracemalloc.start()

        start_time = time.time()
        peak_memory = 0

        while time.time() - start_time < duration:
            # è·å–å½“å‰å†…å­˜ä½¿ç”¨
            current, peak = tracemalloc.get_traced_memory()
            peak_memory = max(peak_memory, peak)

            # è·å–è¿›ç¨‹å†…å­˜
            memory_info = self.process.memory_info()

            print(f"ğŸ“Š [{time.time() - start_time:.1f}s] "
                  f"RSS: {memory_info.rss / 1024 / 1024:.1f}MB, "
                  f"Peak: {peak / 1024 / 1024:.1f}MB")

            time.sleep(5)

        tracemalloc.stop()

        print(f"ğŸ“ˆ å†…å­˜åˆ†æå®Œæˆï¼Œå³°å€¼ä½¿ç”¨: {peak_memory / 1024 / 1024:.2f} MB")

    @profile
    def profile_function(self, func, *args, **kwargs):
        """å‡½æ•°æ€§èƒ½åˆ†æ"""
        print(f"ğŸ” åˆ†æå‡½æ•°æ€§èƒ½: {func.__name__}")
        start_time = time.time()
        start_memory = self.process.memory_info().rss

        try:
            result = func(*args, **kwargs)

            end_time = time.time()
            end_memory = self.process.memory_info().rss

            print(f"â±ï¸ æ‰§è¡Œæ—¶é—´: {end_time - start_time:.4f} ç§’")
            print(f"ğŸ§  å†…å­˜å˜åŒ–: {(end_memory - start_memory) / 1024 / 1024:.2f} MB")

            return result

        except Exception as e:
            print(f"âŒ å‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    monitor = PerformanceMonitor()

    print("ğŸš€ å¯åŠ¨æ€§èƒ½ç›‘æ§...")
    print(f"ğŸ“Š è¿›ç¨‹ID: {os.getpid()}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {os.getcwd()}")

    while True:
        print("\\n" + "="*50)
        print(f"â° è¿è¡Œæ—¶é—´: {time.time() - monitor.start_time:.1f} ç§’")

        # ç›‘æ§å„é¡¹æŒ‡æ ‡
        monitor.monitor_memory_usage()
        monitor.monitor_cpu_usage()
        monitor.monitor_disk_io()

        # ç­‰å¾…ä¸‹æ¬¡ç›‘æ§
        time.sleep(10)

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\\nğŸ‘‹ ç›‘æ§å·²åœæ­¢")
    except Exception as e:
        print(f"âŒ ç›‘æ§å‡ºé”™: {e}")
        sys.exit(1)
```

---

## ğŸ“‹ æ€»ç»“

è¿™ä¸ªè¯¦ç»†çš„å‚è€ƒæ–‡æ¡£æä¾›äº†ï¼š

1. **å®Œæ•´çš„ä»£ç ç¤ºä¾‹** - ç­–ç•¥å·¥å‚ã€ä¾èµ–æ³¨å…¥ã€CQRSæ¨¡å¼
2. **è¯¦ç»†çš„é…ç½®å‚æ•°** - pyproject.tomlã€pytest.iniç­‰
3. **æ€§èƒ½ä¼˜åŒ–æŒ‡å—** - æ•°æ®åº“ã€ç¼“å­˜ã€APIä¼˜åŒ–
4. **è´¨é‡ä¿®å¤å·¥å…·** - è‡ªåŠ¨åŒ–ä¿®å¤è„šæœ¬å’Œé«˜çº§ç”¨æ³•
5. **æ•…éšœæ’é™¤æ–¹æ¡ˆ** - ä¾èµ–å†²çªã€æµ‹è¯•å¤±è´¥ã€Dockeré—®é¢˜
6. **æ€§èƒ½ç›‘æ§å·¥å…·** - å†…å­˜ã€CPUã€ç£ç›˜IOç›‘æ§

é€šè¿‡åˆ†å±‚æ–‡æ¡£æ¶æ„ï¼Œå¼€å‘è€…å¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©åˆé€‚çš„æ–‡æ¡£å±‚çº§ï¼š
- **å¿«é€Ÿå…¥é—¨**: ä½¿ç”¨ CLAUDE.md
- **æ·±å…¥å¼€å‘**: å‚è€ƒ CLAUDE_DETAILED.md

---

*æ–‡æ¡£ç‰ˆæœ¬: v1.0 | æœ€åæ›´æ–°: 2025-11-16*